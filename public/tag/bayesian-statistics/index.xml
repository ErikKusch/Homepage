<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bayesian Statistics | Erik Kusch</title>
    <link>https://www.erikkusch.com/tag/bayesian-statistics/</link>
      <atom:link href="https://www.erikkusch.com/tag/bayesian-statistics/index.xml" rel="self" type="application/rss+xml" />
    <description>Bayesian Statistics</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-gb</language><copyright>© 2024</copyright><lastBuildDate>Tue, 08 Nov 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://www.erikkusch.com/img/%C3%A5motdalshytta.jpg</url>
      <title>Bayesian Statistics</title>
      <link>https://www.erikkusch.com/tag/bayesian-statistics/</link>
    </image>
    
    <item>
      <title>Bayesian Network Inference</title>
      <link>https://www.erikkusch.com/courses/bayes-nets/inference/</link>
      <pubDate>Tue, 08 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/bayes-nets/inference/</guid>
      <description>&lt;style&gt;

blockquote{
color:#633a00;
}

&lt;/style&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;p&gt;Most of the material in these chapters has already been covered in previous material, so the following summary is rather brief:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/content/courses/bayes-nets/8-Bayesian-Network-Inference_08-11-22.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bayesian Network Inference&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Please refer to earlier material for introductions of queries, structure learning, and parameter learning in theory and in &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;exercises&#34;&gt;Exercises&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to the exercises at the end of chapter 4 in 
&lt;a href=&#34;https://link.springer.com/book/10.1007/978-1-4614-6446-4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bayesian Networks in R with Applications in Systems Biology&lt;/a&gt; by by Radhakrishnan Nagarajan, Marco Scutari &amp;amp; Sophie Lèbre and Part 4 in 
&lt;a href=&#34;https://www.bnlearn.com/book-crc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bayesian Networks with Examples in R&lt;/a&gt; by M. Scutari and J.-B. Denis. Much of my inspiration for these solutions, where necessary, by consulting the solutions provided by the authors themselves as in the appendix.&lt;/p&gt;
&lt;h3 id=&#34;r-environment&#34;&gt;&lt;code&gt;R&lt;/code&gt; Environment&lt;/h3&gt;
&lt;p&gt;For today&amp;rsquo;s exercise, I load the following packages:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(bnlearn)
library(gRain)
library(GeneNet)
library(penalized)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;nagarajan-41&#34;&gt;Nagarajan 4.1&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Apply the junction tree algorithm to the validated network structure from Sachs et al. (2005), and draw the resulting undirected triangulated graph.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Taken directly from the solutions:&lt;/p&gt;
&lt;img src=&#34;https://www.erikkusch.com/courses/bayes-nets/Nagarajan4_1.JPG&#34; width=&#34;900&#34;/&gt;
&lt;h3 id=&#34;nagarajan-42&#34;&gt;Nagarajan 4.2&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Consider the Sachs et al. (2005) data used in Sect. 4.2.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;First, let&amp;rsquo;s read the data in like it was done in the book:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;isachs &amp;lt;- read.table(&amp;quot;sachs.interventional.txt&amp;quot;, header = TRUE, colClasses = &amp;quot;factor&amp;quot;)
isachs &amp;lt;- isachs[, 1:11]
for (i in names(isachs)) {
  levels(isachs[, i]) &amp;lt;- c(&amp;quot;LOW&amp;quot;, &amp;quot;AVG&amp;quot;, &amp;quot;HIGH&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This .txt file can be downloaded from 
&lt;a href=&#34;https://www.bnlearn.com/book-useR/code/sachs.interventional.txt.gz&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h4 id=&#34;part-a&#34;&gt;Part A&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Perform parameter learning with the &lt;code&gt;bn.fit&lt;/code&gt; function from &lt;code&gt;bnlearn&lt;/code&gt; and the validated network structure. How do the maximum likelihood estimates differ from the Bayesian ones, and how do the latter vary as the imaginary sample size increases?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sachs_DAG &amp;lt;- model2network(paste0(
  &amp;quot;[PKC][PKA|PKC][praf|PKC:PKA]&amp;quot;,
  &amp;quot;[pmek|PKC:PKA:praf][p44.42|pmek:PKA]&amp;quot;,
  &amp;quot;[pakts473|p44.42:PKA][P38|PKC:PKA]&amp;quot;,
  &amp;quot;[pjnk|PKC:PKA][plcg][PIP3|plcg]&amp;quot;,
  &amp;quot;[PIP2|plcg:PIP3]&amp;quot;
))
f4.1_mle &amp;lt;- bn.fit(sachs_DAG, isachs, method = &amp;quot;mle&amp;quot;)
f4.1_bayes1 &amp;lt;- bn.fit(sachs_DAG, isachs, method = &amp;quot;bayes&amp;quot;, iss = 1)
f4.1_bayes10 &amp;lt;- bn.fit(sachs_DAG, isachs, method = &amp;quot;bayes&amp;quot;, iss = 10)
f4.1_bayes100 &amp;lt;- bn.fit(sachs_DAG, isachs, method = &amp;quot;bayes&amp;quot;, iss = 100)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I omit the outputs of the individual objects created above here for space.&lt;/p&gt;
&lt;p&gt;From a theoretical standpoint mle estimates may contain NA values while bayes-inferred estimates do not. That being said, I did not see any NA outputs in the maximum likelihood estimates here.&lt;/p&gt;
&lt;p&gt;As far as iss is concerned, higher iss values result in smoother estimates.&lt;/p&gt;
&lt;h4 id=&#34;part-b&#34;&gt;Part B&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Node &lt;code&gt;PKA&lt;/code&gt; is parent of all the nodes in the &lt;code&gt;praf → pmek → p44.42 → pakts473&lt;/code&gt; chain. Use the junction tree algorithm to explore how our beliefs on those nodes change when we have evidence that &lt;code&gt;PKA&lt;/code&gt; is &lt;code&gt;“LOW”&lt;/code&gt;, and when &lt;code&gt;PKA&lt;/code&gt; is &lt;code&gt;“HIGH”&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mle_jtree &amp;lt;- compile(as.grain(f4.1_mle))
query &amp;lt;- c(&amp;quot;praf&amp;quot;, &amp;quot;pmek&amp;quot;, &amp;quot;p44.42&amp;quot;, &amp;quot;pakts473&amp;quot;)

## baseline query
querygrain(mle_jtree, nodes = query)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $pmek
## pmek
##       LOW       AVG      HIGH 
## 0.5798148 0.3066667 0.1135185 
## 
## $praf
## praf
##       LOW       AVG      HIGH 
## 0.5112963 0.2835185 0.2051852 
## 
## $p44.42
## p44.42
##       LOW       AVG      HIGH 
## 0.1361111 0.6062963 0.2575926 
## 
## $pakts473
## pakts473
##        LOW        AVG       HIGH 
## 0.60944444 0.31037037 0.08018519
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## low evidence
mle_jprop &amp;lt;- setFinding(mle_jtree, nodes = &amp;quot;PKA&amp;quot;, states = &amp;quot;LOW&amp;quot;)
querygrain(mle_jprop, nodes = query)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $pmek
## pmek
##        LOW        AVG       HIGH 
## 0.35782443 0.08874046 0.55343511 
## 
## $praf
## praf
##       LOW       AVG      HIGH 
## 0.1145038 0.1746183 0.7108779 
## 
## $p44.42
## p44.42
##       LOW       AVG      HIGH 
## 0.3435115 0.1965649 0.4599237 
## 
## $pakts473
## pakts473
##       LOW       AVG      HIGH 
## 0.2967557 0.2977099 0.4055344
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## high evidence
mle_jprop &amp;lt;- setFinding(mle_jtree, nodes = &amp;quot;PKA&amp;quot;, states = &amp;quot;HIGH&amp;quot;)
querygrain(mle_jprop, nodes = query)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $pmek
## pmek
##         LOW         AVG        HIGH 
## 0.981418919 0.016891892 0.001689189 
## 
## $praf
## praf
##        LOW        AVG       HIGH 
## 0.83614865 0.13006757 0.03378378 
## 
## $p44.42
## p44.42
##        LOW        AVG       HIGH 
## 0.07263514 0.68918919 0.23817568 
## 
## $pakts473
## pakts473
##       LOW       AVG      HIGH 
## 0.7652027 0.2347973 0.0000000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;PKA&lt;/code&gt; inhibits all other nodes. When &lt;code&gt;PKA&lt;/code&gt; is &lt;code&gt;HIGH&lt;/code&gt; then the &lt;code&gt;LOW&lt;/code&gt; probability of all other nodes increases.&lt;/p&gt;
&lt;p&gt;When &lt;code&gt;PKA&lt;/code&gt; is &lt;code&gt;HIGH&lt;/code&gt;, the activity of all the proteins corresponding to the query nodes is inhibited (the &lt;code&gt;LOW&lt;/code&gt; probability increases and the &lt;code&gt;HIGH&lt;/code&gt; decreases). When &lt;code&gt;PKA&lt;/code&gt; is &lt;code&gt;LOW&lt;/code&gt;, the opposite is true (the LOW probability decreases and the &lt;code&gt;HIGH&lt;/code&gt; increases).&lt;/p&gt;
&lt;h4 id=&#34;part-c&#34;&gt;Part C&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Similarly, explore the effects on &lt;code&gt;pjnk&lt;/code&gt; of evidence on &lt;code&gt;PIP2&lt;/code&gt;, &lt;code&gt;PIP3&lt;/code&gt;, and &lt;code&gt;plcg&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mle_jprop &amp;lt;- setFinding(mle_jtree,
  nodes = c(&amp;quot;PIP2&amp;quot;, &amp;quot;PIP3&amp;quot;, &amp;quot;plcg&amp;quot;),
  states = rep(&amp;quot;LOW&amp;quot;, 3)
)

## baseline query
querygrain(mle_jtree, nodes = &amp;quot;pjnk&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $pjnk
## pjnk
##        LOW        AVG       HIGH 
## 0.53944444 0.38277778 0.07777778
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## low evidence
querygrain(mle_jprop, nodes = &amp;quot;pjnk&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $pjnk
## pjnk
##        LOW        AVG       HIGH 
## 0.53944444 0.38277778 0.07777778
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Turns out &lt;code&gt;pjnk&lt;/code&gt; is unaffected by the others. The DAG shown in the answers to exercise Nagarajan 4.1 supports this.&lt;/p&gt;
&lt;h3 id=&#34;nagarajan-43&#34;&gt;Nagarajan 4.3&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Consider the marks data set analyzed in Sect. 2.3.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(marks)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-a-1&#34;&gt;Part A&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Learn both the network structure and the parameters with likelihood based approaches, i.e., BIC or AIC, for structure learning and maximum likelihood estimates for the parameters.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;f4.3_dag &amp;lt;- hc(marks, score = &amp;quot;bic-g&amp;quot;)
f4.3_dag
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   Bayesian network learned via Score-based methods
## 
##   model:
##    [MECH][VECT|MECH][ALG|MECH:VECT][ANL|ALG][STAT|ALG:ANL] 
##   nodes:                                 5 
##   arcs:                                  6 
##     undirected arcs:                     0 
##     directed arcs:                       6 
##   average markov blanket size:           2.40 
##   average neighbourhood size:            2.40 
##   average branching factor:              1.20 
## 
##   learning algorithm:                    Hill-Climbing 
##   score:                                 BIC (Gauss.) 
##   penalization coefficient:              2.238668 
##   tests used in the learning procedure:  34 
##   optimized:                             TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;f4.3_bn &amp;lt;- bn.fit(f4.3_dag, marks)
f4.3_bn
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   Bayesian network parameters
## 
##   Parameters of node MECH (Gaussian distribution)
## 
## Conditional density: MECH
## Coefficients:
## (Intercept)  
##    38.95455  
## Standard deviation of the residuals: 17.48622 
## 
##   Parameters of node VECT (Gaussian distribution)
## 
## Conditional density: VECT | MECH
## Coefficients:
## (Intercept)         MECH  
##  34.3828788    0.4160755  
## Standard deviation of the residuals: 11.01373 
## 
##   Parameters of node ALG (Gaussian distribution)
## 
## Conditional density: ALG | MECH + VECT
## Coefficients:
## (Intercept)         MECH         VECT  
##  25.3619809    0.1833755    0.3577122  
## Standard deviation of the residuals: 8.080725 
## 
##   Parameters of node ANL (Gaussian distribution)
## 
## Conditional density: ANL | ALG
## Coefficients:
## (Intercept)          ALG  
##   -3.574130     0.993156  
## Standard deviation of the residuals: 10.50248 
## 
##   Parameters of node STAT (Gaussian distribution)
## 
## Conditional density: STAT | ALG + ANL
## Coefficients:
## (Intercept)          ALG          ANL  
## -11.1920114    0.7653499    0.3164056  
## Standard deviation of the residuals: 12.60646
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-b-1&#34;&gt;Part B&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Query the network learned in the previous point for the probability to have the marks for both &lt;code&gt;STAT&lt;/code&gt; and &lt;code&gt;MECH&lt;/code&gt; above 60, given evidence that the mark for &lt;code&gt;ALG&lt;/code&gt; is at most 60. Are the two variables independent given the evidence on &lt;code&gt;ALG&lt;/code&gt;?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cpquery(f4.3_bn, event = (STAT &amp;gt; 60) &amp;amp; (MECH &amp;gt; 60), evidence = (ALG &amp;lt;= 60), n = 1e7)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.009562571
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cpquery(f4.3_bn, event = (STAT &amp;gt; 60), evidence = (ALG &amp;lt;= 60), n = 1e7)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.08289571
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cpquery(f4.3_bn, event = (MECH &amp;gt; 60), evidence = (ALG &amp;lt;= 60), n = 1e7)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0683385
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The conditional probability of the two outcomes (0.0095912) is not the same as the product of their corresponding marginal probabilities (0.0056668). Conclusively, we can say that &lt;code&gt;STAT&lt;/code&gt; and &lt;code&gt;MECH&lt;/code&gt; are not independent conditional on &lt;code&gt;ALG&lt;/code&gt;.&lt;/p&gt;
&lt;h4 id=&#34;part-c-1&#34;&gt;Part C&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;What is the (conditional) probability of having an average vote (in the [60,70] range) in both &lt;code&gt;VECT&lt;/code&gt; and &lt;code&gt;MECH&lt;/code&gt; while having an outstanding vote in &lt;code&gt;ALG&lt;/code&gt; (at least 90)?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cpquery(f4.3_bn,
  event = ((MECH &amp;gt;= 60) &amp;amp; (MECH &amp;lt;= 70)) | ((VECT &amp;gt;= 60) &amp;amp; (VECT &amp;lt;= 70)),
  evidence = (ALG &amp;gt;= 90),
  n = 1e7
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2872254
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;nagarajan-44&#34;&gt;Nagarajan 4.4&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Using the dynamic Bayesian network &lt;code&gt;dbn2&lt;/code&gt; from Sect. 4.3, investigate the effects of genes &lt;code&gt;257710_at&lt;/code&gt; and &lt;code&gt;255070_at&lt;/code&gt; observed at time t-2 on gene &lt;code&gt;265768_at&lt;/code&gt; at time t.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is the network in the chapter according to the errata corrige 
&lt;a href=&#34;https://www.bnlearn.com/book-useR/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(arth800)
subset &amp;lt;- c(60, 141, 260, 333, 365, 424, 441, 512, 521, 578, 789, 799)
arth12 &amp;lt;- arth800.expr[, subset]
x &amp;lt;- arth12[1:(nrow(arth12) - 2), ]
y &amp;lt;- arth12[-(1:2), &amp;quot;265768_at&amp;quot;]
lambda &amp;lt;- optL1(response = y, penalized = x, trace = FALSE)$lambda
lasso.t &amp;lt;- penalized(response = y, penalized = x, lambda1 = lambda, trace = FALSE)
y &amp;lt;- arth12[-(1:2), &amp;quot;245094_at&amp;quot;]
colnames(x)[12] &amp;lt;- &amp;quot;245094_at1&amp;quot;
lambda &amp;lt;- optL1(response = y, penalized = x, trace = FALSE)$lambda
lasso.s &amp;lt;- penalized(response = y, penalized = x, lambda1 = lambda, trace = FALSE)
## errate comes in here
dbn2 &amp;lt;- empty.graph(c(
  &amp;quot;265768_at&amp;quot;, &amp;quot;245094_at1&amp;quot;,
  &amp;quot;258736_at&amp;quot;, &amp;quot;257710_at&amp;quot;, &amp;quot;255070_at&amp;quot;,
  &amp;quot;245319_at&amp;quot;, &amp;quot;245094_at&amp;quot;
))
dbn2 &amp;lt;- set.arc(dbn2, &amp;quot;245094_at&amp;quot;, &amp;quot;265768_at&amp;quot;)
for (node in names(coef(lasso.s))[-c(1, 6)]) {
  dbn2 &amp;lt;- set.arc(dbn2, node, &amp;quot;245094_at&amp;quot;)
}
dbn2 &amp;lt;- set.arc(dbn2, &amp;quot;245094_at1&amp;quot;, &amp;quot;245094_at&amp;quot;)
dbn2.data &amp;lt;- as.data.frame(x[, nodes(dbn2)[1:6]])
dbn2.data[, &amp;quot;245094_at&amp;quot;] &amp;lt;- y
dbn2.data[, &amp;quot;245094_at1&amp;quot;] &amp;lt;- arth12[2:(nrow(arth12) - 1), &amp;quot;245094_at&amp;quot;]
dbn2.fit &amp;lt;- bn.fit(dbn2, dbn2.data)
## errata stops here
dbn2.fit[[&amp;quot;265768_at&amp;quot;]] &amp;lt;- lasso.t
dbn2.fit[[&amp;quot;245094_at&amp;quot;]] &amp;lt;- lasso.s
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is the solution to the exercise:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(42)
cpquery(dbn2.fit, event = (`265768_at` &amp;gt; 8), evidence = (`257710_at` &amp;gt; 8))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3590734
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cpquery(dbn2.fit, event = (`265768_at` &amp;gt; 8), evidence = (`255070_at` &amp;gt; 8))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5753049
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cpquery(dbn2.fit, event = (`265768_at` &amp;gt; 8), evidence = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.4396
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;High expression levels of gene 257710_at at time t −2 reduce the probability of high expression levels of gene 265768_at at time t; the opposite is true for gene 255070_at.&lt;/p&gt;
&lt;h3 id=&#34;scutari-41&#34;&gt;Scutari 4.1&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Consider the survey data set from Chapter 1.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The data can be obtained from 
&lt;a href=&#34;https://www.bnlearn.com/book-crc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;survey &amp;lt;- read.table(&amp;quot;survey.txt&amp;quot;, header = TRUE, colClasses = &amp;quot;factor&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remember, this is the corresponding DAG we know to be true:&lt;/p&gt;
&lt;img src=&#34;https://www.erikkusch.com/courses/networksscutari/Fig1.1.JPG&#34; width=&#34;900&#34;/&gt;
&lt;h4 id=&#34;part-a-2&#34;&gt;Part A&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Learn a BN with the IAMB algorithm and the asymptotic mutual information test.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;s4.1_dag &amp;lt;- iamb(survey, test = &amp;quot;mi&amp;quot;)
s4.1_dag
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   Bayesian network learned via Constraint-based methods
## 
##   model:
##     [undirected graph]
##   nodes:                                 6 
##   arcs:                                  4 
##     undirected arcs:                     4 
##     directed arcs:                       0 
##   average markov blanket size:           1.33 
##   average neighbourhood size:            1.33 
##   average branching factor:              0.00 
## 
##   learning algorithm:                    IAMB 
##   conditional independence test:         Mutual Information (disc.) 
##   alpha threshold:                       0.05 
##   tests used in the learning procedure:  85
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-b-2&#34;&gt;Part B&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Learn a second BN with IAMB but using only the first 100 observations of the data set. Is there a significant loss of information in the resulting BN compared to the BN learned from the whole data set?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;s4.1_dagB &amp;lt;- iamb(survey[1:1e2, ], test = &amp;quot;mi&amp;quot;)
s4.1_dagB
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   Bayesian network learned via Constraint-based methods
## 
##   model:
##     [undirected graph]
##   nodes:                                 6 
##   arcs:                                  1 
##     undirected arcs:                     1 
##     directed arcs:                       0 
##   average markov blanket size:           0.33 
##   average neighbourhood size:            0.33 
##   average branching factor:              0.00 
## 
##   learning algorithm:                    IAMB 
##   conditional independence test:         Mutual Information (disc.) 
##   alpha threshold:                       0.05 
##   tests used in the learning procedure:  42
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We discover far fewer arcs!&lt;/p&gt;
&lt;h4 id=&#34;part-c-2&#34;&gt;Part C&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Repeat the structure learning in the previous point with IAMB and the Monte Carlo and sequential Monte Carlo mutual information tests. How do the resulting networks compare with the BN learned with the asymptotic test? Is the increased execution time justified?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;s4.1_dagC_mcmc &amp;lt;- iamb(survey[1:1e2, ], test = &amp;quot;mc-mi&amp;quot;)
s4.1_dagC_mcmc
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   Bayesian network learned via Constraint-based methods
## 
##   model:
##     [undirected graph]
##   nodes:                                 6 
##   arcs:                                  1 
##     undirected arcs:                     1 
##     directed arcs:                       0 
##   average markov blanket size:           0.33 
##   average neighbourhood size:            0.33 
##   average branching factor:              0.00 
## 
##   learning algorithm:                    IAMB 
##   conditional independence test:         Mutual Information (disc., MC) 
##   alpha threshold:                       0.05 
##   permutations:                          5000 
##   tests used in the learning procedure:  38
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;s4.1_dagC_smc &amp;lt;- iamb(survey[1:1e2, ], test = &amp;quot;smc-mi&amp;quot;)
s4.1_dagC_smc
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   Bayesian network learned via Constraint-based methods
## 
##   model:
##     [undirected graph]
##   nodes:                                 6 
##   arcs:                                  1 
##     undirected arcs:                     1 
##     directed arcs:                       0 
##   average markov blanket size:           0.33 
##   average neighbourhood size:            0.33 
##   average branching factor:              0.00 
## 
##   learning algorithm:                    IAMB 
##   conditional independence test:         Mutual Information (disc., Seq. MC) 
##   alpha threshold:                       0.05 
##   permutations:                          5000 
##   tests used in the learning procedure:  38
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We do not discover more arcs, and the outputs of the two asymptotic tests are equal for this case:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;all.equal(s4.1_dagC_mcmc, s4.1_dagC_smc, s4.1_dagB)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;scutari-42&#34;&gt;Scutari 4.2&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Consider again the survey data set from Chapter 1.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The data can be obtained from 
&lt;a href=&#34;https://www.bnlearn.com/book-crc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;survey &amp;lt;- read.table(&amp;quot;survey.txt&amp;quot;, header = TRUE, colClasses = &amp;quot;factor&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-a-3&#34;&gt;Part A&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Learn a BN using Bayesian posteriors for both structure and parameter learning, in both cases with &lt;code&gt;iss = 5&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;s4.2_dag &amp;lt;- hc(survey, score = &amp;quot;bde&amp;quot;, iss = 5)
s4.2_bn &amp;lt;- bn.fit(s4.2_dag, survey, method = &amp;quot;bayes&amp;quot;, iss = 5)
modelstring(s4.2_bn)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;[R][E|R][T|R][A|E][O|E][S|E]&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-b-3&#34;&gt;Part B&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Repeat structure learning with hc and 3 random restarts and with tabu. How do the BNs differ? Is there any evidence of numerical or convergence problems?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;s4.2_hc &amp;lt;- hc(survey, score = &amp;quot;bde&amp;quot;, iss = 5, restart = 3)
modelstring(s4.2_hc)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;[T][R|T][E|R][A|E][O|E][S|E]&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;s4.2_tabu &amp;lt;- tabu(survey, score = &amp;quot;bde&amp;quot;, iss = 5)
modelstring(s4.2_tabu)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;[O][S][E|O:S][A|E][R|E][T|R]&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Bayesian networks inferred here differ quite substantially in their DAG structures.&lt;/p&gt;
&lt;p&gt;The random-start hill-climbing algorithm builds a DAG structure closer to the validated structure which is supported by the &lt;code&gt;score&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;score(s4.2_hc, survey)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -1998.432
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;score(s4.2_tabu, survey)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -1999.733
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-c-3&#34;&gt;Part C&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Use increasingly large subsets of the survey data to check empirically that BIC and BDe are asymptotically equivalent.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(42)
breaks &amp;lt;- seq(from = 10, to = 100, by = 10) # percentage of data
analysis_df &amp;lt;- data.frame(
  bde = NA,
  bic = NA,
  breaks = NA
)
for (k in 1:1e3) {
  bde_vec &amp;lt;- c()
  bic_vec &amp;lt;- c()
  for (i in breaks) {
    samp &amp;lt;- sample(1:nrow(survey), nrow(survey) / i)
    samp &amp;lt;- survey[samp, ]
    s4.2_bde &amp;lt;- hc(samp, score = &amp;quot;bde&amp;quot;, iss = 5)
    s4.2_bic &amp;lt;- hc(samp, score = &amp;quot;bic&amp;quot;)
    bde_vec &amp;lt;- c(bde_vec, score(s4.2_bde, survey))
    bic_vec &amp;lt;- c(bic_vec, score(s4.2_bic, survey))
  }
  analysis_df &amp;lt;- rbind(
    analysis_df,
    data.frame(
      bde = bde_vec,
      bic = bic_vec,
      breaks = breaks
    )
  )
}

analysis_df &amp;lt;- na.omit(analysis_df)

plot(
  x = analysis_df$breaks,
  y = abs(analysis_df$bde - analysis_df$bic)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-11-08-network-inference_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;scutari-43&#34;&gt;Scutari 4.3&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Consider the marks data set from Section 4.7.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(marks)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-a-4&#34;&gt;Part A&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Create a bn object describing the graph in the bottom right panel of Figure 4.5 and call it &lt;code&gt;mdag&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mdag &amp;lt;- model2network(paste0(
  &amp;quot;[ANL][MECH][LAT|ANL:MECH]&amp;quot;,
  &amp;quot;[VECT|LAT][ALG|LAT][STAT|LAT]&amp;quot;
))
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-b-4&#34;&gt;Part B&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Construct the skeleton, the CPDAG and the moral graph of &lt;code&gt;mdag&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mdag_skel &amp;lt;- skeleton(mdag)
mdag_cpdag &amp;lt;- cpdag(mdag)
mdag_moral &amp;lt;- moral(mdag)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-c-4&#34;&gt;Part C&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Discretise the marks data using &amp;ldquo;interval&amp;rdquo; discretisation with 2, 3 and 4 intervals.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dmarks_2 &amp;lt;- discretize(marks, &amp;quot;interval&amp;quot;, breaks = 2)
dmarks_3 &amp;lt;- discretize(marks, &amp;quot;interval&amp;quot;, breaks = 3)
dmarks_4 &amp;lt;- discretize(marks, &amp;quot;interval&amp;quot;, breaks = 4)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-d&#34;&gt;Part D&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Perform structure learning with hc on each of the discretised data sets; how do the resulting DAGs differ?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;hc_2 &amp;lt;- hc(dmarks_2)
modelstring(hc_2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;[MECH][VECT|MECH][ALG|VECT][ANL|ALG][STAT|ALG]&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;hc_3 &amp;lt;- hc(dmarks_3)
modelstring(hc_3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;[MECH][ALG|MECH][ANL|ALG][STAT|ALG][VECT|ANL]&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;hc_4 &amp;lt;- hc(dmarks_4)
modelstring(hc_4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;[MECH][VECT][ALG][ANL|ALG][STAT|ANL]&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Quite evidently, as we increase the number of intervals, we break conditional relationships so much so that fewer arcs are identified.&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.2.1 (2022-06-23 ucrt)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19044)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_Germany.utf8  LC_CTYPE=English_Germany.utf8    LC_MONETARY=English_Germany.utf8 LC_NUMERIC=C                     LC_TIME=English_Germany.utf8    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] penalized_0.9-52    survival_3.4-0      GeneNet_1.2.16      fdrtool_1.2.17      longitudinal_1.1.13 corpcor_1.6.10      gRain_1.3.11        gRbase_1.8.7        bnlearn_4.8.1      
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.9          highr_0.9           bslib_0.4.0         compiler_4.2.1      BiocManager_1.30.18 jquerylib_0.1.4     R.methodsS3_1.8.2   R.utils_2.12.0      tools_4.2.1        
## [10] digest_0.6.29       jsonlite_1.8.0      evaluate_0.16       R.cache_0.16.0      lattice_0.20-45     pkgconfig_2.0.3     rlang_1.0.5         igraph_1.3.4        Matrix_1.5-1       
## [19] graph_1.74.0        cli_3.3.0           rstudioapi_0.14     Rgraphviz_2.40.0    yaml_2.3.5          parallel_4.2.1      blogdown_1.13       xfun_0.33           fastmap_1.1.0      
## [28] styler_1.8.0        stringr_1.4.1       knitr_1.40          vctrs_0.4.1         sass_0.4.2          stats4_4.2.1        grid_4.2.1          R6_2.5.1            RBGL_1.72.0        
## [37] rmarkdown_2.16      bookdown_0.29       purrr_0.3.4         magrittr_2.0.3      splines_4.2.1       BiocGenerics_0.42.0 htmltools_0.5.3     stringi_1.7.8       cachem_1.0.6       
## [46] R.oo_1.25.0
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Dynamic Bayesian Networks</title>
      <link>https://www.erikkusch.com/courses/bayes-nets/dynamic/</link>
      <pubDate>Tue, 01 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/bayes-nets/dynamic/</guid>
      <description>&lt;style&gt;

blockquote{
color:#633a00;
}

&lt;/style&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/content/courses/bayes-nets/Dynamic-BNs.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dynamic Bayesian Networks&lt;/a&gt; by 
&lt;a href=&#34;https://gregor-mathes.netlify.app/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gregor Mathes&lt;/a&gt; (one of our study group members)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;exercises&#34;&gt;Exercises&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to the exercises at the end of chapter 3 in 
&lt;a href=&#34;https://link.springer.com/book/10.1007/978-1-4614-6446-4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bayesian Networks in R with Applications in Systems Biology&lt;/a&gt; by by Radhakrishnan Nagarajan, Marco Scutari &amp;amp; Sophie Lèbre. Much of my inspiration for these solutions, where necessary, by consulting the solutions provided by the authors themselves as in the appendix.&lt;/p&gt;
&lt;h3 id=&#34;r-environment&#34;&gt;&lt;code&gt;R&lt;/code&gt; Environment&lt;/h3&gt;
&lt;p&gt;For today&amp;rsquo;s exercise, I load the following packages:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(vars)
library(lars)
library(GeneNet)
library(G1DBN) # might have to run remotes::install_version(&amp;quot;G1DBN&amp;quot;, &amp;quot;3.1.1&amp;quot;) first
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;nagarajan-31&#34;&gt;Nagarajan 3.1&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Consider the &lt;code&gt;Canada&lt;/code&gt; data set from the vars package, which we analyzed in Sect. 3.5.1.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(Canada)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-a&#34;&gt;Part A&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Load the data set from the &lt;code&gt;vars&lt;/code&gt; package and investigate its properties using the exploratory analysis techniques covered in Chap. 1.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(Canada)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Time-Series [1:84, 1:4] from 1980 to 2001: 930 930 930 931 933 ...
##  - attr(*, &amp;quot;dimnames&amp;quot;)=List of 2
##   ..$ : NULL
##   ..$ : chr [1:4] &amp;quot;e&amp;quot; &amp;quot;prod&amp;quot; &amp;quot;rw&amp;quot; &amp;quot;U&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Canada)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        e              prod             rw              U         
##  Min.   :928.6   Min.   :401.3   Min.   :386.1   Min.   : 6.700  
##  1st Qu.:935.4   1st Qu.:404.8   1st Qu.:423.9   1st Qu.: 7.782  
##  Median :946.0   Median :406.5   Median :444.4   Median : 9.450  
##  Mean   :944.3   Mean   :407.8   Mean   :440.8   Mean   : 9.321  
##  3rd Qu.:950.0   3rd Qu.:410.7   3rd Qu.:461.1   3rd Qu.:10.607  
##  Max.   :961.8   Max.   :418.0   Max.   :470.0   Max.   :12.770
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-b&#34;&gt;Part B&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Estimate a VAR(1) process for this data set.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(var1 &amp;lt;- VAR(Canada, p = 1, type = &amp;quot;const&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## VAR Estimation Results:
## ======================= 
## 
## Estimated coefficients for equation e: 
## ====================================== 
## Call:
## e = e.l1 + prod.l1 + rw.l1 + U.l1 + const 
## 
##          e.l1       prod.l1         rw.l1          U.l1         const 
##    1.17353629    0.14479389   -0.07904568    0.52438144 -192.56360758 
## 
## 
## Estimated coefficients for equation prod: 
## ========================================= 
## Call:
## prod = e.l1 + prod.l1 + rw.l1 + U.l1 + const 
## 
##         e.l1      prod.l1        rw.l1         U.l1        const 
##   0.08709510   1.01970070  -0.02629309   0.32299246 -81.55109611 
## 
## 
## Estimated coefficients for equation rw: 
## ======================================= 
## Call:
## rw = e.l1 + prod.l1 + rw.l1 + U.l1 + const 
## 
##        e.l1     prod.l1       rw.l1        U.l1       const 
##  0.06381103 -0.13551199  0.96872851 -0.19538479 11.61375726 
## 
## 
## Estimated coefficients for equation U: 
## ====================================== 
## Call:
## U = e.l1 + prod.l1 + rw.l1 + U.l1 + const 
## 
##         e.l1      prod.l1        rw.l1         U.l1        const 
##  -0.19293575  -0.08086896   0.07538624   0.47530976 186.80892410
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-c&#34;&gt;Part C&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Build the auto-regressive matrix $A$ and the constant matrix $B$ defining the VAR(1) model.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## base object creation
base_mat &amp;lt;- matrix(0, 4, 5)
colnames(base_mat) &amp;lt;- c(&amp;quot;e&amp;quot;, &amp;quot;prod&amp;quot;, &amp;quot;rw&amp;quot;, &amp;quot;U&amp;quot;, &amp;quot;constant&amp;quot;)
p &amp;lt;- 0.05
## object filling
pos &amp;lt;- which(coef(var1)$e[, &amp;quot;Pr(&amp;gt;|t|)&amp;quot;] &amp;lt; p)
base_mat[1, pos] &amp;lt;- coef(var1)$e[pos, &amp;quot;Estimate&amp;quot;]
pos &amp;lt;- which(coef(var1)$prod[, &amp;quot;Pr(&amp;gt;|t|)&amp;quot;] &amp;lt; p)
base_mat[2, pos] &amp;lt;- coef(var1)$prod[pos, &amp;quot;Estimate&amp;quot;]
pos &amp;lt;- which(coef(var1)$rw[, &amp;quot;Pr(&amp;gt;|t|)&amp;quot;] &amp;lt; p)
base_mat[3, pos] &amp;lt;- coef(var1)$rw[pos, &amp;quot;Estimate&amp;quot;]
pos &amp;lt;- which(coef(var1)$U[, &amp;quot;Pr(&amp;gt;|t|)&amp;quot;] &amp;lt; p)
base_mat[4, pos] &amp;lt;- coef(var1)$U[pos, &amp;quot;Estimate&amp;quot;]
## final objects
(A &amp;lt;- base_mat[, 1:4])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               e        prod          rw         U
## [1,]  1.1735363  0.14479389 -0.07904568 0.5243814
## [2,]  0.0000000  1.01970070  0.00000000 0.0000000
## [3,]  0.0000000 -0.13551199  0.96872851 0.0000000
## [4,] -0.1929358 -0.08086896  0.07538624 0.4753098
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(B &amp;lt;- base_mat[, 5])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -192.5636    0.0000    0.0000  186.8089
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-d&#34;&gt;Part D&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Compare the results with the LASSO matrix when estimating the L1-penalty with cross-validation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## data preparation
data_df &amp;lt;- Canada[-nrow(Canada), ] # remove last row of data
## Lasso
Lasso_ls &amp;lt;- lapply(colnames(Canada), function(gene) {
  y &amp;lt;- Canada[-1, gene] # remove first row of data, and select only target gene
  lars(y = y, x = data_df, type = &amp;quot;lasso&amp;quot;) # LASSO matrix
})
## Cross-validation
CV_ls &amp;lt;- lapply(1:ncol(Canada), function(gene) {
  y &amp;lt;- Canada[-1, gene] # remove first row of data, and select only target gene
  lasso.cv &amp;lt;- cv.lars(y = y, x = data_df, mode = &amp;quot;fraction&amp;quot;)
  frac &amp;lt;- lasso.cv$index[which.min(lasso.cv$cv)]
  predict(Lasso_ls[[gene]], s = frac, type = &amp;quot;coef&amp;quot;, mode = &amp;quot;fraction&amp;quot;)
})
## output
rbind(
  CV_ls[[1]]$coefficients,
  CV_ls[[2]]$coefficients,
  CV_ls[[3]]$coefficients,
  CV_ls[[4]]$coefficients
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                e        prod           rw          U
## [1,]  1.17353629  0.14479389 -0.079045685  0.5243814
## [2,]  0.02570001  1.02314558 -0.004878295  0.1994059
## [3,]  0.09749788 -0.11991692  0.954389035 -0.1023845
## [4,] -0.17604953 -0.08192783  0.069502065  0.5086115
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And for comparison the previously identified $A$:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;A
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               e        prod          rw         U
## [1,]  1.1735363  0.14479389 -0.07904568 0.5243814
## [2,]  0.0000000  1.01970070  0.00000000 0.0000000
## [3,]  0.0000000 -0.13551199  0.96872851 0.0000000
## [4,] -0.1929358 -0.08086896  0.07538624 0.4753098
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-e&#34;&gt;Part E&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;What can you conclude?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The whole point of LASSO, as far as I understand it, is to shrink parameter estimates towards 0 often times reaching 0 exactly. In the above this has not happened for many parameters, but is the case with the estimation provided by &lt;code&gt;vars&lt;/code&gt;. I assume this might be because there just aren&amp;rsquo;t enough variables and/or observations in time.&lt;/p&gt;
&lt;h3 id=&#34;nagarajan-32&#34;&gt;Nagarajan 3.2&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Consider the &lt;code&gt;arth800&lt;/code&gt; data set from the GeneNet package, which we analyzed in Sects. 3.5.2 and 3.5.3.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(arth800)
data(arth800.expr)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-a-1&#34;&gt;Part A&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Load the data set from the &lt;code&gt;GeneNet&lt;/code&gt; package. The time series expression of the 800 genes is included in a data set called &lt;code&gt;arth800.expr&lt;/code&gt;. Investigate its properties using the exploratory analysis techniques covered in Chap. 1.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(arth800.expr)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  &#39;longitudinal&#39; num [1:22, 1:800] 10.04 10.11 9.77 10.06 10.02 ...
##  - attr(*, &amp;quot;dimnames&amp;quot;)=List of 2
##   ..$ : chr [1:22] &amp;quot;0-1&amp;quot; &amp;quot;0-2&amp;quot; &amp;quot;1-1&amp;quot; &amp;quot;1-2&amp;quot; ...
##   ..$ : chr [1:800] &amp;quot;AFFX-Athal-GAPDH_3_s_at&amp;quot; &amp;quot;AFFX-Athal-Actin_3_f_at&amp;quot; &amp;quot;267612_at&amp;quot; &amp;quot;267520_at&amp;quot; ...
##  - attr(*, &amp;quot;time&amp;quot;)= num [1:11] 0 1 2 4 8 12 13 14 16 20 ...
##  - attr(*, &amp;quot;repeats&amp;quot;)= num [1:11] 2 2 2 2 2 2 2 2 2 2 ...
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(arth800.expr)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Longitudinal data:
##  800 variables measured at 11 different time points
##  Total number of measurements per variable: 22 
##  Repeated measurements: yes 
## 
##  To obtain the measurement design call &#39;get.time.repeats()&#39;.
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-b-1&#34;&gt;Part B&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;For this practical exercise, we will work on a subset of variables (one for each gene) having a large variance. Compute the variance of each of the 800 variables, plot the various variance values in decreasing order, and create a data set with the variables greater than 2.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## variance calculation
variance &amp;lt;- diag(var(arth800.expr))
## plotting
plot(sort(variance, decreasing = TRUE), type = &amp;quot;l&amp;quot;, ylab = &amp;quot;Variance&amp;quot;)
abline(h = 2, lty = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-11-01-nagara-dynamic_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## variables with variances greater than 2
dataVar2 &amp;lt;- arth800.expr[, which(variance &amp;gt; 2)]
dim(dataVar2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 22 49
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-c-1&#34;&gt;Part C&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Can you fit a VAR process with a usual approach from this data set?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I don&amp;rsquo;t think so. There are more variables (genes) than there are samples (time steps):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dim(dataVar2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 22 49
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-d-1&#34;&gt;Part D&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Which alternative approaches can be used to fit a VAR process from this data set?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The chapter discusses these alternatives:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LASSO&lt;/li&gt;
&lt;li&gt;James-Stein Shrinkage&lt;/li&gt;
&lt;li&gt;Low-order conditional dependency approximation&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;part-e-1&#34;&gt;Part E&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Estimate a dynamic Bayesian network with each of the alternative approaches presented in this chapter.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;First, I prepare the data by re-ordering them:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## make the data sequential for both repetitions
dataVar2seq &amp;lt;- dataVar2[c(seq(1, 22, by = 2), seq(2, 22, by = 2)), ]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;LASSO&lt;/em&gt; with the &lt;code&gt;lars&lt;/code&gt; package:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x &amp;lt;- dataVar2seq[-c(21:22), ] # remove final rows (end of sequences)
Lasso_ls &amp;lt;- lapply(colnames(dataVar2seq), function(gene) {
  y &amp;lt;- dataVar2seq[-(1:2), gene]
  lars(y = y, x = x, type = &amp;quot;lasso&amp;quot;)
})
CV_ls &amp;lt;- lapply(1:ncol(dataVar2seq), function(gene) {
  y &amp;lt;- dataVar2seq[-(1:2), gene]
  lasso.cv &amp;lt;- cv.lars(y = y, x = x, mode = &amp;quot;fraction&amp;quot;, plot.it = FALSE)
  frac &amp;lt;- lasso.cv$index[which.min(lasso.cv$cv)]
  predict(Lasso_ls[[gene]], s = frac, type = &amp;quot;coef&amp;quot;, mode = &amp;quot;fraction&amp;quot;)
})
Lasso_mat &amp;lt;- matrix(0, dim(dataVar2seq)[2], dim(dataVar2seq)[2])
for (i in 1:dim(Lasso_mat)[1]) {
  Lasso_mat[i, ] &amp;lt;- CV_ls[i][[1]]$coefficients
}
sum(Lasso_mat != 0) # number of arcs
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 456
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(sort(abs(Lasso_mat), decr = TRUE)[1:500], type = &amp;quot;l&amp;quot;, ylab = &amp;quot;Absolute coefficients&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-11-01-nagara-dynamic_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;James-Stein shrinkage&lt;/em&gt; with the &lt;code&gt;GeneNet&lt;/code&gt; package:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;DBNGeneNet &amp;lt;- ggm.estimate.pcor(dataVar2, method = &amp;quot;dynamic&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Estimating optimal shrinkage intensity lambda (correlation matrix): 0.0539
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;DBNGeneNet.edges &amp;lt;- network.test.edges(DBNGeneNet) # p-values, q-values and posterior probabilities for each potential arc
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Estimate (local) false discovery rates (partial correlations):
## Step 1... determine cutoff point
## Step 2... estimate parameters of null distribution and eta0
## Step 3... compute p-values and estimate empirical PDF/CDF
## Step 4... compute q-values and local fdr
## Step 5... prepare for plotting
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-11-01-nagara-dynamic_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(DBNGeneNet.edges[, &amp;quot;prob&amp;quot;], type = &amp;quot;l&amp;quot;) # arcs probability by decreasing order
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-11-01-nagara-dynamic_files/figure-html/unnamed-chunk-14-2.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sum(DBNGeneNet.edges$prob &amp;gt; 0.95) # arcs with prob &amp;gt; 0.95
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;First-order conditional dependency&lt;/em&gt; with the &lt;code&gt;G1DBN&lt;/code&gt; package:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;G1DB_BN &amp;lt;- DBNScoreStep1(dataVar2seq, method = &amp;quot;ls&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Treating 49 vertices:
## 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;G1DB_BN &amp;lt;- DBNScoreStep2(G1DB_BN$S1ls, dataVar2seq, method = &amp;quot;ls&amp;quot;, alpha1 = 0.5)
plot(sort(G1DB_BN, decreasing = TRUE), type = &amp;quot;l&amp;quot;, ylab = &amp;quot;Arcs’ p-values&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-11-01-nagara-dynamic_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;nagarajan-33&#34;&gt;Nagarajan 3.3&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Consider the dimension reduction approaches used in the previous exercise and the &lt;code&gt;arth800&lt;/code&gt; data set from the &lt;code&gt;GeneNet&lt;/code&gt; package.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(arth800)
data(arth800.expr)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-a-2&#34;&gt;Part A&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;For a comparative analysis of the different approaches, select the top 50 arcs for each approach (function &lt;code&gt;BuildEdges&lt;/code&gt; from the &lt;code&gt;G1DBN&lt;/code&gt; package can be used to that end).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;LASSO&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;lasso_tresh &amp;lt;- mean(sort(abs(Lasso_mat), decreasing = TRUE)[50:51]) # Lasso_mat from exercise 3.2
lasso_50 &amp;lt;- BuildEdges(score = -abs(Lasso_mat), threshold = -lasso_tresh)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;James-Stein shrinkage&lt;/em&gt; with the &lt;code&gt;GeneNet&lt;/code&gt; package:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;DBNGeneNet_50 &amp;lt;- cbind(DBNGeneNet.edges[1:50, &amp;quot;node1&amp;quot;], DBNGeneNet.edges[1:50, &amp;quot;node2&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;First-order conditional dependency&lt;/em&gt; with the &lt;code&gt;G1DBN&lt;/code&gt; package:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;G1DBN_tresh &amp;lt;- mean(sort(G1DB_BN)[50:51])
G1DBN.edges &amp;lt;- BuildEdges(score = G1DB_BN, threshold = G1DBN_tresh, prec = 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-b-2&#34;&gt;Part B&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Plot the four inferred networks with the function plot from package &lt;code&gt;G1DBN&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Four inferred networks? I assume the exercise so far wanted me to also analyse the data using the LASSO approach with the SIMoNe (&lt;code&gt;simone&lt;/code&gt;) package. I will skip over that one and continue with the three I have:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;par(mfrow = c(1, 3))

## LASSO
LASSO_plot &amp;lt;- graph.edgelist(cbind(lasso_50[, 1], lasso_50[, 2]))
Lasso_layout &amp;lt;- layout.fruchterman.reingold(LASSO_plot)
plot(LASSO_plot,
  layout = Lasso_layout,
  edge.arrow.size = 0.5, vertex.size = 10,
  main = &amp;quot;LASSO&amp;quot;
)

## James-Stein
DBN_plot &amp;lt;- graph.edgelist(DBNGeneNet_50)
# DBN_layout &amp;lt;- layout.fruchterman.reingold(DBN_plot)
plot(DBN_plot,
  layout = Lasso_layout,
  edge.arrow.size = 0.5, vertex.size = 10,
  main = &amp;quot;GeneNet&amp;quot;
)

## First-order conditional
G1DBN_plot &amp;lt;- graph.edgelist(cbind(G1DBN.edges[, 1], G1DBN.edges[, 2]))
# G1DBN_layout = layout.fruchterman.reingold(G1DBN_plot)
plot(G1DBN_plot,
  layout = Lasso_layout,
  edge.arrow.size = 0.5, vertex.size = 10,
  main = &amp;quot;G1DBN&amp;quot;
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-11-01-nagara-dynamic_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h4 id=&#34;part-c-2&#34;&gt;Part C&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;How many arcs are common to the four inferred networks?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## extract edges
LASSO_el &amp;lt;- as_edgelist(LASSO_plot)
DBN_el &amp;lt;- as_edgelist(DBN_plot)
G1DBN_el &amp;lt;- as_edgelist(G1DBN_plot)

## number of repeated edges in pairwise comparisons
sum(duplicated(rbind(LASSO_el, DBN_el)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sum(duplicated(rbind(LASSO_el, G1DBN_el)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 6
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sum(duplicated(rbind(DBN_el, G1DBN_el)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;### all at once
sum(duplicated(rbind(LASSO_el, DBN_el, G1DBN_el)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 7
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-d-2&#34;&gt;Part D&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Are the top 50 arcs of each inferred network similar? What can you conclude?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;No, they are not. I can conclude that different dimension reductions produce different DAG structures.&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.2.1 (2022-06-23 ucrt)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19044)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_Germany.utf8  LC_CTYPE=English_Germany.utf8    LC_MONETARY=English_Germany.utf8 LC_NUMERIC=C                     LC_TIME=English_Germany.utf8    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] G1DBN_3.1.1         igraph_1.3.4        GeneNet_1.2.16      fdrtool_1.2.17      longitudinal_1.1.13 corpcor_1.6.10      lars_1.3            vars_1.5-6          lmtest_0.9-40      
## [10] urca_1.3-3          strucchange_1.5-3   sandwich_3.0-2      zoo_1.8-10          MASS_7.3-58.1      
## 
## loaded via a namespace (and not attached):
##  [1] highr_0.9         bslib_0.4.0       compiler_4.2.1    jquerylib_0.1.4   R.methodsS3_1.8.2 R.utils_2.12.0    tools_4.2.1       digest_0.6.29     jsonlite_1.8.0    evaluate_0.16    
## [11] nlme_3.1-159      R.cache_0.16.0    lattice_0.20-45   pkgconfig_2.0.3   rlang_1.0.5       cli_3.3.0         rstudioapi_0.14   yaml_2.3.5        blogdown_1.13     xfun_0.33        
## [21] fastmap_1.1.0     styler_1.8.0      stringr_1.4.1     knitr_1.40        vctrs_0.4.1       sass_0.4.2        grid_4.2.1        R6_2.5.1          rmarkdown_2.16    bookdown_0.29    
## [31] purrr_0.3.4       magrittr_2.0.3    htmltools_0.5.3   stringi_1.7.8     cachem_1.0.6      R.oo_1.25.0
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Graph Theory &amp; Bayes</title>
      <link>https://www.erikkusch.com/courses/bayes-nets/introduction/</link>
      <pubDate>Tue, 13 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/bayes-nets/introduction/</guid>
      <description>&lt;style&gt;

blockquote{
color:#633a00;
}


&lt;/style&gt; 
&lt;p&gt;This session of our study group did not include any practical material. For the summary of the theory discussed in this session, please refer to the slides linked below.&lt;/p&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/content/courses/bayes-nets/1-Bayes-_-Graph-Theory_13-09-22.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Multinomial Bayesian Networks</title>
      <link>https://www.erikkusch.com/courses/bayes-nets/part-1/</link>
      <pubDate>Tue, 13 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/bayes-nets/part-1/</guid>
      <description>&lt;style&gt;

blockquote{
color:#633a00;
}

&lt;/style&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/content/courses/bayes-nets/3-Multinomial-Networks_03-10-22.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Multinomial Bayesian Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;exercises&#34;&gt;Exercises&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to the exercises at the end of Part 1 in 
&lt;a href=&#34;https://www.bnlearn.com/book-crc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bayesian Networks with Examples in R&lt;/a&gt; by M. Scutari and J.-B. Denis. I have created these notes as a part of a study-partnership with 
&lt;a href=&#34;https://www.linkedin.com/in/frederik-have-kallesoe-0584889b/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Frederik Kallesøe&lt;/a&gt;. Much of my inspiration for these solutions, where necessary, has been obtained either from chatting with Frederik or by consulting the solutions provided by the authors themselves as in the appendix.&lt;/p&gt;
&lt;h3 id=&#34;r-environment&#34;&gt;&lt;code&gt;R&lt;/code&gt; Environment&lt;/h3&gt;
&lt;p&gt;For today&amp;rsquo;s exercise, I load the following packages:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(bnlearn)
library(gRain)
library(ggplot2)
library(lattice)
library(gridExtra)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;scutari-11&#34;&gt;Scutari 1.1&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Consider the DAG for the survey studied in this chapter and shown in Figure 1.1.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here&amp;rsquo;s the DAG in question:&lt;/p&gt;
&lt;img src=&#34;https://www.erikkusch.com/courses/networksscutari/Fig1.1.JPG&#34; width=&#34;900&#34;/&gt;
&lt;h4 id=&#34;part-1&#34;&gt;Part 1.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;List the parents and the children of each node.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Node&lt;/th&gt;
&lt;th&gt;Parent(s)&lt;/th&gt;
&lt;th&gt;Child(ren)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Age (A)&lt;/td&gt;
&lt;td&gt;{ }&lt;/td&gt;
&lt;td&gt;E&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Sex (S)&lt;/td&gt;
&lt;td&gt;{ }&lt;/td&gt;
&lt;td&gt;E&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Education (E)&lt;/td&gt;
&lt;td&gt;A, S&lt;/td&gt;
&lt;td&gt;O, R&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Occupation (O)&lt;/td&gt;
&lt;td&gt;E&lt;/td&gt;
&lt;td&gt;E&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Residence (R)&lt;/td&gt;
&lt;td&gt;E&lt;/td&gt;
&lt;td&gt;E&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Travel (T)&lt;/td&gt;
&lt;td&gt;O, R&lt;/td&gt;
&lt;td&gt;{ }&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;part-2&#34;&gt;Part 2.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;List all the fundamental connections present in the DAG, and classify them as either serial, divergent or convergent.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Fundamental connections are those paths who contain three vertices/nodes. In directed graphs, they can be classified into three different categories depending on flow of dependencies.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Path&lt;/th&gt;
&lt;th&gt;Classification&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;A → E ← S&lt;/td&gt;
&lt;td&gt;Convergent&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;A → E → O&lt;/td&gt;
&lt;td&gt;Serial&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;A → E → R&lt;/td&gt;
&lt;td&gt;Serial&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;S → E → O&lt;/td&gt;
&lt;td&gt;Serial&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;S → E → R&lt;/td&gt;
&lt;td&gt;Serial&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;O ← E → R&lt;/td&gt;
&lt;td&gt;Divergent&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;E → O → T&lt;/td&gt;
&lt;td&gt;Serial&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;E → R → T&lt;/td&gt;
&lt;td&gt;Serial&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;O → T ← R&lt;/td&gt;
&lt;td&gt;Convergent&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;part-3&#34;&gt;Part 3.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Add an arc from Age to Occupation, and another arc from Travel to Education. Is the resulting graph still a valid BN? If not, why?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let&amp;rsquo;s take this one arc at a time:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A → O. Adding this arc does not lead to the introduction of any cycles and so the Bayesian Network (BN) remains valid. I have added this graph to the figure from the book and highlighted it in green just below.&lt;/li&gt;
&lt;li&gt;T → E. Adding this arc does introduce cyclic paths along T → E → R → T and T → E → O → T thus resulting in a non-valid BN. I have highlighted the added arc in red and shaded the cyclic paths in orange below.&lt;/li&gt;
&lt;/ol&gt;
&lt;img src=&#34;https://www.erikkusch.com/courses/networksscutari/Fig1.1_Arcs.JPG&#34; width=&#34;900&#34;/&gt;
&lt;h3 id=&#34;scutari-12&#34;&gt;Scutari 1.2&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Consider the probability distribution from the survey in Section 1.3.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The data can be obtained from 
&lt;a href=&#34;https://www.bnlearn.com/book-crc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;survey &amp;lt;- read.table(&amp;quot;survey.txt&amp;quot;, header = TRUE, colClasses = &amp;quot;factor&amp;quot;)
A.lv &amp;lt;- c(&amp;quot;young&amp;quot;, &amp;quot;adult&amp;quot;, &amp;quot;old&amp;quot;)
S.lv &amp;lt;- c(&amp;quot;M&amp;quot;, &amp;quot;F&amp;quot;)
E.lv &amp;lt;- c(&amp;quot;high&amp;quot;, &amp;quot;uni&amp;quot;)
O.lv &amp;lt;- c(&amp;quot;emp&amp;quot;, &amp;quot;self&amp;quot;)
R.lv &amp;lt;- c(&amp;quot;small&amp;quot;, &amp;quot;big&amp;quot;)
T.lv &amp;lt;- c(&amp;quot;car&amp;quot;, &amp;quot;train&amp;quot;, &amp;quot;other&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-1-1&#34;&gt;Part 1.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Compute the number of configurations of the parents of each node.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;code&gt;A&lt;/code&gt; and &lt;code&gt;S&lt;/code&gt; have no parents (refer back to the DAG in exercise 1.1). Therefore, we are only interested in the configurations of parental nodes for &lt;code&gt;E&lt;/code&gt;, &lt;code&gt;O&lt;/code&gt;, &lt;code&gt;R&lt;/code&gt;, and &lt;code&gt;T&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(E &amp;lt;- length(A.lv) * length(S.lv))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 6
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(O &amp;lt;- length(E.lv))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(R &amp;lt;- length(E.lv))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(T &amp;lt;- length(O.lv) * length(R.lv))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a simple exercise of combinatorics. The number of parental configurations is simply the number of states each parental node can be in multiplied by the same for all other parental nodes.&lt;/p&gt;
&lt;h4 id=&#34;part-2-1&#34;&gt;Part 2.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Compute the number of parameters of the local distributions.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;All of this comes down to how many parameters we need to estimate to accurately represent the probability distributions belonging to each node in our DAG. Since all probabilities per node sum up to 1, we effectively only ever need to estimate a number $n-1$ parameters for each node with $n$ being the number of states said node can be in. Let&amp;rsquo;s walk through this.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;A&lt;/code&gt; has 3 states, so need to estimate 2 parameters ($p_A = 2$). &lt;code&gt;S&lt;/code&gt; has 2 states hence we need 1 parameter for this node ($p_S = 1$).&lt;/p&gt;
&lt;p&gt;Now we arrive at &lt;code&gt;E&lt;/code&gt; and things get more complicated. The probability distribution for &lt;code&gt;E&lt;/code&gt; comes in two parts - one for &lt;code&gt;&amp;quot;high&amp;quot;&lt;/code&gt; and one for &lt;code&gt;&amp;quot;low&amp;quot;&lt;/code&gt; education level. Both of these contain additional probability distributions of combinations of the levels of &lt;code&gt;S&lt;/code&gt; and &lt;code&gt;A&lt;/code&gt;. To obtain the number of parameters needed to describe this 3-dimensional distribution, we can simply calculate $p_E = n_S * n_A * (n_E-1) = 2 * 3 * 1 = 6$.&lt;/p&gt;
&lt;p&gt;Moving on to &lt;code&gt;O&lt;/code&gt; and &lt;code&gt;R&lt;/code&gt;. Both of these need 2 parameters ($p_O = p_r = 2$) because of their two-dimensional distributions being made up of two levels of education and two levels occupation and residency respectively ($2 * (2-1) = 2$).&lt;/p&gt;
&lt;p&gt;Lastly, we arrive at &lt;code&gt;T&lt;/code&gt; which we need 8 parameters for ($p_T = 8$). Holy smokes. Why? Basically, this is a repeat of what we did for &lt;code&gt;E&lt;/code&gt;. We have a three-dimensional distribution with three levels in T-Space, two levels in-space, and two more levels in O-Space. To arrive at the number of parameters we simply do $p_T = (n_T-1) * n_o * n_R = 2 * 2 * 2 = 8$.&lt;/p&gt;
&lt;h4 id=&#34;part-3-1&#34;&gt;Part 3.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Compute the number of parameters of the global distribution.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We can sum all of the local parameters up to arrive at $p_{total} = p_A + p_S + p_E + p_O + p_R + p_T = 2+1+6+2+2+8 = 21$.&lt;/p&gt;
&lt;p&gt;And in &lt;code&gt;R&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# define DAG structure
dag &amp;lt;- model2network(&amp;quot;[A][S][E|A:S][O|E][R|E][T|O:R]&amp;quot;)
# define local distributions
A.prob &amp;lt;- array(c(0.30, 0.50, 0.20), dim = 3, dimnames = list(A = A.lv))
S.prob &amp;lt;- array(c(0.60, 0.40), dim = 2, dimnames = list(S = S.lv))
E.prob &amp;lt;- array(
  c(
    0.75, 0.25, 0.72, 0.28, 0.88, 0.12, 0.64, 0.36, 0.70,
    0.30, 0.90, 0.10
  ),
  dim = c(2, 3, 2),
  dimnames = list(E = E.lv, A = A.lv, S = S.lv)
)
O.prob &amp;lt;- array(c(0.96, 0.04, 0.92, 0.08),
  dim = c(2, 2),
  dimnames = list(O = O.lv, E = E.lv)
)
R.prob &amp;lt;- array(c(0.25, 0.75, 0.20, 0.80),
  dim = c(2, 2),
  dimnames = list(R = R.lv, E = E.lv)
)
T.prob &amp;lt;- array(
  c(
    0.48, 0.42, 0.10, 0.56, 0.36, 0.08, 0.58, 0.24, 0.18,
    0.70, 0.21, 0.09
  ),
  dim = c(3, 2, 2),
  dimnames = list(T = T.lv, O = O.lv, R = R.lv)
)
# define set of local distributions
cpt &amp;lt;- list(
  A = A.prob, S = S.prob, E = E.prob, O = O.prob,
  R = R.prob, T = T.prob
)
# create BN
bn &amp;lt;- custom.fit(dag, cpt)
# obtain parameters
nparams(bn)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 21
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that I pulled the probabilities for the distributions from the book and their values are irrelevant to the number of parameters.&lt;/p&gt;
&lt;h4 id=&#34;part-4&#34;&gt;Part 4.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Add an arc from Education to Travel. Recompute the factorisation into local distributions shown in Equation (1.1). How does the number of parameters of each local distribution change?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Adding E → T to Equation (1.1) results in:
$$P(A, S, E, O, R, T) = P(A) P(S) P(E | A, S) P(O | E) P(R | E) P(T | E, O, R)$$&lt;/p&gt;
&lt;p&gt;Now that &lt;code&gt;T&lt;/code&gt; is dependant on &lt;code&gt;E&lt;/code&gt; as well as the previous parents, the number of free parameters of the local distribution of &lt;code&gt;T&lt;/code&gt; increases
to 16 ($p_E = 16$). This is because our local distribution of &lt;code&gt;T&lt;/code&gt; is now four-dimensional resulting in $p_T = (n_T-1) * n_o * n_R * n_E = 2 * 2 * 2 * 2 = 16$.&lt;/p&gt;
&lt;p&gt;All other local distributions remain the same.&lt;/p&gt;
&lt;h3 id=&#34;scutari-13&#34;&gt;Scutari 1.3&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Consider again the DAG for the survey.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;part-1-2&#34;&gt;Part 1.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Create an object of class &lt;code&gt;bn&lt;/code&gt; for the DAG.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here&amp;rsquo;s the simplest way of doing this by specifying the model string:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# define DAG structure
bn &amp;lt;- model2network(&amp;quot;[A][S][E|A:S][O|E][R|E][T|O:R]&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-2-2&#34;&gt;Part 2.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Use the functions in &lt;code&gt;bnlearn&lt;/code&gt; and the R object created in the previous point to extract the nodes and the arcs of the DAG. Also extract the parents and the children of each node.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here we go:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;nodes(bn)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;A&amp;quot; &amp;quot;E&amp;quot; &amp;quot;O&amp;quot; &amp;quot;R&amp;quot; &amp;quot;S&amp;quot; &amp;quot;T&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;arcs(bn)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      from to 
## [1,] &amp;quot;A&amp;quot;  &amp;quot;E&amp;quot;
## [2,] &amp;quot;S&amp;quot;  &amp;quot;E&amp;quot;
## [3,] &amp;quot;E&amp;quot;  &amp;quot;O&amp;quot;
## [4,] &amp;quot;E&amp;quot;  &amp;quot;R&amp;quot;
## [5,] &amp;quot;O&amp;quot;  &amp;quot;T&amp;quot;
## [6,] &amp;quot;R&amp;quot;  &amp;quot;T&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sapply(X = nodes(bn), FUN = bnlearn::parents, x = bn)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $A
## character(0)
## 
## $E
## [1] &amp;quot;A&amp;quot; &amp;quot;S&amp;quot;
## 
## $O
## [1] &amp;quot;E&amp;quot;
## 
## $R
## [1] &amp;quot;E&amp;quot;
## 
## $S
## character(0)
## 
## $T
## [1] &amp;quot;O&amp;quot; &amp;quot;R&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sapply(X = nodes(bn), FUN = bnlearn::children, x = bn)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $A
## [1] &amp;quot;E&amp;quot;
## 
## $E
## [1] &amp;quot;O&amp;quot; &amp;quot;R&amp;quot;
## 
## $O
## [1] &amp;quot;T&amp;quot;
## 
## $R
## [1] &amp;quot;T&amp;quot;
## 
## $S
## [1] &amp;quot;E&amp;quot;
## 
## $T
## character(0)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-3-2&#34;&gt;Part 3.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Print the model formula from &lt;code&gt;bn&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;modelstring(bn)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;[A][S][E|A:S][O|E][R|E][T|O:R]&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-4-1&#34;&gt;Part 4.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Fit the parameters of the network from the data stored in survey.txt using their Bayesian estimators and save the result into an object of class &lt;code&gt;bn.fit&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bn_full &amp;lt;- bn.fit(bn, data = survey, method = &amp;quot;bayes&amp;quot;, iss = 10)
class(bn_full)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;bn.fit&amp;quot;      &amp;quot;bn.fit.dnet&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-5&#34;&gt;Part 5.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Remove the arc from Education to Occupation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bn_sparse &amp;lt;- drop.arc(bn, from = &amp;quot;E&amp;quot;, to = &amp;quot;O&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-6&#34;&gt;Part 6.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Fit the parameters of the modified network. Which local distributions change, and how?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bn_sparse &amp;lt;- bn.fit(bn_sparse, data = survey, method = &amp;quot;bayes&amp;quot;, iss = 10)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We already now that the only local distribution which should change is that of &lt;code&gt;O&lt;/code&gt;. Let&amp;rsquo;s check that:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dim(coef(bn_full$O))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2 2
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dim(coef(bn_sparse$O))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Quite evidently, the local distribution of &lt;code&gt;O&lt;/code&gt; has become much simpler in our sparse Bayesian Network. Why? Because it has no parent node now which would parse additional information and complexity onto it.&lt;/p&gt;
&lt;h3 id=&#34;scutari-14&#34;&gt;Scutari 1.4&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Re-create the &lt;code&gt;bn.mle&lt;/code&gt; object used in Section 1.4.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bn.mle &amp;lt;- bn.fit(dag, data = survey, method = &amp;quot;mle&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-1-3&#34;&gt;Part 1.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Compare the distribution of Occupation conditional on Age with the corresponding marginal distribution using &lt;code&gt;querygrain&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## creating object ready for gRain functions
junction &amp;lt;- compile(as.grain(bn.mle))
## Overall query
query_over &amp;lt;- querygrain(junction, nodes = &amp;quot;O&amp;quot;)$O
## Marginal query when A is young
jage &amp;lt;- setEvidence(junction, &amp;quot;A&amp;quot;, states = &amp;quot;young&amp;quot;)
query_young &amp;lt;- querygrain(jage, nodes = &amp;quot;O&amp;quot;)$O
## Marginal query when A is adult
jage &amp;lt;- setEvidence(junction, &amp;quot;A&amp;quot;, states = &amp;quot;adult&amp;quot;)
query_adult &amp;lt;- querygrain(jage, nodes = &amp;quot;O&amp;quot;)$O
## Marginal query when A is old
jage &amp;lt;- setEvidence(junction, &amp;quot;A&amp;quot;, states = &amp;quot;old&amp;quot;)
query_old &amp;lt;- querygrain(jage, nodes = &amp;quot;O&amp;quot;)$O
## Combining queries
queries_df &amp;lt;- rbind(query_over, query_young, query_adult, query_old)
rownames(queries_df) &amp;lt;- c(&amp;quot;Overall&amp;quot;, &amp;quot;Young&amp;quot;, &amp;quot;Adult&amp;quot;, &amp;quot;Old&amp;quot;)
queries_df
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               emp       self
## Overall 0.9660248 0.03397517
## Young   0.9644166 0.03558340
## Adult   0.9636485 0.03635151
## Old     0.9738915 0.02610849
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we can see, conditioning on &lt;code&gt;A&lt;/code&gt; does not influence the distribution of &lt;code&gt;O&lt;/code&gt; that much.&lt;/p&gt;
&lt;h4 id=&#34;part-2-3&#34;&gt;Part 2.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;How many random observations are needed for &lt;code&gt;cpquery&lt;/code&gt; to produce estimates of the parameters of these two distributions with a precision of ±0.01?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I find this question to be difficult to understand. What I assume I am tasked with is to compare the distribution of Occupation conditional on Age (&lt;code&gt;query_over&lt;/code&gt;: 0.97, 0.03) with the estimates produced by &lt;code&gt;cpquery&lt;/code&gt; given some evidence (i.e. parental node configuration). This would mean comparing each query EXCEPT for query_over to it&amp;rsquo;s counterpart with &lt;code&gt;cpquery()&lt;/code&gt;. That&amp;rsquo;s a tad excessive, and so I only compare &lt;code&gt;query_young&lt;/code&gt; (0.96, 0.04) from above with the results obtained by &lt;code&gt;cpquery()&lt;/code&gt;. What I am looking at is: &amp;ldquo;How high do my sample sizes have to be in &lt;code&gt;cpquery()&lt;/code&gt; to be within a ±0.01 margin of &lt;code&gt;query_young&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Luckily, &lt;code&gt;query_young&lt;/code&gt; only has two values and so I can tell &lt;code&gt;cpquery()&lt;/code&gt; to only compute one of them as the other follows logically by subtracting the former from 1.&lt;/p&gt;
&lt;p&gt;Here, I want to test this for likelihood weighting (&lt;code&gt;lw&lt;/code&gt;) and logic sampling (&lt;code&gt;ls&lt;/code&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# create test list and test sequence
precis_ls &amp;lt;- as.list(c(0, 0))
names(precis_ls) &amp;lt;- c(&amp;quot;LW&amp;quot;, &amp;quot;LS&amp;quot;)
n_seq &amp;lt;- as.integer(seq(from = 1e2, to = 1e5, length.out = 1e2))
# iterate over our sample sizes
for (i in n_seq) {
  precis_ls$LW &amp;lt;- c(
    precis_ls$LW,
    cpquery(bn.mle, event = (O == &amp;quot;emp&amp;quot;), evidence = list(A = &amp;quot;young&amp;quot;), method = &amp;quot;lw&amp;quot;, n = i)
  )
  precis_ls$LS &amp;lt;- c(
    precis_ls$LS,
    cpquery(bn.mle, event = (O == &amp;quot;emp&amp;quot;), evidence = (A == &amp;quot;young&amp;quot;), method = &amp;quot;ls&amp;quot;, n = i)
  )
}
# remove first positions which were blanks
precis_ls$LW &amp;lt;- precis_ls$LW[-1]
precis_ls$LS &amp;lt;- precis_ls$LS[-1]
# plotting the results
plot_df &amp;lt;- data.frame(
  N = c(n_seq, n_seq),
  Precision = c(
    query_young[1] - precis_ls$LW,
    query_young[1] - precis_ls$LS
  ),
  Method = rep(c(&amp;quot;Likelihood Weighting&amp;quot;, &amp;quot;Logical Sampling&amp;quot;), each = length(n_seq))
)
ggplot(data = plot_df, aes(x = N, y = Precision, col = Method)) +
  geom_line(size = 1.5) +
  geom_hline(yintercept = 0.01) +
  geom_hline(yintercept = -0.01) +
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-04-13-crc_part1_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As is evident from this plot, we do not need much in terms of sample to arrive at highly precise results using &lt;code&gt;cpquery()&lt;/code&gt; with either method. Still, to be safe, I would probably always run with &lt;code&gt;n = 1e3&lt;/code&gt; at least.&lt;/p&gt;
&lt;h4 id=&#34;part-3-3&#34;&gt;Part 3.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Use the functions in &lt;code&gt;bnlearn&lt;/code&gt; to extract the DAG from &lt;code&gt;bn.mle&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dag &amp;lt;- bn.net(bn.mle)
dag
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   Random/Generated Bayesian network
## 
##   model:
##    [A][S][E|A:S][O|E][R|E][T|O:R] 
##   nodes:                                 6 
##   arcs:                                  6 
##     undirected arcs:                     0 
##     directed arcs:                       6 
##   average markov blanket size:           2.67 
##   average neighbourhood size:            2.00 
##   average branching factor:              1.00 
## 
##   generation algorithm:                  Empty
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-4-2&#34;&gt;Part 4.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Which nodes d-separate Age and Occupation?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sapply(nodes(dag), function(z) dsep(dag, &amp;quot;A&amp;quot;, &amp;quot;O&amp;quot;, z))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     A     E     O     R     S     T 
##  TRUE  TRUE  TRUE FALSE FALSE FALSE
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;scutari-15&#34;&gt;Scutari 1.5&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Implement an R function for BN inference via rejection sampling using the description provided in Section 1.4 as a reference.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;From the book:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;In rejection sampling, we generate random independent observations from the BN. Then we count how many match the evidence we are conditioning on and how many of those observations also match the event whose probability we are computing; the estimated conditional probability is the ratio between the latter and the former.&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rejection.sampling &amp;lt;- function(bn, nsim, event.node, event.value, evidence.node, evidence.value) {
  sims &amp;lt;- rbn(x = bn, n = nsim) # random samples for each node from a Bayesian network
  m1 &amp;lt;- sims[sims[, evidence.node] == evidence.value, ] # retain only those samples where our evidence node matches the evidence condition
  m2 &amp;lt;- m1[m1[, event.node] == event.value, ] # retain only those samples where our event node matches the event condition
  return(nrow(m2) / nrow(m1)) # how many percent of the evidence samples also return the event state?
}
rejection.sampling(
  bn = bn.mle, nsim = 10^4,
  event.node = &amp;quot;O&amp;quot;, event.value = &amp;quot;emp&amp;quot;,
  evidence.node = &amp;quot;A&amp;quot;, evidence.value = &amp;quot;young&amp;quot;
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9640978
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;scutari-16&#34;&gt;Scutari 1.6&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Using the &lt;code&gt;dag&lt;/code&gt; and &lt;code&gt;bn&lt;/code&gt; objects from Sections 1.2 and 1.3:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The DAG in question is the same as &lt;code&gt;dag&lt;/code&gt; in these solutions. The BN is the same as &lt;code&gt;bn.mle&lt;/code&gt; or &lt;code&gt;bn_full&lt;/code&gt;. Since I do this for the Bayesian part of Bayesian networks, I use the latter.&lt;/p&gt;
&lt;h4 id=&#34;part-1-4&#34;&gt;Part 1.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Plot the DAG using &lt;code&gt;graphviz.plot&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Easy enough:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;graphviz.plot(dag)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-04-13-crc_part1_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h4 id=&#34;part-2-4&#34;&gt;Part 2.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Plot the DAG again, highlighting the nodes and the arcs that are part of one or more v-structures.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A bit meaningless of a plot because all of these nodes are involved in v-structures. However, the paths E → O, and E → R are not highlighted as v-structures. Why? Because they are sequential paths rather than convergent or divergent.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;vs &amp;lt;- vstructs(dag, arcs = TRUE)
hl &amp;lt;- list(nodes = unique(as.character(vs)), arcs = vs)
graphviz.plot(dag, highlight = hl)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-04-13-crc_part1_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h4 id=&#34;part-3-4&#34;&gt;Part 3.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Plot the DAG one more time, highlighting the path leading from Age to Occupation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;All we need to do is highlight the paths A → E and E → O:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;hl &amp;lt;- matrix(c(&amp;quot;A&amp;quot;, &amp;quot;E&amp;quot;, &amp;quot;E&amp;quot;, &amp;quot;O&amp;quot;), nc = 2, byrow = TRUE)
graphviz.plot(bn.mle, highlight = list(arcs = hl))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-04-13-crc_part1_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h4 id=&#34;part-4-3&#34;&gt;Part 4.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Plot the conditional probability table of Education.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There is a ready-made function for that!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bn.fit.barchart(bn_full$E)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-04-13-crc_part1_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;1440&#34; /&gt;
Across all age ranges, women are much higher educated (on average) than men.&lt;/p&gt;
&lt;h4 id=&#34;part-5-1&#34;&gt;Part 5.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Compare graphically the distributions of Education for male and female interviewees.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here, we simply need to extract the relevant proportions and need them into a barchart.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;junction &amp;lt;- compile(as.grain(bn.mle))
jmale &amp;lt;- setEvidence(junction, &amp;quot;S&amp;quot;, states = &amp;quot;M&amp;quot;)
jfemale &amp;lt;- setEvidence(junction, &amp;quot;S&amp;quot;, states = &amp;quot;F&amp;quot;)
p1 &amp;lt;- barchart(querygrain(jmale, nodes = &amp;quot;E&amp;quot;)$E, main = &amp;quot;Male&amp;quot;, xlim = c(0, 1))
p2 &amp;lt;- barchart(querygrain(jfemale, nodes = &amp;quot;E&amp;quot;)$E, main = &amp;quot;Female&amp;quot;, xlim = c(0, 1))
grid.arrange(p1, p2, ncol = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-04-13-crc_part1_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.2.1 (2022-06-23 ucrt)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19044)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_Germany.utf8  LC_CTYPE=English_Germany.utf8    LC_MONETARY=English_Germany.utf8 LC_NUMERIC=C                     LC_TIME=English_Germany.utf8    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] gridExtra_2.3   lattice_0.20-45 ggplot2_3.3.6   gRain_1.3.11    gRbase_1.8.7    bnlearn_4.8.1  
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.9          assertthat_0.2.1    digest_0.6.29       utf8_1.2.2          R6_2.5.1            stats4_4.2.1        evaluate_0.16       highr_0.9           blogdown_1.13      
## [10] pillar_1.8.1        rlang_1.0.5         rstudioapi_0.14     Rgraphviz_2.40.0    jquerylib_0.1.4     R.utils_2.12.0      R.oo_1.25.0         Matrix_1.5-1        rmarkdown_2.16     
## [19] styler_1.8.0        labeling_0.4.2      stringr_1.4.1       igraph_1.3.4        munsell_0.5.0       compiler_4.2.1      xfun_0.33           pkgconfig_2.0.3     BiocGenerics_0.42.0
## [28] htmltools_0.5.3     tidyselect_1.1.2    tibble_3.1.8        bookdown_0.29       fansi_1.0.3         dplyr_1.0.9         withr_2.5.0         R.methodsS3_1.8.2   grid_4.2.1         
## [37] RBGL_1.72.0         jsonlite_1.8.0      gtable_0.3.1        lifecycle_1.0.2     DBI_1.1.3           magrittr_2.0.3      scales_1.2.1        graph_1.74.0        cli_3.3.0          
## [46] stringi_1.7.8       cachem_1.0.6        farver_2.1.1        bslib_0.4.0         vctrs_0.4.1         generics_0.1.3      tools_4.2.1         R.cache_0.16.0      glue_1.6.2         
## [55] purrr_0.3.4         parallel_4.2.1      fastmap_1.1.0       yaml_2.3.5          colorspace_2.0-3    BiocManager_1.30.18 knitr_1.40          sass_0.4.2
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>(Bayesian) Networks &amp; R</title>
      <link>https://www.erikkusch.com/courses/bayes-nets/networksr/</link>
      <pubDate>Tue, 27 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/bayes-nets/networksr/</guid>
      <description>&lt;style&gt;

blockquote{
color:#633a00;
}

&lt;/style&gt; 
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/content/courses/bayes-nets/2-Networks-in-R_27-09-22.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Networks in &lt;code&gt;R&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;exercises&#34;&gt;Exercises&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to the exercises at the end of chapter 1 in 
&lt;a href=&#34;https://link.springer.com/book/10.1007/978-1-4614-6446-4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bayesian Networks in R with Applications in Systems Biology&lt;/a&gt;  by Radhakrishnan Nagarajan, Marco Scutari &amp;amp; Sophie Lèbre. Much of my inspiration for these solutions, where necessary, by consulting the solutions provided by the authors themselves as in the appendix.&lt;/p&gt;
&lt;h3 id=&#34;nagarajan-11&#34;&gt;Nagarajan 1.1&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;1.1. Consider a directed acyclic graph with n nodes.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;(a) Show that at least one node must not have any incoming arc, i.e., the graph must contain at least one root node.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A graph without a root node violates the premise of acyclicity inherent to Bayesian Networks.&lt;/p&gt;
&lt;p&gt;Assuming our graph is directed with G = (&lt;strong&gt;V&lt;/strong&gt;, A), and assuming that there is no root node present, for any vertex $V_i$ that we chose of the set &lt;strong&gt;V&lt;/strong&gt;, we can find a path that $V_i \rightarrow &amp;hellip; \rightarrow V_n$ which spans all nodes in &lt;strong&gt;V&lt;/strong&gt;. However, $V_n$ must have an outgoing arc ($V_n \rightarrow &amp;hellip;$) which must connect to any of the nodes already on the path between $V_i$ and $V_n$ thereby incurring a loop or cycle.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;(b) Show that such a graph can have at most n(n−1) arcs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This can be explained as an iterative process. Starting with any node $V_1 \in \boldsymbol V$, not violating the prerequisite for acyclicity, $V_1$ may only contain $n-1$ outgoing arcs (an arc linking to itself would create a cycle). Continuing now to another node $V_2 \in \boldsymbol V$ and $V_2 \neq V_1$, this node may only contain $n-2$ outgoing arcs as any arc linking to itself or $V_1$ would introduce a cycle or loop.&lt;/p&gt;
&lt;p&gt;Continuing this process to its logical conclusion of $V_n$, we can summarise the number of possible outgoing arcs thusly:&lt;/p&gt;
&lt;p&gt;\begin{equation}
A \leq (n-1) + (n-2) + &amp;hellip; + (1) = \left( n\atop{2} \right) = \frac{n(n-1)}{2}
\end{equation}&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;(c) Show that a path can span at most n−1 arcs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Any arc contains two nodes - one tail and one head. Therefore, a path spanning n arcs would have to contain n+1 vertices, thus passing trough one vertex twice and introducing a cycle.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;(d) Describe an algorithm to determine the topological ordering of the graph.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Two prominent examples are 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Breadth-first_search&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;breadth-first&lt;/a&gt; (BF) and 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Depth-first_search&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;depth-first&lt;/a&gt; (DF) algorithms.&lt;/p&gt;
&lt;p&gt;The former (BF) attempts to locate a node that satisfies a certain query conditions by exploring a graph from a root node and subsequently evaluating nodes which are equidistant to the root (at a distance of one arc). If none of these nodes satisfies the search criteria, the distance to root node is increased by an additional arc distance.&lt;/p&gt;
&lt;p&gt;DF, on the other hand, starts at a root node and fully explores a randomly chosen path until either a node satisfying the search criteria is reached or the path has ended in which case a new path is explored starting at the root node.&lt;/p&gt;
&lt;h3 id=&#34;nagarajan-12&#34;&gt;Nagarajan 1.2&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;1.2. Consider the graphs shown in Fig. 1.1.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;!-- Figure 1.1 is the following: --&gt;
&lt;img src=&#34;https://www.erikkusch.com/courses/bayes-nets/Nagara1.1.JPG&#34; width=&#34;900&#34;/&gt;
&lt;blockquote&gt;
&lt;p&gt;(a) Obtain the skeleton of the partially directed and directed graphs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I start by loading the &lt;code&gt;visNetwork&lt;/code&gt; &lt;code&gt;R&lt;/code&gt; package. I chose this package simply because I like how it creates interactive visualisations - try it out! Click some of the vertices below or try and drag them around.&lt;/p&gt;
&lt;p&gt;I also load additional html libraries with which I can include the thml outputs produced by &lt;code&gt;visNetwork&lt;/code&gt; in this blog:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(visNetwork)
library(htmlwidgets)
library(htmltools)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, I register the node set as well as the two arc sets we are working with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;V &amp;lt;- data.frame(
  id = c(&amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;C&amp;quot;, &amp;quot;D&amp;quot;, &amp;quot;E&amp;quot;),
  label = c(&amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;C&amp;quot;, &amp;quot;D&amp;quot;, &amp;quot;E&amp;quot;)
)
A_directed &amp;lt;- data.frame(
  from = c(&amp;quot;C&amp;quot;, &amp;quot;C&amp;quot;, &amp;quot;C&amp;quot;, &amp;quot;D&amp;quot;, &amp;quot;A&amp;quot;),
  to = c(&amp;quot;E&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;D&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;B&amp;quot;)
)
A_partial &amp;lt;- data.frame(
  from = c(&amp;quot;E&amp;quot;, &amp;quot;D&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;C&amp;quot;, &amp;quot;C&amp;quot;),
  to = c(&amp;quot;B&amp;quot;, &amp;quot;E&amp;quot;, &amp;quot;D&amp;quot;, &amp;quot;D&amp;quot;, &amp;quot;A&amp;quot;)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that I still have to register a direction even for undirected edges.&lt;/p&gt;
&lt;p&gt;Now, to visualise the skeletons, we simply plot the graphs without any edge directionality:&lt;/p&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for the plotting call of the skeleton of the directed graph:&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## skeleton of directed graph
Nagara1.2aD &amp;lt;- visNetwork(
  nodes = V,
  edges = A_directed
) %&amp;gt;%
  visNodes(
    shape = &amp;quot;circle&amp;quot;,
    font = list(size = 40, color = &amp;quot;white&amp;quot;),
    color = list(
      background = &amp;quot;darkgrey&amp;quot;,
      border = &amp;quot;black&amp;quot;,
      highlight = &amp;quot;orange&amp;quot;
    ),
    shadow = list(enabled = TRUE, size = 10)
  ) %&amp;gt;%
  visEdges(color = list(
    color = &amp;quot;green&amp;quot;,
    highlight = &amp;quot;red&amp;quot;
  )) %&amp;gt;%
  visLayout(randomSeed = 42)
saveWidget(Nagara1.2aD, &amp;quot;Nagara1.2aD.html&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;iframe seamless src=&#34;https://www.erikkusch.com/courses/bayes-nets/Nagara1.2aD.html&#34; width=&#34;900&#34; height=&#34;500&#34;&gt;&lt;/iframe&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for the plotting call of the skeleton of the partially directed graph:&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## skeleton of partially directed graph
Nagara1.2aP &amp;lt;- visNetwork(
  nodes = V,
  edges = A_partial
) %&amp;gt;%
  visNodes(
    shape = &amp;quot;circle&amp;quot;,
    font = list(size = 40, color = &amp;quot;white&amp;quot;),
    color = list(
      background = &amp;quot;darkgrey&amp;quot;,
      border = &amp;quot;black&amp;quot;,
      highlight = &amp;quot;orange&amp;quot;
    ),
    shadow = list(enabled = TRUE, size = 10)
  ) %&amp;gt;%
  visEdges(color = list(
    color = &amp;quot;green&amp;quot;,
    highlight = &amp;quot;red&amp;quot;
  )) %&amp;gt;%
  visLayout(randomSeed = 42)

saveWidget(Nagara1.2aP, &amp;quot;Nagara1.2aP.html&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;iframe seamless src=&#34;https://www.erikkusch.com/courses/bayes-nets/Nagara1.2aP.html&#34; width=&#34;900&#34; height=&#34;500&#34;&gt;&lt;/iframe&gt;
&lt;blockquote&gt;
&lt;p&gt;(b) Enumerate the acyclic graphs that can be obtained by orienting the undirected arcs of the partially directed graph.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Right. Sorting this out using &lt;code&gt;visNetwork&lt;/code&gt; would be a royal pain. So I instead opt for using &lt;code&gt;igraph&lt;/code&gt; for this exercise.&lt;/p&gt;
&lt;p&gt;First, I load the necessary &lt;code&gt;R&lt;/code&gt; package:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(igraph)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, I consider the edge and node set of the partially directed graph:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;V &amp;lt;- data.frame(id = c(&amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;C&amp;quot;, &amp;quot;D&amp;quot;, &amp;quot;E&amp;quot;))
A_partial &amp;lt;- data.frame(
  from = c(&amp;quot;E&amp;quot;, &amp;quot;D&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;C&amp;quot;, &amp;quot;C&amp;quot;),
  to = c(&amp;quot;B&amp;quot;, &amp;quot;E&amp;quot;, &amp;quot;D&amp;quot;, &amp;quot;D&amp;quot;, &amp;quot;A&amp;quot;)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To enumerate all DAGs from these sets, I may only change the directions of the last three edges listed in my edge set.&lt;/p&gt;
&lt;p&gt;Objectively, this exercise could be solved by simply &lt;em&gt;thinking&lt;/em&gt; about the different constellations. &lt;strong&gt;However&lt;/strong&gt;, I code for a living. One may argue that thinking is not necessarily in my wheelhouse (on may also claim the opposite). Hence, I will automate the procedure.&lt;/p&gt;
&lt;p&gt;To do so, I will need to create all possible combinations of directions of currently undirected edges in the graph in question and check whether they create cycles or not.&lt;/p&gt;
&lt;p&gt;To do so, I load the &lt;code&gt;bnlearn&lt;/code&gt; package as it comes with an error message when trying to assign an arc set that introduces cycles. I also register a base graph consisting of only our vertex set:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(bnlearn)
base_bn &amp;lt;- empty.graph(nodes = V$id)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, I simply loop over my three arcs in question and alternate which direction they are pointing. At the deepest level of this monstrosity, I am then assingning the final arcs to the base graph and only retain it if the cyclicity error has not been thrown:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;A_iter &amp;lt;- A_partial
A_ls &amp;lt;- list()
counter &amp;lt;- 1
for (AD in 1:2) {
  A_iter[3, ] &amp;lt;- if (AD == 1) {
    A_partial[3, ]
  } else {
    rev(A_partial[3, ])
  }
  for (CD in 1:2) {
    A_iter[4, ] &amp;lt;- if (CD == 1) {
      A_partial[4, ]
    } else {
      rev(A_partial[4, ])
    }
    for (CA in 1:2) {
      A_iter[5, ] &amp;lt;- if (CA == 1) {
        A_partial[5, ]
      } else {
        rev(A_partial[5, ])
      }
      A_check &amp;lt;- tryCatch(
        {
          arcs(base_bn) &amp;lt;- A_iter
        },
        error = function(e) {
          &amp;quot;error&amp;quot;
        }
      )
      if (all(A_check != &amp;quot;error&amp;quot;)) {
        A_ls[[counter]] &amp;lt;- base_bn
        counter &amp;lt;- counter + 1
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How many valid DAGs did this result in?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;length(A_ls)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 6
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s plot these out and be done with it:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;par(mfrow = c(2, 3))
for (plot_iter in A_ls) {
  dag_igraph &amp;lt;- graph_from_edgelist(arcs(plot_iter))
  plot(dag_igraph,
    layout = layout.circle,
    vertex.size = 30
  )
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-09-27-practice_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;(c) List the arcs that can be reversed (i.e., turned in the opposite direction), one at a time, without introducing cycles in the directed graph.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;All arcs of the directed graph can be reversed without introducing cycles.&lt;/p&gt;
&lt;h3 id=&#34;nagarajan-13&#34;&gt;Nagarajan 1.3&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;1.3. The (famous) iris data set reports the measurements in centimeters of the sepal length and width and the petal length and width for 50 flowers from each of 3 species of iris (“setosa,” “versicolor,” and “virginica”).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;(a) Load the iris data set (it is included in the datasets package, which is part of the base R distribution and does not need to be loaded explicitly) and read its manual page.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(iris)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;?iris
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;(b) Investigate the structure of the data set.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(iris)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &#39;data.frame&#39;:	150 obs. of  5 variables:
##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...
##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...
##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...
##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...
##  $ Species     : Factor w/ 3 levels &amp;quot;setosa&amp;quot;,&amp;quot;versicolor&amp;quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;(c) Compare the sepal length among the three species by plotting histograms side by side.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(ggplot2)
ggplot(iris, aes(x = Sepal.Length)) +
  geom_histogram() +
  facet_wrap(~Species) +
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-09-27-practice_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;(d) Repeat the previous point using boxplots.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(ggplot2)
ggplot(iris, aes(x = Species, y = Sepal.Length)) +
  geom_boxplot() +
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-09-27-practice_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;nagarajan-14&#34;&gt;Nagarajan 1.4&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;1.4. Consider again the iris data set from Exercise 1.3.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;(a) Write the data frame holding iris data frame into a space-separated text file named “iris.txt,” and read it back into a second data frame called iris2.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;write.table(iris, file = &amp;quot;iris.txt&amp;quot;, row.names = FALSE)
iris2 &amp;lt;- read.table(&amp;quot;iris.txt&amp;quot;, header = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;(b) Check that iris and iris2 are identical.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;identical(iris, iris2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why are they not identical? That&amp;rsquo;s because &lt;code&gt;iris&lt;/code&gt; stores &lt;code&gt;Species&lt;/code&gt; as a &lt;code&gt;factor&lt;/code&gt;. Information which is lost when writing to a .txt file. Let&amp;rsquo;s reformat this and check again:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;iris2$Species &amp;lt;- factor(iris2$Species)
identical(iris, iris2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;(c) Repeat the previous two steps with a file compressed with bzip2 named “iris.txt.bz2.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bzfd &amp;lt;- bzfile(&amp;quot;iris.txt.bz2&amp;quot;, open = &amp;quot;w&amp;quot;)
write.table(iris, file = bzfd, row.names = FALSE)
close(bzfd)
bzfd &amp;lt;- bzfile(&amp;quot;iris.txt.bz2&amp;quot;, open = &amp;quot;r&amp;quot;)
iris2 &amp;lt;- read.table(bzfd, header = TRUE)
close(bzfd)
iris2$Species &amp;lt;- factor(iris2$Species)
identical(iris, iris2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;(d) Save iris directly (e.g., without converting it to a text table) into a file called “iris.rda,” and read it back.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;save(iris, file = &amp;quot;iris.rda&amp;quot;)
load(&amp;quot;iris.rda&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;(e) List all R objects in the global environment and remove all of them apart from iris.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ls()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;A_check&amp;quot;     &amp;quot;A_directed&amp;quot;  &amp;quot;A_iter&amp;quot;      &amp;quot;A_ls&amp;quot;        &amp;quot;A_partial&amp;quot;   &amp;quot;AD&amp;quot;          &amp;quot;base_bn&amp;quot;     &amp;quot;bzfd&amp;quot;        &amp;quot;CA&amp;quot;          &amp;quot;CD&amp;quot;          &amp;quot;counter&amp;quot;     &amp;quot;dag_igraph&amp;quot;  &amp;quot;iris&amp;quot;        &amp;quot;iris2&amp;quot;      
## [15] &amp;quot;Nagara1.2aD&amp;quot; &amp;quot;Nagara1.2aP&amp;quot; &amp;quot;plot_iter&amp;quot;   &amp;quot;V&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;l &amp;lt;- ls()
rm(list = l[l != &amp;quot;iris&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;(f) Exit the R saving the contents of the current session.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;quit(save = &amp;quot;yes&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;nagarajan-15&#34;&gt;Nagarajan 1.5&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;1.5. Consider the gaussian.test data set included in bnlearn.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;(a) Print the column names.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;colnames(gaussian.test)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;C&amp;quot; &amp;quot;D&amp;quot; &amp;quot;E&amp;quot; &amp;quot;F&amp;quot; &amp;quot;G&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;(b) Print the range and the quartiles of each variable.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## range
for (var in names(gaussian.test)) {
  print(range(gaussian.test[, var]))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -2.246520  4.847388
## [1] -10.09456  14.21538
## [1] -15.80996  32.44077
## [1] -9.043796 26.977326
## [1] -3.558768 11.494383
## [1] -1.170247 45.849594
## [1] -1.365823 12.409607
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## quantiles
for (var in names(gaussian.test)) {
  print(quantile(gaussian.test[, var]))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         0%        25%        50%        75%       100% 
## -2.2465197  0.3240041  0.9836491  1.6896519  4.8473877 
##           0%          25%          50%          75%         100% 
## -10.09455807  -0.01751825   2.00025495   4.07692065  14.21537969 
##         0%        25%        50%        75%       100% 
## -15.809961   3.718150   8.056369  12.373614  32.440769 
##        0%       25%       50%       75%      100% 
## -9.043796  5.984274  8.994232 12.164417 26.977326 
##        0%       25%       50%       75%      100% 
## -3.558768  2.095676  3.508567  4.873497 11.494383 
##        0%       25%       50%       75%      100% 
## -1.170247 17.916175 21.982997 26.330886 45.849594 
##        0%       25%       50%       75%      100% 
## -1.365823  3.738940  5.028420  6.344179 12.409607
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;(c) Print all the observations for which A falls in the interval [3,4] and B in (−∞,−5]∪[10,∞).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;TestA &amp;lt;- (gaussian.test[, &amp;quot;A&amp;quot;] &amp;gt;= 3) &amp;amp; (gaussian.test[, &amp;quot;A&amp;quot;] &amp;lt;= 4)
TestB &amp;lt;- (gaussian.test[, &amp;quot;B&amp;quot;] &amp;lt;= -4) | (gaussian.test[, &amp;quot;B&amp;quot;] &amp;gt;= 4)
gaussian.test[TestA &amp;amp; TestB, ]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             A         B          C           D         E        F         G
## 134  3.000115  5.677259 19.6165914 13.90710134 1.2399546 29.66208 6.1184117
## 171  3.912097  5.000325 19.5261459 13.31373543 0.2807556 32.51754 7.3699037
## 954  3.735995  4.052434 18.1856087 12.09431457 3.7080579 26.97241 3.0517690
## 1034 3.317637  4.837303 18.1044125 13.32279487 2.9563555 28.40041 3.9876290
## 1042 3.372036  4.197395 17.6533233 12.52777893 4.8983597 31.08633 3.7283984
## 1078 3.157623  5.184670 19.0430676 13.89072391 3.2572347 37.08646 8.9429136
## 1127 3.141251  4.269507 17.1939411 12.42629251 2.6622288 29.53266 4.4132469
## 1237 3.031727  4.341881 17.6020776 12.43625964 2.7949498 27.74622 5.4084005
## 1755 3.052266  4.612071 18.1180989 13.19968750 5.6471997 27.78189 1.5129846
## 1819 3.290631  7.143942 22.6867359 16.14048867 4.8648686 37.51501 8.0241134
## 2030 3.289162  5.787095 20.0194963 14.52479092 5.2423368 37.46786 7.6643575
## 2153 3.037955  4.797399 17.1331929 13.58727751 2.7524554 31.84176 5.5202624
## 2179 3.114916  5.414628 19.3677155 14.14501162 3.4438786 31.06106 3.8799064
## 2576 3.458761  4.471637 18.1722539 13.36940122 2.3688318 23.60165 0.7689322
## 2865 3.140513  7.269383 22.5686681 16.86773531 4.0260061 36.17848 6.1028714
## 3035 3.476832  4.109519 17.0599625 11.55995675 4.4027366 29.83397 3.6381844
## 3133 3.667252  4.953129 19.6575484 13.31833594 5.0080665 31.78321 3.9031780
## 3434 3.418895  6.412021 21.8072576 15.48090391 5.6847825 29.41806 1.3941551
## 3481 3.050811  6.203747 21.1427355 15.39309314 4.2068982 33.79386 5.1348785
## 3573 3.612315  4.741220 18.4841065 13.08992732 5.4532191 31.22224 3.3986084
## 3695 3.284053  4.899003 17.7691812 13.73467842 4.5814578 39.96773 9.5808916
## 3893 3.070645  6.111989 20.6963754 15.34110860 2.0329921 29.92680 4.1352188
## 3999 3.493238  5.307218 19.1502279 14.10123450 4.8567953 35.69970 5.8485995
## 4144 3.045976  4.925688 18.8388604 13.28690773 7.3473994 34.12657 4.0527848
## 4164 3.624343  5.411443 20.3400016 13.35879870 7.4107565 32.20646 2.5717899
## 4220 3.133246  4.950543 17.9604182 13.91087282 4.2105519 30.01146 3.7960603
## 4229 3.119493  7.255816 22.4329800 16.59886045 2.4893039 33.62491 5.0819631
## 4258 3.777205  7.189762 23.9019969 16.75321069 4.0727603 37.66095 6.6653238
## 4671 3.455920 -4.198865  0.1452861 -0.27503006 1.9954874 16.60534 4.7261660
## 4703 3.301100 -4.109750  0.2244686 -0.07441052 6.2531813 23.58530 6.4627941
## 4739 3.010097  9.775164 28.1861733 20.54659992 5.1594216 40.50032 5.8467280
## 4779 3.215547  6.393758 20.7043512 15.59370075 3.2628127 33.35600 5.8151547
## 4866 3.873728 -4.257339  0.8213114 -0.31665717 0.2758219 14.94325 5.4011586
## 4987 3.058566  8.128704 24.9419446 18.56396890 5.8402279 35.29171 4.4032448
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;(d) Sample 50 rows without replacement.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I&amp;rsquo;m leaving out the output to save some space.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(42)
gaussian.test[sample(50, replace = FALSE), ]
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;(e) Draw a bootstrap sample (e.g., sample 5,000 observations with replacement) and compute the mean of each variable.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;colMeans(gaussian.test[sample(5000, replace = TRUE), ])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         A         B         C         D         E         F         G 
##  1.007428  2.076592  8.157574  9.101292  3.532690 22.160643  4.998093
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;(f) Standardize each variable.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I&amp;rsquo;m leaving out the oputput to save some space.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;scale(gaussian.test)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;nagarajan-16&#34;&gt;Nagarajan 1.6&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;1.6. Generate a data frame with 100 observations for the following variables:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;(a) A categorical variable with two levels, low and high. The first 50 observations should be set to low, the others to high.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;A &amp;lt;- factor(c(rep(&amp;quot;low&amp;quot;, 50), rep(&amp;quot;high&amp;quot;, 50)), levels = c(&amp;quot;low&amp;quot;, &amp;quot;high&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;(b) A categorical variable with two levels, good and bad, nested within the first variable, i.e., the first 25 observations should be set to good, the second 25 to bad, and so on.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;nesting &amp;lt;- c(rep(&amp;quot;good&amp;quot;, 25), rep(&amp;quot;bad&amp;quot;, 25))
B &amp;lt;- factor(rep(nesting, 2), levels = c(&amp;quot;good&amp;quot;, &amp;quot;bad&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;(c) A continuous, numerical variable following a Gaussian distribution with mean 2 and variance 4 when the first variable is equal to low and with mean 4 and variance 1 if the first variable is equal to high. In addition, compute the standard deviation of the last variable for each configuration of the first two variables.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(42)
C &amp;lt;- c(rnorm(50, mean = 2, sd = 2), rnorm(50, mean = 4, sd = 1))
data &amp;lt;- data.frame(A = A, B = B, C = C)
aggregate(C ~ A + B, data = data, FUN = sd)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      A    B         C
## 1  low good 2.6127294
## 2 high good 0.9712271
## 3  low  bad 1.8938398
## 4 high  bad 0.8943556
&lt;/code&gt;&lt;/pre&gt;
&lt;!-- ### Scutari Exercises --&gt;
&lt;!-- These are answers and solutions to the exercises at the end of Part 5 in [Bayesian Networks with Examples in R](https://www.bnlearn.com/book-crc/) by M. Scutari and J.-B. Denis. Much of my inspiration for these solutions, where necessary, by consulting the solutions provided by the authors themselves as in the appendix. --&gt;
&lt;!-- #### Scutari 5.1 --&gt;
&lt;!-- &gt; One essential task in any analysis is to import and export the R objects describing models from different packages. This is all the more true in the case of BN modelling, as no package implements all of structure learning, parameter learning and inference. --&gt;
&lt;!-- &gt; 1. Create the dag.bnlearn object from Section 5.1.1. --&gt;
&lt;!-- &gt; 2. Export it to deal. --&gt;
&lt;!-- &gt;3. Import the result back into bnlearn. --&gt;
&lt;!-- &gt; 4. Export dag.bnlearn to catnet and import it back in bnlearn. --&gt;
&lt;!-- &gt; 5. Perform parameter learning using the discretised dmarks and dag.bnlearn and export it to a DSC file, which can be read in Hugin and GeNIe. --&gt;
&lt;!-- #### Scutari 5.2 --&gt;
&lt;!-- &gt; Learn a GBN from the marks data (without the LAT variable) using pcalg and a custom test that defines dependence as significant if the corresponding partial correlation is greater than 0.50. --&gt;
&lt;!-- #### Scutari 5.3 --&gt;
&lt;!-- &gt; Reproduce the example of structure learning from Section 5.1.1 using deal, but set the imaginary sample size to 20. How does the resulting network change? --&gt;
</description>
    </item>
    
    <item>
      <title>Gaussian Bayesian Networks</title>
      <link>https://www.erikkusch.com/courses/bayes-nets/part-2/</link>
      <pubDate>Tue, 01 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/bayes-nets/part-2/</guid>
      <description>&lt;style&gt;

blockquote{
color:#633a00;
}

&lt;/style&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/content/courses/bayes-nets/4-Gaussian-Networks_11-10-22.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gaussian Bayesian Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;exercises&#34;&gt;Exercises&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to the exercises at the end of Part 2 in 
&lt;a href=&#34;https://www.bnlearn.com/book-crc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bayesian Networks with Examples in R&lt;/a&gt; by M. Scutari and J.-B. Denis. Much of my inspiration for these solutions, where necessary, by consulting the solutions provided by the authors themselves as in the appendix.&lt;/p&gt;
&lt;h3 id=&#34;r-environment&#34;&gt;&lt;code&gt;R&lt;/code&gt; Environment&lt;/h3&gt;
&lt;p&gt;For today&amp;rsquo;s exercise, I load the following packages:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(bnlearn)
library(ggplot2)
library(tidyr)
library(tidybayes)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;scutari-21&#34;&gt;Scutari 2.1&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Prove that Equation (2.2) implies Equation (2.3).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Equation 2.2 reads:&lt;/p&gt;
&lt;p&gt;$$f(C | G = g) \neq f(C)$$&lt;/p&gt;
&lt;p&gt;Equation 2.3 reads:&lt;/p&gt;
&lt;p&gt;$$f(G | C = c) \neq f(G)$$&lt;/p&gt;
&lt;p&gt;So how do we go about demonstrating that the first implies the latter? Well, we are using Bayesian theory here so why not use the Bayes&#39; theorem? So let&amp;rsquo;s start by rewriting equation 2.2:&lt;/p&gt;
&lt;p&gt;$$f(C | G) = \frac{f(C, G)}{f(G)} = \frac{f(G | C) f(C)}{f(G)}$$
So how does this relate to the question that equation 2.2 implies equation 2.3? Well, if $f(C|G) = f(C)$ then this equation would reveal that $f(G|C) = f(G)$ (so that the $f(G)$ terms factor out). Our proof stipulates that these statements aren&amp;rsquo;t true, but one still implies the other and we land of quod erat demonstrandum.&lt;/p&gt;
&lt;h3 id=&#34;scutari-22&#34;&gt;Scutari 2.2&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Within the context of the DAG shown in Figure 2.1, prove that Equation (2.5) is true using Equation (2.6).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is the DAG in question:&lt;/p&gt;
&lt;img src=&#34;https://www.erikkusch.com/courses/networksscutari/Screenshot 2021-06-02 135704.png&#34; width=&#34;900&#34;/&gt;
&lt;p&gt;The equation to prove (2.5) is:&lt;/p&gt;
&lt;p&gt;$$f(N, W | V = v) = f(N | V = v) f(W | V = v)$$&lt;/p&gt;
&lt;p&gt;and we use this equation (2.6) for our proof:&lt;/p&gt;
&lt;p&gt;$$f(G, E, V, N, W, C) = f(G) f(E) f(V | G, E) f(N | V) f(W | V) f(C | N, W)$$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start the proof by integrating over all variables that aren&amp;rsquo;t $N$, $W$, and $V$ (the variables contained in the equation we are tasked to prove):&lt;/p&gt;
&lt;p&gt;$$f(V, W, N) = \int_G \int_E \int_Cf(G,E,V,N,W,C)$$&lt;/p&gt;
&lt;p&gt;We do this to remove all but the variables we are after from our equation so let&amp;rsquo;s follow this rationale:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\int_G \int_E \int_Cf(G,E,V,N,W,C) = &amp;amp;f(V) f(N|V) f(W|V)  \newline
&amp;amp;\times \left( \int_G \int_E f(G) f(E) f(V|G,E) \right) \newline
&amp;amp;\left( \int_C f(C|N,W) \right)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Simplifying this mess, we arrive at:&lt;/p&gt;
&lt;p&gt;$$f(V, W, N) = f(V) f(N|V) f(W|V)$$&lt;/p&gt;
&lt;p&gt;Finally, we can obtain our original formula:&lt;/p&gt;
&lt;p&gt;$$f(W,N|V) = \frac{f(V,W,N)}{f(V)} = \frac{f(V) f(N|V) f(W|V)}{f(V)} = f(N|V) f(W|N)$$&lt;/p&gt;
&lt;p&gt;Another case of the quod erat demonstrandums.&lt;/p&gt;
&lt;h3 id=&#34;scutari-23&#34;&gt;Scutari 2.3&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Compute the marginal variance of the two nodes with two parents from the local distributions proposed in Table 2.1. Why is it much more complicated for C than for V?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Table 2.1 is hardly a table at all, but I did locate it. Basically, it is an amalgamation of the probability distributions proposed for the DAG from the previous exercise:&lt;/p&gt;
&lt;img src=&#34;https://www.erikkusch.com/courses/networksscutari/Screenshot 2021-06-02 141404.png&#34; width=&#34;900&#34;/&gt;
&lt;p&gt;Note that the parameter $07v$ in the second-to-last row should read $0.7v$.&lt;/p&gt;
&lt;p&gt;The two nodes we are after are $V$ and $C$. Since the task already tells us that the computation of the marginal variance for $V$ is easier than for $C$, I start with this one.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Computation for $V$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Simply translating the probability distribution into a linear model, we receive:&lt;/p&gt;
&lt;p&gt;$$V = -10.35534 + 0.5G + 0.70711E + \epsilon_V$$&lt;/p&gt;
&lt;p&gt;with the variances of our independent variables $G$, $E$, and $\epsilon_V$ being $10^2$, $10^2$, and $5^2$ respectively. Consequently the variance of $V$ can be calculated as follows:&lt;/p&gt;
&lt;p&gt;$$VAR(V) = 0.5^2VAR(G) + 0.70711^2VAR(E) + VAR(\epsilon_V)$$
$$VAR(V) = 0.5^210^2+0.70711^210^2+5^2 = 10$$&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Computation for $C$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For $C$, we can transform our portability distribution into a linear model again:&lt;/p&gt;
&lt;p&gt;$$C = 0.3N+0.7W+\epsilon_C$$&lt;/p&gt;
&lt;p&gt;this time, however the &lt;strong&gt;predictors variables&lt;/strong&gt; are &lt;strong&gt;not independent&lt;/strong&gt; since they share node $V$ as their parent. Consequently, we have to compute their covariance:&lt;/p&gt;
&lt;p&gt;$$COV(N,W) = COV(0.1V, 0.7V) = 0.1 * 0.7 * Var(V) = 0.1 * 0.7 * 10^2$$&lt;/p&gt;
&lt;p&gt;So we actually needed to calculate the variance for $V$ to even be able to calculate the variance for $C$. Let&amp;rsquo;s round this out now, then:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
Var(C) &amp;amp;= 0.3^2 * VAR(N) + 0.7^2VAR(W) \newline
&amp;amp;+ VAR(\epsilon_C) + 2 * 0.3 * 0.7 * COV(N,W)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Now, I simply plug the values into the formula and arrive at:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
Var(C) &amp;amp;= 0.3^2 * 9.949874^2+0.7^2 * 7.141428 \newline
&amp;amp;+6.25^2+2 * 0.3 * 0.7 * 0.1 * 0.7 * 10^2  \newline
&amp;amp; = 54.4118
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Curiously, the book suggest this as the solution:&lt;/p&gt;
&lt;p&gt;$$Var(C) = (0.3^2+0.7^2+0.3 * 0.7 * 0.14)10^2+6.25^2 = 100.0024$$&lt;/p&gt;
&lt;p&gt;I am not sure where the values for VAR(N) and VAR(W) have gone here. If anyone who is reading this knows the answer to it, please contact me and let me know as well.&lt;/p&gt;
&lt;h3 id=&#34;scutari-24&#34;&gt;Scutari 2.4&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Write an R script using only the &lt;code&gt;rnorm&lt;/code&gt; and &lt;code&gt;cbind&lt;/code&gt; functions to create a 100 × 6 matrix of 100 observations simulated from the BN defined in Table 2.1. Compare the result with those produced by a call to &lt;code&gt;cpdist&lt;/code&gt; function.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To simulate a table of observation using the formulae in the probability distribution collection from the previous question (Table 1), we simply select random values for all parent nodes according to their distributions and let the distributions for all offspring nodes do the rest. One important note here, is that the &lt;code&gt;rnorm()&lt;/code&gt; function in &lt;code&gt;R&lt;/code&gt; takes as an argument of variation the standard deviation $\sigma$ rather than the variance $\sigma^2$:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(42) # making things reproducible
n &amp;lt;- 1e2 # number of replicates
G &amp;lt;- rnorm(n, 50, 10)
E &amp;lt;- rnorm(n, 50, 10)
V &amp;lt;- rnorm(n, -10.35534 + 0.5 * G + 0.70711 * E, 5)
N &amp;lt;- rnorm(n, 45 + 0.1 * V, 9.949874)
W &amp;lt;- rnorm(n, 15 + 0.7 * V, 7.141428)
C &amp;lt;- rnorm(n, 0.3 * N + 0.7 * W, 6.25)
sim1 &amp;lt;- data.frame(cbind(G, E, V, N, W, C))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we do this using the &lt;code&gt;cpdist()&lt;/code&gt; function. To do so, we first have to create our Bayesian Network:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dag.bnlearn &amp;lt;- model2network(&amp;quot;[G][E][V|G:E][N|V][W|V][C|N:W]&amp;quot;)
disE &amp;lt;- list(coef = c(&amp;quot;(Intercept)&amp;quot; = 50), sd = 10)
disG &amp;lt;- list(coef = c(&amp;quot;(Intercept)&amp;quot; = 50), sd = 10)
disV &amp;lt;- list(coef = c(&amp;quot;(Intercept)&amp;quot; = -10.35534, E = 0.70711, G = 0.5), sd = 5)
disN &amp;lt;- list(coef = c(&amp;quot;(Intercept)&amp;quot; = 45, V = 0.1), sd = 9.949874)
disW &amp;lt;- list(coef = c(&amp;quot;(Intercept)&amp;quot; = 15, V = 0.7), sd = 7.141428)
disC &amp;lt;- list(coef = c(&amp;quot;(Intercept)&amp;quot; = 0, N = 0.3, W = 0.7), sd = 6.25)
dis.list &amp;lt;- list(E = disE, G = disG, V = disV, N = disN, W = disW, C = disC)
gbn.bnlearn &amp;lt;- custom.fit(dag.bnlearn, dist = dis.list)
sim2 &amp;lt;- data.frame(cpdist(gbn.bnlearn, nodes = nodes(gbn.bnlearn), evidence = TRUE))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;this is pretty much exactly what is done in the chapter.&lt;/p&gt;
&lt;p&gt;So let&amp;rsquo;s compare these simulation outputs:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# preparing all data together in one data frame for plotting
sim1$sim &amp;lt;- 1
sim2$sim &amp;lt;- 2
Plot_df &amp;lt;- rbind(sim1, sim2[, match(colnames(sim1), colnames(sim2))])
Plot_df &amp;lt;- gather(data = Plot_df, key = &amp;quot;node&amp;quot;, value = &amp;quot;value&amp;quot;, G:C)
Plot_df$sim &amp;lt;- as.factor(Plot_df$sim)
## plotting
ggplot(Plot_df, aes(x = value, y = sim)) +
  stat_halfeye() +
  facet_wrap(~node, scales = &amp;quot;free&amp;quot;) +
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-25-crc_part2_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As is apparent from this, all results fall close to the expected values of roughly 50. There are noticeable differences between the simulations. I would suggest that these are due to the fairly low sample size for &lt;code&gt;sim1&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;scutari-25&#34;&gt;Scutari 2.5&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Imagine two ways other than changing the size of the points (as in Section 2.7.2) to introduce a third variable in the plot.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The plot in question is this one:&lt;/p&gt;
&lt;img src=&#34;https://www.erikkusch.com/courses/networksscutari/Screenshot 2021-06-02 163301.png&#34; width=&#34;900&#34;/&gt;
&lt;p&gt;this plot is aimed at showing the distribution of $C$ when both $E$ and $V$ vary. Here, the variation in $V$ is shown along the x-axis, while the variation of $E$ is contained within the sizes of the circles. The y-axis represents the values of $C$ according to its distributions.&lt;/p&gt;
&lt;p&gt;So how else could we add information of $E$ to a plot of $V$ and $C$? I reckon we could:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Make three scatter plots. One for each pairing of our variables.&lt;/li&gt;
&lt;li&gt;Represent the values of $E$ with a colour saturation gradient.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;scutari-26&#34;&gt;Scutari 2.6&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Can GBNs be extended to log-normal distributions? If so how, if not, why?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;GBNs are Gaussian Bayesian Networks - Bayesian Networks where each node follows a Gaussian distribution.&lt;/p&gt;
&lt;p&gt;Yes, absolutely they can! We can simply take the logarithm of all initial variables and apply the GBN right away. Of course, all values that shall be transformed using the logarithm have to be positive.&lt;/p&gt;
&lt;h3 id=&#34;scutari-27&#34;&gt;Scutari 2.7&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;How can we generalise GBNs as defined in Section 2.3 in order to make each node’s variance depend on the node’s parents?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I see absolutely no problem here. Let&amp;rsquo;s say we have two nodes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$A$; parent node with a constant variance&lt;/li&gt;
&lt;li&gt;$B$; child node with a variance dependant the parent node&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then we can easily define the variance of $B$ given $A$ ($VAR(B|A)$) as follows:&lt;/p&gt;
&lt;p&gt;$$VAR(B|A) = \left(A-E(A)\right)^2 * \sigma^2_B$$&lt;/p&gt;
&lt;h3 id=&#34;scutari-28&#34;&gt;Scutari 2.8&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;From the first three lines of Table 2.1, prove that the joint distribution of E, G and V is trivariate normal.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This one is a doozy and I really needed to consult the solutions in the book for this one. Let&amp;rsquo;s first remind ourselves of the first lines of said table:&lt;/p&gt;
&lt;img src=&#34;https://www.erikkusch.com/courses/networksscutari/Screenshot 2021-06-02 141404_2.png&#34; width=&#34;900&#34;/&gt;
&lt;p&gt;To approach this problem it is useful to point out that the logarithm of the density of a multivariate normal distribution is defined as such:&lt;/p&gt;
&lt;p&gt;$$f(x) \propto -\frac{1}{2}(x-\mu)^T\sum^{-2}(x-\mu)$$&lt;/p&gt;
&lt;p&gt;with $x$ being a random vector and $\mu$ denoting our expectation. $\sum$ identifies the covariance matrix.&lt;/p&gt;
&lt;p&gt;Simplifying this, we can transform our variables $G$, $E$, and $V$ to give them a zero marginal expectation and a unity marginal variance. That is a very long-winded way of saying: we normalise our variables:&lt;/p&gt;
&lt;p&gt;$$\overline G = \frac{G-E(G)}{\sqrt{VAR(G)}} = \frac{G-50}{10} \sim Normal(0, 1)$$
$$\overline E = \frac{E-E(E)}{\sqrt{VAR(E)}} = \frac{E-50}{10} \sim Normal(0, 1)$$
$$\overline V = \frac{V-E(V)}{\sqrt{VAR(V)}} = \frac{V-50}{10}$$
Solving for $\overline V | \overline G, \overline E$, we obtain:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\overline V | \overline G, \overline E = Normal\left(\frac{1}{2} \overline G + \sqrt{\frac{1}{2}} \overline E , (\frac{1}{2})^2 \right)
\end{equation}&lt;/p&gt;
&lt;!-- $$\overline V | \overline G, \overline E = Normal\left(\frac{1}{2} \overline G + \sqrt{\frac{1}{2}} \overline E , (\frac{1}{2})^2 \right)$$ --&gt;
&lt;p&gt;I have to honestly that I don&amp;rsquo;t quite understand how this happened and if anyone reading this has intuition for this solution, please let me know.&lt;/p&gt;
&lt;p&gt;Now, we can compute the joint density distribution of these three normalised variables:&lt;/p&gt;
&lt;p&gt;$$\begin{eqnarray}  f(\overline G, \overline E, \overline V) &amp;amp;\propto&amp;amp; f(\overline G)+f(\overline E)+f(\overline V | \overline G, \overline E) \newline 
&amp;amp;=&amp;amp; -\frac{g^2}{2}-\frac{e^2}{2}-2 \left( v- \frac{1}{2}g - \sqrt{\frac{1}{2}}e \right)^2     \newline
&amp;amp;=&amp;amp; -\begin{bmatrix} g \newline e \newline v\end{bmatrix}^T \begin{bmatrix} 1 &amp;amp; \frac{\sqrt{2}}{2} &amp;amp; -1\newline \frac{\sqrt{2}}{2} &amp;amp; \frac{3}{2} &amp;amp; -\sqrt{2} \newline -1 &amp;amp; -\sqrt{2} &amp;amp; 2 \end{bmatrix} \begin{bmatrix} g \newline e \newline v\end{bmatrix}  \newline
&amp;amp;=&amp;amp; -\frac{1}{2} \begin{bmatrix} g \newline e \newline v\end{bmatrix}^T \begin{bmatrix} 1 &amp;amp; 0 &amp;amp; \frac{1}{2}\newline 0 &amp;amp; 1 &amp;amp; \frac{1}{2} \newline \frac{1}{2} &amp;amp; \sqrt{\frac{1}{2}} &amp;amp; 1 \end{bmatrix} \begin{bmatrix} g \newline e \newline v\end{bmatrix}  \newline
\end{eqnarray}$$&lt;/p&gt;
&lt;p&gt;I have to admit that most of this is, as of right now, beyond me as I came to this book for the &amp;ldquo;&lt;em&gt;applications in R&lt;/em&gt;&amp;rdquo; in the first place. The book concludes that this results in:&lt;/p&gt;
&lt;p&gt;$$VAR \left( \begin{bmatrix} \overline G \newline \overline E \newline \overline V\end{bmatrix} \right) = \begin{bmatrix} 1 &amp;amp; 0 &amp;amp; \frac{1}{2}\newline 0 &amp;amp; 1 &amp;amp; \frac{1}{2} \newline \frac{1}{2} &amp;amp; \sqrt{\frac{1}{2}} &amp;amp; 1 \end{bmatrix} $$&lt;/p&gt;
&lt;p&gt;which results in our proof.&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.2.1 (2022-06-23 ucrt)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19044)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_Germany.utf8  LC_CTYPE=English_Germany.utf8    LC_MONETARY=English_Germany.utf8 LC_NUMERIC=C                     LC_TIME=English_Germany.utf8    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] tidybayes_3.0.2 tidyr_1.2.0     ggplot2_3.3.6   bnlearn_4.8.1  
## 
## loaded via a namespace (and not attached):
##  [1] styler_1.8.0         tidyselect_1.1.2     xfun_0.33            bslib_0.4.0          purrr_0.3.4          lattice_0.20-45      colorspace_2.0-3     vctrs_0.4.1          generics_0.1.3      
## [10] htmltools_0.5.3      yaml_2.3.5           utf8_1.2.2           rlang_1.0.5          R.oo_1.25.0          jquerylib_0.1.4      pillar_1.8.1         glue_1.6.2           withr_2.5.0         
## [19] DBI_1.1.3            R.utils_2.12.0       distributional_0.3.1 R.cache_0.16.0       lifecycle_1.0.2      stringr_1.4.1        posterior_1.3.1      munsell_0.5.0        blogdown_1.13       
## [28] gtable_0.3.1         R.methodsS3_1.8.2    coda_0.19-4          evaluate_0.16        labeling_0.4.2       knitr_1.40           fastmap_1.1.0        parallel_4.2.1       fansi_1.0.3         
## [37] highr_0.9            arrayhelpers_1.1-0   backports_1.4.1      checkmate_2.1.0      scales_1.2.1         cachem_1.0.6         jsonlite_1.8.0       abind_1.4-5          farver_2.1.1        
## [46] tensorA_0.36.2       digest_0.6.29        svUnit_1.0.6         stringi_1.7.8        bookdown_0.29        dplyr_1.0.9          grid_4.2.1           ggdist_3.2.0         cli_3.3.0           
## [55] tools_4.2.1          magrittr_2.0.3       sass_0.4.2           tibble_3.1.8         pkgconfig_2.0.3      ellipsis_0.3.2       assertthat_0.2.1     rmarkdown_2.16       rstudioapi_0.14     
## [64] R6_2.5.1             compiler_4.2.1
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 01 &amp; 02</title>
      <link>https://www.erikkusch.com/courses/rethinking/chapter-02/</link>
      <pubDate>Tue, 05 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/rethinking/chapter-02/</guid>
      <description>&lt;h1 id=&#34;small-worlds-in-large-worlds&#34;&gt;Small Worlds in Large Worlds&lt;/h1&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/rethinking/1_11-12-2020_SUMMARY_The-Golem-Of-Prague.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides Chapter 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/rethinking/2_18-12-2020_SUMMARY_-Basics-of-Bayesian-Inference-and-Counting.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides Chapter 2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to the exercises at the end of chapter 2 in 
&lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Satistical Rethinking 2&lt;/a&gt; by Richard McElreath. I have created these notes as a part of my ongoing involvement in the 
&lt;a href=&#34;https://www.erikkusch.com/project/aubayes/&#34;&gt;AU Bayes Study Group&lt;/a&gt;. Much of my inspiration for these solutions, where necessary, has been obtained from 
&lt;a href=&#34;https://github.com/jffist/statistical-rethinking-solutions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Taras Svirskyi&lt;/a&gt; and 
&lt;a href=&#34;https://jmgirard.com/statistical-rethinking-ch2/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jeffrey Girard&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;easy-exercises&#34;&gt;Easy Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-e1&#34;&gt;Practice E1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Which of the expressions below correspond to the statement: the probability of rain on Monday?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$Pr(rain)$&lt;/li&gt;
&lt;li&gt;$Pr(rain|Monday)$&lt;/li&gt;
&lt;li&gt;$Pr(Monday|rain)$&lt;/li&gt;
&lt;li&gt;$\frac{Pr(rain,Monday)}{Pr(Monday)}$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2, $Pr(rain|Monday)$ - reads as &amp;ldquo;the probability of rain, given that it is Monday&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;4, $\frac{Pr(rain|Monday)}{Pr(Monday)}$ - reads as &amp;ldquo;the probability that is raining and a Monday, divided by the probability of it being a Monday&amp;rdquo; which is the same as &amp;ldquo;the probability of rain, given that it is Monday. This is simply just the Bayes theorem in action.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;practice-e2&#34;&gt;Practice E2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Which of the following statements corresponds to the expression: $Pr(Monday|rain)$?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The probability of rain on Monday.&lt;/li&gt;
&lt;li&gt;The probability of rain, given that it is Monday.&lt;/li&gt;
&lt;li&gt;The probability that it is Monday, given that it is raining.&lt;/li&gt;
&lt;li&gt;The probability that it is Monday and that it is raining.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;3, The probability that it is Monday, given that it is raining.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;practice-e3&#34;&gt;Practice E3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Which of the expressions below correspond to the statement: the probability that it is Monday, given that it is raining?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$Pr(Monday|rain)$&lt;/li&gt;
&lt;li&gt;$Pr(rain|Monday)$&lt;/li&gt;
&lt;li&gt;$Pr(rain|Monday)Pr(Monday)$&lt;/li&gt;
&lt;li&gt;$\frac{Pr(rain|Monday)Pr(Monday)}{Pr(rain)}$&lt;/li&gt;
&lt;li&gt;$\frac{Pr(Monday|rain)Pr(rain)}{Pr(Monday)}$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1, $Pr(Monday|rain)$&lt;/li&gt;
&lt;li&gt;4, $\frac{Pr(rain|Monday)Pr(Monday)}{Pr(rain)}$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;practice-e4&#34;&gt;Practice E4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; The Bayesian statistician Bruno de Finetti (1906-1985) began his book on probability theory with the declaration: “PROBABILITY DOES NOT EXIST.” The capitals appeared in the original, so I imagine de Finetti wanted us to shout the statement. What he meant is that probability is a device for describing uncertainty from the perspective of an observer with limited knowledge; it has no objective reality. Discuss the globe tossing example from the chapter, in light of this statement. What does it mean to say “the probability of water is 0.7”?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Completely uninformed and looking only at random samples, we would come to the conclusion that 70% of the Earth are covered by water. However, this estimate could be heavily biased once confounding factors are taken into account. Factors such as inaccuracy of the globe model we are tossing when compared to the real globe (i.e. the model is perfectly spherical, but the Earth itself is a flattened ellipsoid).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;medium-exercises&#34;&gt;Medium Exercises&lt;/h2&gt;
&lt;p&gt;This is where we get into &lt;code&gt;R&lt;/code&gt; application of Bayes. Here, I load a few packages to make my outputs a bit nicer to look at:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rm(list = ls())
library(ggplot2)
library(cowplot)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-m1--m2&#34;&gt;Practice M1 &amp;amp; M2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Recall the globe tossing model from the chapter. Compute and plot the grid approximate posterior distribution for each of the following sets of observations. In each case, assume a uniform prior for p.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;W,W,W&lt;/li&gt;
&lt;li&gt;W,W,W,L&lt;/li&gt;
&lt;li&gt;L,W,W,L,W,W,W&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now assume a prior for p that is equal to zero when p&amp;lt;0.5 and is a positive constant when p≥0.5. Again compute and plot the grid approximate posterior distribution for each of the sets of observations in the problem just above.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Register the data we observed
data1 &amp;lt;- c(&amp;quot;W&amp;quot;, &amp;quot;W&amp;quot;, &amp;quot;W&amp;quot;)
data2 &amp;lt;- c(&amp;quot;W&amp;quot;, &amp;quot;W&amp;quot;, &amp;quot;W&amp;quot;, &amp;quot;L&amp;quot;)
data3 &amp;lt;- c(&amp;quot;L&amp;quot;, &amp;quot;W&amp;quot;, &amp;quot;W&amp;quot;, &amp;quot;L&amp;quot;, &amp;quot;W&amp;quot;, &amp;quot;W&amp;quot;, &amp;quot;W&amp;quot;)
data_ls &amp;lt;- list(data1, data2, data3)
names(data_ls) &amp;lt;- c(paste(data1, collapse = &amp;quot;&amp;quot;), paste(data2, collapse = &amp;quot;&amp;quot;), paste(data3, collapse = &amp;quot;&amp;quot;))

# Define grid to sample
p_grid &amp;lt;- seq(0, 1, length.out = 25)
ng &amp;lt;- length(p_grid)

# Register the priors
priors_ls &amp;lt;- list(NA, NA)
names(priors_ls) &amp;lt;- c(&amp;quot;uniform prior (M1)&amp;quot;, &amp;quot;disjunct prior (M2)&amp;quot;)
priors_ls[[1]] &amp;lt;- rep(1, ng)
priors_ls[[2]] &amp;lt;- ifelse(p_grid &amp;lt; 0.5, 0, 1)

# Generate a list within which to store plots for later output all together
plot_ls &amp;lt;- as.list(rep(NA, length(data_ls) * length(priors_ls)))
counter &amp;lt;- 1

# Calculate Posteriors
for (i in 1:length(data_ls)) { # loop over data sets
  data &amp;lt;- data_ls[[i]] # extract data from list
  n &amp;lt;- length(data) # number of observations
  w &amp;lt;- sum(data == &amp;quot;W&amp;quot;) # number of observed water
  for (k in 1:length(priors_ls)) { # loop over priors
    prior &amp;lt;- priors_ls[[k]] # extract prior
    likelihood &amp;lt;- dbinom(w, n, p_grid) # calculate likelihood of water in grid
    posterior &amp;lt;- likelihood * prior # compute posterior
    posterior &amp;lt;- posterior / sum(posterior) # standardise posterior
    # save data to data frame for ggplot
    df &amp;lt;- data.frame(
      param = p_grid,
      prob = posterior,
      ptype = rep(&amp;quot;posterior&amp;quot;, ng)
    )
    # ggplotting to plot list
    plot_ls[[counter]] &amp;lt;- ggplot(df, aes(x = param, y = prob)) +
      geom_line() +
      geom_point() +
      theme_bw() +
      labs(title = paste(names(data_ls)[i], names(priors_ls)[k], sep = &amp;quot; &amp;quot;))
    counter &amp;lt;- counter + 1
  } # end of priors loop
} # end of data set loop
plot_grid(plotlist = plot_ls, ncol = 2, labels = &amp;quot;AUTO&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-05-statistical-rethinking-chapter-02_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;practice-m3&#34;&gt;Practice M3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Suppose there are two globes, one for Earth and one for Mars. The Earth globe is 70% covered in water. The Mars globe is 100% land. Further suppose that one of these globes–you don’t know which–was tossed in the air and produces a “land” observation. Assume that each globe was equally likely to be tossed. Show that the posterior probability that the globe was the Earth, conditional on seeing “land” ($Pr(Earth|land)$), is 0.23.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;br&gt;
From the question, we know that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$P(land|Earth) = 1-0.7 = 0.3$&lt;/li&gt;
&lt;li&gt;$P(land|Mars) = 1$&lt;/li&gt;
&lt;li&gt;$P(Earth) = P(Mars) = .5$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;According to Bayes&#39; theorem, we know that:&lt;br&gt;
$P(Earth|Land) = \frac{P(land|Earth)P(Earth)}{P(land)}$&lt;/p&gt;
&lt;p&gt;Conclusively, we are only missing $P(land)$ which we can calculate from:&lt;/p&gt;
&lt;p&gt;$P(land) = P(land|Earth)P(Earth)+P(land|Mars)Pr(Mars)$&lt;br&gt;
$= 0.3&lt;em&gt;0.5+1&lt;/em&gt;0.5=0.65$&lt;/p&gt;
&lt;p&gt;Finally, we plug all these values into the above Bayes&#39; theorem and obtain:
$P(Earth|Land) = \frac{0.3*0.5}{0.65} = 0.2307692$&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;R&lt;/code&gt;, we can do this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(p_e_l &amp;lt;- (1 - 0.7) * 0.5 / ((1 - 0.7) * 0.5 + 1. * 0.5))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2307692
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-m4&#34;&gt;Practice M4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Suppose you have a deck with only three cards. Each card has two sides, and each side is either black or white. One card has two black sides. The second card has one black and one white side. The third card has two white sides. Now suppose all three cards are placed in a bag and shuffled. Someone reaches into the bag and pulls out a card and places it flat on a table. A black side is shown facing up, but you don’t know the colour of the side facing down. Show that the probability that the other side is also black is 2/3. Use the counting method (Section 2 of the chapter) to approach this problem. This means counting up the ways that each card could produce the observed data (a black card facing up on the table).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;br&gt;
I made a small visualisation of this here:
&lt;img src=&#34;https://www.erikkusch.com/courses/rethinking/Ch02_Cards.jpg&#34; width=&#34;900&#34;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;By Counting:&lt;/em&gt;&lt;br&gt;
As you can see, out of all draws that start with a black side facing up on the first draw (3 total paths), only two conclude to the other side also being black. Thus, the probability is two thirds.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Bayes&#39; Theorem:&lt;/em&gt;&lt;br&gt;
$P(Second = B|First = B) = \frac{P(Second = B, First = B)}{P(First = B)} = \frac{P(BB)}{P(BB)+P(WB)*P(First = B|WB)}$
$= \frac{1/3}{(1/3 + 1/3 * 1/2)} = 2/3$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Via &lt;em&gt;&lt;code&gt;R&lt;/code&gt;&lt;/em&gt;:&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bb.ways &amp;lt;- 2
wb.ways &amp;lt;- 1
ww.ways &amp;lt;- 0
likelihood &amp;lt;- c(bb.ways, wb.ways, ww.ways)
prior &amp;lt;- c(1, 1, 1)
posterior &amp;lt;- prior * likelihood
posterior &amp;lt;- posterior / sum(posterior)
posterior[1] == 2 / 3
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-m5&#34;&gt;Practice M5&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Now suppose there are four cards: BB, BW, WW, and another BB. Again suppose a card is drawn from the bag and a black side appears face up. Again calculate the probability that the other side is black.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;br&gt;
Here&amp;rsquo;s an update of my previous visualisation for this:
&lt;img src=&#34;https://www.erikkusch.com/courses/rethinking/Ch02_Cards2.jpg&#34; width=&#34;900&#34;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;By Counting:&lt;/em&gt;&lt;br&gt;
As you can see, out of all draws that start with a black side facing up on the first draw (5 total paths), four conclude to the other side also being black. Thus, the probability is 4/5 which is 0.8.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Bayes&#39; Theorem:&lt;/em&gt;&lt;br&gt;
$P(Second = B|First = B) = \frac{P(Second = B, First = B)}{P(First = B)} = \frac{P(BB)}{P(BB)+P(WB)*P(First = B|WB)}$
$=\frac{1/2}{(1/2 + 1/4 * 1/2)} = 4/5$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Via &lt;em&gt;&lt;code&gt;R&lt;/code&gt;&lt;/em&gt;:&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bb.ways &amp;lt;- 2
wb.ways &amp;lt;- 1
ww.ways &amp;lt;- 0
likelihood &amp;lt;- c(bb.ways, wb.ways, ww.ways)
prior &amp;lt;- c(2, 1, 1)
posterior &amp;lt;- prior * likelihood
posterior &amp;lt;- posterior / sum(posterior)
posterior[1] == 4 / 5
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-m6&#34;&gt;Practice M6&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Imagine that black ink is heavy, and so cards with black sides are heavier than cards with white sides. As a result, it’s less likely that a card with black sides is pulled from the bag. So again assume that there are three cards: BB, BW, and WW. After experimenting a number of times, you conclude that for every way to pull the BB card from the bag, there are 2 ways to pull the BW card and 3 ways to pull the WW card. Again suppose that a card is pulled and a black side appears face up. Show that the probability the other side is black is now 0.5. Use the counting method, as before.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;br&gt;
Here&amp;rsquo;s an update of my previous visualisation for this:
&lt;img src=&#34;https://www.erikkusch.com/courses/rethinking/Ch02_Cards3.jpg&#34; width=&#34;900&#34;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;By Counting:&lt;/em&gt;&lt;br&gt;
Out of all draws that start with a black side facing up on the first draw (4 total paths), only 2 now conclude to the other side also being black. Thus, the probability is 1/2 which is 0.5.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Bayes&#39; Theorem:&lt;/em&gt;&lt;br&gt;
$P(Second = B|First = B) = \frac{P(Second = B, First = B)}{P(First = B)} = \frac{P(BB)}{P(BB)+P(WB)*P(First = B|WB)}$
$= \frac{1/6}{(1/6 + 2/6 * 1/2)} = 1/2$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Via &lt;em&gt;&lt;code&gt;R&lt;/code&gt;&lt;/em&gt;:&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bb.ways &amp;lt;- 2
wb.ways &amp;lt;- 1
ww.ways &amp;lt;- 0
likelihood &amp;lt;- c(bb.ways, wb.ways, ww.ways)
prior &amp;lt;- c(1, 2, 3)
posterior &amp;lt;- prior * likelihood
posterior &amp;lt;- posterior / sum(posterior)
posterior[1] == 0.5
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-m7&#34;&gt;Practice M7&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Assume again the original card problem, with a single card showing a black side face up. Before looking at the other side, we draw another card from the bag and lay it face up on the table. The face that is shown on the new card is white. Show that the probability that the first card, the one showing a black side, has black on its other side is now 0.75. Use the counting method, if you can. Hint: Treat this like the sequence of globe tosses, counting all the ways to see each observation, for each possible first card.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;br&gt;
Here&amp;rsquo;s an update of my previous visualisation for this:
&lt;img src=&#34;https://www.erikkusch.com/courses/rethinking/Ch02_Cards4.jpg&#34; width=&#34;900&#34;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;By Counting:&lt;/em&gt;&lt;br&gt;
Out of all draws that start with a black side facing up on the first draw (12 total paths), 8 now conclude to the next card being places white-side facing up. Thus, the probability is 8/12 which is 0.75.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Via &lt;em&gt;&lt;code&gt;R&lt;/code&gt;&lt;/em&gt;:&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;card.bb.likelihood &amp;lt;- 2 * 3 # bb pulled first (either side), next card is ww (either side) or wb (white-side up)
card.wb.likelihood &amp;lt;- 1 * 2 # wb pulled black side up, next card is ww (either side)
card.ww.likelihood &amp;lt;- 0
likelihood &amp;lt;- c(card.bb.likelihood, card.wb.likelihood, card.ww.likelihood)
prior &amp;lt;- c(1, 1, 1)
posterior &amp;lt;- prior * likelihood
posterior &amp;lt;- posterior / sum(posterior)
posterior[1] == 0.75
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;hard-exercises&#34;&gt;Hard Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-h1&#34;&gt;Practice H1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Suppose there are two species of panda bear. Both are equally common in the wild and live in the same place. They look exactly alike and eat the same food, and there is yet no genetic assay capable of telling them apart. They differ however in family sizes. Species A gives birth to twins 10% of the time, otherwise birthing a single infant. Species B births twins 20% of the time, otherwise birthing singleton infants. Assume these numbers are known with certainty, from many years of field research. Now suppose you are managing a captive panda breeding program. You have a new female panda of unknown species, and she has just given birth to twins. What is the probability that her next birth will also be twins?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;By Hand:&lt;/em&gt;&lt;br&gt;
From the question, we know that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$Pr(twins|A)=0.1$&lt;/li&gt;
&lt;li&gt;$Pr(twins|B)=0.2$&lt;/li&gt;
&lt;li&gt;$Pr(A)=0.5$&lt;/li&gt;
&lt;li&gt;$Pr(B)=0.5$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We know want to calculate $Pr(twins)$ which is given as:&lt;/p&gt;
&lt;p&gt;$Pr(twins)=Pr(twins|A)Pr(A)+Pr(twins|B)Pr(B)$&lt;/p&gt;
&lt;p&gt;For this, however, we need to know the probability of our individual belonging to species A or B, respectively. For this, we will use the Bayes&#39; Theorem:&lt;/p&gt;
&lt;p&gt;$Pr(A|twins)=\frac{Pr(twins|A)Pr(A)}{Pr(twins)}=\frac{0.1*(0.5)}{0.15}=1/3$&lt;br&gt;
$Pr(B|twins)=\frac{Pr(twins|B)Pr(B)}{Pr(twins)}=\frac{0.2*(0.5)}{0.15}=2/3$&lt;/p&gt;
&lt;p&gt;These values can be used as $Pr(A)$ and $Pr(B)$ respectively in the formula to compute $Pr(twins)$ above as such:&lt;/p&gt;
&lt;p&gt;$Pr(twins) = 0.1&lt;em&gt;1/3 + 0.2&lt;/em&gt;2/3 = 1/6$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In&lt;/em&gt; &lt;code&gt;R&lt;/code&gt;&lt;em&gt;:&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;p_twins_A &amp;lt;- 0.1
p_twins_B &amp;lt;- 0.2
likelihood &amp;lt;- c(p_twins_A, p_twins_B)
prior &amp;lt;- c(1, 1)
posterior &amp;lt;- prior * likelihood
posterior &amp;lt;- posterior / sum(posterior)
sum(posterior * likelihood) == 1 / 6
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-h2&#34;&gt;Practice H2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Recall all the facts from the problem above. Now compute the probability that the panda we have is from species A, assuming we have observed only the first birth and that it was twins.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; We already now from our answer above that it is 1/3.&lt;br&gt;
&lt;em&gt;By Hand:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;$Pr(A|twins)=\frac{Pr(twins|A)Pr(A)}{Pr(twins)}=\frac{0.1*(0.5)}{0.15}=1/3$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In&lt;/em&gt; &lt;code&gt;R&lt;/code&gt;&lt;em&gt;:&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;p_twins_A &amp;lt;- 0.1
p_twins_B &amp;lt;- 0.2
likelihood &amp;lt;- c(p_twins_A, p_twins_B)
prior &amp;lt;- c(1, 1)
posterior &amp;lt;- prior * likelihood
posterior &amp;lt;- posterior / sum(posterior)
posterior[1] # 0.33
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3333333
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-h3&#34;&gt;Practice H3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Continuing on from the previous problem, suppose the same panda mother has a second birth and that it is not twins, but a singleton infant. Compute the posterior probability that this panda is species A.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;By Hand:&lt;/em&gt;&lt;br&gt;
The probability of birthing singleton infants given a certain species-membership is given by:&lt;/p&gt;
&lt;p&gt;$Pr(single|A)=1–Pr(twins|A)=1–0.1=0.9$&lt;br&gt;
$Pr(single|B)=1–Pr(twins|B)=1–0.2=0.8$&lt;/p&gt;
&lt;p&gt;We already know species membership probability given the first birth having been a twin-birth:&lt;/p&gt;
&lt;p&gt;$Pr(A)=1/3$&lt;br&gt;
$Pr(B)=2/3$&lt;/p&gt;
&lt;p&gt;Next, we require the probability of a singleton birth overall, which we calculate as follows:&lt;/p&gt;
&lt;p&gt;$Pr(single)=Pr(single|A)Pr(A)+Pr(single|B)Pr(B)$&lt;br&gt;
$=0.9(1/3)+0.8(2/3)=5/6$&lt;/p&gt;
&lt;p&gt;Finally, we are ready to use the Bayes&#39; Theorem:&lt;/p&gt;
&lt;p&gt;$Pr(A|single)=\frac{Pr(single|A)Pr(A)}{Pr(single)}=\frac{0.9(1/3)}{5/6}=0.36$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In&lt;/em&gt; &lt;code&gt;R&lt;/code&gt;&lt;em&gt;:&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## TWO STEPS WITH UPDATING
p_twins_A &amp;lt;- 0.1
p_twins_B &amp;lt;- 0.2
# first Bayesian update
likelihood_twins &amp;lt;- c(p_twins_A, p_twins_B)
prior &amp;lt;- c(1, 1)
posterior &amp;lt;- prior * likelihood_twins
posterior &amp;lt;- posterior / sum(posterior)
# second Bayesian update
likelihood_single &amp;lt;- c(1 - p_twins_A, 1 - p_twins_B)
prior &amp;lt;- posterior
posterior &amp;lt;- prior * likelihood_single
posterior &amp;lt;- posterior / sum(posterior)
posterior[1] == 0.36
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## IN ONE STEP
p_twins_A &amp;lt;- 0.1
p_twins_B &amp;lt;- 0.2
# likelihood of two events (p(twins_step1 &amp;amp; single_step2|species=X))
likelihood_twins_single &amp;lt;- c(
  p_twins_A * (1 - p_twins_A),
  p_twins_B * (1 - p_twins_B)
)
prior &amp;lt;- c(1, 1)
posterior &amp;lt;- prior * likelihood_twins_single
posterior &amp;lt;- posterior / sum(posterior)
posterior[1] == 0.36
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-h4&#34;&gt;Practice H4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; A common boast of Bayesian statisticians is that Bayesian inferences makes it easy to use all of the data, even if the data are of different types. So suppose now that a veterinarian comes along who has a new genetic test that she claims can identify the species of our mother panda. But the test, like all tests, is imperfect. This is the information you have about the test:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The probability it correctly identifies a species A panda is 0.8.&lt;/li&gt;
&lt;li&gt;The probability it correctly identifies a species B panda is 0.65.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The vet administers the test to your panda and tells you that the test is positive for species A. First ignore your previous information from the births and compute the posterior probability that your panda is species A. Then redo your calculation, now using the birth data as well.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;By Hand:&lt;/em&gt;&lt;br&gt;
Again, the question hands us some information:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$Pr(+A|A)=0.8$&lt;/li&gt;
&lt;li&gt;$Pr(+A|B)=1 − 0.65 = 0.35$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Importantly here, the test was positive for species A. I had previously overlooked this. Thanks to 
&lt;a href=&#34;https://fariasaramis.github.io/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Aramis Farias&lt;/a&gt; for pointing this out to me!&lt;/p&gt;
&lt;p&gt;Currently, we have to assume that both species are equally likely here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$Pr(A)=0.5$&lt;/li&gt;
&lt;li&gt;$Pr(B)=0.5$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now, to calculate $Pr(A|+)$, we require $Pr(+)$:&lt;/p&gt;
&lt;p&gt;$Pr(+)=Pr(+|A)Pr(A)+Pr(+|B)Pr(B)$&lt;br&gt;
$=0.8(0.5)+0.35(0.5)=0.575$&lt;/p&gt;
&lt;p&gt;Finally, we calculate $Pr(A|+)$:
$Pr(A|+)=\frac{Pr(+A|A)Pr(A)}{Pr(+)}=\frac{0.8(0.5)}{0.575}\sim 0.6987$&lt;/p&gt;
&lt;p&gt;Taking into account our previous knowledge on births, we have to set:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$Pr(A)=0.36$&lt;/li&gt;
&lt;li&gt;$Pr(B)=1-Pr(A)=0.64$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We plug these values into the formulae above and obtain:&lt;/p&gt;
&lt;p&gt;$Pr(+)=Pr(+|A)Pr(A)+Pr(+|B)Pr(B)$&lt;br&gt;
$=0.8(0.36)+0.35(0.64)=0.512$&lt;/p&gt;
&lt;p&gt;Finally, we calculate $Pr(A|+)$:
$Pr(A|+)=\frac{Pr(+A|A)Pr(A)}{Pr(+)}=\frac{0.8(0.36)}{0.512}=0.5625$&lt;/p&gt;
&lt;p&gt;Given our test alone, we are probably overestimating assignment to species A, here.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In&lt;/em&gt; &lt;code&gt;R&lt;/code&gt;&lt;em&gt;:&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# WITHOUT BIRTH-INFORMATION
likelihood_test &amp;lt;- c(0.8, 0.35)
prior &amp;lt;- c(1, 1)
posterior_vet &amp;lt;- prior * likelihood_test
posterior_vet &amp;lt;- posterior_vet / sum(posterior_vet)
posterior_vet[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6956522
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# WITH BIRT-INFORMATION
p_twins_A &amp;lt;- 0.1
p_twins_B &amp;lt;- 0.2
# likelihood of two events (p(twins_step1 &amp;amp; single_step2|species=X))
likelihood_twins_single &amp;lt;- c(
  p_twins_A * (1 - p_twins_A),
  p_twins_B * (1 - p_twins_B)
)
prior &amp;lt;- posterior_vet
posterior &amp;lt;- prior * likelihood_twins_single
posterior &amp;lt;- posterior / sum(posterior)
posterior[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5625
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.2.3 (2023-03-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Big Sur ... 10.16
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] cowplot_1.1.1 ggplot2_3.4.1
## 
## loaded via a namespace (and not attached):
##  [1] highr_0.10        bslib_0.4.2       compiler_4.2.3    pillar_1.8.1      jquerylib_0.1.4   R.methodsS3_1.8.2 R.utils_2.12.2    tools_4.2.3       digest_0.6.31     jsonlite_1.8.4   
## [11] evaluate_0.20     lifecycle_1.0.3   tibble_3.2.1      gtable_0.3.1      R.cache_0.16.0    pkgconfig_2.0.3   rlang_1.1.0       cli_3.6.0         rstudioapi_0.14   yaml_2.3.7       
## [21] blogdown_1.16     xfun_0.37         fastmap_1.1.1     withr_2.5.0       dplyr_1.1.0       styler_1.9.1      knitr_1.42        generics_0.1.3    vctrs_0.6.1       sass_0.4.5       
## [31] tidyselect_1.2.0  grid_4.2.3        glue_1.6.2        R6_2.5.1          fansi_1.0.4       rmarkdown_2.20    bookdown_0.33     farver_2.1.1      purrr_1.0.1       magrittr_2.0.3   
## [41] scales_1.2.1      htmltools_0.5.4   colorspace_2.1-0  labeling_0.4.2    utf8_1.2.3        munsell_0.5.0     cachem_1.0.7      R.oo_1.25.0
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Hybrid Bayesian Networks</title>
      <link>https://www.erikkusch.com/courses/bayes-nets/part-3/</link>
      <pubDate>Tue, 08 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/bayes-nets/part-3/</guid>
      <description>&lt;style&gt;

blockquote{
color:#633a00;
}

&lt;/style&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/content/courses/bayes-nets/5-Hybrid-Bayesian-Networks_18-10-22.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hybrid Bayesian Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/content/courses/bayes-nets/Hybrid-BNs.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hybrid Bayesian Networks&lt;/a&gt; by 
&lt;a href=&#34;https://github.com/arielsaffer&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ariel Saffer&lt;/a&gt; (one of our study group members)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;exercises&#34;&gt;Exercises&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to the exercises at the end of Part 3 in 
&lt;a href=&#34;https://www.bnlearn.com/book-crc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bayesian Networks with Examples in R&lt;/a&gt; by M. Scutari and J.-B. Denis. Much of my inspiration for these solutions, where necessary, by consulting the solutions provided by the authors themselves as in the appendix.&lt;/p&gt;
&lt;h3 id=&#34;r-environment&#34;&gt;&lt;code&gt;R&lt;/code&gt; Environment&lt;/h3&gt;
&lt;p&gt;For today&amp;rsquo;s exercise, I load the following packages:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(bnlearn)
library(rjags)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;scutari-31&#34;&gt;Scutari 3.1&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Explain why it is logical to get a three-step function for the discretised approach in Figure 3.2.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Figure 3.2 shows this discretisation:&lt;/p&gt;
&lt;img src=&#34;https://www.erikkusch.com/courses/networksscutari/Screenshot 2021-06-08 153234.png&#34; width=&#34;900&#34;/&gt;
&lt;p&gt;the reason we obtain a three-step function here is down to the intervals that were chosen to bin the continuous diametre data into discrete categories:&lt;/p&gt;
&lt;p&gt;$$&amp;lt;6.16$$
$$[6.16; 6.19]$$
$$&amp;gt;6.19$$&lt;/p&gt;
&lt;p&gt;Owing to these intervals, we fit all of our continuous data into three categories which mirror the three-step function portrayed in the above figure.&lt;/p&gt;
&lt;h3 id=&#34;scutari-32&#34;&gt;Scutari 3.2&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Starting from the BUGS model in Section 3.1.1, write another BUGSmodel for the discretised model proposed in Section 3.1.2. The functions required for this task are described in the JAGS manual.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The model for the hybrid case (which we are to adapt to the discretised approach) reads as such:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;model{ 
  csup ~ dcat(sp); 
  cdiam ~ dnorm(mu[csup], 1/sigma^2);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To adapt it for discretised data, I simply change the outcome distribution for &lt;code&gt;cdiam&lt;/code&gt; to also be categorical:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;model{ 
  csup ~ dcat(sp); 
  cdiam ~ dcat(Diams[, csup]);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where &lt;code&gt;Diams&lt;/code&gt; is a matrix that contains probabilities for the different diameters (rows) for suppliers (columns).&lt;/p&gt;
&lt;h3 id=&#34;scutari-33&#34;&gt;Scutari 3.3&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Let d = 6.0, 6.1, 6.2, 6.4.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;Using the BUGS model proposed in Section 3.1.1, write the &lt;code&gt;R&lt;/code&gt; script to estimate $P(S = s1 | D = d)$ for the continuous approach demonstrated in the same section.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;I start by simply repeating the code in section 3.1.1:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sp &amp;lt;- c(0.5, 0.5)
mu &amp;lt;- c(6.1, 6.25)
sigma &amp;lt;- 0.05
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From here, I take inspiration from the book in the later section on the crop model and write a sampling loop for which to retain samples. First, I create some objects to store data and outputs as well as do some housekeeping:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## house-keeping
set.seed(42) # there are random processes here

## object creation
diameters_vec &amp;lt;- c(6.0, 6.1, 6.2, 6.4)
prob_vec &amp;lt;- rep(NA, length(diameters_vec))
names(prob_vec) &amp;lt;- diameters_vec
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, I greatly dislike the reliance on model files that the book insists on and so I register my JAGS model code as an active text connection in &lt;code&gt;R&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;jags_mod &amp;lt;- textConnection(&amp;quot;model{
                           csup ~ dcat(sp);
                           cdiam ~ dnorm(mu[csup], 1/sigma^2);
                           }&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, I am ready to estimate $P(S = s1 | D = d)$:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;for (samp_iter in seq(length(diameters_vec))) {
  # create data list
  jags.data &amp;lt;- list(sp = sp, mu = mu, sigma = sigma, cdiam = diameters_vec[samp_iter])
  # compile model
  model &amp;lt;- jags.model(file = jags_mod, data = jags.data, quiet = TRUE)
  update(model, n.iter = 1e4)
  # sample model and retrieve vector of supplier identity (containing values 1 and 2)
  simu &amp;lt;- coda.samples(model = model, variable.names = &amp;quot;csup&amp;quot;, n.iter = 2e4, thin = 20)[[1]]
  # compute probability of supplier 1
  prob_vec[samp_iter] &amp;lt;- sum(simu == 1) / length(simu)
}
prob_vec
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     6   6.1   6.2   6.4 
## 1.000 0.982 0.197 0.000
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Using the BUGS model obtained in Exercise 3.2, write the &lt;code&gt;R&lt;/code&gt; script to estimate $P(S = s1 | D = d)$ for the discretised approach suggested in Section 3.1.2.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;Again, I start by typing out important parameters from the book:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sp &amp;lt;- c(0.5, 0.5)
Diams &amp;lt;- matrix(c(0.88493, 0.07914, 0.03593, 0.03593, 0.07914, 0.88493), 3)
Diams
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         [,1]    [,2]
## [1,] 0.88493 0.03593
## [2,] 0.07914 0.07914
## [3,] 0.03593 0.88493
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once more, I perform housekeeping and object creation prior to sampling. This time, however, I create a probability matrix to store the probability of each rod diametre in each diametre class belonging to supplier 1:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## house-keeping
set.seed(42) # there are random processes here

## object creation
diameters_vec &amp;lt;- c(6.0, 6.1, 6.2, 6.4)
cdiam_vec &amp;lt;- 1:3
dimnames &amp;lt;- list(
  paste(&amp;quot;cdiam&amp;quot;, cdiam_vec, sep = &amp;quot;_&amp;quot;),
  diameters_vec
)
prob_mat &amp;lt;- matrix(rep(NA, 12), ncol = 4)
dimnames(prob_mat) &amp;lt;- dimnames
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And there&amp;rsquo;s our model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;jags_mod &amp;lt;- textConnection(&amp;quot;model{
                            csup ~ dcat(sp);
                            cdiam ~ dcat(Diams[, csup]);
                            }&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, I am ready to estimate $P(S = s1 | D = d)$:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;for (samp_iter in seq(length(cdiam_vec))) {
  # create data list
  jags.data &amp;lt;- list(sp = sp, Diams = Diams, cdiam = cdiam_vec[samp_iter])
  # compile model
  model &amp;lt;- jags.model(file = jags_mod, data = jags.data, quiet = TRUE)
  update(model, n.iter = 1e4)
  # sample model and retrieve vector of supplier identity (containing values 1 and 2)
  simu &amp;lt;- coda.samples(model = model, variable.names = &amp;quot;csup&amp;quot;, n.iter = 2e4, thin = 20)[[1]]
  # compute probability of supplier 1
  prob_mat[samp_iter, ] &amp;lt;- sum(simu == 1) / length(simu)
}
prob_mat
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             6   6.1   6.2   6.4
## cdiam_1 0.966 0.966 0.966 0.966
## cdiam_2 0.490 0.490 0.490 0.490
## cdiam_3 0.035 0.035 0.035 0.035
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;And check the results with Figure 3.2.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Looking at figure 3.2, I&amp;rsquo;d argue the above probabilities align with the figure:&lt;/p&gt;
&lt;img src=&#34;https://www.erikkusch.com/courses/bayes-nets/Scutari3.2.png&#34; width=&#34;900&#34;/&gt;
&lt;h3 id=&#34;scutari-34&#34;&gt;Scutari 3.4&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;In Section 3.1.1, the probability that the supplier is &lt;code&gt;s1&lt;/code&gt; knowing that the diameter is 6.2 was estimated to be 0.1824 which is not identical to the value obtained with JAGS.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;Explain why the calculation with the &lt;code&gt;R&lt;/code&gt; function &lt;code&gt;dnorm&lt;/code&gt; is right and why the value 0.1824 is correct. Can you explain why the JAGS result is not exact? Propose a way to improve it.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;Since either function relies on random processes, differences in seeds may explain the difference in inference. To improve the accuracy of the JAGS result, I would suggest increasing the sample size that led to its creation.&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Would this value be different if we modify the marginal distribution for the two suppliers?&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;Yes. Marginal distributions are essential to the Bayes&#39; Theorem and a change thereof would necessitate a change in inference.&lt;/p&gt;
&lt;h3 id=&#34;scutari-35&#34;&gt;Scutari 3.5&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Revisiting the discretisation in Section 3.1.2, compute the conditional probability tables for  $ D | S $ and  $ S | D $  when the interval boundaries are set to  $ (6.10, 6.18) $ instead of  $ (6.16, 6.19)$ . Compared to the results presented in Section 3.1.2, what is your conclusion?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let&amp;rsquo;s start by repeating book code and updating the intervals to obtain $D | S$:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mu &amp;lt;- c(6.1, 6.25)
sigma &amp;lt;- 0.05
limits &amp;lt;- c(6.10, 6.18)
dsd &amp;lt;- matrix(
  c(
    diff(c(0, pnorm(limits, mu[1], sigma), 1)),
    diff(c(0, pnorm(limits, mu[2], sigma), 1))
  ),
  3, 2
)
dimnames(dsd) &amp;lt;- list(D = c(&amp;quot;thin&amp;quot;, &amp;quot;average&amp;quot;, &amp;quot;thick&amp;quot;), S = c(&amp;quot;s1&amp;quot;, &amp;quot;s2&amp;quot;))
dsd
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          S
## D                 s1          s2
##   thin    0.50000000 0.001349898
##   average 0.44520071 0.079406761
##   thick   0.05479929 0.919243341
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To obtain $ S | D $, we apply Bayes&#39; Theorem:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;jointd &amp;lt;- dsd / 2 # dive by 2 to get joint distribution over suppliers of which we have 2
mardd &amp;lt;- rowSums(jointd) # marginal distribution of diametre class irrespective of supplier
dds &amp;lt;- t(jointd / mardd) # find conditional probabilites
dds
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     D
## S           thin   average      thick
##   s1 0.997307473 0.8486359 0.05625964
##   s2 0.002692527 0.1513641 0.94374036
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One of our new limits in in fact the mean diametre supplied by supplier 1. That is clearly not a helpful limit as it simply shifts probability to supplier 1.&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.2.1 (2022-06-23 ucrt)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19044)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_Germany.utf8  LC_CTYPE=English_Germany.utf8    LC_MONETARY=English_Germany.utf8 LC_NUMERIC=C                     LC_TIME=English_Germany.utf8    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] rjags_4-13    coda_0.19-4   bnlearn_4.8.1
## 
## loaded via a namespace (and not attached):
##  [1] bslib_0.4.0       compiler_4.2.1    pillar_1.8.1      jquerylib_0.1.4   R.methodsS3_1.8.2 R.utils_2.12.0    tools_4.2.1       digest_0.6.29     jsonlite_1.8.0    evaluate_0.16    
## [11] lifecycle_1.0.2   R.cache_0.16.0    lattice_0.20-45   rlang_1.0.5       cli_3.3.0         rstudioapi_0.14   yaml_2.3.5        parallel_4.2.1    blogdown_1.13     xfun_0.33        
## [21] fastmap_1.1.0     styler_1.8.0      stringr_1.4.1     knitr_1.40        vctrs_0.4.1       sass_0.4.2        grid_4.2.1        glue_1.6.2        R6_2.5.1          fansi_1.0.3      
## [31] rmarkdown_2.16    bookdown_0.29     purrr_0.3.4       magrittr_2.0.3    htmltools_0.5.3   utf8_1.2.2        stringi_1.7.8     cachem_1.0.6      R.oo_1.25.0
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 03</title>
      <link>https://www.erikkusch.com/courses/rethinking/chapter-03/</link>
      <pubDate>Thu, 07 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/rethinking/chapter-03/</guid>
      <description>&lt;h1 id=&#34;sampling-the-imaginary&#34;&gt;Sampling the Imaginary&lt;/h1&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/rethinking/2_18-12-2020_SUMMARY_-Basics-of-Bayesian-Inference-and-Counting.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides Chapter 3&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to the exercises at the end of chapter 3 in 
&lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Satistical Rethinking 2&lt;/a&gt; by Richard McElreath. I have created these notes as a part of my ongoing involvement in the 
&lt;a href=&#34;https://www.erikkusch.com/project/aubayes/&#34;&gt;AU Bayes Study Group&lt;/a&gt;. Much of my inspiration for these solutions, where necessary, has been obtained from 
&lt;a href=&#34;https://jmgirard.com/statistical-rethinking-ch3/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jeffrey Girard&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Loading &lt;code&gt;rethinking&lt;/code&gt; package for visualisations:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rm(list = ls())
library(rethinking)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;easy-exercises&#34;&gt;Easy Exercises&lt;/h2&gt;
&lt;p&gt;These problems use the samples from the posterior distribution for the globe tossing example. This code will give you a specific set of samples, so that you can check your answers exactly. Use the values in &lt;code&gt;samples&lt;/code&gt; to answer the questions that follow.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;p_grid &amp;lt;- seq(from = 0, to = 1, length.out = 1000)
prior &amp;lt;- rep(1, 1000)
likelihood &amp;lt;- dbinom(6, size = 9, prob = p_grid)
posterior &amp;lt;- likelihood * prior
posterior &amp;lt;- posterior / sum(posterior)
set.seed(100)
samples &amp;lt;- sample(p_grid, prob = posterior, size = 1e4, replace = TRUE)
hist(samples)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-03_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;practice-e1&#34;&gt;Practice E1&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; How much posterior probability lies below $p=0.2$?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# how the book did it
sum(samples &amp;lt; .2) / length(samples)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4e-04
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# easier way
mean(samples &amp;lt; 0.2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4e-04
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-e2&#34;&gt;Practice E2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; How much posterior probability lies above $p=0.8$?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mean(samples &amp;gt; 0.8)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1116
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-e3&#34;&gt;Practice E3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; How much posterior probability lies between $p=0.2$ and $p=0.8$?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mean(samples &amp;gt; 0.2 &amp;amp; samples &amp;lt; 0.8)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.888
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-e4&#34;&gt;Practice E4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; 20% of the posterior probability lies below which value of $p$?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;quantile(samples, 0.2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       20% 
## 0.5185185
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-e5&#34;&gt;Practice E5&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; 20% of the posterior probability lies above which value of $p$?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;quantile(samples, 0.8)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       80% 
## 0.7557558
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-e6&#34;&gt;Practice E6&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Which values of $p$ contain the narrowest interval equal to 66% of the posterior probability?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;HPDI(samples, prob = 0.66)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     |0.66     0.66| 
## 0.5085085 0.7737738
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-e7&#34;&gt;Practice E7&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Which values of p contain 66% of the posterior probability, assuming equal posterior probability both below and above the interval?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;PI(samples, prob = 0.66)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       17%       83% 
## 0.5025025 0.7697698
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;medium-exercises&#34;&gt;Medium Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-m1&#34;&gt;Practice M1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Suppose the globe tossing data had turned out to be 8 water in 15 tosses. Construct the posterior distribution, using grid approximation. Use the same flat prior as before.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;p_grid &amp;lt;- seq(from = 0, to = 1, length.out = 1000)
prior &amp;lt;- rep(1, 1000)
likelihood &amp;lt;- dbinom(8, size = 15, prob = p_grid)
posterior &amp;lt;- likelihood * prior
posterior &amp;lt;- posterior / sum(posterior)
plot(p_grid, posterior, type = &amp;quot;l&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-03_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;p_grid[which.max(posterior)]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5335335
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-m2&#34;&gt;Practice M2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Draw 10,000 samples from the grid approximation from above. Then use the samples to calculate the 90% HPDI for $p$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(42)
samples &amp;lt;- sample(p_grid, prob = posterior, replace = TRUE, size = 1e+4)
hist(samples)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-03_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;HPDI(samples, prob = .9)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      |0.9      0.9| 
## 0.3393393 0.7267267
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mean(samples)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5295147
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;median(samples)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5305305
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-m3&#34;&gt;Practice M3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Construct a posterior predictive check for this model and data. This means simulate the distribution of samples, averaging over the posterior uncertainty in $p$. What is the probability of observing 8 water in 15 tosses?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;n &amp;lt;- 15
set.seed(42)
dumdata &amp;lt;- rbinom(10000, size = n, prob = samples)
simplehist(dumdata)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-03_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mean(dumdata == 8)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1419
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-m4&#34;&gt;Practice M4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Using the posterior distribution constructed from the new (8/15) data, now calculate the probability of observing 6 water in 9 tosses.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;likelihood_6of9 &amp;lt;- dbinom(6, size = 9, prob = p_grid)
prior_6of9 &amp;lt;- posterior
(p_6of9 &amp;lt;- sum(likelihood_6of9 * prior_6of9))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1763898
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Alternatively, we can generate the data using the same seed as above:
set.seed(100)
dumdata_6of9 &amp;lt;- rbinom(10000, size = 9, prob = samples)
simplehist(dumdata_6of9)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-03_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mean(dumdata_6of9 == 6)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1765
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-m5&#34;&gt;Practice M5&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Start over at 3M1, but now use a prior that is zero below $p=0.5$ and a constant above $p=0.5$. This corresponds to prior information that a majority of the Earth’s surface is water. Repeat each problem above and compare the inferences. What difference does the better prior make? If it helps, compare inferences (using both priors) to the true value $p=0.7$.&lt;/p&gt;
&lt;h4 id=&#34;part-a&#34;&gt;Part A&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Construct the posterior distribution, using grid approximation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;p_grid &amp;lt;- seq(from = 0, to = 1, length.out = 1000)
prior &amp;lt;- ifelse(p_grid &amp;lt; 0.5, 0, 0.5)
likelihood &amp;lt;- dbinom(8, size = 15, prob = p_grid)
posterior &amp;lt;- likelihood * prior
posterior &amp;lt;- posterior / sum(posterior)
plot(p_grid, posterior, type = &amp;quot;l&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-03_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;p_grid[which.max(posterior)]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5335335
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-b&#34;&gt;Part B&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Draw 10,000 samples from the grid approximation from above. Then use the samples to calculate the 90% HPDI for $p$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(42)
samples &amp;lt;- sample(p_grid, prob = posterior, replace = TRUE, size = 1e+4)
hist(samples)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-03_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;HPDI(samples, prob = .9)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      |0.9      0.9| 
## 0.5005005 0.7117117
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mean(samples)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6067921
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;median(samples)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5945946
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-c&#34;&gt;Part C&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; What is the probability of observing 8 water in 15 tosses?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;n &amp;lt;- 15
set.seed(42)
dumdata &amp;lt;- rbinom(10000, size = n, prob = samples)
simplehist(dumdata)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-03_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mean(dumdata == 8)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1516
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(dumdata) / 1e+4
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## dumdata
##      1      2      3      4      5      6      7      8      9     10     11     12     13     14     15 
## 0.0002 0.0004 0.0038 0.0125 0.0329 0.0671 0.1188 0.1516 0.1734 0.1712 0.1276 0.0823 0.0398 0.0157 0.0027
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-d&#34;&gt;Part D&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Using the posterior distribution constructed from the new (8/15) data, now calculate the probability of observing 6 water in 9 tosses.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;likelihood_6of9 &amp;lt;- dbinom(6, size = 9, prob = p_grid)
prior_6of9 &amp;lt;- posterior
(p_6of9 &amp;lt;- sum(likelihood_6of9 * prior_6of9))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2323071
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Alternatively, we can generate the data using the same seed as above:
set.seed(100)
dumdata_6of9 &amp;lt;- rbinom(10000, size = 9, prob = samples)
simplehist(dumdata_6of9)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-03_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mean(dumdata_6of9 == 6)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2321
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-m6&#34;&gt;Practice M6&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Suppose you want to estimate the Earth’s proportion of water very precisely. Specifically, you want the 99% percentile interval of the posterior distribution of $p$ to be only 0.05 wide. This means the distance between the upper and lower bound of the interval should be 0.05. How many times will you have to toss the globe to do this?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Solution taken from 
&lt;a href=&#34;https://github.com/rmcelreath/statrethinking_winter2019/blob/master/homework/week01_solutions.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Richard McElreath&lt;/a&gt; and altered by myself.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;f &amp;lt;- function(N) {
  p_true &amp;lt;- 0.7
  W &amp;lt;- rbinom(1, size = N, prob = p_true)
  prob_grid &amp;lt;- seq(0, 1, length.out = 1000)
  prior &amp;lt;- rep(1, 1000)
  prob_data &amp;lt;- dbinom(W, size = N, prob = prob_grid)
  posterior &amp;lt;- prob_data * prior
  posterior &amp;lt;- posterior / sum(posterior)
  samples &amp;lt;- sample(p_grid, prob = posterior, size = 1e4, replace = TRUE)
  PI99 &amp;lt;- PI(samples, .99)
  as.numeric(PI99[2] - PI99[1])
}
Nlist &amp;lt;- c(20, 50, 100, 200, 500, 1000, 2000)
Nlist &amp;lt;- rep(Nlist, each = 100)
width &amp;lt;- sapply(Nlist, f)
plot(Nlist, width)
abline(h = 0.05, col = &amp;quot;red&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-03_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;hard-exercises&#34;&gt;Hard Exercises&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(homeworkch3)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-h1&#34;&gt;Practice H1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Using grid approximation, compute the posterior distribution for the probability of a birth being a boy. Assume a uniform prior probability. Which parameter value maximizes the posterior probability?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;n_boys &amp;lt;- sum(c(birth1, birth2))
n_ttl &amp;lt;- length(birth1) + length(birth2)
n_pgrid &amp;lt;- 1000
p_grid &amp;lt;- seq(0, 1, length.out = n_pgrid)
prior &amp;lt;- rep(1, n_pgrid)
likelihood &amp;lt;- dbinom(n_boys, size = n_ttl, prob = p_grid)
posterior &amp;lt;- likelihood * prior
posterior &amp;lt;- posterior / sum(posterior)
plot(p_grid, posterior, type = &amp;quot;l&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-03_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;p_grid[which.max(posterior)]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5545546
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-h2&#34;&gt;Practice H2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Using the $sample()$ function, draw 10000 random parameter values from the posterior distribution you calculated above. Use these samples to estimate the 50%, 89% and 97% highest posterior density intervals.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;n_ptrials &amp;lt;- 1e4
p_samples &amp;lt;- sample(p_grid, size = n_ptrials, prob = posterior, replace = TRUE)
(hpi_50 &amp;lt;- HPDI(p_samples, .5))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      |0.5      0.5| 
## 0.5265265 0.5735736
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(hpi_89 &amp;lt;- HPDI(p_samples, .89))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     |0.89     0.89| 
## 0.4964965 0.6076076
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(hpi_97 &amp;lt;- HPDI(p_samples, .97))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     |0.97     0.97| 
## 0.4784785 0.6276276
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;for (w in c(.5, .89, .97)) {
  hpi &amp;lt;- HPDI(p_samples, w)
  print(sprintf(&amp;quot;HPDI %d%% [%f, %f]&amp;quot;, w * 100, hpi[1], hpi[2]))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;HPDI 50% [0.526527, 0.573574]&amp;quot;
## [1] &amp;quot;HPDI 89% [0.496496, 0.607608]&amp;quot;
## [1] &amp;quot;HPDI 97% [0.478478, 0.627628]&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mean(p_samples)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5545528
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;median(p_samples)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5545546
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-h3&#34;&gt;Practice H3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Use &lt;code&gt;rbinom()&lt;/code&gt; to simulate 10000 replicates of 200 births. You should end up with 10000 numbers, each one a count of boys out of 200 births. Compare the distribution of predicted numbers of boys to the actual count in the data (111 boys out of 200 births). There are many good ways to visualize the simulations, but the &lt;code&gt;dens()&lt;/code&gt; command (part of the &lt;code&gt;rethinking&lt;/code&gt; package) is probably the easiest way in this case. Does it look like the model fits the data well? That is, does the distribution of predictions include the actual observation as a central, likely outcome?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;n_btrials &amp;lt;- 1e4 # birth observations
set.seed(42)
b_sample &amp;lt;- rbinom(n_btrials, size = n_ttl, prob = p_samples)
simplehist(b_sample)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-03_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mean(b_sample)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 110.9792
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;median(b_sample)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 111
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dens(b_sample)
abline(v = n_boys, col = &amp;quot;red&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-03_files/figure-html/unnamed-chunk-22-2.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;practice-h4&#34;&gt;Practice H4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Now compare 10000 counts of boys from 100 simulated first-borns only to the number of boys in the first births, &lt;code&gt;birth1&lt;/code&gt;. How does the model look in this light?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;n_boys_b1 &amp;lt;- sum(birth1)
n_ttl_b1 &amp;lt;- length(birth1)
n_btrials &amp;lt;- 1e4 # birth observations
likelihood &amp;lt;- dbinom(sum(birth1), size = length(birth1), prob = p_grid)
posterior &amp;lt;- likelihood * prior
posterior &amp;lt;- posterior / sum(posterior)
samples &amp;lt;- sample(p_grid, prob = posterior, size = 1e4, replace = TRUE)
set.seed(42)
b_sample &amp;lt;- rbinom(n_btrials, size = n_ttl_b1, prob = samples)
simplehist(b_sample)
abline(v = n_boys_b1, col = &amp;quot;red&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-03_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mean(b_sample)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 51.0102
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;median(b_sample)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 51
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dens(b_sample)
abline(v = n_boys_b1, col = &amp;quot;red&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-03_files/figure-html/unnamed-chunk-23-2.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;practice-h5&#34;&gt;Practice H5&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; The model assumes that sex of first and second births are independent. To check this assumption, focus now on second births that followed female first-borns. Compare 10,000 simulated counts of boys to only those second births that followed girls. To do this correctly, you need to count the number of first-borns who were girls and simulate that many births, 10,000 times. Compare the counts of boys in your simulations to the actual observed count of boys following girls. How does the model look in this light? Any guesses what is going on in these data?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;n_ttl_g1 &amp;lt;- sum(birth1 == 0)
n_ttl_g1b2 &amp;lt;- sum(birth2[birth1 == 0])
print(sprintf(&amp;quot;there were %d boys born after first girl. There were ttl %d cases&amp;quot;, n_ttl_g1b2, n_ttl_g1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;there were 39 boys born after first girl. There were ttl 49 cases&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;n_btrials &amp;lt;- 1e4 # birth observations
b_sample &amp;lt;- rbinom(n_btrials, size = n_ttl_g1, prob = p_samples)
mean(b_sample)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 27.1431
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;median(b_sample)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 27
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;simplehist(b_sample)
abline(v = n_ttl_g1b2, col = &amp;quot;red&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-03_files/figure-html/unnamed-chunk-24-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The model underestimates number of boys for the second child after the first girl. Gender of the second child is not independent from the first one.&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.5 (2021-03-31)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19043)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.1252  LC_CTYPE=English_United Kingdom.1252    LC_MONETARY=English_United Kingdom.1252 LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.1252    
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] rethinking_2.13      rstan_2.21.2         ggplot2_3.3.3        StanHeaders_2.21.0-7
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.7         mvtnorm_1.1-1      lattice_0.20-41    prettyunits_1.1.1  ps_1.6.0           assertthat_0.2.1   digest_0.6.27      utf8_1.2.1         V8_3.4.1           R6_2.5.0          
## [11] backports_1.2.1    stats4_4.0.5       evaluate_0.14      coda_0.19-4        highr_0.9          blogdown_1.3       pillar_1.6.0       rlang_0.4.11       curl_4.3.2         callr_3.7.0       
## [21] jquerylib_0.1.4    R.utils_2.10.1     R.oo_1.24.0        rmarkdown_2.7      styler_1.4.1       stringr_1.4.0      loo_2.4.1          munsell_0.5.0      compiler_4.0.5     xfun_0.22         
## [31] pkgconfig_2.0.3    pkgbuild_1.2.0     shape_1.4.5        htmltools_0.5.1.1  tidyselect_1.1.0   tibble_3.1.1       gridExtra_2.3      bookdown_0.22      codetools_0.2-18   matrixStats_0.61.0
## [41] fansi_0.4.2        crayon_1.4.1       dplyr_1.0.5        withr_2.4.2        MASS_7.3-53.1      R.methodsS3_1.8.1  grid_4.0.5         jsonlite_1.7.2     gtable_0.3.0       lifecycle_1.0.0   
## [51] DBI_1.1.1          magrittr_2.0.1     scales_1.1.1       RcppParallel_5.1.2 cli_3.0.0          stringi_1.5.3      bslib_0.2.4        ellipsis_0.3.2     generics_0.1.0     vctrs_0.3.7       
## [61] rematch2_2.1.2     tools_4.0.5        R.cache_0.14.0     glue_1.4.2         purrr_0.3.4        processx_3.5.1     yaml_2.2.1         inline_0.3.17      colorspace_2.0-0   knitr_1.33        
## [71] sass_0.3.1
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Additional Static Exercises</title>
      <link>https://www.erikkusch.com/courses/bayes-nets/static/</link>
      <pubDate>Tue, 25 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/bayes-nets/static/</guid>
      <description>&lt;style&gt;

blockquote{
color:#633a00;
}

&lt;/style&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/content/courses/bayes-nets/Static-Bayesian-Networks.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Static Bayesian Networks&lt;/a&gt; by 
&lt;a href=&#34;https://www.linkedin.com/in/felipe-sanchez-22b335135/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Felipe Sanchez&lt;/a&gt; (one of our study group members)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For detailed summary slides, please consult the separate sections on 
&lt;a href=&#34;https://www.erikkusch.com/courses/bayes-nets/part-1/&#34;&gt;Multinomial&lt;/a&gt;, 
&lt;a href=&#34;https://www.erikkusch.com/courses/bayes-nets/part-2/&#34;&gt;Gaussian&lt;/a&gt;, and 
&lt;a href=&#34;https://www.erikkusch.com/courses/bayes-nets/part-3/&#34;&gt;Hybrid&lt;/a&gt; Bayesian Networks.&lt;/p&gt;
&lt;h2 id=&#34;exercises&#34;&gt;Exercises&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to the exercises at the end of chapter 2 in 
&lt;a href=&#34;https://link.springer.com/book/10.1007/978-1-4614-6446-4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bayesian Networks in R with Applications in Systems Biology&lt;/a&gt; by by Radhakrishnan Nagarajan, Marco Scutari &amp;amp; Sophie Lèbre. Much of my inspiration for these solutions, where necessary, by consulting the solutions provided by the authors themselves as in the appendix.&lt;/p&gt;
&lt;h3 id=&#34;r-environment&#34;&gt;&lt;code&gt;R&lt;/code&gt; Environment&lt;/h3&gt;
&lt;p&gt;For today&amp;rsquo;s exercise, I load the following packages:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(bnlearn)
library(igraph)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;nagarajan-21&#34;&gt;Nagarajan 2.1&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Consider the &lt;code&gt;asia&lt;/code&gt; synthetic data set from Lauritzen and Spiegelhalter (1988), which describes the diagnosis of a patient at a chest clinic who has just come back from a trip to Asia and is showing dyspnea.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;part-a&#34;&gt;Part A&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Load the data set from the &lt;code&gt;bnlearn&lt;/code&gt; package and investigate its characteristics using the exploratory analysis techniques covered in Chap. 1.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(asia)
str(asia)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &#39;data.frame&#39;:	5000 obs. of  8 variables:
##  $ A: Factor w/ 2 levels &amp;quot;no&amp;quot;,&amp;quot;yes&amp;quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ S: Factor w/ 2 levels &amp;quot;no&amp;quot;,&amp;quot;yes&amp;quot;: 2 2 1 1 1 2 1 2 2 2 ...
##  $ T: Factor w/ 2 levels &amp;quot;no&amp;quot;,&amp;quot;yes&amp;quot;: 1 1 2 1 1 1 1 1 1 1 ...
##  $ L: Factor w/ 2 levels &amp;quot;no&amp;quot;,&amp;quot;yes&amp;quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ B: Factor w/ 2 levels &amp;quot;no&amp;quot;,&amp;quot;yes&amp;quot;: 2 1 1 2 1 1 1 2 2 2 ...
##  $ E: Factor w/ 2 levels &amp;quot;no&amp;quot;,&amp;quot;yes&amp;quot;: 1 1 2 1 1 1 1 1 1 1 ...
##  $ X: Factor w/ 2 levels &amp;quot;no&amp;quot;,&amp;quot;yes&amp;quot;: 1 1 2 1 1 1 1 1 1 1 ...
##  $ D: Factor w/ 2 levels &amp;quot;no&amp;quot;,&amp;quot;yes&amp;quot;: 2 1 2 2 2 2 1 2 2 2 ...
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(asia)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    A          S          T          L          B          E          X          D       
##  no :4958   no :2485   no :4956   no :4670   no :2451   no :4630   no :4431   no :2650  
##  yes:  42   yes:2515   yes:  44   yes: 330   yes:2549   yes: 370   yes: 569   yes:2350
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-b&#34;&gt;Part B&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Create a &lt;code&gt;bn&lt;/code&gt; object with the network structure described in the manual page of &lt;code&gt;asia&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dag_2.1 &amp;lt;- model2network(&amp;quot;[A][S][T|A][L|S][B|S][D|B:E][E|T:L][X|E]&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-c&#34;&gt;Part C&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Derive the skeleton, the moral graph, and the CPDAG representing the equivalence class of the network. Plot them using &lt;code&gt;graphviz.plot&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# object creation
skel_2.1 &amp;lt;- skeleton(dag_2.1)
moral_2.1 &amp;lt;- moral(dag_2.1)
equclass_2.1 &amp;lt;- cpdag(dag_2.1)
# plotting
par(mfrow = c(1, 3))
graphviz.plot(skel_2.1, main = &amp;quot;Skeleton&amp;quot;)
graphviz.plot(moral_2.1, main = &amp;quot;Moral&amp;quot;)
graphviz.plot(equclass_2.1, main = &amp;quot;Equivalence Class&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-10-25-nagara-static_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h4 id=&#34;part-d&#34;&gt;Part D&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Identify the parents, the children, the neighbors, and the Markov blanket of each node.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# parents
sapply(nodes(dag_2.1), bnlearn::parents, x = dag_2.1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $A
## character(0)
## 
## $B
## [1] &amp;quot;S&amp;quot;
## 
## $D
## [1] &amp;quot;B&amp;quot; &amp;quot;E&amp;quot;
## 
## $E
## [1] &amp;quot;L&amp;quot; &amp;quot;T&amp;quot;
## 
## $L
## [1] &amp;quot;S&amp;quot;
## 
## $S
## character(0)
## 
## $T
## [1] &amp;quot;A&amp;quot;
## 
## $X
## [1] &amp;quot;E&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# children
sapply(nodes(dag_2.1), bnlearn::children, x = dag_2.1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $A
## [1] &amp;quot;T&amp;quot;
## 
## $B
## [1] &amp;quot;D&amp;quot;
## 
## $D
## character(0)
## 
## $E
## [1] &amp;quot;D&amp;quot; &amp;quot;X&amp;quot;
## 
## $L
## [1] &amp;quot;E&amp;quot;
## 
## $S
## [1] &amp;quot;B&amp;quot; &amp;quot;L&amp;quot;
## 
## $T
## [1] &amp;quot;E&amp;quot;
## 
## $X
## character(0)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# neighbors
sapply(nodes(dag_2.1), bnlearn::nbr, x = dag_2.1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $A
## [1] &amp;quot;T&amp;quot;
## 
## $B
## [1] &amp;quot;D&amp;quot; &amp;quot;S&amp;quot;
## 
## $D
## [1] &amp;quot;B&amp;quot; &amp;quot;E&amp;quot;
## 
## $E
## [1] &amp;quot;D&amp;quot; &amp;quot;L&amp;quot; &amp;quot;T&amp;quot; &amp;quot;X&amp;quot;
## 
## $L
## [1] &amp;quot;E&amp;quot; &amp;quot;S&amp;quot;
## 
## $S
## [1] &amp;quot;B&amp;quot; &amp;quot;L&amp;quot;
## 
## $T
## [1] &amp;quot;A&amp;quot; &amp;quot;E&amp;quot;
## 
## $X
## [1] &amp;quot;E&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# markov blanket
sapply(nodes(dag_2.1), bnlearn::mb, x = dag_2.1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $A
## [1] &amp;quot;T&amp;quot;
## 
## $B
## [1] &amp;quot;D&amp;quot; &amp;quot;E&amp;quot; &amp;quot;S&amp;quot;
## 
## $D
## [1] &amp;quot;B&amp;quot; &amp;quot;E&amp;quot;
## 
## $E
## [1] &amp;quot;B&amp;quot; &amp;quot;D&amp;quot; &amp;quot;L&amp;quot; &amp;quot;T&amp;quot; &amp;quot;X&amp;quot;
## 
## $L
## [1] &amp;quot;E&amp;quot; &amp;quot;S&amp;quot; &amp;quot;T&amp;quot;
## 
## $S
## [1] &amp;quot;B&amp;quot; &amp;quot;L&amp;quot;
## 
## $T
## [1] &amp;quot;A&amp;quot; &amp;quot;E&amp;quot; &amp;quot;L&amp;quot;
## 
## $X
## [1] &amp;quot;E&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;nagarajan-22&#34;&gt;Nagarajan 2.2&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Using the network structures created in Exercise 2.1 for the asia data set, produce the following plots with &lt;code&gt;graphviz.plot&lt;/code&gt;:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;part-a-1&#34;&gt;Part A&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;A plot of the CPDAG of the equivalence class in which the arcs belonging to a v-structure are highlighted (either with a different color or using a thicker line width).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;graphviz.plot(equclass_2.1,
  highlight = list(arcs = vstructs(equclass_2.1, arcs = TRUE), lwd = 2, col = &amp;quot;red&amp;quot;)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-10-25-nagara-static_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h4 id=&#34;part-b-1&#34;&gt;Part B&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Fill the nodes with different colors according to their role in the diagnostic process: causes (“visit to Asia” and “smoking”), effects (“tuberculosis,” “lung cancer,” and “bronchitis”), and the diagnosis proper (“chest X-ray,” “dyspnea,” and “either tuberculosis or lung cancer/bronchitis”).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;No clue on how to do this with &lt;code&gt;graphviz.plot&lt;/code&gt; and the solution provided in the book results in an error message. Instead, I use &lt;code&gt;igraph&lt;/code&gt; for plotting:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# create igraph object
equclass_igraph &amp;lt;- graph_from_edgelist(arcs(equclass_2.1))
# assign colours, effects = red; causes = green; diagnosis = blue
V(equclass_igraph)$color &amp;lt;- c(&amp;quot;blue&amp;quot;, &amp;quot;red&amp;quot;, &amp;quot;green&amp;quot;, &amp;quot;green&amp;quot;, &amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;, &amp;quot;red&amp;quot;, &amp;quot;green&amp;quot;)
V(equclass_igraph)$name &amp;lt;- c(&amp;quot;Visit to Asia&amp;quot;, &amp;quot;Tubercolosis&amp;quot;, &amp;quot;Bronchitis&amp;quot;, &amp;quot;Dyspnoea&amp;quot;, &amp;quot;Smoking&amp;quot;, &amp;quot;Tuberculosis vs Cancer&amp;quot;, &amp;quot;X-Ray&amp;quot;, &amp;quot;Lung Cancer&amp;quot;)
# plotting
plot(equclass_igraph,
  layout = layout.circle,
  vertex.size = 30,
  vertex.label.color = &amp;quot;black&amp;quot;
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-10-25-nagara-static_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h4 id=&#34;part-c-1&#34;&gt;Part C&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Explore different layouts by changing the &lt;code&gt;layout&lt;/code&gt; and &lt;code&gt;shape&lt;/code&gt; arguments.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;par(mfrow = c(2, 5))
layout &amp;lt;- c(&amp;quot;dot&amp;quot;, &amp;quot;neato&amp;quot;, &amp;quot;twopi&amp;quot;, &amp;quot;circo&amp;quot;, &amp;quot;fdp&amp;quot;)
shape &amp;lt;- c(&amp;quot;ellipse&amp;quot;, &amp;quot;circle&amp;quot;)
for (l in layout) {
  for (s in shape) {
    graphviz.plot(equclass_2.1, shape = s, layout = l, main = paste(l, s))
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-10-25-nagara-static_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;nagarajan-23&#34;&gt;Nagarajan 2.3&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Consider the &lt;code&gt;marks&lt;/code&gt; data set analyzed in Sect. 2.3.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(marks)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-a-2&#34;&gt;Part A&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Discretize the data using a quantile transform and different numbers of intervals (say, from 2 to 5). How does the network structure learned from the resulting data sets change as the number of intervals increases?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;intervals &amp;lt;- 2:5
par(mfrow = c(1, length(intervals)))
for (int in intervals) {
  disc_data &amp;lt;- discretize(marks, breaks = int, method = &amp;quot;quantile&amp;quot;)
  graphviz.plot(hc(disc_data), main = int)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-10-25-nagara-static_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The network structure becomes flatter. I reckon this is caused by the loss of information as the number of intervals is increased and variables are discretised with no regard for joint distributions.&lt;/p&gt;
&lt;h4 id=&#34;part-b-2&#34;&gt;Part B&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Repeat the discretization using interval discretization using up to 5 intervals, and compare the resulting networks with the ones obtained previously with quantile discretization.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;intervals &amp;lt;- 2:5
par(mfrow = c(1, length(intervals)))
for (int in intervals) {
  disc_data &amp;lt;- discretize(marks, breaks = int, method = &amp;quot;interval&amp;quot;)
  graphviz.plot(hc(disc_data), main = int)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-10-25-nagara-static_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Although the specific placement of the nodes changes between the two discretisation approaches, the general pattern of loss of arcs as number of intervals increases stays constant.&lt;/p&gt;
&lt;h4 id=&#34;part-c-2&#34;&gt;Part C&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Does Hartemink’s discretization algorithm perform better than either quantile or interval discretization? How does its behavior depend on the number of initial breaks?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;intervals &amp;lt;- 2:5
par(mfrow = c(1, length(intervals)))
for (int in intervals) {
  disc_data &amp;lt;- discretize(marks, breaks = int, method = &amp;quot;hartemink&amp;quot;, ibreaks = 50, idisc = &amp;quot;interval&amp;quot;)
  graphviz.plot(hc(disc_data), main = int)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-10-25-nagara-static_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This form of discretisation seems more robust when assessing how accurately the DAG structure is learned when number of intervals is increased.&lt;/p&gt;
&lt;h3 id=&#34;nagarajan-24&#34;&gt;Nagarajan 2.4&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;The ALARM network (Beinlich et al. 1989) is a Bayesian network designed to provide an alarm message system for patients hospitalized in intensive care units (ICU). Since ALARM is commonly used as a benchmark in literature, a synthetic data set of 5000 observations generated from this network is available from bnlearn as &lt;code&gt;alarm&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(alarm)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-a-3&#34;&gt;Part A&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Create a &lt;code&gt;bn&lt;/code&gt; object for the “true” structure of the network using the model string provided in its manual page.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;true_bn &amp;lt;- model2network(paste(&amp;quot;[HIST|LVF][CVP|LVV]&amp;quot;, &amp;quot;[PCWP|LVV][HYP][LVV|HYP:LVF][LVF]&amp;quot;,
  &amp;quot;[STKV|HYP:LVF][ERLO][HRBP|ERLO:HR]&amp;quot;, &amp;quot;[HREK|ERCA:HR][ERCA][HRSA|ERCA:HR][ANES]&amp;quot;,
  &amp;quot;[APL][TPR|APL][ECO2|ACO2:VLNG][KINK]&amp;quot;, &amp;quot;[MINV|INT:VLNG][FIO2][PVS|FIO2:VALV]&amp;quot;,
  &amp;quot;[SAO2|PVS:SHNT][PAP|PMB][PMB][SHNT|INT:PMB]&amp;quot;, &amp;quot;[INT][PRSS|INT:KINK:VTUB][DISC][MVS]&amp;quot;,
  &amp;quot;[VMCH|MVS][VTUB|DISC:VMCH]&amp;quot;, &amp;quot;[VLNG|INT:KINK:VTUB][VALV|INT:VLNG]&amp;quot;,
  &amp;quot;[ACO2|VALV][CCHL|ACO2:ANES:SAO2:TPR]&amp;quot;, &amp;quot;[HR|CCHL][CO|HR:STKV][BP|CO:TPR]&amp;quot;,
  sep = &amp;quot;&amp;quot;
))
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-b-3&#34;&gt;Part B&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Compare the networks learned with different constraint-based algorithms with the true one, both in terms of structural differences and using either BIC or BDe.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# learning
bn.gs &amp;lt;- gs(alarm)
bn.iamb &amp;lt;- iamb(alarm)
bn.inter &amp;lt;- inter.iamb(alarm)
# plotting
par(mfrow = c(2, 2))
graphviz.plot(true_bn, main = &amp;quot;True Structure&amp;quot;)
graphviz.plot(bn.gs, main = &amp;quot;Grow-Shrink&amp;quot;)
graphviz.plot(bn.iamb, main = &amp;quot;IAMB&amp;quot;)
graphviz.plot(bn.inter, main = &amp;quot;Inter-IAMB&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-10-25-nagara-static_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# comparisons
unlist(bnlearn::compare(true_bn, bn.gs))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## tp fp fn 
##  5 14 41
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;unlist(bnlearn::compare(true_bn, bn.iamb))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## tp fp fn 
## 16 18 30
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;unlist(bnlearn::compare(true_bn, bn.inter))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## tp fp fn 
## 27 11 19
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Scores
score(cextend(true_bn), alarm, type = &amp;quot;bde&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -218063
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;score(cextend(bn.gs), alarm, type = &amp;quot;bde&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -337116.1
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;score(cextend(bn.iamb), alarm, type = &amp;quot;bde&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -263670.8
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;score(cextend(bn.inter), alarm, type = &amp;quot;bde&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -259922.1
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-c-3&#34;&gt;Part C&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;The overall performance of constraint-based algorithms suggests that the asymptotic $\chi^2$ conditional independence tests may not be appropriate for analyzing &lt;code&gt;alarm&lt;/code&gt;. Are permutation or shrinkage tests better choices?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This should improve the performance drastically. However, computational time is so high that I refuse to run this code. Even the first call to &lt;code&gt;gs()&lt;/code&gt; below takes more than 12 hours to run. I don&amp;rsquo;t know what the authors of the exercise material had envisioned the learning outcome of this to be.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bn.gs2 &amp;lt;- gs(alarm, test = &amp;quot;smc-x2&amp;quot;)
bn.iamb2 &amp;lt;- iamb(alarm, test = &amp;quot;smc-x2&amp;quot;)
bn.inter2 &amp;lt;- inter.iamb(alarm, test = &amp;quot;smc-x2&amp;quot;)
unlist(compare(true_bn, bn.gs2))
unlist(compare(true_bn, bn.iamb2))
unlist(compare(true_bn, bn.inter2))
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-d-1&#34;&gt;Part D&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;How are the above learning strategies affected by changes to &lt;code&gt;alpha&lt;/code&gt;?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Shrinkage should also improves structure learning performance, but computational time should be much lower than it is with permutation tests. Much like with the previous exercise, however, the code below just takes too long for my liking to finish running.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bn.gs3 &amp;lt;- gs(alarm, test = &amp;quot;smc-x2&amp;quot;, alpha = 0.01)
bn.iamb3 &amp;lt;- iamb(alarm, test = &amp;quot;smc-x2&amp;quot;, alpha = 0.01)
bn.inter3 &amp;lt;- inter.iamb(alarm, test = &amp;quot;smc-x2&amp;quot;, alpha = 0.01)
unlist(compare(true, bn.gs3))
unlist(compare(true, bn.iamb3))
unlist(compare(true, bn.inter3))
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;nagarajan-25&#34;&gt;Nagarajan 2.5&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Consider again the &lt;code&gt;alarm&lt;/code&gt; network used in Exercise 2.4.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;part-a-4&#34;&gt;Part A&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Learn its structure with hill-climbing and tabu search, using the posterior density BDe as a score function. How does the network structure change with the imaginary sample size &lt;code&gt;iss&lt;/code&gt;?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;par(mfrow = c(2, 5))
for (iss in c(1, 5, 10, 20, 50)) {
  bn &amp;lt;- hc(alarm, score = &amp;quot;bde&amp;quot;, iss = iss)
  main &amp;lt;- paste(&amp;quot;hc(..., iss = &amp;quot;, iss, &amp;quot;)&amp;quot;, sep = &amp;quot;&amp;quot;)
  sub &amp;lt;- paste(narcs(bn), &amp;quot;arcs&amp;quot;)
  graphviz.plot(bn, main = main, sub = sub)
}
for (iss in c(1, 5, 10, 20, 50)) {
  bn &amp;lt;- tabu(alarm, score = &amp;quot;bde&amp;quot;, iss = iss)
  main &amp;lt;- paste(&amp;quot;tabu(..., iss = &amp;quot;, iss, &amp;quot;)&amp;quot;, sep = &amp;quot;&amp;quot;)
  sub &amp;lt;- paste(narcs(bn), &amp;quot;arcs&amp;quot;)
  graphviz.plot(bn, main = main, sub = sub)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-10-25-nagara-static_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The number of arcs increases with &lt;code&gt;iss&lt;/code&gt;. Large values of &lt;code&gt;iss&lt;/code&gt; over-smooth the data and thus result in networks with similar scores and therefore allow for many arcs to be included in the final networks.&lt;/p&gt;
&lt;h4 id=&#34;part-b-4&#34;&gt;Part B&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Does the length of the tabu list have a significant impact on the network structures learned with &lt;code&gt;tabu&lt;/code&gt;?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;par(mfrow = c(1, 5))
for (n in c(10, 15, 20, 50, 100)) {
  bn &amp;lt;- tabu(alarm, score = &amp;quot;bde&amp;quot;, tabu = n)
  bde &amp;lt;- score(bn, alarm, type = &amp;quot;bde&amp;quot;)
  main &amp;lt;- paste(&amp;quot;tabu(..., tabu = &amp;quot;, n, &amp;quot;)&amp;quot;, sep = &amp;quot;&amp;quot;)
  sub &amp;lt;- paste(ntests(bn), &amp;quot;steps, score&amp;quot;, bde)
  graphviz.plot(bn, main = main, sub = sub)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-10-25-nagara-static_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Increasing the tabu length severely affects the learned structure of the final network. Firstly, it does so by increasing the raw number of network structures explored by tabu. Secondly, getting stuck in local maxima becomes increasingly unlikely.&lt;/p&gt;
&lt;h4 id=&#34;part-c-4&#34;&gt;Part C&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;How does the BIC score compare with BDe at different sample sizes in terms of structure and score of the learned network?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;par(mfrow = c(2, 6))
for (n in c(100, 200, 500, 1000, 2000, 5000)) {
  bn.bde &amp;lt;- hc(alarm[1:n, ], score = &amp;quot;bde&amp;quot;)
  bn.bic &amp;lt;- hc(alarm[1:n, ], score = &amp;quot;bic&amp;quot;)
  bde &amp;lt;- score(bn.bde, alarm, type = &amp;quot;bde&amp;quot;)
  bic &amp;lt;- score(bn.bic, alarm, type = &amp;quot;bic&amp;quot;)
  main &amp;lt;- paste(&amp;quot;BDe, sample size&amp;quot;, n)
  sub &amp;lt;- paste(ntests(bn.bde), &amp;quot;steps, score&amp;quot;, bde)
  graphviz.plot(bn.bde, main = main, sub = sub)
  main &amp;lt;- paste(&amp;quot;BIC, sample size&amp;quot;, n)
  sub &amp;lt;- paste(ntests(bn.bic), &amp;quot;steps, score&amp;quot;, bic)
  graphviz.plot(bn.bic, main = main, sub = sub)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-10-25-nagara-static_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The networks become more similar as sample size increases. At small sample sizes, BIC results in sparser networks than BDe.&lt;/p&gt;
&lt;h3 id=&#34;nagarajan-26&#34;&gt;Nagarajan 2.6&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Consider the observational data set from Sachs et al. (2005) used in Sect. 2.5.1 (the original data set, not the discretized one).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;part-a-5&#34;&gt;Part A&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Evaluate the networks learned by hill-climbing with BIC and BGe using cross-validation and the log-likelihood loss function.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The sachs data file is available
&lt;a href=&#34;https://www.bnlearn.com/book-useR/code/sachs.data.txt.gz&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sachs &amp;lt;- read.table(&amp;quot;sachs.data.txt&amp;quot;, header = TRUE)
bn.bic &amp;lt;- hc(sachs, score = &amp;quot;bic-g&amp;quot;)
bn.cv(bn.bic, data = sachs)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   k-fold cross-validation for Bayesian networks
## 
##   target network structure:
##    [praf][PIP2][p44.42][PKC][pmek|praf][PIP3|PIP2][pakts473|p44.42][P38|PKC][plcg|PIP3][PKA|p44.42:pakts473][pjnk|PKC:P38] 
##   number of folds:                       10 
##   loss function:                         Log-Likelihood Loss (Gauss.) 
##   expected loss:                         65.48251
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bn.bge &amp;lt;- hc(sachs, score = &amp;quot;bge&amp;quot;)
bn.cv(bn.bge, data = sachs)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   k-fold cross-validation for Bayesian networks
## 
##   target network structure:
##    [praf][plcg][PIP2][p44.42][PKC][pmek|praf][PIP3|PIP2][pakts473|p44.42][P38|PKC][PKA|p44.42:pakts473][pjnk|PKC:P38] 
##   number of folds:                       10 
##   loss function:                         Log-Likelihood Loss (Gauss.) 
##   expected loss:                         65.3477
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The BGe network fits the data slightly better than the BIC-network.&lt;/p&gt;
&lt;h4 id=&#34;part-b-5&#34;&gt;Part B&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Use bootstrap resampling to evaluate the distribution of the number of arcs present in each of the networks learned in the previous point. Do they differ significantly?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(42)
narcs.bic &amp;lt;- bn.boot(sachs, algorithm = &amp;quot;hc&amp;quot;, algorithm.args = list(score = &amp;quot;bic-g&amp;quot;), statistic = narcs)
narcs.bge &amp;lt;- bn.boot(sachs, algorithm = &amp;quot;hc&amp;quot;, algorithm.args = list(score = &amp;quot;bge&amp;quot;), statistic = narcs)
narcs.bic &amp;lt;- unlist(narcs.bic)
narcs.bge &amp;lt;- unlist(narcs.bge)
par(mfrow = c(1, 2))
hist(narcs.bic, main = &amp;quot;BIC&amp;quot;, freq = FALSE)
curve(dnorm(x, mean = mean(narcs.bic), sd = sd(narcs.bic)), add = TRUE, col = 2)
hist(narcs.bge, main = &amp;quot;BGe&amp;quot;, freq = FALSE)
curve(dnorm(x, mean = mean(narcs.bge), sd = sd(narcs.bge)), add = TRUE, col = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-10-25-nagara-static_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Number-of-arc-distributions are markedly different between BIC and BGe networks.&lt;/p&gt;
&lt;h4 id=&#34;part-c-5&#34;&gt;Part C&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Compute the averaged network structure for &lt;code&gt;sachs&lt;/code&gt; using hill-climbing with BGe and different imaginary sample sizes. How does the value of the significance threshold change as &lt;code&gt;iss&lt;/code&gt; increases?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(42)
t &amp;lt;- c()
iss &amp;lt;- c(5, 10, 20, 50, 100)
for (i in iss) {
  s &amp;lt;- boot.strength(sachs,
    algorithm = &amp;quot;hc&amp;quot;,
    algorithm.args = list(score = &amp;quot;bge&amp;quot;, iss = i)
  )
  t &amp;lt;- c(t, attr(s, &amp;quot;threshold&amp;quot;))
}
t
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.780 0.415 0.430 0.380 0.440
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.2.1 (2022-06-23 ucrt)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19044)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_Germany.utf8  LC_CTYPE=English_Germany.utf8    LC_MONETARY=English_Germany.utf8 LC_NUMERIC=C                     LC_TIME=English_Germany.utf8    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] igraph_1.3.4  bnlearn_4.8.1
## 
## loaded via a namespace (and not attached):
##  [1] highr_0.9           bslib_0.4.0         compiler_4.2.1      jquerylib_0.1.4     R.methodsS3_1.8.2   R.utils_2.12.0      tools_4.2.1         digest_0.6.29       jsonlite_1.8.0     
## [10] evaluate_0.16       R.cache_0.16.0      pkgconfig_2.0.3     rlang_1.0.5         graph_1.74.0        cli_3.3.0           rstudioapi_0.14     Rgraphviz_2.40.0    yaml_2.3.5         
## [19] parallel_4.2.1      blogdown_1.13       xfun_0.33           fastmap_1.1.0       styler_1.8.0        stringr_1.4.1       knitr_1.40          sass_0.4.2          vctrs_0.4.1        
## [28] grid_4.2.1          stats4_4.2.1        R6_2.5.1            rmarkdown_2.16      bookdown_0.29       purrr_0.3.4         magrittr_2.0.3      htmltools_0.5.3     BiocGenerics_0.42.0
## [37] stringi_1.7.8       cachem_1.0.6        R.oo_1.25.0
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 04</title>
      <link>https://www.erikkusch.com/courses/rethinking/chapter-04/</link>
      <pubDate>Fri, 08 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/rethinking/chapter-04/</guid>
      <description>&lt;h1 id=&#34;geocentric-models&#34;&gt;Geocentric Models&lt;/h1&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/rethinking/3__08-01-2021_SUMMARY_-Linear-Regressions.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides Chapter 4&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to the exercises at the end of chapter 4 in 
&lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Satistical Rethinking 2&lt;/a&gt; by Richard McElreath. I have created these notes as a part of my ongoing involvement in the 
&lt;a href=&#34;https://www.erikkusch.com/project/aubayes/&#34;&gt;AU Bayes Study Group&lt;/a&gt;. Much of my inspiration for these solutions, where necessary, has been obtained from 
&lt;a href=&#34;https://jmgirard.com/statistical-rethinking-ch4/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jeffrey Girard&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;easy-exercises&#34;&gt;Easy Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-e1&#34;&gt;Practice E1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; In the model definition below, which line is the likelihood?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$y_i \sim Normal(\mu, \sigma)$&lt;/li&gt;
&lt;li&gt;$\mu \sim Normal(0, 10)$&lt;/li&gt;
&lt;li&gt;$\sigma \sim Uniform(0, 10)$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; 1, $y_i \sim Normal(\mu, \sigma)$ - This is the likelihood specification (see also page 84 in the book). Everything else is a specification of priors.&lt;/p&gt;
&lt;h3 id=&#34;practice-e2&#34;&gt;Practice E2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; In the model definition just above, how many parameters are in the posterior distribution?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; $y_i$ is not to be estimated, but represents the data we have at hand and want to understand through parameters. Both $\mu$ and $\sigma$ are parameters which we attempt to estimate.&lt;/p&gt;
&lt;h3 id=&#34;practice-e3&#34;&gt;Practice E3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Using the model definition above, write down the appropriate form of Bayes’ theorem that includes the proper likelihood and priors.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Following the specification in the &amp;ldquo;Overthinking&amp;rdquo; box on page 87, we can write the Bayes&#39; Theorem for the model above as:&lt;/p&gt;
&lt;p&gt;$Pr(\mu, \sigma | y) = \frac{\prod_i Normal(y_i| \mu, \sigma) Normal(\mu| 0, 10) Uniform(\sigma | 0, 10)}{\int\int\prod_i Normal(y_i| \mu, \sigma) Normal(\mu| 0, 10) Uniform(\sigma | 0, 10) d\mu d\sigma}$&lt;/p&gt;
&lt;h3 id=&#34;practice-e4&#34;&gt;Practice E4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; In the model definition below, which line is the linear model?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$y_i \sim Normal(\mu, \sigma)$&lt;/li&gt;
&lt;li&gt;$\mu_i=\alpha+\beta x_i$&lt;/li&gt;
&lt;li&gt;$\alpha \sim Normal(0,10)$&lt;/li&gt;
&lt;li&gt;$\beta \sim Normal(0,1)$&lt;/li&gt;
&lt;li&gt;$\sigma \sim Uniform(0,10)$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; The linear model is &lt;em&gt;deterministic&lt;/em&gt; in nature (i.e. the parameters determine the value of the response variable). This is identified with the mathematical notation of $=$. Therefore, line 2. $\mu_i=\alpha+\beta x_i$ is the linear model. All other specifications are &lt;em&gt;stochastic&lt;/em&gt; links (identified with $\sim$).&lt;/p&gt;
&lt;h3 id=&#34;practice-e5&#34;&gt;Practice E5&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; In the model definition just above, how many parameters are in the posterior distribution?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Three. $\alpha$, $\beta$, and $\sigma$ are the parameters we estimate. $y_i \sim Normal(\mu, \sigma)$ is the likelihood.&lt;/p&gt;
&lt;h2 id=&#34;medium-exercises&#34;&gt;Medium Exercises&lt;/h2&gt;
&lt;p&gt;This is where we get into &lt;code&gt;R&lt;/code&gt; applications.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rm(list = ls())
library(rethinking)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-m1&#34;&gt;Practice M1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; For the model definition below, simulate observed heights from the prior (not the posterior).&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$y_i \sim Normal(\mu, \sigma)$&lt;/li&gt;
&lt;li&gt;$\mu \sim Normal(0, 10)$&lt;/li&gt;
&lt;li&gt;$\sigma \sim Uniform(0, 10)$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(42)
sample_mu &amp;lt;- rnorm(1e4, 0, 10)
sample_sigma &amp;lt;- runif(1e4, 0, 10)
prior_y &amp;lt;- rnorm(1e4, sample_mu, sample_sigma)
dens(prior_y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-04_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;practice-m2&#34;&gt;Practice M2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Translate the model just above into a &lt;code&gt;quap()&lt;/code&gt; formula.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;formula &amp;lt;- alist(
  y ~ dnorm(mu, sigma),
  mu ~ dnorm(0, 10),
  sigma ~ dunif(0, 10)
)
formula
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## y ~ dnorm(mu, sigma)
## 
## [[2]]
## mu ~ dnorm(0, 10)
## 
## [[3]]
## sigma ~ dunif(0, 10)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-m3&#34;&gt;Practice M3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Translate the &lt;code&gt;quap()&lt;/code&gt; model formula below into a mathematical model definition.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;flist &amp;lt;- alist(
  y ~ dnorm(mu, sigma),
  mu &amp;lt;- a + b * x,
  a ~ dnorm(0, 50),
  b ~ dunif(0, 10),
  sigma ~ dunif(0, 50)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$y_i \sim Normal(\mu, \sigma)$&lt;/li&gt;
&lt;li&gt;$\mu_i=\alpha+\beta x_i$&lt;/li&gt;
&lt;li&gt;$\alpha \sim Normal(0,50)$&lt;/li&gt;
&lt;li&gt;$\beta \sim Uniform(0,10)$&lt;/li&gt;
&lt;li&gt;$\sigma \sim Uniform(0,50)$&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;practice-m4&#34;&gt;Practice M4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; A sample of students is measured for height each year for 3 years. After the third year, you want to fit a linear regression predicting height using year as a predictor. Write down the mathematical model definition for this regression, using any variable names and priors you choose. Be prepared to defend you choice of priors.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Firstly, I start by identifying the likelihood of heights $h$. I assume these to be normally distributed around some mean $\mu$, with a standard deviation of $\sigma$:&lt;/p&gt;
&lt;p&gt;$h_i = Normal(\mu, \sigma)$&lt;/p&gt;
&lt;p&gt;$\mu$ is the mean of heights and can be obtained as follows:&lt;/p&gt;
&lt;p&gt;$\mu_i = \alpha + \beta x_i$&lt;/p&gt;
&lt;p&gt;Setting aside the issue of independence here - each student shows up in the data multiple times, thus making the time-series of heights dependent and potentially autocorrelated - no age range has been specified for our students in question. This leaves us with little information regarding potential priors (elementary school students are much smaller than university students). Therefore, I chose a weak prior with a large range. I call this prior for the height intercept $\alpha$ and assume a normal distribution with a wide range:&lt;/p&gt;
&lt;p&gt;$\alpha \sim Normal(150,25)$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(42)
dens(rnorm(1e4, 150, 25))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-04_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;$\beta$ represents the average increase of in height for each year. Again, the potentially large age range means we have to use a somewhat uninformative prior because people grow very differently at different ages. The distribution here could be argued to be uniform or normal. I am going with normal because it emphasises a much more peaked distribution of growth rates with an emphasis for the mean:&lt;/p&gt;
&lt;p&gt;$\beta \sim Normal(4,1)$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(42)
dens(rnorm(1e4, 4, 2))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-04_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As you can see, this choice of prior for $\alpha$ would allow for negative growth rates which might be an issue.&lt;/p&gt;
&lt;p&gt;Finally, we need to identify our $\sigma$ where I specify a uniform range that covers the full range of heights when given a large range of students (in age, that is):&lt;/p&gt;
&lt;p&gt;$\sigma \sim Uniform(0,30)$&lt;/p&gt;
&lt;p&gt;Plugging this one into our height simulation, we get a wide but overall sensible range:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(42)
dens(rnorm(1e4, 150, 30))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-04_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Personally, I think a second-order polynomial regression could be sensible here to account for the change in growth rate over time, but I assume a latter question will deal with that.&lt;/p&gt;
&lt;h3 id=&#34;practice-m5&#34;&gt;Practice M5&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Now suppose I remind you that every student got taller each year. Does this information lead you to change your choice of priors? How?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Yes, I can change my prior for $\beta$ to always be positive by using a log-normal distribution:&lt;/p&gt;
&lt;p&gt;$\beta \sim LogNormal(2,0.5)$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(42)
dens(rlnorm(1e4, 2, .5))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-04_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;1440&#34; /&gt;
Now this makes much more sense!&lt;/p&gt;
&lt;!-- ## Practice M6 --&gt;
&lt;!-- **Question:** Now suppose I tell you that the average height in the first year was 120 cm and that every student got taller each year. Does this information lead you to change your choice of priors? How?   --&gt;
&lt;!-- **Answer:** This really changes things. We now know that we are talking about school-age students (around 6-8 years of age). This means we can alter our priors quite significantly as follows:   --&gt;
&lt;!-- $\alpha$ - We know the average height to be 120cm, and can set a relatively small standard deviation on that distribution:   --&gt;
&lt;!-- $\alpha \sim Normal(150,25)$   --&gt;
&lt;!-- $\beta$ - We know that children this age grow much faster than older people and so can increase our prior for this with more certainty:   --&gt;
&lt;!-- $\beta \sim Normal(7,1)$ --&gt;
&lt;!-- $\sigma$ - this is the standard deviation of the overall height likelihood function. We can make this more conservative, knowing that we have a tighter age range: --&gt;
&lt;!-- $\sigma \sim Uniform(0,20)$ --&gt;
&lt;h3 id=&#34;practice-m6&#34;&gt;Practice M6&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Now suppose I tell you that the variance among heights for students of the same age is never more than 64 cm. How does this lead you to revise your priors?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; The variance is the square product of $\sigma$, so we know that $\sigma$ never exceeds $\sqrt(64) = 8$ and can update our prior accordingly:&lt;/p&gt;
&lt;p&gt;$\sigma \sim Uniform(0,8)$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(42)
dens(runif(1e4, 0, 8))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-04_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;hard-exercises&#34;&gt;Hard Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-h1&#34;&gt;Practice H1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; The weights listed below were recorded in the !Kung census, but heights were not recorded for these individuals. Provide predicted heights and 89% intervals (either HPDI or PI) for each of these individuals. That is, fill in the table below, using model-based predictions.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Individual&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;weight&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;expected height&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;89% interval&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;46.95&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;43.72&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;64.78&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;32.59&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;54.63&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(&amp;quot;Howell1&amp;quot;)
d &amp;lt;- Howell1
formula &amp;lt;- alist(
  height ~ dnorm(mu, sigma),
  mu &amp;lt;- a + b * weight,
  a ~ dnorm(150, 30), # a relatively short height with a large SD to account for large age spread in data set and the overall smaller stature of the peoples in the !Kung census
  b ~ dlnorm(0, 1), # I want strictly positive weight-height relationships
  sigma ~ dunif(0, 50) # rather large SD
)
(m &amp;lt;- quap(formula, data = d))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Quadratic approximate posterior distribution
## 
## Formula:
## height ~ dnorm(mu, sigma)
## mu &amp;lt;- a + b * weight
## a ~ dnorm(150, 30)
## b ~ dlnorm(0, 1)
## sigma ~ dunif(0, 50)
## 
## Posterior means:
##         a         b     sigma 
## 75.550586  1.761449  9.345982 
## 
## Log-likelihood: -1987.71
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we can obtain the posterior distributions for our weight values in the table:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;new_weight &amp;lt;- c(46.95, 43.72, 64.78, 32.59, 54.63)
pred_height &amp;lt;- link(m, data = data.frame(weight = new_weight))
expected &amp;lt;- apply(pred_height, 2, mean)
interval &amp;lt;- apply(pred_height, 2, HPDI, prob = 0.89)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we merge this into a data frame:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data.frame(
  individual = 1:5,
  weight = new_weight,
  expected = expected,
  lower = interval[1, ],
  upper = interval[2, ]
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   individual weight expected    lower    upper
## 1          1  46.95 158.2948 157.4868 159.1242
## 2          2  43.72 152.6012 151.9167 153.3826
## 3          3  64.78 189.7242 188.3729 191.2549
## 4          4  32.59 132.9821 132.3565 133.6889
## 5          5  54.63 171.8326 170.9136 173.0242
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-h2&#34;&gt;Practice H2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Select out all the rows in the Howell1 data with ages below 18 years of age. If you do it right, you should end up with a new data frame with 192 rows in it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;d2 &amp;lt;- Howell1[Howell1$age &amp;lt; 18, ]
weight_bar &amp;lt;- mean(d2$weight)
nrow(d2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 192
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-a&#34;&gt;Part A&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Fit a linear regression to these data, using &lt;code&gt;quap()&lt;/code&gt;. Present and interpret the estimates. For every 10 units of increase in weight, how much taller does the model predict a child gets?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;formula &amp;lt;- alist(
  height ~ dnorm(mu, sigma),
  mu &amp;lt;- a + b * (weight - weight_bar),
  a ~ dnorm(110, 30),
  b ~ dlnorm(0, 1),
  sigma ~ dunif(0, 60)
)
m &amp;lt;- quap(formula, data = d2)
precis(m)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean        sd       5.5%      94.5%
## a     108.319563 0.6087746 107.346624 109.292503
## b       2.716656 0.0683154   2.607475   2.825838
## sigma   8.437165 0.4305635   7.749042   9.125289
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For a 10-unit increase in weight, we see a 27.17cm increase in height.&lt;/p&gt;
&lt;h4 id=&#34;part-b&#34;&gt;Part B&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Plot the raw data, with height on the vertical axis and weight on the horizontal axis. Superimpose the MAP regression line and 89% HPDI for the mean. Also superimpose the 89% HPDI for predicted heights.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Data for plot
weight.seq &amp;lt;- seq(from = min(d2$weight), to = max(d2$weight), by = 1) # sequence to do predictions for
mu &amp;lt;- link(m, data = data.frame(weight = weight.seq)) # do predictions
mu.mean &amp;lt;- apply(mu, 2, mean) # calculate mean
mu.HPDI &amp;lt;- apply(mu, 2, HPDI, prob = 0.89) # identify interval
sim.height &amp;lt;- sim(m, data = list(weight = weight.seq)) # simulate full predictions
height.HPDI &amp;lt;- apply(sim.height, 2, HPDI, prob = 0.89) # identify interval

# Plotting
plot(height ~ weight, data = d2, col = col.alpha(rangi2, 0.7)) # base plot
lines(weight.seq, mu.mean) # add mean line
shade(mu.HPDI, weight.seq) # add hdpi interval
shade(height.HPDI, weight.seq) # add full-hdpi interval
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-04_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h4 id=&#34;part-c&#34;&gt;Part C&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; What aspects of the model fit concern you? Describe the kinds of assumptions you would change, if any, to improve the model. You don’t have to write any new code. Just explain what the model appears to be doing a bad job of, and what you hypothesize would be a better model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; The model woefully overpredicts height at the low-weight end of the spectrum as well as the upper end of the weight spectrum. At the mid-range of the weight spectrum, our model underpredicts height. It looks as though the data fall onto a curve and so we could potentially do better with a polynomial model.&lt;/p&gt;
&lt;h3 id=&#34;practice-h3&#34;&gt;Practice H3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Suppose a colleague of yours, who works on allometry, glances at the practice problems just above. Your colleague exclaims, “That’s silly. Everyone knows that it’s only the logarithm of body weight that scales with height!” Let’s take your colleague’s advice and see what happens.&lt;/p&gt;
&lt;h4 id=&#34;part-a-1&#34;&gt;Part A&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Model the relationship between height (cm) and the natural logarithm of weight (log-kg). Use the entire Howell1 data frame, all 544 rows, adults and non-adults. Fit this model, using the quadratic approximation:&lt;/p&gt;
&lt;p&gt;$h_i ∼ Normal(\mu_i, \sigma)$&lt;br&gt;
$\mu_i = \alpha + \beta log(w_i)$&lt;br&gt;
$\alpha ∼ Normal(178, 20)$&lt;br&gt;
$\beta ∼ LogNormal(0, 1)$&lt;br&gt;
$\sigma ∼ Uniform(0, 50)$&lt;/p&gt;
&lt;p&gt;where $h_i$ is the height of individual $i$ and $w_i$ is the weight (in kg) of individual $i$. The function for computing a natural log in &lt;code&gt;R&lt;/code&gt; is just &lt;code&gt;log()&lt;/code&gt;. Can you interpret the resulting estimates?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;d &amp;lt;- Howell1
formula &amp;lt;- alist(
  height ~ dnorm(mu, sigma),
  mu &amp;lt;- a + b * log(weight),
  a ~ dnorm(178, 20),
  b ~ dlnorm(0, 1),
  sigma ~ dunif(0, 50)
)
m &amp;lt;- quap(formula, data = d)
precis(m)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean        sd       5.5%      94.5%
## a     -22.874329 1.3343120 -25.006817 -20.741840
## b      46.817801 0.3823300  46.206764  47.428838
## sigma   5.137168 0.1558908   4.888025   5.386312
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our $\alpha$ estimate seems to be out-of-line at -22.87. This is simply the predicted height when the weight is 0 log-kg and thus somewhat uninformative. $\beta$ tells us that our individual grow, on average, by 46.82cm per increase in log-kg by 1. The standard deviation around our height predictions is 5.14.&lt;/p&gt;
&lt;p&gt;Transforming a variable makes interpreting diffuclt.&lt;/p&gt;
&lt;h4 id=&#34;part-b-1&#34;&gt;Part B&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Begin with this plot: &lt;code&gt;plot(height ~ weight, data = Howell1), col = col.alpha(rangi2, 0.4))&lt;/code&gt;. Then use samples from the quadratic approximate posterior of the model in (a) to superimpose on the plot: (1) the predicted mean height as a function of weight, (2) the 97% HPDI for the mean, and (3) the 97% HPDI for predicted heights.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(height ~ weight, data = d, col = col.alpha(rangi2, 0.4))
# Estimate and plot the quap regression line and 97% HPDI for the mean
weight.seq &amp;lt;- seq(from = min(d$weight), to = max(d$weight), by = 1)
mu &amp;lt;- link(m, data = data.frame(weight = weight.seq))
mu.mean &amp;lt;- apply(mu, 2, mean)
mu.HPDI &amp;lt;- apply(mu, 2, HPDI, prob = 0.97)
lines(weight.seq, mu.mean)
shade(mu.HPDI, weight.seq)
# Estimate and plot the 97% HPDI for the predicted heights
sim.height &amp;lt;- sim(m, data = list(weight = weight.seq))
height.HPDI &amp;lt;- apply(sim.height, 2, HPDI, prob = 0.97)
shade(height.HPDI, weight.seq)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-04_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;1440&#34; /&gt;
Yup, this does fit much more neatly.&lt;/p&gt;
&lt;h3 id=&#34;practice-h4&#34;&gt;Practice H4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Plot the prior predictive distribution for the polynomial regression model in the chapter. You can modify the code that plots the linear regression prior predictive distribution. Can you modify the prior distributions of $\alpha$, $\beta_1$, and $\beta_2$ so that the prior predictions stay within the biologically reasonable outcome space? That is to say: Do not try to fit the data by hand. But do try to keep the curves consistent with what you know about height and weight, before seeing these exact data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(&amp;quot;Howell1&amp;quot;)
d &amp;lt;- Howell1
# standardising weight
d$weight_s &amp;lt;- with(d, (weight - mean(weight)) / sd(weight))
# quadratic weight
d$weight_s2 &amp;lt;- d$weight_s^2

# MODEL
M_Poly &amp;lt;- quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu &amp;lt;- a + b1 * weight_s + b2 * weight_s2,
    a ~ dnorm(178, 20),
    b1 ~ dlnorm(0, 1),
    b2 ~ dnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  data = d
)
precis(M_Poly)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean        sd       5.5%      94.5%
## a     146.054799 0.3689900 145.465082 146.644517
## b1     21.734548 0.2888949  21.272839  22.196258
## b2     -7.800570 0.2742037  -8.238800  -7.362339
## sigma   5.774487 0.1764685   5.492456   6.056517
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The prior prediction can be obtained using &lt;code&gt;extract.prior()&lt;/code&gt;. The obtained sample can then be passed on to the &lt;code&gt;link()&lt;/code&gt; function for the weight space in question. Since we want to try multiple priors, I use a function that takes the &lt;code&gt;alist&lt;/code&gt; object as well as the number of predicted curves as arguments. This function-idea has been adapted from 
&lt;a href=&#34;https://gregor-mathes.netlify.app/2021/01/01/rethinking-chapter-4/#question-4m5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gregor Mathes&lt;/a&gt; solution which is based on the use of &lt;code&gt;tidyverse&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidyr) # we don&#39;t get around using the function pivot_longer()
library(ggplot2)
modify_prior_poly &amp;lt;- function(my_alist, N) {
  # set seed for reproducibility
  set.seed(42)
  # fit model
  m_poly &amp;lt;- quap(my_alist, data = d)
  # make weight sequence with both standardised weight and the square of it
  weight_seq &amp;lt;- seq(from = min(d$weight), to = max(d$weight), by = 1)
  weight_seq &amp;lt;- data.frame(
    weight = weight_seq,
    weight_s = (weight_seq - mean(weight_seq)) / sd(weight_seq),
    weight_s2 = ((weight_seq - mean(weight_seq)) / sd(weight_seq))^2
  )
  # extract samples from the prior
  m_poly_prior &amp;lt;- extract.prior(m_poly, n = N)
  # now apply the polynomial equation to the priors to get predicted heights
  m_poly_mu &amp;lt;- link(
    m_poly,
    post = m_poly_prior,
    data = list(
      weight_s = weight_seq$weight_s,
      weight_s2 = weight_seq$weight_s2
    )
  )
  m_poly_mu &amp;lt;- as.data.frame(m_poly_mu)
  m_poly_mu &amp;lt;- as.data.frame(pivot_longer(m_poly_mu, cols = everything(), values_to = &amp;quot;height&amp;quot;))
  m_poly_mu$weight &amp;lt;- rep(weight_seq$weight, N)
  m_poly_mu$type &amp;lt;- rep(as.character(1:N), each = length(weight_seq$weight))
  # plot it
  ggplot(m_poly_mu) +
    geom_line(aes(x = weight, y = height, group = type), alpha = 0.5) +
    geom_hline(yintercept = c(0, 272), colour = &amp;quot;steelblue4&amp;quot;) +
    annotate(
      geom = &amp;quot;text&amp;quot;,
      x = c(6, 12),
      y = c(11, 285),
      label = c(&amp;quot;Embryo&amp;quot;, &amp;quot;World&#39;s tallest person&amp;quot;),
      colour = c(rep(&amp;quot;steelblue4&amp;quot;, 2))
    ) +
    labs(x = &amp;quot;Weight in kg&amp;quot;, y = &amp;quot;Height in cm&amp;quot;) +
    theme_minimal()
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s run this for our initial model specification:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;modify_prior_poly(
  my_alist = alist(
    height ~ dnorm(mu, sigma),
    mu &amp;lt;- a + b1 * weight_s + b2 * weight_s2,
    a ~ dnorm(178, 20),
    b1 ~ dlnorm(0, 1),
    b2 ~ dnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  N = 40
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-04_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The priors should cover the whole biologically sensible space (unless we have some really strong indication for this not being the case). Let&amp;rsquo;s start by decreasing the mean for $\alpha$ and increasing its standard deviation:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;modify_prior_poly(
  my_alist = alist(
    height ~ dnorm(mu, sigma),
    mu &amp;lt;- a + b1 * weight_s + b2 * weight_s2,
    a ~ dnorm(130, 35), # decrease mean and increase sd
    b1 ~ dlnorm(0, 1),
    b2 ~ dnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  N = 40
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-04_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Better, but not quite there yet. The lines themselves could do with stronger positive relationship here between weight and height. We know this relationship to be stronger:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;modify_prior_poly(
  my_alist = alist(
    height ~ dnorm(mu, sigma),
    mu &amp;lt;- a + b1 * weight_s + b2 * weight_s2,
    a ~ dnorm(130, 35),
    b1 ~ dlnorm(1, 1), # increase mean, but not sd (we don&#39;t want negative relationships)
    b2 ~ dnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  N = 40
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-04_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I am already happy with this. However, we can see some downward curving weight-height relationships here. That&amp;rsquo;s probably not what we find in the real-world and so we might want to force these relationships to always curve upwards, by having a positive $\beta_2$ with a narrow log-normal distribution:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;modify_prior_poly(
  my_alist = alist(
    height ~ dnorm(mu, sigma),
    mu &amp;lt;- a + b1 * weight_s + b2 * weight_s2,
    a ~ dnorm(130, 35),
    b1 ~ dlnorm(1, 1),
    b2 ~ dlnorm(0, .05), # force positive parameter
    sigma ~ dunif(0, 50)
  ),
  N = 40
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-04_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Phew. I can&amp;rsquo;t think of more, to be honest. This looks good to me.&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.5 (2021-03-31)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19043)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.1252  LC_CTYPE=English_United Kingdom.1252    LC_MONETARY=English_United Kingdom.1252 LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.1252    
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] tidyr_1.1.3          rethinking_2.13      rstan_2.21.2         ggplot2_3.3.3        StanHeaders_2.21.0-7
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.7         mvtnorm_1.1-1      lattice_0.20-41    prettyunits_1.1.1  ps_1.6.0           assertthat_0.2.1   digest_0.6.27      utf8_1.2.1         V8_3.4.1           R6_2.5.0          
## [11] backports_1.2.1    stats4_4.0.5       evaluate_0.14      coda_0.19-4        highr_0.9          blogdown_1.3       pillar_1.6.0       rlang_0.4.11       curl_4.3.2         callr_3.7.0       
## [21] jquerylib_0.1.4    R.utils_2.10.1     R.oo_1.24.0        rmarkdown_2.7      styler_1.4.1       labeling_0.4.2     stringr_1.4.0      loo_2.4.1          munsell_0.5.0      compiler_4.0.5    
## [31] xfun_0.22          pkgconfig_2.0.3    pkgbuild_1.2.0     shape_1.4.5        htmltools_0.5.1.1  tidyselect_1.1.0   tibble_3.1.1       gridExtra_2.3      bookdown_0.22      codetools_0.2-18  
## [41] matrixStats_0.61.0 fansi_0.4.2        crayon_1.4.1       dplyr_1.0.5        withr_2.4.2        MASS_7.3-53.1      R.methodsS3_1.8.1  grid_4.0.5         jsonlite_1.7.2     gtable_0.3.0      
## [51] lifecycle_1.0.0    DBI_1.1.1          magrittr_2.0.1     scales_1.1.1       RcppParallel_5.1.2 cli_3.0.0          stringi_1.5.3      farver_2.1.0       bslib_0.2.4        ellipsis_0.3.2    
## [61] generics_0.1.0     vctrs_0.3.7        rematch2_2.1.2     tools_4.0.5        R.cache_0.14.0     glue_1.4.2         purrr_0.3.4        processx_3.5.1     yaml_2.2.1         inline_0.3.17     
## [71] colorspace_2.0-0   knitr_1.33         sass_0.3.1
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 04 (Extra Material)</title>
      <link>https://www.erikkusch.com/courses/rethinking/chapter-04b/</link>
      <pubDate>Fri, 15 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/rethinking/chapter-04b/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to additional exercises from previous versions of the end of chapter 4 in 
&lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Satistical Rethinking 2&lt;/a&gt; by Richard McElreath. I have created these notes as a part of my ongoing involvement in the 
&lt;a href=&#34;https://www.erikkusch.com/project/aubayes/&#34;&gt;AU Bayes Study Group&lt;/a&gt;. Much of my inspiration for these solutions, where necessary, has been obtained from 
&lt;a href=&#34;https://gregor-mathes.netlify.app/2021/01/01/rethinking-chapter-4/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gregor Mathes&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;practice-1&#34;&gt;Practice 1&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Refit model &lt;code&gt;m4.3&lt;/code&gt; from the chapter but omit the mean weight &lt;code&gt;xbar&lt;/code&gt;. Compare the new model’s posterior to that of the original model. In particular, look at the covariance among the parameters. What is difference?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; 
Let&amp;rsquo;s firstly refit the model &lt;code&gt;m4.3&lt;/code&gt; using the code on pages 100 &amp;amp; 101:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rethinking)
data(Howell1)
d &amp;lt;- Howell1
d2 &amp;lt;- d[d$age &amp;gt;= 18, ]
# define the average weight, x-bar
xbar &amp;lt;- mean(d2$weight)
# fit original model
m4.3 &amp;lt;- quap(alist(
  height ~ dnorm(mu, sigma),
  mu &amp;lt;- a + b * (weight - xbar),
  a ~ dnorm(178, 20),
  b ~ dlnorm(0, 1),
  sigma ~ dunif(0, 50)
), data = d2)
# fit reduced model
m4.3_reduced &amp;lt;- quap(alist(
  height ~ dnorm(mu, sigma),
  mu &amp;lt;- a + b * weight,
  a ~ dnorm(178, 20),
  b ~ dlnorm(0, 1),
  sigma ~ dunif(0, 50)
), data = d2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do we compare these models and their posteriors? Here, I want to look at three things:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Covariances between parameters estimates&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;round(vcov(m4.3), digits = 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           a     b sigma
## a     0.073 0.000 0.000
## b     0.000 0.002 0.000
## sigma 0.000 0.000 0.037
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;round(vcov(m4.3_reduced), digits = 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            a      b sigma
## a      3.601 -0.078 0.009
## b     -0.078  0.002 0.000
## sigma  0.009  0.000 0.037
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we can see, the &lt;strong&gt;covariances increase&lt;/strong&gt; quite a bit when omitting &lt;code&gt;xbar&lt;/code&gt; and this &lt;strong&gt;not centring&lt;/strong&gt;.&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Summaries of each parameter in the posterior&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(extract.samples(m4.3))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        a               b              sigma      
##  Min.   :153.6   Min.   :0.7505   Min.   :4.324  
##  1st Qu.:154.4   1st Qu.:0.8738   1st Qu.:4.947  
##  Median :154.6   Median :0.9027   Median :5.076  
##  Mean   :154.6   Mean   :0.9023   Mean   :5.076  
##  3rd Qu.:154.8   3rd Qu.:0.9307   3rd Qu.:5.205  
##  Max.   :155.7   Max.   :1.0443   Max.   :5.773
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(extract.samples(m4.3_reduced))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        a               b              sigma      
##  Min.   :108.2   Min.   :0.7290   Min.   :4.434  
##  1st Qu.:113.2   1st Qu.:0.8632   1st Qu.:4.945  
##  Median :114.5   Median :0.8911   Median :5.071  
##  Mean   :114.5   Mean   :0.8911   Mean   :5.072  
##  3rd Qu.:115.8   3rd Qu.:0.9195   3rd Qu.:5.199  
##  Max.   :121.7   Max.   :1.0403   Max.   :5.833
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Between the two models, neither $\beta$ (&lt;code&gt;b&lt;/code&gt;) nor $\sigma$ (&lt;code&gt;sigma&lt;/code&gt;) differ greatly. However, our posterior estimate of $\alpha$ (&lt;code&gt;a&lt;/code&gt;) is quite a bit lower in the reduced model than it is in the original model.
This is down to the interpretation of the $\alpha$ parameter itself. In the original model, $\alpha$ denotes the average height of a person at the mean weight in the data set. Since we removed the &lt;code&gt;xbar&lt;/code&gt; component in the reduced model, $\alpha$ now identifies the average height of a person of weight $0kg$ - a nonsense metric.&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Predictions and Intervals&lt;br&gt;
Here, I have written a function that takes a model object, data, and some additional arguments to automate plot generation:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot.predictions &amp;lt;- function(X, Y, data, model, main) {
  XOrig &amp;lt;- X
  X &amp;lt;- data[, colnames(data) == X]
  Y &amp;lt;- data[, colnames(data) == Y]
  plot(Y ~ X, col = col.alpha(rangi2, 0.8), main = main)
  # Estimate and plot the quap regression line and 97% HPDI for the mean
  weight.seq &amp;lt;- seq(from = min(X), to = max(X), length.out = 1000)
  predict_df &amp;lt;- data.frame(XOrig = weight.seq)
  colnames(predict_df) &amp;lt;- XOrig
  mu &amp;lt;- link(model, data = predict_df)
  mu.mean &amp;lt;- apply(mu, 2, mean)
  mu.HPDI &amp;lt;- apply(mu, 2, HPDI, prob = 0.97)
  lines(weight.seq, mu.mean)
  shade(mu.HPDI, weight.seq)
  # Estimate and plot the 97% HPDI for the predicted heights
  predict_ls &amp;lt;- list(weight = weight.seq)
  names(predict_ls) &amp;lt;- XOrig
  sim.height &amp;lt;- sim(model, data = predict_ls)
  height.HPDI &amp;lt;- apply(sim.height, 2, HPDI, prob = 0.97)
  shade(height.HPDI, weight.seq)
}

plot.predictions(X = &amp;quot;weight&amp;quot;, Y = &amp;quot;height&amp;quot;, data = d2, model = m4.3, main = &amp;quot;Original Model&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-15-statistical-rethinking-chapter-04b_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot.predictions(X = &amp;quot;weight&amp;quot;, Y = &amp;quot;height&amp;quot;, data = d2, model = m4.3_reduced, main = &amp;quot;Reduced Model&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-15-statistical-rethinking-chapter-04b_files/figure-html/unnamed-chunk-4-2.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So. Does centring or not change the predictions of our model? No, it does not. At least in this case.&lt;/p&gt;
&lt;h2 id=&#34;practice-2&#34;&gt;Practice 2&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; In the chapter, we used 15 knots with the cherry blossom spline. Increase the number of knots and observe what happens to the resulting spline. Then adjust also the width of the prior on the weights - change the standard deviation of the prior and watch what happens. What do you think the combination of knot number and the prior on the weights controls?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; 
Again, I start with code from the book - pages 118, 120 &amp;amp; 122 to be precise - and implement it into a function for easy changing of model specifications:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rethinking)
library(splines)
data(cherry_blossoms)
d &amp;lt;- cherry_blossoms
d2 &amp;lt;- d[complete.cases(d$temp), ] # complete cases on temp

cherry_spline &amp;lt;- function(n_Knots, StdV) {
  # knot list
  knot_list &amp;lt;- quantile(d2$year, probs = seq(0, 1, length.out = n_Knots))[-c(1, n_Knots)]
  # basis function
  B &amp;lt;- bs(d2$year,
    knots = knot_list,
    degree = 3, intercept = TRUE
  )
  # Run quap model
  m4.7 &amp;lt;- quap(alist(
    T ~ dnorm(mu, sigma),
    mu &amp;lt;- a + B %*% w,
    a ~ dnorm(6, 10),
    w ~ dnorm(0, StdV),
    sigma ~ dexp(1)
  ),
  data = list(T = d2$temp, B = B, StdV = StdV),
  start = list(w = rep(0, ncol(B)))
  )
  # get 97% posterior interval for mean and plot
  mu &amp;lt;- link(m4.7)
  mu_PI &amp;lt;- apply(mu, 2, PI, 0.97)
  plot(d2$year, d2$temp,
    col = col.alpha(rangi2, 0.3), pch = 16,
    main = paste(&amp;quot;Knots:&amp;quot;, n_Knots, &amp;quot;-&amp;quot;, &amp;quot;Prior Weight:&amp;quot;, StdV)
  )
  shade(mu_PI, d2$year, col = col.alpha(&amp;quot;black&amp;quot;, 0.5))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s start by &lt;strong&gt;increasing the number of knots&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cherry_spline(n_Knots = 15, StdV = 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-15-statistical-rethinking-chapter-04b_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cherry_spline(n_Knots = 20, StdV = 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-15-statistical-rethinking-chapter-04b_files/figure-html/unnamed-chunk-6-2.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cherry_spline(n_Knots = 30, StdV = 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-15-statistical-rethinking-chapter-04b_files/figure-html/unnamed-chunk-6-3.png&#34; width=&#34;1440&#34; /&gt;
The more knots we use, the more flexible the resulting function becomes. It fits the data better, but might overfit if we try to do predictions.&lt;/p&gt;
&lt;p&gt;Now, we &lt;strong&gt;change the prior weights&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cherry_spline(n_Knots = 15, StdV = 1) # base standard deviation
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-15-statistical-rethinking-chapter-04b_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cherry_spline(n_Knots = 15, StdV = .1) # decreased standard deviation
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-15-statistical-rethinking-chapter-04b_files/figure-html/unnamed-chunk-7-2.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cherry_spline(n_Knots = 15, StdV = 100) # increased standard deviation
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-15-statistical-rethinking-chapter-04b_files/figure-html/unnamed-chunk-7-3.png&#34; width=&#34;1440&#34; /&gt;
As I &lt;strong&gt;decrease the standard deviation for the prior or the weights&lt;/strong&gt;, I see that the resulting function becomes &lt;strong&gt;less flexible&lt;/strong&gt;. I expected our function to become less flexible as I lower the &lt;code&gt;StdV&lt;/code&gt; parameter since a lower standard deviation here will increase the weights and thus give each base function more of say in determining the overall function globally, making the result smoother.&lt;/p&gt;
&lt;h2 id=&#34;practice-3&#34;&gt;Practice 3&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Return to &lt;code&gt;data(cherry_blossoms)&lt;/code&gt; and model the association between blossom date (&lt;code&gt;doy&lt;/code&gt;) and March temperature (&lt;code&gt;temp&lt;/code&gt;). Note that there are many missing values in both variables. You may consider a linear model, a polynomial, or a spline on temperature. How well does temperature rend predict the blossom trend?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rethinking)
library(splines)
data(cherry_blossoms)
d &amp;lt;- cherry_blossoms[, 2:3]
d2 &amp;lt;- na.omit(d)
d2$temps &amp;lt;- scale(d2$temp)
with(d2, plot(temps, doy,
  xlab = &amp;quot;Centred Temperature in March&amp;quot;, ylab = &amp;quot;Day in Year&amp;quot;
))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-15-statistical-rethinking-chapter-04b_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There is a seemingly negative relationship here, but there is also a lot of noise. I expect polynomial or spline approaches to capture too much of that noise and opt for a simple linear regression instead:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# define average temp
xbar &amp;lt;- mean(d2$temp)

# fit modell
cherry_linear &amp;lt;- quap(
  alist(
    doy ~ dnorm(mu, sigma),
    mu &amp;lt;- a + b * (temp - xbar),
    a ~ dnorm(115, 30),
    b ~ dnorm(-2, 5),
    sigma ~ dunif(0, 50)
  ),
  data = d2
)

# output
precis(cherry_linear)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean        sd       5.5%      94.5%
## a     104.921713 0.2106637 104.585032 105.258394
## b      -2.990211 0.3078719  -3.482249  -2.498172
## sigma   5.910003 0.1489654   5.671927   6.148078
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With average temperatures in March, cherries blossom on day 105 of the year. With every increase of 1°C in temperature in March, cherries blossom - on average - 3 earlier. Our PI shows that we are pretty certain of this relationship. Let&amp;rsquo;s plot this to finish:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot.predictions(X = &amp;quot;temp&amp;quot;, Y = &amp;quot;doy&amp;quot;, data = d2, model = cherry_linear, main = &amp;quot;Cherry Blossoms&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-15-statistical-rethinking-chapter-04b_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;1440&#34; /&gt;
Off, that&amp;rsquo;s quite some uncertainty there. I guess we aren&amp;rsquo;t doing a tremendous job at predicting cherry blossom dates depending on temperature in March with this model.&lt;/p&gt;
&lt;h2 id=&#34;practice-4&#34;&gt;Practice 4&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Simulate the prior predictive distribution for the cherry blossom spline in the chapter. Adjust the prior on the weights and observe what happens. What do you think the prior on the weight is doing?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I haven&amp;rsquo;t solved this myself (yet). In the meantime, you can consult the answer provided by 
&lt;a href=&#34;https://gregor-mathes.netlify.app/2021/01/01/rethinking-chapter-4/#question-4h6&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gregor Mathes&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;practice-5&#34;&gt;Practice 5&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; The cherry blossom spline in the chapter used an intercept a, but technically it doesn’t require one. The first basis function could substitute for the intercept. Try refitting the cherry blossom spline without the intercept. What else about the model do you need to change to make this work?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rethinking)
library(splines)
data(cherry_blossoms)
d &amp;lt;- cherry_blossoms
d2 &amp;lt;- d[complete.cases(d$temp), ] # complete cases on temp

n_Knots &amp;lt;- 15
# knot list
knot_list &amp;lt;- quantile(d2$year, probs = seq(0, 1, length.out = n_Knots))[-c(1, n_Knots)]
# basis function
B &amp;lt;- bs(d2$year,
  knots = knot_list,
  degree = 3, intercept = FALSE
)
# Run quap model
m4.7 &amp;lt;- quap(alist(
  T ~ dnorm(mu, sigma),
  mu &amp;lt;- B %*% w,
  a ~ dnorm(6, 10),
  w ~ dnorm(0, 1),
  sigma ~ dexp(1)
),
data = list(T = d2$temp, B = B),
start = list(w = rep(0, ncol(B)))
)
# get 97% posterior interval for mean and plot
mu &amp;lt;- link(m4.7)
mu_PI &amp;lt;- apply(mu, 2, PI, 0.97)
plot(d2$year, d2$temp,
  col = col.alpha(rangi2, 0.3), pch = 16,
  main = &amp;quot;No Intercept&amp;quot;
)
shade(mu_PI, d2$year, col = col.alpha(&amp;quot;black&amp;quot;, 0.5))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-15-statistical-rethinking-chapter-04b_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We need to change the deterministic formula in the model as well as the creation of basis functions by setting &lt;code&gt;Intercept = FALSE&lt;/code&gt; in the &lt;code&gt;bs()&lt;/code&gt; function call.&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.5 (2021-03-31)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19043)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.1252  LC_CTYPE=English_United Kingdom.1252    LC_MONETARY=English_United Kingdom.1252 LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.1252    
## 
## attached base packages:
## [1] splines   parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] rethinking_2.13      rstan_2.21.2         ggplot2_3.3.6        StanHeaders_2.21.0-7
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.7         mvtnorm_1.1-1      lattice_0.20-41    prettyunits_1.1.1  ps_1.6.0           assertthat_0.2.1   digest_0.6.27      utf8_1.2.1         V8_3.4.1           R6_2.5.0          
## [11] backports_1.2.1    stats4_4.0.5       evaluate_0.14      coda_0.19-4        highr_0.9          blogdown_1.3       pillar_1.6.0       rlang_0.4.11       curl_4.3.2         callr_3.7.0       
## [21] jquerylib_0.1.4    R.utils_2.10.1     R.oo_1.24.0        rmarkdown_2.7      styler_1.4.1       stringr_1.4.0      loo_2.4.1          munsell_0.5.0      compiler_4.0.5     xfun_0.22         
## [31] pkgconfig_2.0.3    pkgbuild_1.2.0     shape_1.4.5        htmltools_0.5.1.1  tidyselect_1.1.0   tibble_3.1.1       gridExtra_2.3      bookdown_0.22      codetools_0.2-18   matrixStats_0.61.0
## [41] fansi_0.4.2        crayon_1.4.1       dplyr_1.0.5        withr_2.4.2        MASS_7.3-53.1      R.methodsS3_1.8.1  grid_4.0.5         jsonlite_1.7.2     gtable_0.3.0       lifecycle_1.0.0   
## [51] DBI_1.1.1          magrittr_2.0.1     scales_1.1.1       RcppParallel_5.1.2 cli_3.0.0          stringi_1.5.3      bslib_0.2.4        ellipsis_0.3.2     generics_0.1.0     vctrs_0.3.7       
## [61] rematch2_2.1.2     tools_4.0.5        R.cache_0.14.0     glue_1.4.2         purrr_0.3.4        processx_3.5.1     yaml_2.2.1         inline_0.3.17      colorspace_2.0-0   knitr_1.33        
## [71] sass_0.3.1
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 05</title>
      <link>https://www.erikkusch.com/courses/rethinking/chapter-05/</link>
      <pubDate>Thu, 21 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/rethinking/chapter-05/</guid>
      <description>&lt;h1 id=&#34;the-many-variables--the-spurious-waffles&#34;&gt;The Many Variables &amp;amp; The Spurious Waffles&lt;/h1&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/rethinking/5__22-01-2021_SUMMARY_-Multiple-Regression.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides Chapter 5&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to the exercises at the end of chapter 5 in 
&lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Satistical Rethinking 2&lt;/a&gt; by Richard McElreath. I have created these notes as a part of my ongoing involvement in the 
&lt;a href=&#34;https://www.erikkusch.com/project/aubayes/&#34;&gt;AU Bayes Study Group&lt;/a&gt;. Much of my inspiration for these solutions, where necessary, has been obtained from 
&lt;a href=&#34;https://jmgirard.com/statistical-rethinking-ch5/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jeffrey Girard&lt;/a&gt;, and This 
&lt;a href=&#34;https://github.com/jffist/statistical-rethinking-solutions/blob/master/ch05_hw.R&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub Repository&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;easy-exercises&#34;&gt;Easy Exercises&lt;/h2&gt;
&lt;!-- Despite it not being specifically asked of me, I decide to show some examples in `R`even for the easy exercises and need some packages: --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- rm(list=ls()) --&gt;
&lt;!-- library(rethinking) --&gt;
&lt;!-- library(ggplot2) --&gt;
&lt;!-- library(GGally) --&gt;
&lt;!-- library(dagitty) --&gt;
&lt;!-- ``` --&gt;
&lt;h3 id=&#34;practice-e1&#34;&gt;Practice E1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Which of the linear models below are multiple regressions?&lt;/p&gt;
&lt;p&gt;(1) $μ_i=α+βx_i$&lt;br&gt;
(2) $μ_i=β_xx_i+β_zz_i$&lt;br&gt;
(3) $μ_i=α+β(x_i–z_i)$&lt;br&gt;
(4) $μ_i=α+β_xx_i+β_zz_i$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  &lt;em&gt;2 and 4 are multiple regressions.&lt;/em&gt;&lt;br&gt;
Model 1 does only considers one predictor variable ($x_i$) and can thus not be a multiple regression. Models 2 and 4 contain multiple predictors with ($x_i$ and $z_i$) with separate slope parameters ($\beta_x$ and $\beta_z$) and thus qualify to be considered multiple regressions. The presence or absence of an intercept parameter ($\alpha$) does not change this interpretation.&lt;/p&gt;
&lt;p&gt;Model 3 is a tad out there. While it only contains one slope parameter ($\beta$), it does make use of two variables ($x_i$ and $z_i$). It can be rewritten as $μ_i=α+βx_i–βz_i$. Now, the notation is in line with models 2 and 4, but has a fixed slope for both. For that reason, I do not think that it is a multiple regression.&lt;/p&gt;
&lt;h3 id=&#34;practice-e2&#34;&gt;Practice E2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Write down a multiple regression to evaluate the claim: &lt;em&gt;Animal diversity is linearly related to latitude, but only after controlling for plant diversity&lt;/em&gt;. You just need to write down the model definition.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; $Div_ {Animals} = \alpha + \beta_{Lat}Lat + \beta_{Plants}Div_{Plants}$&lt;/p&gt;
&lt;h3 id=&#34;practice-e3&#34;&gt;Practice E3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Write down a multiple regression to evaluate the claim: &lt;em&gt;Neither amount of funding nor size of laboratory is by itself a good predictor of time to PhD degree; but together these variables are both positively associated with time to degree&lt;/em&gt;. Write down the model definition and indicate which side of zero each slope parameter should be on.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; $T = \alpha + \beta_FF + \beta_SS$&lt;br&gt;
with $T$ (time to PhD), $F$ (funding), and $S$ (size of laboratory).&lt;/p&gt;
&lt;p&gt;On a side-note, I am a bit unclear as to whether the question should really state that they have a &amp;ldquo;positive&amp;rdquo; effect as this indicates, by intuition, a decrease in time to PhD, but in statistical terms, denote the exact opposite.&lt;/p&gt;
&lt;p&gt;Since the combined effect of both slopes is supposed to be positive, the individual slopes must both be positive (or negative, depending on what you understand by &amp;ldquo;positive effect on time&amp;rdquo;).&lt;/p&gt;
&lt;!-- I decided to simulate one case in `R`: --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- N &lt;- 1e3 # number of samples --&gt;
&lt;!-- rho &lt;- .9 # controls correlation between predictors --&gt;
&lt;!-- set.seed(42) --&gt;
&lt;!-- F &lt;- rnorm(n = N, mean = 10, sd = 1) # funding --&gt;
&lt;!-- S &lt;- rnorm(n = N, mean = -rho*F, sd = sqrt(1-rho^2)) # size --&gt;
&lt;!-- d &lt;- data.frame(F=F, S=S,  --&gt;
&lt;!--                 T=rnorm(n = N, mean = F + S, sd = 3) # T dependant F and S --&gt;
&lt;!--                 ) --&gt;
&lt;!-- ggpairs(d) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- Now that I have the data, I want to run individual models for $T$ as predicted by $S$ and $F$, respectively, to make sure neither are a good predictor in isolation: --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- ### Size Model --&gt;
&lt;!-- modE3_S &lt;- quap( --&gt;
&lt;!--   alist( --&gt;
&lt;!--     T ~ dnorm(mu, sigma), --&gt;
&lt;!--     mu &lt;- a + bS*S, --&gt;
&lt;!--     a ~ dnorm(0,10), --&gt;
&lt;!--     bS ~ dnorm(0, 1), --&gt;
&lt;!--     sigma ~ dunif(0, 10) --&gt;
&lt;!--   ), --&gt;
&lt;!--   data = d --&gt;
&lt;!-- )  --&gt;
&lt;!-- precis(modE3_S) --&gt;
&lt;!-- ### Funding Model --&gt;
&lt;!-- modE3_F &lt;- quap( --&gt;
&lt;!--   alist( --&gt;
&lt;!--     T ~ dnorm(mu, sigma), --&gt;
&lt;!--     mu &lt;- a + bF*F, --&gt;
&lt;!--     a ~ dnorm(0,10), --&gt;
&lt;!--     bF ~ dnorm(0, 1), --&gt;
&lt;!--     sigma ~ dunif(0, 10) --&gt;
&lt;!--   ), --&gt;
&lt;!--   data = d --&gt;
&lt;!-- )  --&gt;
&lt;!-- precis(modE3_F) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- Cool. Neither are good predictors by themselves. How about if we combine them? --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- modE3_FS &lt;- quap( --&gt;
&lt;!--   alist( --&gt;
&lt;!--     T ~ dnorm(mu, sigma), --&gt;
&lt;!--     mu &lt;- a + bF*F + bS*S, --&gt;
&lt;!--     a ~ dnorm(0,10), --&gt;
&lt;!--     bF ~ dnorm(0, 1), --&gt;
&lt;!--     bS ~ dnorm(0, 1), --&gt;
&lt;!--     sigma ~ dunif(0, 10) --&gt;
&lt;!--   ), --&gt;
&lt;!--   data = d --&gt;
&lt;!-- )  --&gt;
&lt;!-- precis(modE3_FS) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- Ok. In a shared model, both funding and size of laboratory are still pretty useless as predictors in my simulated data. Maybe I can get a clearer picture of this when plotting the models: --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- par(mfrow = c(1,2)) --&gt;
&lt;!-- ## SIZE --&gt;
&lt;!-- plot(T~S, d, col = col.alpha(&#34;blue&#34;,0.5), main = &#34;Time vs. Size&#34;) --&gt;
&lt;!-- ## Single Effect --&gt;
&lt;!-- S.seq &lt;- seq(-14, -4, by=0.1) --&gt;
&lt;!-- modE3_S.Link &lt;- link(modE3_S, data=list(S=S.seq), n=1000) --&gt;
&lt;!-- modE3_S.mean &lt;- apply(modE3_S.Link, 2, mean) --&gt;
&lt;!-- modE3_S.pi &lt;- apply(modE3_S.Link, 2, PI) --&gt;
&lt;!-- lines(S.seq, modE3_S.mean, col=&#39;red&#39;) --&gt;
&lt;!-- shade(modE3_S.pi, S.seq, col = col.alpha(&#34;red&#34;,0.15)) --&gt;
&lt;!-- ## Combined Effect --&gt;
&lt;!-- modE3_FS_S &lt;- link(modE3_FS, data=data.frame(S=S.seq, F=mean(d$F)), n=1000) --&gt;
&lt;!-- modE3_FS_S.mean &lt;- apply(modE3_FS_S, 2, mean) --&gt;
&lt;!-- modE3_FS_S.pi &lt;- apply(modE3_FS_S, 2, PI) --&gt;
&lt;!-- lines(S.seq, modE3_FS_S.mean, col=&#39;green&#39;) --&gt;
&lt;!-- shade(modE3_FS_S.pi, S.seq, col = col.alpha(&#34;green&#34;,0.5)) --&gt;
&lt;!-- ## FUNDING --&gt;
&lt;!-- plot(T~F, d, col = col.alpha(&#34;blue&#34;,0.5), main = &#34;Time vs. Funding&#34;) --&gt;
&lt;!-- # Single Effect --&gt;
&lt;!-- F.seq &lt;- seq(7, 14, by=0.1) --&gt;
&lt;!-- modE3_F.Link &lt;- link(modE3_F, data=list(F=F.seq), n=1000) --&gt;
&lt;!-- modE3_F.mean &lt;- apply(modE3_F.Link, 2, mean) --&gt;
&lt;!-- modE3_F.pi &lt;- apply(modE3_F.Link, 2, PI) --&gt;
&lt;!-- lines(F.seq, modE3_F.mean, col=&#39;red&#39;) --&gt;
&lt;!-- shade(modE3_F.pi, F.seq, col = col.alpha(&#34;red&#34;,0.15)) --&gt;
&lt;!-- ## Combined Effect --&gt;
&lt;!-- modE3_FS_F &lt;- link(modE3_FS, data=data.frame(S=mean(d$S), F=F.seq), n=1000) --&gt;
&lt;!-- modE3_FS_F.mean &lt;- apply(modE3_FS_F, 2, mean) --&gt;
&lt;!-- modE3_FS_F.pi &lt;- apply(modE3_FS_F, 2, PI) --&gt;
&lt;!-- lines(F.seq, modE3_FS_F.mean, col=&#39;green&#39;) --&gt;
&lt;!-- shade(modE3_FS_F.pi, F.seq, col = col.alpha(&#34;green&#34;,0.5)) --&gt;
&lt;!-- ``` --&gt;
&lt;h3 id=&#34;practice-e4&#34;&gt;Practice E4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Suppose you have a single categorical predictor with 4 levels (unique values), labelled $A$, $B$, $C$ and $D$. Let $A_i$ be an indicator variable that is 1 where case $i$ is in category $A$. Also suppose $B_i$, $C_i$, and $D_i$ for the other categories. Now which of the following linear models are inferentially equivalent ways to include the categorical variable in a regression? Models are inferentially equivalent when it’s possible to compute one posterior distribution from the posterior distribution of another model.&lt;/p&gt;
&lt;p&gt;(1) $μ_i=α+β_AA_i+β_BB_i+β_DD_i$&lt;br&gt;
(2) $μ_i=α+β_AA_i+β_BB_i+β_CC_i+β_DD_i$&lt;br&gt;
(3) $μ_i=α+β_BB_i+β_CC_i+β_DD_i$&lt;br&gt;
(4) $μ_i=α_AA_i+α_BB_i+α_CC_i+α_DD_i$&lt;br&gt;
(5) $μ_i=α_A(1–Bi–Ci–Di)+α_BB_i+α_CC_i+α_DD_i$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; &lt;em&gt;Model 1 and 3-5 are inferentially equivalent.&lt;/em&gt;&lt;br&gt;
Models 1 and 3 both make use of 3 of the 4 total indicator variables which means that we can always derive the the parameter estimates for the 4th indicator variable from a combination of the three parameter estimates present as well as the intercept. Model 4 is akin to an index variable approach which is inferentially the same as an indicator approach (Models 1 and 3). Model 5 is the same as Model 4, so long as we assume that each observation has to belong to one of the four indicator variables.&lt;/p&gt;
&lt;h2 id=&#34;medium-exercises&#34;&gt;Medium Exercises&lt;/h2&gt;
&lt;p&gt;Time to get into &lt;code&gt;R&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rm(list = ls())
library(rethinking)
library(ggplot2)
library(GGally)
library(dagitty)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-m1&#34;&gt;Practice M1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Invent your own example of a spurious correlation. An outcome variable should be correlated with both predictor variables. But when both predictors are entered in the same model, the correlation between the outcome and one of the predictors should mostly vanish (or at least be greatly reduced).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Here, I follow the example in the Overthinking box on page 138. I create an example in which I consider a relationship between standardised vegetative height, standardised air temperature that vanishes once standardised elevation enters the picture.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(42)
N &amp;lt;- 1e2
Elev &amp;lt;- rnorm(n = N, mean = 0, sd = 1)
VegHeight &amp;lt;- rnorm(n = 100, mean = -Elev, sd = 1)
AirTemp &amp;lt;- rnorm(n = N, mean = Elev, sd = 2)
d &amp;lt;- data.frame(Elev, VegHeight, AirTemp)
ggpairs(d)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-21-statistical-rethinking-chapter-05_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;First, I need to show that there even is a (spurious) association between vegetation height (&lt;code&gt;VegHeight&lt;/code&gt;) and air temperature (&lt;code&gt;AirTemp&lt;/code&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m &amp;lt;- quap(
  alist(
    VegHeight ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bAT * AirTemp,
    a ~ dnorm(0, 1),
    bAT ~ dnorm(0, 1),
    sigma ~ dunif(0, 2)
  ),
  data = d
)
precis(m)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean         sd       5.5%       94.5%
## a     -0.1163981 0.13088036 -0.3255702  0.09277397
## bAT   -0.1334516 0.06168793 -0.2320409 -0.03486242
## sigma  1.3201363 0.09334783  1.1709484  1.46932412
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, let&amp;rsquo;s see what happens when I add elevation data (&lt;code&gt;Elev&lt;/code&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m &amp;lt;- quap(
  alist(
    VegHeight ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bAT * AirTemp + bEL * Elev,
    a ~ dnorm(0, 1),
    bAT ~ dnorm(0, 1),
    bEL ~ dnorm(0, 1),
    sigma ~ dunif(0, 2)
  ),
  data = d
)
precis(m)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              mean         sd        5.5%      94.5%
## a     -0.08752358 0.08934620 -0.23031606  0.0552689
## bAT    0.03291017 0.04470657 -0.03853957  0.1043599
## bEL   -0.98905981 0.09190458 -1.13594107 -0.8421785
## sigma  0.89659689 0.06340391  0.79526519  0.9979286
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Conclusively, according to our simulated data and the multiple regression analysis, increasing elevation leads to decreased vegetation height directly. Since the bivariate effect of decreasing air temperature leading to decreases in vegetation height vanishes when we include elevation data, we deem this association to be spurious.&lt;/p&gt;
&lt;h3 id=&#34;practice-m2&#34;&gt;Practice M2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Invent your own example of a masked relationship. An outcome variable should be correlated with both predictor variables, but in opposite directions. And the two predictor variables should be correlated with one another.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Again, I take inspiration from an Overthinking box. This time from page 156. Here, I create an example of of species richness (&lt;code&gt;S&lt;/code&gt;) as driven by human footprint (&lt;code&gt;H&lt;/code&gt;), and latitude centred on the equator (&lt;code&gt;L&lt;/code&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;N &amp;lt;- 1e2
rho &amp;lt;- 0.6
L &amp;lt;- rnorm(n = N, mean = 0, sd = 1)
H &amp;lt;- rnorm(n = N, mean = rho * L, sd = sqrt(1 - rho^2))
S &amp;lt;- rnorm(n = N, mean = L - H, sd = 1)
d &amp;lt;- data.frame(S, L, H)
ggpairs(d)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-21-statistical-rethinking-chapter-05_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start with some bivariate models:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Latitude centred on equator
m1 &amp;lt;- quap(
  alist(
    S ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bL * L,
    a ~ dnorm(0, 1),
    bL ~ dnorm(0, 1),
    sigma ~ dunif(0, 2)
  ),
  data = d
)
precis(m1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean         sd        5.5%     94.5%
## a     0.09656591 0.12275009 -0.09961245 0.2927443
## bL    0.26165086 0.13796469  0.04115663 0.4821451
## sigma 1.23678173 0.08745475  1.09701214 1.3765513
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Human footprint
m2 &amp;lt;- quap(
  alist(
    S ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bH * H,
    a ~ dnorm(0, 1),
    bH ~ dnorm(0, 1),
    sigma ~ dunif(0, 2)
  ),
  data = d
)
precis(m2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              mean        sd       5.5%      94.5%
## a      0.07758373 0.1209797 -0.1157652  0.2709327
## bH    -0.30689555 0.1147838 -0.4903422 -0.1234489
## sigma  1.21600759 0.0859864  1.0785847  1.3534305
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, let&amp;rsquo;s combine these into one big multiple regression:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m3 &amp;lt;- quap(
  alist(
    S ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bH * H + bL * L,
    a ~ dnorm(0, 1),
    bH ~ dnorm(0, 1),
    bL ~ dnorm(0, 1),
    sigma ~ dunif(0, 2)
  ),
  data = d
)
precis(m3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              mean         sd       5.5%      94.5%
## a      0.03603387 0.10567367 -0.1328531  0.2049208
## bH    -0.78399406 0.13167742 -0.9944400 -0.5735481
## bL     0.86622135 0.15576555  0.6172779  1.1151648
## sigma  1.05757879 0.07482312  0.9379970  1.1771606
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Both associations became stronger! To link this back to reality, equatoward position has been linked to increased species richness for a long time (through many hypotheses I won&amp;rsquo;t go into here), there is also a global pattern of reduced human footprint around the equator (although this may change&amp;hellip; looking at you, Brazil). Human footprint itself has been unequivocally linked with a decrease in species richness.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s look at this in plotted form:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(coeftab(m3, m2, m1), par = c(&amp;quot;bH&amp;quot;, &amp;quot;bL&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-21-statistical-rethinking-chapter-05_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In a Directed Acyclic Graph, this works out to:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dag &amp;lt;- dagitty(&amp;quot;dag { L -&amp;gt; S L -&amp;gt; H H -&amp;gt; S}&amp;quot;)
coordinates(dag) &amp;lt;- list(x = c(L = 0, S = 1, H = 2), y = c(L = 0, S = 1, H = 0))
drawdag(dag)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-21-statistical-rethinking-chapter-05_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;practice-m3&#34;&gt;Practice M3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; It is sometimes observed that the best predictor of fire risk is the presence of firefighters - States and localities with many firefighters also have more fires. Presumably firefighters do not &lt;em&gt;cause&lt;/em&gt; fires. Nevertheless, this is not a spurious correlation. Instead fires cause firefighters. Consider the same reversal of causal inference in the context of the divorce and marriage data. How might a high divorce rate cause a higher marriage rate? Can you think of a way to evaluate this relationship, using multiple regression?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Divorces introduce un-married individuals to the population. These individuals have already demonstrated a readiness to get married in the first place and so might spike marriage rates via re-marrying. I would test for this by running a multiple regression in which I regress marriage rate on divorce rate and re-marriage rate (this excludes first-marriages). As long as divorce no longer predicts marriage rate once re-marriage rate is known, our hypothesis would be true.&lt;/p&gt;
&lt;h3 id=&#34;practice-m4&#34;&gt;Practice M4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; In the divorce data, States with high numbers of Mormons (members of The Church of Jesus Christ of Latter-day Saints, LDS) have much lower divorce rates than the regression models expected. Find a list of LDS population by State and use those numbers as a predictor variable, predicting divorce rate using marriage rate, median age at marriage, and percent LDS population (possibly standardised). You may want to consider transformations of the raw percent LDS variable.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  Here, I use the percentage values of LDS as obtained through Wikipedia by 
&lt;a href=&#34;https://jmgirard.com/statistical-rethinking-ch5/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jeffrey Girard&lt;/a&gt;. Since there is a large skew in these data due to states with large LDS populations, I apply a log-transformation before standardising:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(&amp;quot;WaffleDivorce&amp;quot;)
d &amp;lt;- WaffleDivorce
d$LDS &amp;lt;- c(0.0077, 0.0453, 0.0610, 0.0104, 0.0194, 0.0270, 0.0044, 0.0057, 0.0041, 0.0075, 0.0082, 0.0520, 0.2623, 0.0045, 0.0067, 0.0090, 0.0130, 0.0079, 0.0064, 0.0082, 0.0072, 0.0040, 0.0045, 0.0059, 0.0073, 0.0116, 0.0480, 0.0130, 0.0065, 0.0037, 0.0333, 0.0041, 0.0084, 0.0149, 0.0053, 0.0122, 0.0372, 0.0040, 0.0039, 0.0081, 0.0122, 0.0076, 0.0125, 0.6739, 0.0074, 0.0113, 0.0390, 0.0093, 0.0046, 0.1161)
d$logLDS &amp;lt;- log(d$LDS)
d$logLDS.s &amp;lt;- (d$logLDS - mean(d$logLDS)) / sd(d$logLDS)
par(mfrow = c(1, 3))
hist(d$LDS)
hist(d$logLDS)
hist(d$logLDS.s)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-21-statistical-rethinking-chapter-05_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now I am ready to build the model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m &amp;lt;- map(
  alist(
    Divorce ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bm * Marriage + ba * MedianAgeMarriage + bl * logLDS.s,
    a ~ dnorm(10, 20),
    bm ~ dnorm(0, 10),
    ba ~ dnorm(0, 10),
    bl ~ dnorm(0, 10),
    sigma ~ dunif(0, 5)
  ),
  data = d
)
precis(m)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              mean         sd      5.5%      94.5%
## a     35.43051636 6.77505687 24.602667 46.2583658
## bm     0.05343619 0.08261404 -0.078597  0.1854694
## ba    -1.02939820 0.22468646 -1.388491 -0.6703058
## bl    -0.60777261 0.29055419 -1.072134 -0.1434109
## sigma  1.37864526 0.13836923  1.157505  1.5997860
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While marriage rate is not a powerful predictor of divorce rate, both age at marriage and percentage of LDS population are strongly negatively associated with divorce rates.&lt;/p&gt;
&lt;h3 id=&#34;practice-m5&#34;&gt;Practice M5&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; One way to reason through multiple causation hypotheses is to imagine detailed mechanisms through which predictor variables may influence outcomes. For example, it is sometimes argued that the price of gasoline (predictor variable) is positively associated with lower obesity rates (outcome variable). However, there are at least two important mechanisms by which the price of gas could reduce obesity. First, it could lead to less driving and therefore more exercise. Second, it could lead to less driving, which leads to less eating out, which leads to less consumption of huge restaurant meals. Can you outline one or more multiple regressions that address these two mechanisms? Assume you can have any predictor data you need.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; $μ_i=α+β_GG_i+β_EE_i+β_RR_i$. I propose we run a multiple regression containing a variable $E$ denoting frequency of exercising, and another variable $R$ which captures the frequency of restaurant visits. We use these alongside the variable $G$ (gasoline price) to model obesity rates.&lt;/p&gt;
&lt;h2 id=&#34;hard-exercises&#34;&gt;Hard Exercises&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Disclaimer:&lt;/strong&gt; All three exercises below use the same data, data(foxes) (part of rethinking). The urban fox (&lt;em&gt;Vulpes vulpes&lt;/em&gt;) is a successful exploiter of human habitat. Since urban foxes move in packs and defend territories, data on habitat quality and population density is also included. The data frame has five columns:&lt;/p&gt;
&lt;p&gt;(1) &lt;code&gt;group&lt;/code&gt;: Number of the social group the individual fox belongs to&lt;br&gt;
(2) &lt;code&gt;avgfood&lt;/code&gt;: The average amount of food available in the territory&lt;br&gt;
(3) &lt;code&gt;groupsize&lt;/code&gt;: The number of foxes in the social group&lt;br&gt;
(4) &lt;code&gt;area&lt;/code&gt;: Size of the territory&lt;br&gt;
(5) &lt;code&gt;weight&lt;/code&gt;: Body weight of the individual fox&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(&amp;quot;foxes&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-h1&#34;&gt;Practice H1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Fit two bivariate Gaussian regressions, using &lt;code&gt;quap&lt;/code&gt;: (1) body weight as a linear function of territory size (&lt;code&gt;area&lt;/code&gt;), and (2) body weight as a linear function of &lt;code&gt;groupsize&lt;/code&gt;. Plot the results of these regressions, displaying the MAP regression line and the 95% interval of the mean. Is either variable important for predicting fox body weight?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Let&amp;rsquo;s start with the models:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Area
d &amp;lt;- foxes
ma &amp;lt;- quap(
  alist(
    weight ~ dnorm(mu, sigma),
    mu &amp;lt;- a + ba * area,
    a ~ dnorm(5, 5),
    ba ~ dnorm(0, 5),
    sigma ~ dunif(0, 5)
  ),
  data = d
)
precis(ma)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean         sd       5.5%     94.5%
## a     4.45430638 0.38955723  3.8317187 5.0768941
## ba    0.02385824 0.11803080 -0.1647778 0.2124943
## sigma 1.17868417 0.07738415  1.0550093 1.3023590
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;area.seq &amp;lt;- seq(from = min(d$area), to = max(d$area), length.out = 1e4)
mu &amp;lt;- link(ma, data = data.frame(area = area.seq))
mu.PI &amp;lt;- apply(mu, 2, PI, prob = 0.95)
plot(weight ~ area, data = d, col = rangi2)
abline(ma)
shade(mu.PI, area.seq)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-21-statistical-rethinking-chapter-05_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Group Size
mg &amp;lt;- quap(
  alist(
    weight ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bg * groupsize,
    a ~ dnorm(5, 5),
    bg ~ dnorm(0, 5),
    sigma ~ dunif(0, 5)
  ),
  data = d
)
precis(mg)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean         sd       5.5%       94.5%
## a      5.0675829 0.32418282  4.5494762  5.58568969
## bg    -0.1238161 0.07038361 -0.2363027 -0.01132946
## sigma  1.1635303 0.07638932  1.0414454  1.28561522
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;groupsize.seq &amp;lt;- seq(from = min(d$groupsize), to = max(d$groupsize), length.out = 1e4)
mu &amp;lt;- link(mg, data = data.frame(groupsize = groupsize.seq))
mu.PI &amp;lt;- apply(mu, 2, PI, prob = 0.95)
plot(weight ~ groupsize, data = d, col = rangi2)
abline(mg)
shade(mu.PI, groupsize.seq)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-21-statistical-rethinking-chapter-05_files/figure-html/unnamed-chunk-13-2.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;According to these two bivariate models, neither &lt;code&gt;area&lt;/code&gt; nor &lt;code&gt;groupsize&lt;/code&gt; have strong associations with &lt;code&gt;weight&lt;/code&gt; of which we could be certain.&lt;/p&gt;
&lt;h3 id=&#34;practice-h2&#34;&gt;Practice H2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Now fit a multiple linear regression with &lt;code&gt;weight&lt;/code&gt; as the outcome and both &lt;code&gt;area&lt;/code&gt; and &lt;code&gt;groupsize&lt;/code&gt; as predictor variables. Plot the predictions of the model for each predictor, holding the other predictor constant at its mean. What does this model say about the importance of each variable? Why do you get different results than you got in the exercise just above?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; First, we run the model itself:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mag &amp;lt;- quap(
  alist(
    weight ~ dnorm(mu, sigma),
    mu &amp;lt;- a + ba * area + bg * groupsize,
    a ~ dnorm(5, 5),
    ba ~ dnorm(0, 5),
    bg ~ dnorm(0, 5),
    sigma ~ dunif(0, 5)
  ),
  data = d
)
precis(mag)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean         sd       5.5%      94.5%
## a      4.4541696 0.36977162  3.8632031  5.0451360
## ba     0.6159473 0.19978363  0.2966545  0.9352402
## bg    -0.4318471 0.12066677 -0.6246959 -0.2389982
## sigma  1.1184516 0.07342985  1.0010965  1.2358067
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, I want to actually have a look at the underlying data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggpairs(d[, 3:5])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-21-statistical-rethinking-chapter-05_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;groupsize&lt;/code&gt; and &lt;code&gt;area&lt;/code&gt; are pretty heavily associated it seems. Finally, we establish counterfactual plots:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Fixing Group Size
G.avg &amp;lt;- mean(d$groupsize)
A.seq &amp;lt;- seq(from = 0, to = 6, length.out = 1e4)
pred.data &amp;lt;- data.frame(
  groupsize = G.avg,
  area = A.seq
)
mu &amp;lt;- link(mag, data = pred.data)
mu.mean &amp;lt;- apply(mu, 2, mean)
mu.PI &amp;lt;- apply(mu, 2, PI, prob = 0.95)
A.sim &amp;lt;- sim(mag, data = pred.data, n = 1e4)
A.PI &amp;lt;- apply(A.sim, 2, PI)
plot(weight ~ area, data = d, type = &amp;quot;n&amp;quot;)
mtext(&amp;quot;groupsize = 4.345&amp;quot;)
lines(A.seq, mu.mean)
shade(mu.PI, A.seq)
shade(A.PI, A.seq)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-21-statistical-rethinking-chapter-05_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Fixing Area
A.avg &amp;lt;- mean(d$area)
G.seq &amp;lt;- seq(from = 1, to = 10, length.out = 1e4)
pred.data &amp;lt;- data.frame(
  groupsize = G.seq,
  area = A.avg
)
mu &amp;lt;- link(mag, data = pred.data)
mu.mean &amp;lt;- apply(mu, 2, mean)
mu.PI &amp;lt;- apply(mu, 2, PI, prob = 0.95)
G.sim &amp;lt;- sim(mag, data = pred.data, n = 1e4)
G.PI &amp;lt;- apply(G.sim, 2, PI)
plot(weight ~ groupsize, data = d, type = &amp;quot;n&amp;quot;)
mtext(&amp;quot;area = 3.169&amp;quot;)
lines(G.seq, mu.mean)
shade(mu.PI, G.seq)
shade(G.PI, G.seq)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-21-statistical-rethinking-chapter-05_files/figure-html/unnamed-chunk-16-2.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is a clear example of a masking relationship. When considered in isolation (bivariate models) neither &lt;code&gt;groupsize&lt;/code&gt; nor &lt;code&gt;area&lt;/code&gt; show clear associations with &lt;code&gt;weight&lt;/code&gt;. However, as soon as we use a multiple regression, we find that &lt;code&gt;weight&lt;/code&gt; declines as &lt;code&gt;groupsize&lt;/code&gt; increases, while &lt;code&gt;area&lt;/code&gt; has the opposite effect. These effects cancel each other out in bivariate settings.&lt;/p&gt;
&lt;h3 id=&#34;practice-h3&#34;&gt;Practice H3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Finally, consider the &lt;code&gt;avgfood&lt;/code&gt; variable. Fit two more multiple regressions: (1) body weight as an additive function of &lt;code&gt;avgfood&lt;/code&gt; and &lt;code&gt;groupsize&lt;/code&gt;, and (2) body weight as an additive function of all three variables, &lt;code&gt;avgfood&lt;/code&gt; and &lt;code&gt;groupsize&lt;/code&gt; and &lt;code&gt;area&lt;/code&gt;. Compare the results of these models to the previous models you&amp;rsquo;ve fit, in the first two exercises.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; First, we require the model with two variables:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m &amp;lt;- quap(
  alist(
    weight ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bf * avgfood + bg * groupsize,
    a ~ dnorm(5, 5),
    bf ~ dnorm(0, 5),
    bg ~ dnorm(0, 5),
    sigma ~ dunif(0, 5)
  ),
  data = d
)
precis(m)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean         sd       5.5%      94.5%
## a      4.1806405 0.42501186  3.5013895  4.8598915
## bf     3.6052545 1.17683527  1.7244445  5.4860646
## bg    -0.5433458 0.15271144 -0.7874082 -0.2992834
## sigma  1.1166611 0.07332213  0.9994782  1.2338441
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we need the model with three variables:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m &amp;lt;- quap(
  alist(
    weight ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bf * avgfood + bg * groupsize + ba * area,
    a ~ dnorm(5, 5),
    bf ~ dnorm(0, 5),
    bg ~ dnorm(0, 5),
    ba ~ dnorm(0, 5),
    sigma ~ dunif(0, 5)
  ),
  data = d
)
precis(m)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean         sd        5.5%      94.5%
## a      4.1010815 0.42308541  3.42490933  4.7772537
## bf     2.3024285 1.39359133  0.07520038  4.5296566
## bg    -0.5926614 0.15385399 -0.83854977 -0.3467730
## ba     0.4017410 0.23609446  0.02441642  0.7790655
## sigma  1.1044452 0.07252194  0.98854116  1.2203493
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-a&#34;&gt;Part A&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Is &lt;code&gt;avgfood&lt;/code&gt; or &lt;code&gt;area&lt;/code&gt; a better predictor of body weight? If you had to choose one or the other to include in a model, which would it be? Support your assessment with any tables or plots you choose.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; According to intuition, I would prefer &lt;code&gt;avgfood&lt;/code&gt; because it is directly and obviously linked to a gain in calories and thus &lt;code&gt;weight&lt;/code&gt; whereas &lt;code&gt;area&lt;/code&gt; is a fairly indirect relationship. However, I still want to test this in &lt;code&gt;R&lt;/code&gt;. Let&amp;rsquo;s first look at the raw data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggpairs(d[, -1])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-21-statistical-rethinking-chapter-05_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;avgfood&lt;/code&gt; and &lt;code&gt;area&lt;/code&gt; are strikingly correlated with one another. To chose the most informative of these two, I build models of &lt;code&gt;weight&lt;/code&gt; solely dependant on standardised records of both (so the slope estimates are comparable in their magnitude) as well as &lt;code&gt;groupsize&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Average Food
d$avgfood.s &amp;lt;- (d$avgfood - mean(d$avgfood)) / sd(d$avgfood)
m &amp;lt;- quap(
  alist(
    weight ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bf * avgfood.s + bg * groupsize,
    a ~ dnorm(5, 5),
    bf ~ dnorm(0, 5),
    bg ~ dnorm(0, 5),
    sigma ~ dunif(0, 5)
  ),
  data = d
)
precis(m)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean         sd       5.5%      94.5%
## a      6.9566906 0.67995576  5.8699900  8.0433913
## bf     0.7450529 0.23844151  0.3639773  1.1261284
## bg    -0.5588002 0.15473498 -0.8060966 -0.3115038
## sigma  1.1166194 0.07331182  0.9994530  1.2337859
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Area
d$area.s &amp;lt;- (d$area - mean(d$area)) / sd(d$area)
m &amp;lt;- quap(
  alist(
    weight ~ dnorm(mu, sigma),
    mu &amp;lt;- a + ba * area.s + bg * groupsize,
    a ~ dnorm(5, 5),
    ba ~ dnorm(0, 5),
    bg ~ dnorm(0, 5),
    sigma ~ dunif(0, 5)
  ),
  data = d
)
precis(m)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean         sd       5.5%      94.5%
## a      6.3903420 0.53148096  5.5409327  7.2397512
## ba     0.5682683 0.18494820  0.2726854  0.8638512
## bg    -0.4283914 0.12001993 -0.6202064 -0.2365764
## sigma  1.1184575 0.07343099  1.0011006  1.2358144
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Given these results, I prefer the stronger relationship between &lt;code&gt;weight&lt;/code&gt; and &lt;code&gt;avgfood&lt;/code&gt; over that relying on &lt;code&gt;area&lt;/code&gt; and would chose a model using only &lt;code&gt;avgfood&lt;/code&gt;.&lt;/p&gt;
&lt;h4 id=&#34;part-b&#34;&gt;Part B&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; When both &lt;code&gt;avgfood&lt;/code&gt; or &lt;code&gt;area&lt;/code&gt; are in the same model, their effects are reduced (closer to zero) and their standard errors are larger than when they are included in separate models. Can you explain this result?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; I am pretty sure that this is our old friend &lt;em&gt;multicollinearity&lt;/em&gt; rearing its ugly head. Since the two are highly correlated, their respective effects become less when controlling for either in the same model.&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.5 (2021-03-31)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19043)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.1252  LC_CTYPE=English_United Kingdom.1252    LC_MONETARY=English_United Kingdom.1252 LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.1252    
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] dagitty_0.3-1        GGally_2.1.2         rethinking_2.13      rstan_2.21.2         ggplot2_3.3.6        StanHeaders_2.21.0-7
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.7         mvtnorm_1.1-1      lattice_0.20-41    prettyunits_1.1.1  ps_1.6.0           assertthat_0.2.1   digest_0.6.27      utf8_1.2.1         V8_3.4.1           plyr_1.8.6        
## [11] R6_2.5.0           backports_1.2.1    stats4_4.0.5       evaluate_0.14      coda_0.19-4        highr_0.9          blogdown_1.3       pillar_1.6.0       rlang_0.4.11       curl_4.3.2        
## [21] callr_3.7.0        jquerylib_0.1.4    R.utils_2.10.1     R.oo_1.24.0        rmarkdown_2.7      styler_1.4.1       labeling_0.4.2     stringr_1.4.0      loo_2.4.1          munsell_0.5.0     
## [31] compiler_4.0.5     xfun_0.22          pkgconfig_2.0.3    pkgbuild_1.2.0     shape_1.4.5        htmltools_0.5.1.1  tidyselect_1.1.0   tibble_3.1.1       gridExtra_2.3      bookdown_0.22     
## [41] codetools_0.2-18   matrixStats_0.61.0 reshape_0.8.8      fansi_0.4.2        crayon_1.4.1       dplyr_1.0.5        withr_2.4.2        MASS_7.3-53.1      R.methodsS3_1.8.1  grid_4.0.5        
## [51] jsonlite_1.7.2     gtable_0.3.0       lifecycle_1.0.0    DBI_1.1.1          magrittr_2.0.1     scales_1.1.1       RcppParallel_5.1.2 cli_3.0.0          stringi_1.5.3      farver_2.1.0      
## [61] bslib_0.2.4        ellipsis_0.3.2     generics_0.1.0     vctrs_0.3.7        boot_1.3-27        rematch2_2.1.2     RColorBrewer_1.1-2 tools_4.0.5        R.cache_0.14.0     glue_1.4.2        
## [71] purrr_0.3.4        processx_3.5.1     yaml_2.2.1         inline_0.3.17      colorspace_2.0-0   knitr_1.33         sass_0.3.1
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 06</title>
      <link>https://www.erikkusch.com/courses/rethinking/chapter-06/</link>
      <pubDate>Thu, 28 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/rethinking/chapter-06/</guid>
      <description>&lt;h1 id=&#34;the-haunted-dag--the-causal-terror&#34;&gt;The Haunted DAG &amp;amp; The Causal Terror&lt;/h1&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/rethinking/6__29-01-2021_SUMMARY_-Confounds.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides Chapter 6&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to the exercises at the end of chapter 6 in 
&lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Satistical Rethinking 2&lt;/a&gt; by Richard McElreath. I have created these notes as a part of my ongoing involvement in the 
&lt;a href=&#34;https://www.erikkusch.com/project/aubayes/&#34;&gt;AU Bayes Study Group&lt;/a&gt;. Much of my inspiration for these solutions, where necessary, has been obtained from 
&lt;a href=&#34;https://sr2-solutions.wjakethompson.com/more-linear-models.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jake Thompson&lt;/a&gt;. The PDF version of 
&lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Satistical Rethinking 2&lt;/a&gt; is lacking some exercises of the print version, it seems. I do not address these here.&lt;/p&gt;
&lt;h2 id=&#34;medium-exercises&#34;&gt;Medium Exercises&lt;/h2&gt;
&lt;p&gt;We are stepping right into &lt;code&gt;R&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rethinking)
library(dagitty)
library(ggdag)
library(ggplot2)
library(tidybayes)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-m1&#34;&gt;Practice M1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Modify the DAG on page 190 to include the variable $V$, an unobserved cause of $C$ and $Y$: variables $C ← V → Y$. Reanalyze the DAG. How many paths connect $X$ to $Y$? Which must be closed? Which should you condition on now?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  Let&amp;rsquo;s start by assigning some coordinates and names of our variables which will end up as nodes in our DAG:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dag_coords &amp;lt;- data.frame(
  name = c(&amp;quot;X&amp;quot;, &amp;quot;U&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;C&amp;quot;, &amp;quot;Y&amp;quot;, &amp;quot;V&amp;quot;),
  x = c(1, 1, 2, 2, 3, 3, 3.5),
  y = c(1, 2, 2.5, 1.5, 2, 1, 1.5)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I add the actual path specifications and make a DAG object:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;DAG_m1 &amp;lt;- dagify(Y ~ X + C + V,
  X ~ U,
  U ~ A,
  B ~ U + C,
  C ~ A + V,
  coords = dag_coords
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, I plot the resulting object:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(DAG_m1, aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_point(shape = 1, stroke = 2, color = &amp;quot;black&amp;quot;) +
  geom_dag_text(color = &amp;quot;black&amp;quot;, size = 10) +
  geom_dag_edges(
    edge_color = &amp;quot;black&amp;quot;, edge_width = 2,
    arrow_directed = grid::arrow(
      length = grid::unit(15, &amp;quot;pt&amp;quot;),
      type = &amp;quot;closed&amp;quot;
    )
  ) +
  theme_void()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-28-statistical-rethinking-chapter-06_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The original DAG on page 190 boasted the following two paths which needed closing:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$X &amp;lt;- U &amp;lt;- A -&amp;gt; C -&amp;gt; Y$&lt;/li&gt;
&lt;li&gt;$X &amp;lt;- U -&amp;gt; B &amp;lt;- C -&amp;gt; Y$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These remain unaltered. Through our novel inclusion of $V$, we now have another two paths that require closing between $X$ and $Y$:&lt;br&gt;
3. $X &amp;lt;- U &amp;lt;- A -&amp;gt; C &amp;lt;- V -&amp;gt; Y$&lt;br&gt;
4. $X &amp;lt;- U -&amp;gt; B &amp;lt;- C &amp;lt;- V -&amp;gt; Y$&lt;/p&gt;
&lt;p&gt;Now, how do we close these paths? First of all, paths 2 and 4 are already closed because $B$ acts as a collider. Path 3 is also closed as $C$ acts as a collider (Thanks to 
&lt;a href=&#34;https://twitter.com/ruxandratesloi1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ruxandra Tesloianu&lt;/a&gt; for pointing this out to me). Path 1 requires closing since $A$ is a fork. If we leave it up to &lt;code&gt;R&lt;/code&gt;, we would condition on the following variables to close all paths between $X$ and $Y$:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;adjustmentSets(DAG_m1, exposure = &amp;quot;X&amp;quot;, outcome = &amp;quot;Y&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## { C, V }
## { A }
## { U }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Alright, let&amp;rsquo;s think about this. Our above DAG does not yet know which variables are unobserved. &lt;code&gt;R&lt;/code&gt; suggests we condition on $C$ and $V$. That&amp;rsquo;s going to be impossible since we don&amp;rsquo;t have data for $V$. Next, we are pointed towards conditioning on $A$ as an alternative. That looks alright. Thirdly, we are prompted to consider conditioning on $U$. Again, we don&amp;rsquo;t have data for that. So, we are only left with one option: &lt;strong&gt;Condition on $A$&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Finally, let&amp;rsquo;s actually give &lt;code&gt;R&lt;/code&gt; all the information we have and rerun the &lt;code&gt;adjustmentSets()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;DAG_m1 &amp;lt;- dagitty(&amp;quot;dag { U [unobserved]
                          V [unobserved]
                          X -&amp;gt; Y
                          X &amp;lt;- U &amp;lt;- A -&amp;gt; C -&amp;gt; Y
                          U -&amp;gt; B &amp;lt;- C
                          C &amp;lt;- V -&amp;gt; Y }&amp;quot;)

adjustmentSets(DAG_m1, exposure = &amp;quot;X&amp;quot;, outcome = &amp;quot;Y&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## { A }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Cool. That&amp;rsquo;s exactly the solution we arrived at earlier as well.&lt;/p&gt;
&lt;h2 id=&#34;hard-exercises&#34;&gt;Hard Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-h1&#34;&gt;Practice H1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Use the Waffle House data, &lt;code&gt;data(WaffleDivorce)&lt;/code&gt;, to find the total causal influence of number of Waffle Houses on divorce rate. Justify your model or models with a causal graph.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  Let&amp;rsquo;s start by recreating the DAG on page 191 with some code from page 192 while sprucing it up with some coordinates for our nodes in the DAG:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Define Paths
DAG_h1 &amp;lt;- dagitty(&amp;quot;dag {
    A -&amp;gt; D
    A -&amp;gt; M -&amp;gt; D
    A &amp;lt;- S -&amp;gt; M
    S -&amp;gt; W -&amp;gt; D
  }&amp;quot;)
# Add Coordinates
coordinates(DAG_h1) &amp;lt;- list(
  x = c(A = 1, S = 1, M = 2, W = 3, D = 3),
  y = c(A = 1, S = 3, M = 2, W = 3, D = 1)
)
# Plotting
ggplot(DAG_h1, aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_text(color = &amp;quot;black&amp;quot;, size = 10) +
  geom_dag_edges(
    edge_color = &amp;quot;black&amp;quot;, edge_width = 2,
    arrow_directed = grid::arrow(
      length = grid::unit(15, &amp;quot;pt&amp;quot;),
      type = &amp;quot;closed&amp;quot;
    )
  ) +
  theme_void()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-28-statistical-rethinking-chapter-06_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s check which variables we need to condition on to allow any subsequent model to identify the causal relationship between $W$ and $D$:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;adjustmentSets(DAG_h1, exposure = &amp;quot;W&amp;quot;, outcome = &amp;quot;D&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## { A, M }
## { S }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We could either condition on $A$ and $M$, or condition only on $S$. The latter seems simpler to me, so I&amp;rsquo;ll run with that! On to build that model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Loading Data
data(WaffleDivorce)
d &amp;lt;- WaffleDivorce
## Scaling Relevant Variables
d$D &amp;lt;- scale(d$Divorce)
d$W &amp;lt;- scale(d$WaffleHouses)
d$S &amp;lt;- scale(d$South)
## Specifying and Running Model
MOD_h1 &amp;lt;- quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bS * S + bW * W,
    a ~ dnorm(0, 0.2),
    c(bS, bW) ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d
)
plot(precis(MOD_h1))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-28-statistical-rethinking-chapter-06_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Our model clearly shows that once we know about whether a state is located in the Southern Contiguous U.S., we don&amp;rsquo;t gain additional information about the local divorce rate by learning about the number of Waffle Houses in the area.&lt;/p&gt;
&lt;h3 id=&#34;practice-h2&#34;&gt;Practice H2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Build a series of models to test the implied conditional independencies of the causal graph you used in the previous problem. If any of the tests fail, how do you think the graph needs to be amended? Does the graph need more or fewer arrows? Feel free to nominate variables that aren&amp;rsquo;t in the data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  Let&amp;rsquo;s start by letting &lt;code&gt;R&lt;/code&gt; identify all the implied conditional independecies:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;impliedConditionalIndependencies(DAG_h1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## A _||_ W | S
## D _||_ S | A, M, W
## M _||_ W | S
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are three models we need to build to assertain the conditional independencies here.&lt;/p&gt;
&lt;p&gt;Firstly, I am reloading the data and standardise all variables of interest:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Loading Data
data(WaffleDivorce)
d &amp;lt;- WaffleDivorce
## Scaling Relevant Variables
d$A &amp;lt;- scale(d$MedianAgeMarriage)
d$D &amp;lt;- scale(d$Divorce)
d$M &amp;lt;- scale(d$Marriage)
d$W &amp;lt;- scale(d$WaffleHouses)
d$S &amp;lt;- scale(d$South)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let&amp;rsquo;s get going with the actual models:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A &lt;em&gt;||&lt;/em&gt; W | S&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;MOD_h2a &amp;lt;- quap(
  alist(
    A ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bS * S + bW * W,
    a ~ dnorm(0, 0.2),
    c(bS, bW) ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d
)
plot(precis(MOD_h2a))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-28-statistical-rethinking-chapter-06_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Conditional independence of $A$ of $W$ given $S$ - confirmed!&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;D &lt;em&gt;||&lt;/em&gt; S | A, M, W&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;MOD_h2b &amp;lt;- quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bA * A + bS * S + bM * M + bW * W,
    a ~ dnorm(0, 0.2),
    c(bA, bS, bM, bW) ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d
)
plot(precis(MOD_h2b))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-28-statistical-rethinking-chapter-06_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Conditional independence of $D$ of $S$ given $A$, $M$, and $W$ - confirmed!&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;M &lt;em&gt;||&lt;/em&gt; W | S&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;MOD_h2c &amp;lt;- quap(
  alist(
    M ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bS * S + bW * W,
    a ~ dnorm(0, 0.2),
    c(bS, bW) ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d
)
plot(precis(MOD_h2c))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-28-statistical-rethinking-chapter-06_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Conditional independence of $M$ of $W$ given $S$ - confirmed!&lt;/p&gt;
&lt;p&gt;I finish this exercise by looking at only the relevant posteriors for each model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Plot_df &amp;lt;- data.frame(
  Posteriors = c(
    extract.samples(MOD_h2a, n = 1e4)$bW,
    extract.samples(MOD_h2b, n = 1e4)$bS,
    extract.samples(MOD_h2c, n = 1e4)$bW
  ),
  Name = rep(c(&amp;quot;bw&amp;quot;, &amp;quot;bS&amp;quot;, &amp;quot;bw&amp;quot;), each = 1e4),
  Model = rep(c(&amp;quot;h2_a&amp;quot;, &amp;quot;h2_b&amp;quot;, &amp;quot;h2_c&amp;quot;), each = 1e4)
)

lbls &amp;lt;- c(
  expression(&amp;quot;Model 1:&amp;quot; ~ beta[W]),
  expression(&amp;quot;Model 2:&amp;quot; ~ beta[S]),
  expression(&amp;quot;Model 3:&amp;quot; ~ beta[W])
)

ggplot(Plot_df, aes(y = Model, x = Posteriors)) +
  stat_halfeye() +
  scale_y_discrete(labels = lbls) +
  labs(x = &amp;quot;Parameter Estimate&amp;quot;, y = &amp;quot;Implied Conditional Independency&amp;quot;) +
  theme_bw() +
  geom_vline(xintercept = 0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-28-statistical-rethinking-chapter-06_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.5 (2021-03-31)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19043)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.1252  LC_CTYPE=English_United Kingdom.1252    LC_MONETARY=English_United Kingdom.1252 LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.1252    
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] tidybayes_2.3.1      ggdag_0.2.3          dagitty_0.3-1        rethinking_2.13      rstan_2.21.2         ggplot2_3.3.6        StanHeaders_2.21.0-7
## 
## loaded via a namespace (and not attached):
##  [1] matrixStats_0.61.0   R.cache_0.14.0       tools_4.0.5          backports_1.2.1      bslib_0.2.4          utf8_1.2.1           R6_2.5.0             DBI_1.1.1            colorspace_2.0-0    
## [10] ggdist_2.4.0         withr_2.4.2          tidyselect_1.1.0     gridExtra_2.3        prettyunits_1.1.1    processx_3.5.1       curl_4.3.2           compiler_4.0.5       cli_3.0.0           
## [19] arrayhelpers_1.1-0   labeling_0.4.2       bookdown_0.22        sass_0.3.1           scales_1.1.1         mvtnorm_1.1-1        callr_3.7.0          stringr_1.4.0        digest_0.6.27       
## [28] rmarkdown_2.7        R.utils_2.10.1       pkgconfig_2.0.3      htmltools_0.5.1.1    styler_1.4.1         highr_0.9            rlang_0.4.11         shape_1.4.5          jquerylib_0.1.4     
## [37] farver_2.1.0         generics_0.1.0       svUnit_1.0.6         jsonlite_1.7.2       dplyr_1.0.5          R.oo_1.24.0          distributional_0.2.2 inline_0.3.17        magrittr_2.0.1      
## [46] loo_2.4.1            Rcpp_1.0.7           munsell_0.5.0        fansi_0.4.2          viridis_0.6.0        lifecycle_1.0.0      R.methodsS3_1.8.1    stringi_1.5.3        yaml_2.2.1          
## [55] ggraph_2.0.5         MASS_7.3-53.1        pkgbuild_1.2.0       plyr_1.8.6           grid_4.0.5           ggrepel_0.9.1        forcats_0.5.1        crayon_1.4.1         lattice_0.20-41     
## [64] graphlayouts_0.7.1   knitr_1.33           ps_1.6.0             pillar_1.6.0         igraph_1.2.6         boot_1.3-27          codetools_0.2-18     stats4_4.0.5         glue_1.4.2          
## [73] evaluate_0.14        blogdown_1.3         V8_3.4.1             RcppParallel_5.1.2   vctrs_0.3.7          tweenr_1.0.2         gtable_0.3.0         purrr_0.3.4          polyclip_1.10-0     
## [82] tidyr_1.1.3          rematch2_2.1.2       assertthat_0.2.1     xfun_0.22            ggforce_0.3.3        tidygraph_1.2.0      coda_0.19-4          viridisLite_0.4.0    tibble_3.1.1        
## [91] ellipsis_0.3.2
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 07</title>
      <link>https://www.erikkusch.com/courses/rethinking/chapter-07/</link>
      <pubDate>Thu, 04 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/rethinking/chapter-07/</guid>
      <description>&lt;h1 id=&#34;ulysses-compass&#34;&gt;Ulysses&#39; Compass&lt;/h1&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/rethinking/7__05-02-2021_SUMMARY_-Model-Evaluation.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides Chapter 7&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to the exercises at the end of chapter 7 in 
&lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Satistical Rethinking 2&lt;/a&gt; by Richard McElreath. I have created these notes as a part of my ongoing involvement in the 
&lt;a href=&#34;https://www.erikkusch.com/project/aubayes/&#34;&gt;AU Bayes Study Group&lt;/a&gt;. Much of my inspiration for these solutions, where necessary, has been obtained from 
&lt;a href=&#34;https://jmgirard.com/statistical-rethinking-ch6/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jeffrey Girard&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;r-environment&#34;&gt;&lt;code&gt;R&lt;/code&gt; Environment&lt;/h2&gt;
&lt;p&gt;For today&amp;rsquo;s exercise, I load the following packages:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rethinking)
library(ggplot2)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;easy-exercises&#34;&gt;Easy Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-e1&#34;&gt;Practice E1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; State the three motivating criteria that define information entropy. Try to express each in your own words.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; The principle of information theory is motivated by the three following criteria:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Continuity&lt;/em&gt;. Uncertainty must be measured on a continuous scale of equal intervals to ensure comparability.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Additivity&lt;/em&gt;. Total uncertainty is derived by adding up the uncertainties associated with each prediction.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Scalability&lt;/em&gt;. Uncertainty scales with number of possible outcomes to reflect changes in certainty just by virtue of different numbers of possible outcomes.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;practice-e2&#34;&gt;Practice E2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Suppose a coin is weighted such that, when it is tossed and lands on a table, it comes up heads 70% of the time. What is the entropy of this coin?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  Following the formula 7.1 on page 210:&lt;/p&gt;
&lt;p&gt;$H(p)=−\sum_{i=1}^n p_ilog(p_i) = −(p_Hlog(p_H)+p_Tlog(p_T))$&lt;/p&gt;
&lt;p&gt;with $p_H$ being probability of heads, and $p_T$ being probability of tails, we can simply plug in our values as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;p &amp;lt;- c(0.7, 0.3)
-sum(p * log(p))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6108643
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thus, the entropy is 0.61.&lt;/p&gt;
&lt;h3 id=&#34;practice-e3&#34;&gt;Practice E3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Suppose a four-sided die is loaded such that, when tossed onto a table, it shows “1” 20%, “2” 25%, “3” 25%, and “4” 30% of the time. What is the entropy of this die?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  Again, we use the formula approach as above:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;p &amp;lt;- c(0.2, 0.25, 0.25, 0.3)
-sum(p * log(p))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.376227
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The entropy of our D4 is 1.38.&lt;/p&gt;
&lt;h3 id=&#34;practice-e4&#34;&gt;Practice E4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Suppose another four-sided die is loaded such that it never shows “4”. The other three sides show equally often. What is the entropy of this die?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; By knowing that one side never shows up, we can omit it altogether and are now looking at a perfectly balanced D3 rather than a D4:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;p &amp;lt;- c(1 / 3, 1 / 3, 1 / 3)
-sum(p * log(p))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.098612
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I have never seen a D3 in real-life, but we could easily imagine a D6 where the numbers 1 through 3 show up twice each. The entropy of our D3 is 1.1.&lt;/p&gt;
&lt;h2 id=&#34;medium-exercises&#34;&gt;Medium Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-m1&#34;&gt;Practice M1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Write down and compare the definitions of AIC, and WAIC. Which of these criteria is most general? Which assumptions are required to transform a more general criterion into a less general one?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;AIC&lt;/em&gt; $= D_{train}+2p$; ($D_{train} =$ in-sample deviance, $p =$ number of parameters estimated in the model). It is built on the assumptions that:&lt;br&gt;
A) Priors are flat or overwhelmed by model&lt;br&gt;
B) Posterior distribution is approximately multivariate Gaussian&lt;br&gt;
C) Sample size $&amp;raquo;$ Number of parameters&lt;/li&gt;
&lt;li&gt;&lt;em&gt;WAIC&lt;/em&gt; $= −2(lppd −\sum_i(var_θ log p(y_i|θ))$; ($y_i =$ observation $i$, $θ =$ posterior distribution, $lppd(y, Θ) =\sum_ilog\frac{1}{S} \sum_Sp(y_i|Θ_s) =$ log-pointwise-predictive-density) . It is built on the assumptions that: Sample size $&amp;raquo;$ Number of parameters.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;WAIC is clearly the more general method here as it comes with less assumptions. To transform WAIC into AIC, we need to assume A. and B. of the assumptions of AIC.&lt;/p&gt;
&lt;h3 id=&#34;practice-m2&#34;&gt;Practice M2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Explain the difference between model &lt;em&gt;selection&lt;/em&gt; and model &lt;em&gt;comparison&lt;/em&gt;. What information is lost under model selection?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Model selection and model comparison both use information criteria and/or cross-validation exercises to determine goddess of fit of models to data set (their explanatory power) as well as their accuracy in terms of making inferences (their predictive power). Where they differ is what these approaches do with the models at hand once the desired information is obtained. In &lt;em&gt;model selection&lt;/em&gt; all but the &amp;ldquo;best&amp;rdquo; model is discarded, whereas under &lt;em&gt;model comparison&lt;/em&gt; we use our new-found information to identify relative model accuracy to assess the influences of different parameters in different models. The latter can lead to understanding causal relationships and identification of confounds in the different models.&lt;/p&gt;
&lt;h3 id=&#34;practice-m3&#34;&gt;Practice M3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; When comparing models with an information criterion, why must all models be fit to exactly the same observations? What would happen to the information criterion values, if the models were fit to different numbers of observations? Perform some experiments, if you are not sure.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  Information criteria are based on deviance. Deviance, in turn, is a sum and not a mean product of all observations. All else being equal, a model with more observations returns a higher deviance and thus worse accuracy according to information criteria.&lt;/p&gt;
&lt;h3 id=&#34;practice-m4&#34;&gt;Practice M4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; What happens to the effective number of parameters, as measured by PSIS or WAIC, as a prior becomes more concentrated? Why? Perform some experiments, if you are not sure.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; The effective number of parameters (or &lt;code&gt;&amp;quot;pWAIC&amp;quot;&lt;/code&gt; in the &lt;code&gt;WAIC()&lt;/code&gt; function output), is the penalty term of our regularisation approaches. As priors become more regularising (i.e. more concentrated on certain prior knowledge or assumptions), the effective number of parameters decreases.&lt;/p&gt;
&lt;p&gt;In the case of WAIC, $p_{WAIC}$ is the variance in the log-likelihoods for each observation in the training data. More concentrated priors constrain this likelihood and subsequent measure of variance, thus reducing it.&lt;/p&gt;
&lt;p&gt;As for PSIS, $P_D$ (effective number of parameters) tells us about the flexibility of the model. Increasingly regularised priors decrease the flexibility of the model.&lt;/p&gt;
&lt;h3 id=&#34;practice-m5&#34;&gt;Practice M5&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Provide an informal explanation of why informative priors reduce overfitting.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  Informative priors constrain the model by making it harder for the model to pick up on extreme parameter values and assign them high posterior probabilities.&lt;/p&gt;
&lt;h3 id=&#34;practice-m6&#34;&gt;Practice M6&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Provide an informal explanation of why overly informative priors result in underfitting.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  Informative priors can constrain a model so much that it becomes impossible for the model to change the prior distribution into an accurate posterior distribution given the data. This can especially problematic when we use informative priors that are born under false premises, stem from bad intuition, or are just plain stupid.&lt;/p&gt;
&lt;h2 id=&#34;hard-exercises&#34;&gt;Hard Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-h1&#34;&gt;Practice H1&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://www.erikkusch.com/courses/rethinking/7H1.JPG&#34; align = &#34;right&#34; width = 272/&gt;  &lt;strong&gt;Question:&lt;/strong&gt; In 2007, The Wall Street Journal published an editorial (“We’re Number One, Alas”) with a graph of corporate tax rates in 29 countries plotted against tax Revenue. A badly fit curve was drawn in (reconstructed to the right), seemingly by hand, to make the argument that the relationship between tax rate and tax Revenue increases and then declines, such that higher tax rates can actually produce less tax Revenue. I want you to actually fit a curve to these data, found in &lt;code&gt;data(Laffer)&lt;/code&gt;. Consider models that use tax rate to predict tax Revenue. Compare, using WAIC or PSIS, a straight-line model to any curved models you like. What do you conclude about the relationship between tax rate and tax Revenue?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; First, I begin by loading the data and standardising my variables:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# data preparation
data(Laffer)
d &amp;lt;- Laffer
d$Rate &amp;lt;- standardize(d$tax_rate)
d$Revenue &amp;lt;- standardize(d$tax_revenue)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this preparation out of the way, I am ready to run three models: Linear, Quadratic, and Cubic. I could run many more than these, but I wager this will be enough.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# linear model
m7H1a &amp;lt;- quap(
  alist(
    Revenue ~ dnorm(mu, sigma),
    mu &amp;lt;- a + b * Rate,
    a ~ dnorm(0, 0.2),
    b ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d
)

# quadratic model
m7H1b &amp;lt;- quap(
  alist(
    Revenue ~ dnorm(mu, sigma),
    mu &amp;lt;- a + b * Rate + b2 * Rate^2,
    a ~ dnorm(0, 0.2),
    c(b, b2) ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d
)

# cubic model
m7H1c &amp;lt;- quap(
  alist(
    Revenue ~ dnorm(mu, sigma),
    mu &amp;lt;- a + b * Rate + b2 * Rate^2 + b3 * Rate^3,
    a ~ dnorm(0, 0.2),
    c(b, b2, b3) ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d
)

# Comparing Models
comparison &amp;lt;- compare(m7H1a, m7H1b, m7H1c)
comparison
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           WAIC       SE     dWAIC      dSE    pWAIC    weight
## m7H1a 87.98410 21.63416 0.0000000       NA 5.528420 0.4448646
## m7H1b 88.79927 24.58351 0.8151711 3.382774 6.990224 0.2959482
## m7H1c 89.06454 24.19943 1.0804389 3.089764 7.070477 0.2591872
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;According to these WAIC values and their Standard Deviations, I cannot make a clear statement as to which relationship between tax rate and tax revenue should be assumed.&lt;/p&gt;
&lt;p&gt;Let me plot these to make a clearer image of what I mean:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## base sequence for predictions
plot_df &amp;lt;- data.frame(Rate = seq(from = min(d$Rate), to = max(d$Rate), length.out = 1e4))
## Predictions for Linear Model
plot_df$m7H1a &amp;lt;- apply(link(m7H1a, data = plot_df), 2, mean)
plot_df$m7H1aLower &amp;lt;- apply(link(m7H1a, data = plot_df), 2, PI, prob = .95)[1, ]
plot_df$m7H1aUpper &amp;lt;- apply(link(m7H1a, data = plot_df), 2, PI, prob = .95)[2, ]
## Predictions for Quadratic Model
plot_df$m7H1b &amp;lt;- apply(link(m7H1b, data = plot_df), 2, mean)
plot_df$m7H1bLower &amp;lt;- apply(link(m7H1b, data = plot_df), 2, PI, prob = .95)[1, ]
plot_df$m7H1bUpper &amp;lt;- apply(link(m7H1b, data = plot_df), 2, PI, prob = .95)[2, ]
## Predictions for Cubic Model
plot_df$m7H1c &amp;lt;- apply(link(m7H1c, data = plot_df), 2, mean)
plot_df$m7H1cLower &amp;lt;- apply(link(m7H1c, data = plot_df), 2, PI, prob = .95)[1, ]
plot_df$m7H1cUpper &amp;lt;- apply(link(m7H1c, data = plot_df), 2, PI, prob = .95)[2, ]
## Plotting
ggplot(plot_df) +
  geom_point(data = d, aes(x = Rate, y = Revenue), size = 2) +
  geom_line(data = plot_df, aes(y = m7H1a, x = Rate, colour = &amp;quot;Linear&amp;quot;), size = 1.5) +
  geom_ribbon(data = plot_df, aes(ymin = m7H1aLower, ymax = m7H1aUpper, x = Rate), alpha = .1) +
  geom_line(data = plot_df, aes(y = m7H1b, x = Rate, colour = &amp;quot;Quadratic&amp;quot;), size = 1.5) +
  geom_ribbon(data = plot_df, aes(ymin = m7H1bLower, ymax = m7H1bUpper, x = Rate), alpha = .1) +
  geom_line(data = plot_df, aes(y = m7H1c, x = Rate, colour = &amp;quot;Cubic&amp;quot;), size = 1.5) +
  geom_ribbon(data = plot_df, aes(ymin = m7H1cLower, ymax = m7H1cUpper, x = Rate), alpha = .1) +
  theme_bw() +
  scale_colour_discrete(name = &amp;quot;Model&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-28-statistical-rethinking-chapter-07_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I think this highlights quite well just how little difference there is in how the models understand the data. What the Wall Street Journal did there was (quite unsurprisingly) utter trite.&lt;/p&gt;
&lt;h3 id=&#34;practice-h2&#34;&gt;Practice H2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; In the &lt;code&gt;Laffer&lt;/code&gt; data, there is one country with a high tax revenue that is an outlier. Use PSIS and WAIC to measure the importance of this outlier in the models you fit in the previous problem. Then use robust regression with a Student’s t distribution to revisit the curve fitting problem. How much does a curved relationship depend upon the outlier point?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Using the &lt;code&gt;rethinking&lt;/code&gt; package, we could identify the outlier by a very high WAIC value in the output of &lt;code&gt;WAIC(..., pointwise = TRUE)&lt;/code&gt;, where &lt;code&gt;...&lt;/code&gt; represents our model name. From the plot, we already now that our outlier is the country with the highest tax revenue, so let&amp;rsquo;s remove that one:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# data preparation
data(Laffer)
d &amp;lt;- Laffer
d$Rate &amp;lt;- standardize(d$tax_rate)
d$Revenue &amp;lt;- standardize(d$tax_revenue)
d &amp;lt;- d[d$tax_revenue != max(d$tax_revenue), ] # removing the outlier
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this preparation out of the way, I am ready to run three models again:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# linear model
m7H2a &amp;lt;- quap(
  alist(
    Revenue ~ dnorm(mu, sigma),
    mu &amp;lt;- a + b * Rate,
    a ~ dnorm(0, 0.2),
    b ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d
)

# quadratic model
m7H2b &amp;lt;- quap(
  alist(
    Revenue ~ dnorm(mu, sigma),
    mu &amp;lt;- a + b * Rate + b2 * Rate^2,
    a ~ dnorm(0, 0.2),
    c(b, b2) ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d
)

# cubic model
m7H2c &amp;lt;- quap(
  alist(
    Revenue ~ dnorm(mu, sigma),
    mu &amp;lt;- a + b * Rate + b2 * Rate^2 + b3 * Rate^3,
    a ~ dnorm(0, 0.2),
    c(b, b2, b3) ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d
)
# Comparing Models
comparison &amp;lt;- compare(m7H2a, m7H2b, m7H2c)
comparison
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           WAIC       SE    dWAIC      dSE    pWAIC    weight
## m7H2b 59.61977 9.702003 0.000000       NA 4.010763 0.6272073
## m7H2c 61.37195 9.630274 1.752179 1.215336 4.863606 0.2611742
## m7H2a 63.07215 9.669994 3.452382 4.146202 4.256780 0.1116184
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Well, the models still greatly overlap in their usefulness.&lt;/p&gt;
&lt;p&gt;Next, let&amp;rsquo;s use a robust regression. I was unsure whether to revert back to the data which contains the outlier here. I chose to do so:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Revert back to full data
data(Laffer)
d &amp;lt;- Laffer
d$Rate &amp;lt;- standardize(d$tax_rate)
d$Revenue &amp;lt;- standardize(d$tax_revenue)
# linear model
m7H2aS &amp;lt;- quap(
  alist(
    Revenue ~ dstudent(2, mu, sigma),
    mu &amp;lt;- a + b * Rate,
    a ~ dnorm(0, 0.2),
    b ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d
)

# quadratic model
m7H2bS &amp;lt;- quap(
  alist(
    Revenue ~ dstudent(2, mu, sigma),
    mu &amp;lt;- a + b * Rate + b2 * Rate^2,
    a ~ dnorm(0, 0.2),
    c(b, b2) ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d
)

# cubic model
m7H2cS &amp;lt;- quap(
  alist(
    Revenue ~ dstudent(2, mu, sigma),
    mu &amp;lt;- a + b * Rate + b2 * Rate^2 + b3 * Rate^3,
    a ~ dnorm(0, 0.2),
    c(b, b2, b3) ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d
)
# Comparing Models
comparison &amp;lt;- compare(m7H2aS, m7H2bS, m7H2cS)
comparison
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            WAIC       SE    dWAIC      dSE    pWAIC     weight
## m7H2bS 70.42437 13.97355 0.000000       NA 3.726454 0.69002496
## m7H2cS 72.57488 13.76632 2.150509 1.370840 4.983440 0.23544401
## m7H2aS 74.87539 13.57939 4.451025 4.933044 4.008494 0.07453103
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What happened in comparison to our original models (including the outlier)?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;comparison &amp;lt;- compare(m7H1a, m7H1b, m7H1c, m7H2a, m7H2b, m7H2c, m7H2aS, m7H2bS, m7H2cS)
comparison
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            WAIC        SE     dWAIC       dSE    pWAIC       weight
## m7H2b  58.70723  9.337612  0.000000        NA 3.548011 6.669909e-01
## m7H2c  61.41111  9.567978  2.703873  1.160931 4.897430 1.725764e-01
## m7H2a  61.59824  9.188906  2.891004  3.943353 3.520610 1.571616e-01
## m7H2bS 70.00986 13.878681 11.302627 10.451899 3.521786 2.343072e-03
## m7H2cS 72.56361 13.867102 13.856375 10.523791 4.983524 6.535011e-04
## m7H2aS 74.30030 13.502465 15.593071 10.302465 3.722951 2.742379e-04
## m7H1b  89.68562 25.603534 30.978388 22.459013 7.469330 1.250975e-07
## m7H1a  90.11345 23.731476 31.406222 20.599460 6.639281 1.010056e-07
## m7H1c  90.25121 25.081664 31.543979 21.929293 7.672332 9.428269e-08
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Removing the outlier definitely made our models perform a lot better across the board. So did using a robust regression.&lt;/p&gt;
&lt;h3 id=&#34;practice-h3&#34;&gt;Practice H3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Consider three fictional Polynesian islands. On each there is a Royal Ornithologist charged by the king with surveying the bird population. They have each found the following proportions of 5 important bird species:&lt;/p&gt;
&lt;img src=&#34;https://www.erikkusch.com/courses/rethinking/7H3.JPG&#34; width=&#34;900&#34;/&gt;
&lt;p&gt;Notice that each row sums to 1, all the birds. This problem has two parts. It is not computationally complicated. But it is conceptually tricky. First, compute the entropy of each island’s bird distribution. Interpret these entropy values. Second, use each island’s bird distribution to predict the other two. This means to compute the K-L Divergence of each island from the others, treating each island as if it were a statistical model of the other islands. You should end up with 6 different K-L Divergence values. Which island predicts the others best? Why?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; First, let&amp;rsquo;s start with the entropies:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# First Island
p1 &amp;lt;- c(0.2, 0.2, 0.2, 0.2, 0.2)
-sum(p1 * log(p1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.609438
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Second Island
p2 &amp;lt;- c(0.8, 0.1, 0.05, 0.025, 0.025)
-sum(p2 * log(p2))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7430039
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Third Island
p3 &amp;lt;- c(0.05, 0.15, 0.7, 0.05, 0.05)
-sum(p3 * log(p3))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9836003
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Entropy is a measure of &lt;strong&gt;uncertainty&lt;/strong&gt;. The higher the entropy, the more uncertain we are of the probability density distribution at hand. Here, the entropies of the islands are ordered (in increasing order) as Island 2, Island 3, and Island 1. This tells us that there is a lot of certainty (in relative terms) of the proportions assigned to each bird species at Island 2 over the other islands. I posit that this is because of the overwhelming presence of species A in Island 2 whilst all other species presences drop drastically to almost being non-existent on Island 2. In plain terms: &amp;ldquo;We are much more certain of which species to find on an island where we know which species is the sole inhabitant when contrasted with an island where all species are present in equal proportions&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Now, let&amp;rsquo;s move on to the computation of K-L distances also known as &lt;strong&gt;Divergence&lt;/strong&gt;. Divergence is calculated as the average difference in log probability between target (p) and model (q). First things first, I need to identify the model (q). The task states to use information on two islands to obtain K-L distances to one target island. So, to identify the model, I need to average out the proportions on two islands and then compare these to the target island. For each of these pairings, I will obtain two K-L distances since these distances are not reversible in their directionality.&lt;/p&gt;
&lt;p&gt;Let me walk you through my first example of obtaining the Divergence between Island 1 and the Island 2 and 3 taken together:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Average the proportions of Island 2 and 3
(q &amp;lt;- apply(cbind(p2, p3), 1, mean))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.4250 0.1250 0.3750 0.0375 0.0375
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Divergence of Island 1 from combined Islands 2 &amp;amp; 3
(D_pq &amp;lt;- sum(p1 * log(p1 / q)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.4871152
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Divergence of combined Islands 2 &amp;amp; 3 from Island 1
(D_qp &amp;lt;- sum(q * log(q / p1)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3717826
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By using our combined knowledge of Islands 2 &amp;amp; 3 to approximate Island 1, we introduce an additional 0.49 of uncertainty. On the contrary, by using our knowledge of Island 1 to approximate the combination of Islands 2 &amp;amp; 3, we introduce an additional 0.37 of uncertainty.&lt;/p&gt;
&lt;p&gt;Now, I repeat this for the other two target islands:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;output &amp;lt;- data.frame(
  Approximated = D_pq,
  Approximator = D_qp
)
# Target: Island 2
q &amp;lt;- apply(cbind(p1, p3), 1, mean)
D_pq &amp;lt;- sum(p2 * log(p2 / q))
D_qp &amp;lt;- sum(q * log(q / p2))
output &amp;lt;- rbind(output, c(D_pq, D_qp))
# Target: Island 3
q &amp;lt;- apply(cbind(p1, p2), 1, mean)
D_pq &amp;lt;- sum(p3 * log(p3 / q))
D_qp &amp;lt;- sum(q * log(q / p3))
output &amp;lt;- rbind(output, c(D_pq, D_qp))
# output
rownames(output) &amp;lt;- c(&amp;quot;Island 1&amp;quot;, &amp;quot;Island 2&amp;quot;, &amp;quot;Island 3&amp;quot;)
output
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          Approximated Approximator
## Island 1    0.4871152    0.3717826
## Island 2    1.2387437    1.2570061
## Island 3    1.0097143    1.1184060
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As it turns out, the divergence between a &amp;ldquo;safe bet&amp;rdquo; (i.e. Island 1 where all species are equally present) and other systems is much smaller than when comparing heavily skewed probability density distributions.&lt;/p&gt;
&lt;h3 id=&#34;practice-h4&#34;&gt;Practice H4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Recall the marriage, age, and happiness collider bias example from Chapter 6. Run models &lt;code&gt;m6.9&lt;/code&gt; and &lt;code&gt;m6.10&lt;/code&gt; again. Compare these two models using WAIC (or LOO, they will produce identical results). Which model is expected to make better predictions? Which model provides the correct causal inference about the influence of age on happiness? Can you explain why the answers to these two questions disagree?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Remember that in these models, &lt;code&gt;m6.9&lt;/code&gt; shows a negative relationship between age and happiness which we know to be untrue because it conditions on the collider of marriage status which itself, is influenced by age and happiness. &lt;code&gt;m6.10&lt;/code&gt; does not condition on said collider and thus does not find a relationship between age and happiness:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## R code 6.21
d &amp;lt;- sim_happiness(seed = 1977, N_years = 1000)
## R code 6.22
d2 &amp;lt;- d[d$age &amp;gt; 17, ] # only adults
d2$A &amp;lt;- (d2$age - 18) / (65 - 18)
## R code 6.23
d2$mid &amp;lt;- d2$married + 1
m6.9 &amp;lt;- quap(
  alist(
    happiness ~ dnorm(mu, sigma),
    mu &amp;lt;- a[mid] + bA * A,
    a[mid] ~ dnorm(0, 1),
    bA ~ dnorm(0, 2),
    sigma ~ dexp(1)
  ),
  data = d2
)
## R code 6.24
m6.10 &amp;lt;- quap(
  alist(
    happiness ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bA * A,
    a ~ dnorm(0, 1),
    bA ~ dnorm(0, 2),
    sigma ~ dexp(1)
  ),
  data = d2
)
## Comparison
compare(m6.9, m6.10)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           WAIC       SE    dWAIC      dSE    pWAIC       weight
## m6.9  2713.971 37.54465   0.0000       NA 3.738532 1.000000e+00
## m6.10 3101.906 27.74379 387.9347 35.40032 2.340445 5.768312e-85
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;According to these information criteria, model &lt;code&gt;m6.9&lt;/code&gt; is a lot better performing (in terms of out-of-sample deviance). However, we know that model &lt;code&gt;m6.10&lt;/code&gt; provides the true causal relationship between age and happiness: none!&lt;/p&gt;
&lt;p&gt;So why would we want to use model &lt;code&gt;m6.9&lt;/code&gt; given the WAIC above instead of model &lt;code&gt;m6.10&lt;/code&gt;. Because by thinking we should do so, we did model selection instead of model comparison! Argh. I stepped right into that one, didn&amp;rsquo;t I? By comparing these two models we can safely say that some confounding must be taking place. WAIC would have us favour model &lt;code&gt;m6.9&lt;/code&gt; simply because it does a better job at predicting the happiness of out-of-sample individuals. Why is that? Because this model identifies the happiness of the different groups of people: the miserable unmarried as well as the ecstatic married ones. Conditioning on the collider added statistical association and so aids predictive accuracy. Thus, while doing better at &lt;strong&gt;predicting&lt;/strong&gt; model &lt;code&gt;m6.9&lt;/code&gt; fails at finding &lt;strong&gt;causality&lt;/strong&gt;. It is important to highlight that again that a model may be good at predicting things without being causally correct.&lt;/p&gt;
&lt;h3 id=&#34;practice-h5&#34;&gt;Practice H5&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Revisit the urban fox data, &lt;code&gt;data(foxes)&lt;/code&gt;, from the previous chapter’s practice problems. Use WAIC or PSIS based model comparison on five different models, each using weight as the outcome, and containing these sets of predictor variables:&lt;/p&gt;
&lt;p&gt;(1) &lt;code&gt;avgfood + groupsize + area&lt;/code&gt;&lt;br&gt;
(2) &lt;code&gt;avgfood + groupsize&lt;/code&gt;&lt;br&gt;
(3) &lt;code&gt;groupsize + area&lt;/code&gt;&lt;br&gt;
(4) &lt;code&gt;avgfood&lt;/code&gt;&lt;br&gt;
(5) &lt;code&gt;area&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Can you explain the relative differences in WAIC scores, using the fox DAG from last week’s home-work? Be sure to pay attention to the standard error of the score differences (&lt;code&gt;dSE&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; The previous chapter in the pdf version I am using did not ask any questions about the fox data. I consulted 
&lt;a href=&#34;https://sr2-solutions.wjakethompson.com/overfitting.html#chapter-7&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jake Thomspon&amp;rsquo;s Blog here&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# data loading and prepping
data(foxes)
d &amp;lt;- foxes
d$area &amp;lt;- scale(d$area)
d$avgfood &amp;lt;- scale(d$avgfood)
d$weight &amp;lt;- scale(d$weight)
d$groupsize &amp;lt;- scale(d$groupsize)
## Models
# (1) `avgfood + groupsize + area`
b7h5_1 &amp;lt;- quap(
  alist(
    weight ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bFood * avgfood + bGroup * groupsize + bArea * area,
    a ~ dnorm(0, .2),
    c(bFood, bGroup, bArea) ~ dnorm(0, 5),
    sigma ~ dexp(1)
  ),
  data = d
)
# (2) `avgfood + groupsize`
b7h5_2 &amp;lt;- quap(
  alist(
    weight ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bFood * avgfood + bGroup * groupsize,
    a ~ dnorm(0, .2),
    c(bFood, bGroup) ~ dnorm(0, 5),
    sigma ~ dexp(1)
  ),
  data = d
)
# (3) `groupsize + area`
b7h5_3 &amp;lt;- quap(
  alist(
    weight ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bGroup * groupsize + bArea * area,
    a ~ dnorm(0, .2),
    c(bGroup, bArea) ~ dnorm(0, 5),
    sigma ~ dexp(1)
  ),
  data = d
)
# (4) `avgfood`
b7h5_4 &amp;lt;- quap(
  alist(
    weight ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bFood * avgfood,
    a ~ dnorm(0, .2),
    bFood ~ dnorm(0, 5),
    sigma ~ dexp(1)
  ),
  data = d
)
# (5) `area`
b7h5_5 &amp;lt;- quap(
  alist(
    weight ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bArea * area,
    a ~ dnorm(0, .2),
    bArea ~ dnorm(0, 5),
    sigma ~ dexp(1)
  ),
  data = d
)
## Comparison
compare(b7h5_1, b7h5_2, b7h5_3, b7h5_4, b7h5_5)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            WAIC       SE      dWAIC      dSE    pWAIC      weight
## b7h5_1 323.3416 16.88472  0.0000000       NA 5.187854 0.410865167
## b7h5_2 323.9809 16.81807  0.6393574 3.894043 4.130907 0.298445216
## b7h5_3 324.0666 16.18771  0.7250103 4.207249 3.945774 0.285933698
## b7h5_4 333.5084 13.79238 10.1668387 8.659625 2.454047 0.002546821
## b7h5_5 333.7929 13.79707 10.4513608 8.704578 2.684646 0.002209099
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, the differences of in the WAIC scores all fall well within the 99% intervals of the differences:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(compare(b7h5_1, b7h5_2, b7h5_3, b7h5_4, b7h5_5))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-28-statistical-rethinking-chapter-07_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;However, we can see that models &lt;code&gt;b7h5_1&lt;/code&gt;, &lt;code&gt;b7h5_2&lt;/code&gt;, and &lt;code&gt;b7h5_3&lt;/code&gt; are nearly identical in their out-of-sample deviance, as are models &lt;code&gt;b7h5_4&lt;/code&gt; and &lt;code&gt;b7h5_5&lt;/code&gt;. To understand this, we want to look at the DAG that underlies this example:&lt;/p&gt;
&lt;img src=&#34;https://www.erikkusch.com/courses/rethinking/FoxDAG.png&#34;/&gt;
&lt;p&gt;Models &lt;code&gt;b7h5_1&lt;/code&gt;, &lt;code&gt;b7h5_2&lt;/code&gt;, and &lt;code&gt;b7h5_3&lt;/code&gt; all use &lt;code&gt;groupsize&lt;/code&gt; and one of/both &lt;code&gt;area&lt;/code&gt; and/or &lt;code&gt;avgfood&lt;/code&gt;. Consequently, all of these models fair the same in their predictive power because there are no open backdoor paths from either &lt;code&gt;area&lt;/code&gt; or &lt;code&gt;avgfood&lt;/code&gt;, as soon as &lt;code&gt;groupsize&lt;/code&gt; is used in conjunction. In other words, the effect of &lt;code&gt;area&lt;/code&gt; while adjusting for &lt;code&gt;groupsize&lt;/code&gt; is the same as the effect of &lt;code&gt;avgfood&lt;/code&gt; while adjusting for &lt;code&gt;groupsize&lt;/code&gt;, because the effect of &lt;code&gt;area&lt;/code&gt; is routed entirely through &lt;code&gt;avgfood&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Likewise, models &lt;code&gt;b7h5_4&lt;/code&gt; and &lt;code&gt;b7h5_5&lt;/code&gt; are nearly identical because these two only contain &lt;code&gt;area&lt;/code&gt; or &lt;code&gt;avgfood&lt;/code&gt; in isolation and all information of &lt;code&gt;area&lt;/code&gt; onto &lt;code&gt;weight&lt;/code&gt; must pass through &lt;code&gt;avgfood&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.5 (2021-03-31)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19043)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.1252  LC_CTYPE=English_United Kingdom.1252    LC_MONETARY=English_United Kingdom.1252 LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.1252    
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] rethinking_2.13      rstan_2.21.2         ggplot2_3.3.6        StanHeaders_2.21.0-7
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.7         mvtnorm_1.1-1      lattice_0.20-41    prettyunits_1.1.1  ps_1.6.0           assertthat_0.2.1   digest_0.6.27      utf8_1.2.1         V8_3.4.1           R6_2.5.0          
## [11] backports_1.2.1    stats4_4.0.5       evaluate_0.14      coda_0.19-4        highr_0.9          blogdown_1.3       pillar_1.6.0       rlang_0.4.11       curl_4.3.2         callr_3.7.0       
## [21] jquerylib_0.1.4    R.utils_2.10.1     R.oo_1.24.0        rmarkdown_2.7      styler_1.4.1       labeling_0.4.2     stringr_1.4.0      loo_2.4.1          munsell_0.5.0      compiler_4.0.5    
## [31] xfun_0.22          pkgconfig_2.0.3    pkgbuild_1.2.0     shape_1.4.5        htmltools_0.5.1.1  tidyselect_1.1.0   tibble_3.1.1       gridExtra_2.3      bookdown_0.22      codetools_0.2-18  
## [41] matrixStats_0.61.0 fansi_0.4.2        crayon_1.4.1       dplyr_1.0.5        withr_2.4.2        MASS_7.3-53.1      R.methodsS3_1.8.1  grid_4.0.5         jsonlite_1.7.2     gtable_0.3.0      
## [51] lifecycle_1.0.0    DBI_1.1.1          magrittr_2.0.1     scales_1.1.1       RcppParallel_5.1.2 cli_3.0.0          stringi_1.5.3      farver_2.1.0       bslib_0.2.4        ellipsis_0.3.2    
## [61] generics_0.1.0     vctrs_0.3.7        rematch2_2.1.2     tools_4.0.5        R.cache_0.14.0     glue_1.4.2         purrr_0.3.4        processx_3.5.1     yaml_2.2.1         inline_0.3.17     
## [71] colorspace_2.0-0   knitr_1.33         sass_0.3.1
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 08</title>
      <link>https://www.erikkusch.com/courses/rethinking/chapter-08/</link>
      <pubDate>Thu, 18 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/rethinking/chapter-08/</guid>
      <description>&lt;h1 id=&#34;conditional-manatees&#34;&gt;Conditional Manatees&lt;/h1&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/rethinking/9__19-02-2021_SUMMARY_-Interactions.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides Chapter 8&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to the exercises at the end of chapter 8 in 
&lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Satistical Rethinking 2&lt;/a&gt; by Richard McElreath. I have created these notes as a part of my ongoing involvement in the 
&lt;a href=&#34;https://www.erikkusch.com/project/aubayes/&#34;&gt;AU Bayes Study Group&lt;/a&gt;. Much of my inspiration for these solutions, where necessary, has been obtained from 
&lt;a href=&#34;https://jmgirard.com/statistical-rethinking-ch7/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jeffrey Girard&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;r-environment&#34;&gt;&lt;code&gt;R&lt;/code&gt; Environment&lt;/h2&gt;
&lt;p&gt;For today&amp;rsquo;s exercise, I load the following packages:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rethinking)
library(ggplot2)
library(viridis)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;easy-exercises&#34;&gt;Easy Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-e1&#34;&gt;Practice E1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; For each of the causal relationships below, name a hypothetical third variable that would lead to an interaction effect.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Bread dough rises because of yeast.&lt;/li&gt;
&lt;li&gt;Education leads to higher income.&lt;/li&gt;
&lt;li&gt;Gasoline makes a car go.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Temperature. Yeast is active only in a certain range of temperatures and varyingly so.&lt;/li&gt;
&lt;li&gt;Age. Time spent in a profession usually comes with raises and thus higher pay as one gets older. This is not the case in all jobs, of course.&lt;/li&gt;
&lt;li&gt;Engine efficiency will interact with the presence of gasoline to determine how much the car will go.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;practice-e2&#34;&gt;Practice E2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Which of the following explanations invokes an interaction?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Caramelizing onions requires cooking over low heat and making sure the onions do not dry out.&lt;/li&gt;
&lt;li&gt;A car will go faster when it has more cylinders or when it has a better fuel injector.&lt;/li&gt;
&lt;li&gt;Most people acquire their political beliefs from their parents, unless they get them instead from their friends.&lt;/li&gt;
&lt;li&gt;Intelligent animal species tend to be either highly social or have manipulative appendages (hands, tentacles, etc.).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Yes&lt;/em&gt;, there is an interaction here. Water and low heat interact to caramelize onions.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Yes&lt;/em&gt;, there is an interaction here. Number of cylinders and quality of fuel injector interact to determine the speed of the car.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;No&lt;/em&gt;, there is no interaction here. You either get your political belief from your parents or your friends. The two do not interact.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Yes&lt;/em&gt;, there is an interaction here. Degree of sociality and possession of manipulative appendages combine to determine intelligence level.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;practice-e3&#34;&gt;Practice E3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; For each of the explanations in E2, write a linear model that expresses the stated relationship.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\mu_i = \beta_T * T_i + \beta_W*W_i + \beta_{TW} * T_i W_i$&lt;/li&gt;
&lt;li&gt;$\mu_i = \beta_C * C_i + \beta_F * F_i + \beta_{CF} * C_i F_i$&lt;/li&gt;
&lt;li&gt;$\mu_i = \beta_P * P_i + \beta_F * F_i$&lt;/li&gt;
&lt;li&gt;$\mu_i = \beta_S * S_i + \beta_A * A_i + \beta_{SA} S_iA_i$&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;medium-exercises&#34;&gt;Medium Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-m1&#34;&gt;Practice M1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Recall the tulips example from the chapter. Suppose another set of treatments adjusted the temperature in the greenhouse over two levels: cold and hot. The data in the chapter were collected at the cold temperature. You find none of the plants grown under the hot temperature developed any blooms at all, regardless of the water and shade levels. Can you explain this result in terms of interactions between water, shade, and temperature?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; We now have a model with a three-way interaction which comes with three two-way interactions:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\mu_i = &amp;amp; \alpha + \beta_T * T_i + \beta_W * W_i + \beta_S * S_i + \newline 
&amp;amp; \beta_{TW} * T_iW_i + \beta_{TS} * T_iS_i + \beta_{WS} * W_iS_i + \newline
&amp;amp; \beta_{TWS} * T_iW_iS_i
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Within this model all parameters work out such that when $T_i = 2$ (hot condition) we obtain $\mu_i = 0$ (no blooms).&lt;/p&gt;
&lt;h3 id=&#34;practice-m2&#34;&gt;Practice M2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Can you invent a regression equation that would make the bloom size zero, whenever the temperature is hot?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Oops. I partially answered that in my previous exercise, but let&amp;rsquo;s go more in detail. Let&amp;rsquo;s first remember some values from the chapter: (1) water is recorded as 1-3 (dry to wet), (2) shade is coded as 1-3 (high to low), and (3) temperature is coded as 0/1 (cold/hot). So now we know that, irrespective of the values of water or shade, as long as $T = 1$, $mu_i$ has to be $0$ in this forumla:&lt;/p&gt;
&lt;p&gt;$$
\begin{split}
\mu_i = &amp;amp;\alpha + \beta_T * T_i + \beta_W * W_i + \beta_S * S_i + \newline
&amp;amp;\beta_{TW} * T_iW_i + \beta_{TS} * T_iS_i + \beta_{WS} * W_iS_i + \newline
&amp;amp;\beta_{TWS} * T_iW_iS_i
\end{split}
$$&lt;/p&gt;
&lt;p&gt;How do we get there? For now, let&amp;rsquo;s set water and shade to 1 and temperature to 1. Doing so will result in a rewriting of the formula above to:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\mu_{i|T=1,W=1,S=1} = \alpha + \beta_T + \beta_W + \beta_S + \beta_{TW} + \beta_{TS} + \beta_{WS} + \beta_{TWS}
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;So now we need to get this formula to always work out to 0, irrespective of values of $\alpha$, $\beta_s$, $\beta_W$, and $\beta_{WS}$. We can do so by having parameters which include the effect of temperature ($\beta_T$, $\beta_{TW}$, $\beta_{TS}$, and $\beta_{TWS}$) counteract the $\alpha$, $\beta_s$, $\beta_W$, and $\beta_{WS}$. This has us rewrite the above formula as:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\mu_{i|T=1,W=1,S=1} = (\alpha + \beta_T) + (\beta_W + \beta_{TW}) + (\beta_S + \beta_{TS}) + (\beta_{WS} + \beta_{TWS})
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;Now for the outcome to be $0$, the contents of the brackets need to be 0, so $\beta_T = -\alpha$, $\beta_{TW} = -\beta_W$, and so on. This morphs our equation into:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\mu_{i|T=1,W=1,S=1} = (\alpha - \alpha) + (\beta_W - \beta_W) + (\beta_S - \beta_S) + (\beta_{WS} - \beta_{WS})
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;So now, irrespective of $W$ or $S$, we will always obtain $\mu_i = 0$ when $T=1$. When $T=0$, all temperature effects drop out and we obtain the same formula as in the book chapter.&lt;/p&gt;
&lt;h3 id=&#34;practice-m3&#34;&gt;Practice M3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; In parts of North America, ravens depend upon wolves for their food. This is because ravens are carnivorous but cannot usually kill or open carcasses of prey. Wolves however can and do kill and tear open animals, and they tolerate ravens co-feeding at their kills. This species relationship is generally described as a “species interaction.” Can you invent a hypothetical set of data on raven population size in which this relationship would manifest as a statistical interaction? Do you think the biological interaction could be linear? Why or why not?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Here&amp;rsquo;s our regression:&lt;/p&gt;
&lt;p&gt;$Ravens∼Normal(μ,σ)$&lt;/p&gt;
&lt;p&gt;$μ=α+β_pPrey+β_wWolves+β_{pw}Prey*Wolves$&lt;/p&gt;
&lt;p&gt;with $Ravens$, $Wolves$, and $Prey$ being the number of ravens, wolves, and prey animals respectively, in a given habitat.&lt;/p&gt;
&lt;p&gt;Now let&amp;rsquo;s make up some data with an in-built interaction effect:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;N &amp;lt;- 1e5 # simulation size
rPW &amp;lt;- 0.2 # correlation between prey and wolf
bP &amp;lt;- 0.05 # regression coefficient for prey
bW &amp;lt;- -0.3 # regression coefficient for wolf
bPW &amp;lt;- 0.2 # regression coefficient for prey-by-wolf interaction
# Simulate data
prey &amp;lt;- as.integer(rnorm(N, mean = 100, sd = 15)) # as.integer, so we have &amp;quot;whole&amp;quot; animals
wolves &amp;lt;- as.integer(rnorm(N, mean = 10 + rPW * prey, sd = 7))
ravens &amp;lt;- as.integer(rnorm(N, mean = 5 + bP * prey + bW * wolves + bPW * wolves * prey, sd = 9))
d &amp;lt;- data.frame(prey = prey, wolves = wolves, ravens = ravens)
# plot the data
par(mfrow = c(1, 2))
plot(ravens ~ prey, data = d, main = &amp;quot;Ravens like prey!&amp;quot;)
plot(ravens ~ wolves, data = d, main = &amp;quot;Ravens like wolves?&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-02-18-statistical-rethinking-chapter-08_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Immediately, we see in our data that, despite us having simulated the data in such a way that ravens do not flock around wolves, when not conditioning on prey, we would think that ravens do flock around wolves.&lt;/p&gt;
&lt;p&gt;Time for a model run:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m &amp;lt;- quap(
  alist(
    ravens ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bP * prey + bW * wolves + bPW * prey * wolves,
    a ~ dnorm(min(d$ravens), 10), # minimum of ravens for intercept prior
    bW ~ dnorm(0, 1),
    bP ~ dnorm(0, 1),
    bPW ~ dnorm(0, 1),
    sigma ~ dnorm(sd(d$ravens), 10) # sd of raven as an initial guess
  ),
  data = d
)
precis(m)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              mean           sd       5.5%       94.5%
## a      5.41309759 0.6926177989  4.3061606  6.52003460
## bW    -0.32805262 0.0233269747 -0.3653336 -0.29077161
## bP     0.04084714 0.0070894052  0.0295169  0.05217738
## bPW    0.20027501 0.0002308127  0.1999061  0.20064390
## sigma  8.97789166 0.0200777428  8.9458035  9.00997977
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And we successfully reconstructed our input interactions.&lt;/p&gt;
&lt;h2 id=&#34;hard-exercises&#34;&gt;Hard Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-h1&#34;&gt;Practice H1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Return to the &lt;code&gt;data(tulips)&lt;/code&gt; example in the chapter. Now include the bed variable as a predictor in the interaction model. Don’t interact bed with the other predictors; just include it as a main effect. Note that bed is categorical. So to use it properly, you will need to either construct dummy variables or rather an index variable, as explained in Chapter 6.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Data
data(tulips)
d &amp;lt;- tulips
d$bed_id &amp;lt;- coerce_index(d$bed)
d$blooms_std &amp;lt;- d$blooms / max(d$blooms) # now on a scale from 0 to 1
d$shade_cent &amp;lt;- d$shade - mean(d$shade) # now on a scale from -1 to 1
d$water_cent &amp;lt;- d$water - mean(d$water) # now on a scale from -1 to 1
## Model
set.seed(20) # setting a seed because I sometimes run out of model iterations here
m.H1 &amp;lt;- quap(alist(
  blooms ~ dnorm(mu, sigma),
  mu &amp;lt;- a[bed_id] + bW * water_cent + bS * shade_cent + bWS * water_cent * shade_cent,
  a[bed_id] ~ dnorm(130, 100),
  bW ~ dnorm(0, 100),
  bS ~ dnorm(0, 100),
  bWS ~ dnorm(0, 100),
  sigma ~ dunif(0, 100)
),
data = d
)
precis(m.H1, depth = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            mean        sd      5.5%     94.5%
## a[1]   97.54986 12.951192  76.85135 118.24837
## a[2]  142.41547 12.950773 121.71763 163.11330
## a[3]  147.11128 12.950771 126.41344 167.80911
## bW     75.12289  9.197989  60.42272  89.82305
## bS    -41.23747  9.196690 -55.93555 -26.53938
## bWS   -52.23345 11.240444 -70.19785 -34.26905
## sigma  39.18206  5.333939  30.65740  47.70673
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-h2&#34;&gt;Practice H2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Use WAIC to compare the model from H1 to a model that omits bed. What do you infer from this comparison? Can you reconcile the WAIC results with the posterior distribution of the bed coefficients?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m.H2 &amp;lt;- quap(
  alist(
    blooms ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bW * water_cent + bS * shade_cent + bWS * water_cent * shade_cent,
    a ~ dnorm(130, 100),
    bW ~ dnorm(0, 100),
    bS ~ dnorm(0, 100),
    bWS ~ dnorm(0, 100),
    sigma ~ dunif(0, 100)
  ),
  data = d,
  start = list(a = mean(d$blooms), bW = 0, bS = 0, bWS = 0, sigma = sd(d$blooms))
)
precis(m.H2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            mean        sd      5.5%     94.5%
## a     129.00797  8.670771 115.15041 142.86554
## bW     74.95946 10.601997  58.01542  91.90350
## bS    -41.14054 10.600309 -58.08188 -24.19920
## bWS   -51.87265 12.948117 -72.56625 -31.17906
## sigma  45.22497  6.152982  35.39132  55.05863
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;compare(m.H1, m.H2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          WAIC        SE    dWAIC      dSE     pWAIC    weight
## m.H2 295.0441  9.873189 0.000000       NA  6.062662 0.6776652
## m.H1 296.5302 10.544146 1.486126 8.181914 10.771689 0.3223348
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model including the bed index variables (&lt;code&gt;m.H2&lt;/code&gt;) shows a slightly better WAIC than the model that contains the bed variable (&lt;code&gt;m.H1&lt;/code&gt;), and most of the Akaike weight. Judging from this (and the variation in the bed intercepts of model &lt;code&gt;m.H1&lt;/code&gt;), we can infer that there&amp;rsquo;s a lot of variability between the flower beds which model &lt;code&gt;m.H1&lt;/code&gt; addresses, but might overfit in doing so. Let me visualise this in a plot:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;post &amp;lt;- extract.samples(m.H1)
post.a &amp;lt;- post$a[, 1]
post.b &amp;lt;- post$a[, 2]
post.c &amp;lt;- post$a[, 3]
dens(post.a, col = &amp;quot;red&amp;quot;, xlim = c(50, 200), ylim = c(0, 0.035))
dens(post.b, col = &amp;quot;blue&amp;quot;, add = TRUE)
dens(post.c, col = &amp;quot;black&amp;quot;, add = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-02-18-statistical-rethinking-chapter-08_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;practice-h3&#34;&gt;Practice H3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Consider again the &lt;code&gt;data(rugged)&lt;/code&gt; data on economic development and terrain ruggedness, examined in this chapter. One of the African countries in that example, Seychelles, is far outside the cloud of other nations, being a rare country with both relatively high GDP and high ruggedness. Seychelles is also unusual, in that it is a group of islands far from the coast of mainland Africa, and its main economic activity is tourism.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(rugged)
d &amp;lt;- rugged
d &amp;lt;- rugged[complete.cases(rugged$rgdppc_2000), ]
d$log_gdp &amp;lt;- log(d$rgdppc_2000)
d$log_gdp_std &amp;lt;- d$log_gdp / mean(d$log_gdp)
d$rugged_std &amp;lt;- d$rugged / max(d$rugged)
d$cid &amp;lt;- ifelse(d$cont_africa == 1, 1, 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-a&#34;&gt;Part A&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Focus on model m8.5 from the chapter. Use WAIC point-wise penalties and PSIS Pareto k values to measure relative influence of each country. By these criteria, is Seychelles influencing the results? Are there other nations that are relatively influential? If so, can you explain why?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;br&gt;
Firstly, the model that the exercise is after is not model m8.5, but model m8.3. Let&amp;rsquo;s run that and look at the PSIS point-wise values:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m.H3a &amp;lt;- quap(alist(
  log_gdp_std ~ dnorm(mu, sigma),
  mu &amp;lt;- a[cid] + b[cid] * (rugged_std - 0.215),
  a[cid] ~ dnorm(1, 0.1),
  b[cid] ~ dnorm(0, 0.3),
  sigma ~ dexp(1)
),
data = d
)
precis(m.H3a, depth = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean          sd        5.5%       94.5%
## a[1]   0.8865690 0.015675759  0.86151610  0.91162189
## a[2]   1.0505745 0.009936639  1.03469382  1.06645515
## b[1]   0.1325281 0.074204674  0.01393472  0.25112152
## b[2]  -0.1425744 0.054749596 -0.23007484 -0.05507398
## sigma  0.1094945 0.005935353  0.10000867  0.11898035
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we look at point-wise WAIC values. I think there&amp;rsquo;s a pretty clear separation of high point-wise WAIC-values in the plot at around 1 so I draw that in and obtain the country names for these:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;WAIC &amp;lt;- WAIC(m.H3a, pointwise = TRUE)
plot(WAIC$WAIC)
abline(h = 1, col = &amp;quot;red&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-02-18-statistical-rethinking-chapter-08_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;as.character(d[WAIC$WAIC &amp;gt; 1, ]$country)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Austria&amp;quot;             &amp;quot;Bangladesh&amp;quot;          &amp;quot;Switzerland&amp;quot;         &amp;quot;Equatorial Guinea&amp;quot;   &amp;quot;Luxembourg&amp;quot;          &amp;quot;Republic of Moldova&amp;quot; &amp;quot;Seychelles&amp;quot;          &amp;quot;Uzbekistan&amp;quot;         
## [9] &amp;quot;Yemen&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we do the same with point-wise PSIS Pareto k values. Again, I believe there is a separation. This time at 0.35:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;PSIS &amp;lt;- PSIS(m.H3a, pointwise = TRUE)
plot(PSIS$k)
abline(h = .35, col = &amp;quot;red&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-02-18-statistical-rethinking-chapter-08_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;as.character(d[PSIS$k &amp;gt; .35, ]$country)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Switzerland&amp;quot; &amp;quot;Lesotho&amp;quot;     &amp;quot;Seychelles&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Honestly, I cannot make too much sense of the countries I obtained via the point-wise WAIC-values, but the point-wise PSIS Pareto k values make obvious sense here. All of these are extremely rugged and much richer than even their surrounding flat countries.&lt;/p&gt;
&lt;h4 id=&#34;part-b&#34;&gt;Part B&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Now use robust regression, as described in the previous chapter. Modify m8.5 to use a Student-t distribution with $ν = 2$. Does this change the results in a substantial way?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m.H3b &amp;lt;- quap(alist(
  log_gdp_std ~ dstudent(2, mu, sigma),
  mu &amp;lt;- a[cid] + b[cid] * (rugged_std - 0.215),
  a[cid] ~ dnorm(1, 0.1),
  b[cid] ~ dnorm(0, 0.3),
  sigma ~ dexp(1)
),
data = d
)
precis(m.H3b, depth = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              mean          sd         5.5%       94.5%
## a[1]   0.86265354 0.016153871  0.836836529  0.88847054
## a[2]   1.04573322 0.010971670  1.028198374  1.06326807
## b[1]   0.11279640 0.075195199 -0.007380055  0.23297285
## b[2]  -0.21362933 0.063538165 -0.315175587 -0.11208307
## sigma  0.08452953 0.006732778  0.073769255  0.09528981
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The parameter estimates changed slightly, but not crazily so. Let&amp;rsquo;s look at the point-wise importance again. This time, I see a split in WAIC values around 2:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;WAIC &amp;lt;- WAIC(m.H3b, pointwise = TRUE)
plot(WAIC$WAIC)
abline(h = 2, col = &amp;quot;red&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-02-18-statistical-rethinking-chapter-08_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;as.character(d[WAIC$WAIC &amp;gt; 2, ]$country)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Switzerland&amp;quot;       &amp;quot;Equatorial Guinea&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now for point-wise PSIS-values. This time, I see don&amp;rsquo;t really a separation so I&amp;rsquo;ll just pull out the highest ranking nations:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;PSIS &amp;lt;- PSIS(m.H3b, pointwise = TRUE)
plot(PSIS$k)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-02-18-statistical-rethinking-chapter-08_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;as.character(d$country[as.numeric(rownames(PSIS[order(PSIS$k), ])[1:5])])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Malawi&amp;quot;    &amp;quot;Ethiopia&amp;quot;  &amp;quot;Yemen&amp;quot;     &amp;quot;Singapore&amp;quot; &amp;quot;Burundi&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Interestingly, only Switzerland shows up in our subset of potentially highly influential nations. That being said, the outliers in the WAIC and point-wise PSIS values are much closer to the main cloud of data points than they were previously. I expect this to be the effect of the ropbust regression.&lt;/p&gt;
&lt;h3 id=&#34;practice-h4&#34;&gt;Practice H4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; The values in &lt;code&gt;data(nettle)&lt;/code&gt; are data on language diversity in 74 nations. The meaning of each column is given below:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;country&lt;/code&gt;: Name of the country&lt;/li&gt;
&lt;li&gt;&lt;code&gt;num.lang&lt;/code&gt;: Number of recognized languages spoken&lt;/li&gt;
&lt;li&gt;&lt;code&gt;area&lt;/code&gt;: Area in square kilometres&lt;/li&gt;
&lt;li&gt;&lt;code&gt;k.pop&lt;/code&gt;: Population, in thousands&lt;/li&gt;
&lt;li&gt;&lt;code&gt;num.stations&lt;/code&gt;: Number of weather stations that provided data for the next two columns&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mean.growing.season&lt;/code&gt;: Average length of growing season, in months&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sd.growing.season&lt;/code&gt;: Standard deviation of length of growing season, in months&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Use these data to evaluate the hypothesis that language diversity is partly a product of food security. The notion is that, in productive ecologies, people don’t need large social networks to buffer them against risk of food shortfalls. This means ethnic groups can be smaller and more self-sufficient, leading to more languages per capita. In contrast, in a poor ecology, there is more subsistence risk, and so human societies have adapted by building larger networks of mutual obligation to provide food insurance. This in turn creates social forces that help prevent languages from diversifying. Specifically, you will try to model the number of languages per capita as the outcome variable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;d$lang.per.cap &amp;lt;- d$num.lang / d$k.pop
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use the logarithm of this new variable as your regression outcome. (A count model would be better here, but you’ll learn those later, in Chapter 11.) This problem is open ended, allowing you to decide how you address the hypotheses and the uncertain advice the modelling provides. If you think you need to use WAIC any place, please do. If you think you need certain priors, argue for them. If you think you need to plot predictions in a certain way, please do. Just try to honestly evaluate the main effects of both &lt;code&gt;mean.growing.season&lt;/code&gt; and &lt;code&gt;sd.growing.season&lt;/code&gt;, as well as their two-way interaction, as outlined in parts (a), (b), and (c) below. If you are not sure which approach to use, try several.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(nettle)
d &amp;lt;- nettle
d$lang.per.cap &amp;lt;- d$num.lang / d$k.pop
d$log_lpc &amp;lt;- log(d$lang.per.cap)
d$log_area &amp;lt;- log(d$area)
d$log_area.c &amp;lt;- d$log_area - mean(d$log_area)
d$mgs.c &amp;lt;- d$mean.growing.season - mean(d$mean.growing.season)
d$sgs.c &amp;lt;- d$sd.growing.season - mean(d$sd.growing.season)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-a-1&#34;&gt;Part A&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Evaluate the hypothesis that language diversity, as measured by &lt;code&gt;log(lang.per.cap)&lt;/code&gt;, is positively associated with the average length of the growing season, &lt;code&gt;mean.growing.season&lt;/code&gt;. Consider &lt;code&gt;log(area)&lt;/code&gt; in your regression(s) as a covariate (not an interaction). Interpret your results.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Let&amp;rsquo;s start this off by building a model that attempts to identify the logarithmic language per capita as defined above (&lt;code&gt;log_lpc&lt;/code&gt;). I use this as my response variable to smooth out the effects of highly multilingual communities which would otherwise look like freakish outliers:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;### Model
m.H4a &amp;lt;- quap(
  alist(
    log_lpc ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bM * mean.growing.season + bA * log_area.c,
    a ~ dnorm(mean(d$log_lpc), sd(d$log_lpc)),
    bM ~ dnorm(0, 2),
    bA ~ dnorm(0, 2),
    sigma ~ dunif(0, 10)
  ),
  data = d
)
precis(m.H4a)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean         sd       5.5%        94.5%
## a     -6.3965435 0.40827385 -7.0490440 -5.744043063
## bM     0.1349813 0.05390024  0.0488383  0.221124289
## bA    -0.2091213 0.13661084 -0.4274518  0.009209185
## sigma  1.3894990 0.11424753  1.2069094  1.572088628
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;### Prediction plot
mean.growing.season.seq &amp;lt;- seq(from = min(d$mean.growing.season), to = max(d$mean.growing.season), length.out = 50)
mu &amp;lt;- link(m.H4a, data = data.frame(mean.growing.season = mean.growing.season.seq, log_area.c = 0), refresh = 0)
mu.mean &amp;lt;- apply(mu, 2, mean)
mu.PI &amp;lt;- apply(mu, 2, PI, prob = 0.97)
plot(log_lpc ~ mean.growing.season, data = d, col = rangi2)
lines(mean.growing.season.seq, mu.mean)
shade(mu.PI, mean.growing.season.seq)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-02-18-statistical-rethinking-chapter-08_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we can see, there&amp;rsquo;s a positive linear relationship between &lt;code&gt;mean.growing.season&lt;/code&gt; and number of languages spoken by a population while also conditioning on area of the nations in question. I highly doubt that there is any causality here, however and instead hypothesise that &lt;code&gt;mean.growing.season&lt;/code&gt; can be used as a proxy for &amp;ldquo;sunniness&amp;rdquo; of a nation which, from anecdotal evidence, is much more appealing to immigration from all over the world than freezing countries with short growing seasons. To follow the rationale of the task however, this is evidence of ample food supply allowing for cultural diversity and thus higher language count.&lt;/p&gt;
&lt;h4 id=&#34;part-b-1&#34;&gt;Part B&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Now evaluate the hypothesis that language diversity is negatively associated with the standard deviation of length of growing season, &lt;code&gt;sd.growing.season&lt;/code&gt;. This hypothesis follows from uncertainty in harvest favouring social insurance through larger social networks and therefore fewer languages. Again, consider &lt;code&gt;log(area)&lt;/code&gt; as a covariate (not an interaction). Interpret your results.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;### Model
m.H4b &amp;lt;- quap(
  alist(
    log_lpc ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bS * sd.growing.season + bA * log_area.c,
    a ~ dnorm(mean(d$log_lpc), sd(d$log_lpc)),
    bS ~ dnorm(0, 5),
    bA ~ dnorm(0, 5),
    sigma ~ dunif(0, 10)
  ),
  data = d
)
precis(m.H4b)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean        sd       5.5%        94.5%
## a     -5.1199557 0.3485729 -5.6770426 -4.562868804
## bS    -0.2005191 0.1824921 -0.4921768  0.091138529
## bA    -0.2433936 0.1552325 -0.4914852  0.004697949
## sigma  1.4384790 0.1182463  1.2494986  1.627459505
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;### Prediction plot
sd.growing.season.seq &amp;lt;- seq(from = min(d$sd.growing.season), to = max(d$sd.growing.season), length.out = 50)
mu &amp;lt;- link(m.H4b, data = data.frame(sd.growing.season = sd.growing.season.seq, log_area.c = 0), refresh = 0)
mu.mean &amp;lt;- apply(mu, 2, mean)
mu.PI &amp;lt;- apply(mu, 2, PI, prob = 0.97)
plot(log_lpc ~ sd.growing.season, data = d, col = rangi2)
lines(sd.growing.season.seq, mu.mean)
shade(mu.PI, sd.growing.season.seq)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-02-18-statistical-rethinking-chapter-08_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Again, I find myself agreeing with the hypothesis seeing how I have identified a negative relationship between &lt;code&gt;sd.growing.season&lt;/code&gt; and language count.&lt;/p&gt;
&lt;h4 id=&#34;part-c&#34;&gt;Part C&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Finally, evaluate the hypothesis that &lt;code&gt;mean.growing.season&lt;/code&gt; and &lt;code&gt;sd.growing.season&lt;/code&gt; interact to synergistically reduce language diversity. The idea is that, in nations with longer average growing seasons, high variance makes storage and redistribution even more important than it would be otherwise. That way, people can cooperate to preserve and protect windfalls to be used during the droughts. These forces in turn may lead to greater social integration and fewer languages.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Again, I build a model which conditions on centred logarithmic area of nations, while building an interaction between &lt;code&gt;mean.growing.season&lt;/code&gt; and &lt;code&gt;sd.growing.season&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m.H4c &amp;lt;- quap(
  alist(
    log_lpc ~ dnorm(mu, sigma),
    mu &amp;lt;- a +
      bM * mean.growing.season +
      bS * sd.growing.season +
      bMS * mean.growing.season * sd.growing.season +
      bA * log_area.c,
    a ~ dnorm(mean(d$log_lpc), sd(d$log_lpc)),
    bM ~ dnorm(0, 5),
    bS ~ dnorm(0, 5),
    bA ~ dnorm(0, 5),
    bMS ~ dnorm(0, 5),
    sigma ~ dunif(0, 10)
  ),
  data = d
)
precis(m.H4c)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               mean         sd       5.5%       94.5%
## a     -6.771794698 0.55161186 -7.6533770 -5.89021241
## bM     0.277261541 0.07288992  0.1607694  0.39375371
## bS     0.327529661 0.37052336 -0.2646382  0.91969756
## bA    -0.005517158 0.15891352 -0.2594917  0.24845734
## bMS   -0.098088334 0.04565210 -0.1710492 -0.02512746
## sigma  1.307674873 0.10763683  1.1356504  1.47969932
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We seem to be quite sure of the interaction effect &lt;code&gt;bMS&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To investiagte the interaction of the two predictors we are interested in, let&amp;rsquo;s produce a Tryptich each:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;par(mfrow = c(2, 3))
# Discretize variables into groups
d$mean.growing.season.group &amp;lt;- cut(
  d$mean.growing.season,
  breaks = quantile(d$mean.growing.season, probs = c(0, 1 / 3, 2 / 3, 1)),
  include.lowest = TRUE,
  dig.lab = 2
)
d$sd.growing.season.group &amp;lt;- cut(
  d$sd.growing.season,
  breaks = quantile(d$sd.growing.season, probs = c(0, 1 / 3, 2 / 3, 1)),
  include.lowest = TRUE,
  dig.lab = 2
)
# Plot first row as mean against SD
mean.growing.season.seq &amp;lt;- seq(from = min(d$mean.growing.season), to = max(d$mean.growing.season), length.out = 50)
for (group in levels(d$sd.growing.season.group)) {
  dt &amp;lt;- d[d$sd.growing.season.group == group, ]
  plot(log_lpc ~ mean.growing.season,
    data = dt, col = rangi2, xlim = c(min(d$mean.growing.season), max(d$mean.growing.season)), ylim = c(min(d$log_lpc), max(d$log_lpc)),
    main = paste(&amp;quot;SD GS = &amp;quot;, group), xlab = &amp;quot;Mean GS&amp;quot;
  )
  mu &amp;lt;- link(m.H4c,
    data = data.frame(mean.growing.season = mean.growing.season.seq, sd.growing.season = mean(dt$sd.growing.season), log_area.c = 0),
    refresh = 0
  )
  mu.mean &amp;lt;- apply(mu, 2, mean)
  mu.PI &amp;lt;- apply(mu, 2, PI, prob = 0.97)
  lines(mean.growing.season.seq, mu.mean)
  lines(mean.growing.season.seq, mu.PI[1, ], lty = 2)
  lines(mean.growing.season.seq, mu.PI[2, ], lty = 2)
}
# Plot second row as SD against mean
sd.growing.season.seq &amp;lt;- seq(from = min(d$sd.growing.season), to = max(d$sd.growing.season), length.out = 50)
for (group in levels(d$mean.growing.season.group)) {
  dt &amp;lt;- d[d$mean.growing.season.group == group, ]
  plot(log_lpc ~ sgs.c,
    data = dt, col = rangi2, xlim = c(min(d$sd.growing.season), max(d$sd.growing.season)), ylim = c(min(d$log_lpc), max(d$log_lpc)),
    main = paste(&amp;quot;Mean GS = &amp;quot;, group), xlab = &amp;quot;SD GS&amp;quot;
  )
  mu &amp;lt;- link(m.H4c,
    data = data.frame(sd.growing.season = sd.growing.season.seq, mean.growing.season = mean(dt$mean.growing.season), log_area.c = 0),
    refresh = 0
  )
  mu.mean &amp;lt;- apply(mu, 2, mean)
  mu.PI &amp;lt;- apply(mu, 2, PI, prob = 0.97)
  lines(sd.growing.season.seq, mu.mean)
  lines(sd.growing.season.seq, mu.PI[1, ], lty = 2)
  lines(sd.growing.season.seq, mu.PI[2, ], lty = 2)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-02-18-statistical-rethinking-chapter-08_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;These plots show that the association between mean length of growing season and language diversity is positive when SD length of growing season is low, but is basically zero when SD length of growing season is high. Similarly, the association between SD length of growing season and language diversity is basically zero when mean length of growing season is low, but is negative when mean length of growing season is high. This is consistent with the hypothesis presented in the question.&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.5 (2021-03-31)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19043)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.1252  LC_CTYPE=English_United Kingdom.1252    LC_MONETARY=English_United Kingdom.1252 LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.1252    
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] viridis_0.6.0        viridisLite_0.4.0    rethinking_2.13      rstan_2.21.2         ggplot2_3.3.6        StanHeaders_2.21.0-7
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.7         mvtnorm_1.1-1      lattice_0.20-41    prettyunits_1.1.1  ps_1.6.0           assertthat_0.2.1   digest_0.6.27      utf8_1.2.1         V8_3.4.1           R6_2.5.0          
## [11] backports_1.2.1    stats4_4.0.5       evaluate_0.14      coda_0.19-4        highr_0.9          blogdown_1.3       pillar_1.6.0       rlang_0.4.11       curl_4.3.2         callr_3.7.0       
## [21] jquerylib_0.1.4    R.utils_2.10.1     R.oo_1.24.0        rmarkdown_2.7      styler_1.4.1       stringr_1.4.0      loo_2.4.1          munsell_0.5.0      compiler_4.0.5     xfun_0.22         
## [31] pkgconfig_2.0.3    pkgbuild_1.2.0     shape_1.4.5        htmltools_0.5.1.1  tidyselect_1.1.0   tibble_3.1.1       gridExtra_2.3      bookdown_0.22      codetools_0.2-18   matrixStats_0.61.0
## [41] fansi_0.4.2        crayon_1.4.1       dplyr_1.0.5        withr_2.4.2        MASS_7.3-53.1      R.methodsS3_1.8.1  grid_4.0.5         jsonlite_1.7.2     gtable_0.3.0       lifecycle_1.0.0   
## [51] DBI_1.1.1          magrittr_2.0.1     scales_1.1.1       RcppParallel_5.1.2 cli_3.0.0          stringi_1.5.3      bslib_0.2.4        ellipsis_0.3.2     generics_0.1.0     vctrs_0.3.7       
## [61] rematch2_2.1.2     tools_4.0.5        R.cache_0.14.0     glue_1.4.2         purrr_0.3.4        processx_3.5.1     yaml_2.2.1         inline_0.3.17      colorspace_2.0-0   knitr_1.33        
## [71] sass_0.3.1
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 09</title>
      <link>https://www.erikkusch.com/courses/rethinking/chapter-09/</link>
      <pubDate>Thu, 25 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/rethinking/chapter-09/</guid>
      <description>&lt;h1 id=&#34;markov-chain-monte-carlo&#34;&gt;Markov Chain Monte Carlo&lt;/h1&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/rethinking/10__26-02-2021_SUMMARY_-MCMC.pptx.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides Chapter 9&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to the exercises at the end of chapter 9 in 
&lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Satistical Rethinking 2&lt;/a&gt; by Richard McElreath. I have created these notes as a part of my ongoing involvement in the 
&lt;a href=&#34;https://www.erikkusch.com/project/aubayes/&#34;&gt;AU Bayes Study Group&lt;/a&gt;. Much of my inspiration for these solutions, where necessary, has been obtained from 
&lt;a href=&#34;https://github.com/jffist/statistical-rethinking-solutions/blob/master/ch08_hw.R&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Taras Svirskyi&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/cavaunpeu/statistical-rethinking/blob/master/chapter-8/homework.R&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;William Wolf&lt;/a&gt;, and 
&lt;a href=&#34;https://www.samples-of-thoughts.com/projects/statistical-rethinking/chapter_8/chp8-ex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Corrie Bartelheimer&lt;/a&gt; as well as the solutions provided to instructors by Richard McElreath himself.&lt;/p&gt;
&lt;h2 id=&#34;r-environment&#34;&gt;&lt;code&gt;R&lt;/code&gt; Environment&lt;/h2&gt;
&lt;p&gt;For today&amp;rsquo;s exercise, I load the following packages:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rethinking)
library(rstan)
library(ggplot2)
library(tidybayes)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;easy-exercises&#34;&gt;Easy Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-e1&#34;&gt;Practice E1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Which of the following is a requirement of the simple Metropolis algorithm?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The parameters must be discrete.&lt;/li&gt;
&lt;li&gt;The likelihood function must be Gaussian.&lt;/li&gt;
&lt;li&gt;The proposal distribution must be symmetric.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Not a requirement. Metropolis can accommodate continuous and discrete parameters.&lt;/li&gt;
&lt;li&gt;Not a requirement. Distribution could be any symmetric distribution. Not just Gaussian.&lt;/li&gt;
&lt;li&gt;This is a requirement.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;practice-e2&#34;&gt;Practice E2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Gibbs sampling is more efficient than the Metropolis algorithm. How does it achieve this extra efficiency? Are there any limitations to the Gibbs sampling strategy?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Gibbs uses adaptive proposals when considering which location in the posterior to sample next. This makes it more efficient because less proposed steps are rejected.&lt;/p&gt;
&lt;h3 id=&#34;practice-e3&#34;&gt;Practice E3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Which sort of parameters can Hamiltonian Monte Carlo not handle? Can you explain why?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Discrete parameters. HMC depends on gradients which to explore using a physics simulation. Discrete parameters would not allow for the construction of any gradients.&lt;/p&gt;
&lt;h3 id=&#34;practice-e4&#34;&gt;Practice E4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Explain the difference between the effective number of samples, &lt;code&gt;n_eff&lt;/code&gt; as calculated by Stan, and the actual number of samples.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  Effective sample number (&lt;code&gt;n_eff&lt;/code&gt;) identifies the number of &amp;lsquo;ideal&amp;rsquo; (i.e. uncorrelated) samples. Since MCMC algorithms explore the posterior as a chain of samples, each sample is usually correlated with the previous one to some extent. Conclusively, &lt;code&gt;n_eff&lt;/code&gt; identifies the number of samples used for estimating the posterior mean/distribution whereas actual number of samples is simply the number of data points we have.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;n_eff&lt;/code&gt; is usually smaller than the actual number of samples (unless we have anti-correlated MCMC samples).&lt;/p&gt;
&lt;h3 id=&#34;practice-e5&#34;&gt;Practice E5&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Which value should &lt;code&gt;Rhat&lt;/code&gt; approach, when a chain is sampling the posterior distribution correctly?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; $\hat{R}$ or &lt;code&gt;Rhat&lt;/code&gt;, in &lt;code&gt;R&lt;/code&gt;, reflects variance within a chain versus variance between chains. If these are the same, $\hat{R}$ will be $1.0$ - i.e.: it does not matter from which chain we would infere parameters and predictions. Values higher than 1.0 can indicate problems in the model. Values much higher than 1 indicate serious issues.&lt;/p&gt;
&lt;h3 id=&#34;practice-e6&#34;&gt;Practice E6&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Sketch a good trace plot for a Markov chain, one that is effectively sampling from the posterior distribution. What is good about its shape? Then sketch a trace plot for a malfunctioning Markov chain. What about its shape indicates malfunction?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Good trace plot&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;y &amp;lt;- rnorm(1e4, mean = 1, sd = 2)
m.E6Good &amp;lt;- ulam(
  alist(
    y ~ dnorm(mu, sigma),
    mu &amp;lt;- alpha,
    alpha ~ dnorm(0, 10),
    sigma ~ dcauchy(0, 1)
  ),
  data = list(y = y),
  cores = 2,
  chains = 2,
  start = list(
    alpha = 0,
    sigma = 1
  )
)
traceplot(m.E6Good)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1000
## [1] 1
## [1] 1000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-02-25-statistical-rethinking-chapter-09_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;1440&#34; /&gt;
These trace plots show that the chains quickly find the region with highest posterior probability and stay there.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Bad trace plot&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;y &amp;lt;- rnorm(1e4, mean = 1, sd = 2)
m.E6Bad &amp;lt;- ulam(
  alist(
    y ~ dnorm(mu, sigma),
    mu &amp;lt;- a1 + a2,
    a1 ~ dnorm(0, 10),
    a2 ~ dnorm(0, 10),
    sigma ~ dcauchy(0, 1)
  ),
  data = list(y = y),
  chains = 2,
  cores = 2,
  start = list(
    a1 = 0,
    a2 = 0,
    sigma = 1
  ),
)
traceplot(m.E6Bad)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1000
## [1] 1
## [1] 1000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-02-25-statistical-rethinking-chapter-09_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is a problem of &lt;em&gt;unidentifiable parameters&lt;/em&gt; as &lt;code&gt;a1&lt;/code&gt; and &lt;code&gt;a2&lt;/code&gt; can cancel each other out to arrive at the correct &lt;code&gt;mu&lt;/code&gt; and so we see non-stationary behaviour in the trace plots of &lt;code&gt;a1&lt;/code&gt; and &lt;code&gt;a2&lt;/code&gt; while the trace plot for &lt;code&gt;sigma&lt;/code&gt; is doing alright.&lt;/p&gt;
&lt;h2 id=&#34;medium-exercises&#34;&gt;Medium Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-m1&#34;&gt;Practice M1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Re-estimate the terrain ruggedness model from the chapter, but now using a uniform prior and an exponential prior for the standard deviation, &lt;code&gt;sigma&lt;/code&gt;. The uniform prior should be &lt;code&gt;dunif(0,10)&lt;/code&gt; and the exponential should be &lt;code&gt;dexp(1)&lt;/code&gt;. Do the different priors have any detectable influence on the posterior distribution?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; The ruggedness model in question is &lt;code&gt;m8.3&lt;/code&gt; in the book (or &lt;code&gt;m9.1&lt;/code&gt; in &lt;code&gt;ulam()&lt;/code&gt; specification). First, I prepare the data like I did 
&lt;a href=&#34;post/2021-02-18-statistical-rethinking-chapter-08/index.Rmd&#34;&gt;previously&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(rugged)
d &amp;lt;- rugged
d$log_gdp &amp;lt;- log(d$rgdppc_2000)
d &amp;lt;- d[complete.cases(d$rgdppc_2000), ]
d$log_gdp_std &amp;lt;- d$log_gdp / mean(d$log_gdp)
d$rugged_std &amp;lt;- d$rugged / max(d$rugged)
d$cid &amp;lt;- ifelse(d$cont_africa == 1, 1, 2)
dd.trim &amp;lt;- list(
  log_gdp_std = d$log_gdp_std,
  rugged_std = d$rugged_std,
  cid = as.integer(d$cid)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s fit that model with the different priors:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Exponential prior for sigma
m.M1Exp &amp;lt;- ulam(
  alist(
    log_gdp_std ~ dnorm(mu, sigma),
    mu &amp;lt;- a[cid] + b[cid] * (rugged_std - 0.215),
    a[cid] ~ dnorm(1, 0.1),
    b[cid] ~ dnorm(0, 0.3),
    sigma ~ dexp(1)
  ),
  data = dd.trim,
  chains = 4,
  cores = 4,
)
## Uniform prior for sigma
m.M1Uni &amp;lt;- ulam(
  alist(
    log_gdp_std ~ dnorm(mu, sigma),
    mu &amp;lt;- a[cid] + b[cid] * (rugged_std - 0.215),
    a[cid] ~ dnorm(1, 0.1),
    b[cid] ~ dnorm(0, 0.3),
    sigma ~ dnorm(0, 10)
  ),
  data = dd.trim,
  chains = 4,
  cores = 4,
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now on to inspect the model. Let&amp;rsquo;s start with the parameter estimates in comparison&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;coeftab(m.M1Exp, m.M1Uni)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       m.M1Exp m.M1Uni
## a[1]     0.89    0.89
## a[2]     1.05    1.05
## b[1]     0.13    0.13
## b[2]    -0.14   -0.14
## sigma    0.11    0.11
## nobs      170     170
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These are strikingly the same. What about the individual model outputs in more detail?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m.M1Exp, depth = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean          sd        5.5%       94.5%    n_eff     Rhat4
## a[1]   0.8870817 0.015625699  0.86196179  0.91173540 2453.919 0.9995577
## a[2]   1.0507770 0.009968219  1.03527611  1.06640703 2834.441 0.9988734
## b[1]   0.1344067 0.074307822  0.01486287  0.25218389 2786.188 0.9993677
## b[2]  -0.1413442 0.054855132 -0.22964887 -0.05187494 2324.832 0.9983652
## sigma  0.1117154 0.006171670  0.10228974  0.12208002 2725.266 0.9988256
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m.M1Uni, depth = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean          sd        5.5%       94.5%    n_eff     Rhat4
## a[1]   0.8865936 0.015580736  0.86128553  0.91082334 2489.074 0.9989360
## a[2]   1.0501777 0.010010613  1.03404118  1.06614541 2152.883 1.0007549
## b[1]   0.1312147 0.074609926  0.01239339  0.24998421 2244.528 0.9993558
## b[2]  -0.1420136 0.054996077 -0.22957192 -0.05372842 2023.621 0.9987402
## sigma  0.1115782 0.006224722  0.10188964  0.12166315 3600.101 0.9990594
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, these are very similar aside from the effective number of samples (&lt;code&gt;n_eff&lt;/code&gt;) which is much higher for all parameter estimates in the model with the exponential prior on &lt;code&gt;sigma&lt;/code&gt; (&lt;code&gt;m.M1Exp&lt;/code&gt;) except for &lt;code&gt;sigma&lt;/code&gt; itself, which boasts a higher &lt;code&gt;n_eff&lt;/code&gt; in the uniform-prior model (&lt;code&gt;m.M1Uni&lt;/code&gt;). As such, we conclude that while the different priors have an impact on &lt;code&gt;n_eff&lt;/code&gt;, they do not change the posterior distributions. Let me visualise this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Plot_df &amp;lt;- data.frame(
  Posteriors = c(
    extract.samples(m.M1Exp, n = 1e4)$sigma,
    extract.samples(m.M1Uni, n = 1e4)$sigma
  ),
  Name = rep(c(&amp;quot;Exp&amp;quot;, &amp;quot;Uni&amp;quot;), each = 1e4),
  Model = rep(c(&amp;quot;m.M1Exp&amp;quot;, &amp;quot;m.M1Uni&amp;quot;), each = 1e4)
)

ggplot(Plot_df, aes(y = Model, x = Posteriors)) +
  stat_halfeye() +
  labs(x = &amp;quot;Parameter Estimate&amp;quot;, y = &amp;quot;Model&amp;quot;) +
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-02-25-statistical-rethinking-chapter-09_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;1440&#34; /&gt;
That really does look the same to me.&lt;/p&gt;
&lt;h3 id=&#34;practice-m2&#34;&gt;Practice M2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; The Cauchy and exponential priors from the terrain ruggedness model are very weak. They can be made more informative by reducing their scale. Compare the &lt;code&gt;dcauchy&lt;/code&gt; and &lt;code&gt;dexp&lt;/code&gt; priors for progressively smaller values of the scaling parameter. As these priors become stronger, how does each influence the posterior distribution?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  I write a &lt;code&gt;for&lt;/code&gt; loop here to minimise code needs:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;RepTimes &amp;lt;- 4 # how many steps I want to try
ScalingFactor &amp;lt;- 10 # by what factor to make priors stronger
# empty lists to store models in
Explist &amp;lt;- as.list(rep(NA, RepTimes))
Caulist &amp;lt;- as.list(rep(NA, RepTimes))
# Loop over all models
for (Mod_Iter in 0:(RepTimes - 1)) {
  dd.trim$ScalingFactor &amp;lt;- ScalingFactor
  dd.trim$Mod_Iter &amp;lt;- Mod_Iter
  ## Exponential prior for sigma
  m.M2Exp &amp;lt;- ulam(
    alist(
      log_gdp_std ~ dnorm(mu, sigma),
      mu &amp;lt;- a[cid] + b[cid] * (rugged_std - 0.215),
      a[cid] ~ dnorm(1, 0.1),
      b[cid] ~ dnorm(0, 0.3),
      sigma ~ dexp(1 * ScalingFactor^Mod_Iter)
    ),
    data = dd.trim,
    chains = 4,
    cores = 4,
  )
  Explist[[Mod_Iter + 1]] &amp;lt;- m.M2Exp
  ## Cauchy prior for sigma
  m.M2Cau &amp;lt;- ulam(
    alist(
      log_gdp_std ~ dnorm(mu, sigma),
      mu &amp;lt;- a[cid] + b[cid] * (rugged_std - 0.215),
      a[cid] ~ dnorm(1, 0.1),
      b[cid] ~ dnorm(0, 0.3),
      sigma ~ dcauchy(0, 1 / ScalingFactor^Mod_Iter)
    ),
    data = dd.trim,
    chains = 4,
    cores = 4,
  )
  Caulist[[Mod_Iter + 1]] &amp;lt;- m.M2Cau
}
coeftab(Explist[[1]], Explist[[2]], Explist[[3]], Explist[[4]])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Explist[[1]] Explist[[2]] Explist[[3]] Explist[[4]]
## a[1]     0.89         0.89         0.89         0.89     
## a[2]     1.05         1.05         1.05         1.05     
## b[1]     0.13         0.13         0.13         0.13     
## b[2]    -0.14        -0.14        -0.14        -0.15     
## sigma    0.11         0.11         0.11         0.09     
## nobs      170          170          170          170
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;coeftab(Caulist[[1]], Caulist[[2]], Caulist[[3]], Caulist[[4]])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Caulist[[1]] Caulist[[2]] Caulist[[3]] Caulist[[4]]
## a[1]     0.89         0.89         0.89         0.89     
## a[2]     1.05         1.05         1.05         1.05     
## b[1]     0.14         0.13         0.13         0.13     
## b[2]    -0.14        -0.14        -0.14        -0.14     
## sigma    0.11         0.11         0.11         0.11     
## nobs      170          170          170          170
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The more restrictive exponential priors decrease the estimate for sigma. On the other hand, the more restrictive cauchy priors have no effect, it seems.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s explore why this is by looking at the priors themselves:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;par(mfrow = c(1, 2))
curve(dexp(x, 1),
  from = 0, to = 5, ylab = &amp;quot;Density&amp;quot;, xlab = &amp;quot;sigma&amp;quot;,
  col = &amp;quot;royalblue4&amp;quot;
)
curve(dexp(x, 10), from = 0, to = 5, add = T)
curve(dexp(x, 100), from = 0, to = 5, add = T, col = col.desat(&amp;quot;red&amp;quot;))
curve(dexp(x, 1000), from = 0, to = 5, add = T, col = col.desat(&amp;quot;green&amp;quot;))
mtext(&amp;quot;Exponential Prior&amp;quot;)
legend(&amp;quot;topright&amp;quot;,
  col = c(&amp;quot;royalblue4&amp;quot;, &amp;quot;black&amp;quot;, col.desat(&amp;quot;red&amp;quot;), col.desat(&amp;quot;green&amp;quot;)),
  lty = c(1, 1, 1), legend = c(&amp;quot;Exp(1)&amp;quot;, &amp;quot;Exp(10)&amp;quot;, &amp;quot;Exp(100)&amp;quot;, &amp;quot;Exp(1000)&amp;quot;), bty = &amp;quot;n&amp;quot;
)

curve(2 * dcauchy(x, 0, 1),
  from = 0, to = 5, ylab = &amp;quot;Density&amp;quot;, xlab = &amp;quot;sigma&amp;quot;,
  col = &amp;quot;royalblue4&amp;quot;
)
curve(2 * dcauchy(x, 0, 0.1), from = 0, to = 5, add = T, col = &amp;quot;black&amp;quot;)
curve(2 * dcauchy(x, 0, 0.01), from = 0, to = 5, add = T, col = col.desat(&amp;quot;red&amp;quot;))
curve(2 * dcauchy(x, 0, 0.001), from = 0, to = 5, add = T, col = col.desat(&amp;quot;green&amp;quot;))
mtext(&amp;quot;Cauchy Prior&amp;quot;)
legend(&amp;quot;topright&amp;quot;,
  col = c(&amp;quot;royalblue4&amp;quot;, &amp;quot;black&amp;quot;, col.desat(&amp;quot;red&amp;quot;), col.desat(&amp;quot;green&amp;quot;)),
  lty = c(1, 1, 1), legend = c(&amp;quot;Cauchy(0, 1)&amp;quot;, &amp;quot;Cauchy(0, 0.1)&amp;quot;, &amp;quot;Cauchy(0, 0.01)&amp;quot;, &amp;quot;Cauchy(0, 0.001)&amp;quot;), bty = &amp;quot;n&amp;quot;
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-02-25-statistical-rethinking-chapter-09_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The cauchy distributions show thicker tails while the exponential distributions quickly concentrate. Hence why a concentrated Cauchy prior allow more flexibility that a concentrated exponential prior.&lt;/p&gt;
&lt;h3 id=&#34;practice-m3&#34;&gt;Practice M3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Re-estimate one of the Stan models from the chapter, but at different numbers of &lt;code&gt;warmup&lt;/code&gt; iterations. Be sure to use the same number of sampling iterations in each case. Compare the &lt;code&gt;n_eff&lt;/code&gt; values.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; The ruggedness model was fine so far so I continue with that one. Here, I build this model with a fixed run length and fixed starting values for each run with changing warmup values:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;start &amp;lt;- list(a = c(1, 1), b = c(0, 0), sigma = 1) # use fixed start values for comparability of runs
m.M3 &amp;lt;- ulam(
  alist(
    log_gdp_std ~ dnorm(mu, sigma),
    mu &amp;lt;- a[cid] + b[cid] * (rugged_std - 0.215),
    a[cid] ~ dnorm(1, 0.1),
    b[cid] ~ dnorm(0, 0.3),
    sigma ~ dexp(1)
  ),
  data = dd.trim,
  start = start,
  chains = 2, cores = 2,
  iter = 100
)
warm_list &amp;lt;- c(5, 10, 100, 500, 1000) # define warmup values to run through
n_eff &amp;lt;- matrix(NA, nrow = length(warm_list), ncol = 5) # first make matrix to hold n_eff results
for (i in 1:length(warm_list)) { # loop over warm_list and collect n_eff
  w &amp;lt;- warm_list[i]
  m_temp &amp;lt;- ulam(m.M3, chains = 2, cores = 2, iter = 1000 + w, warmup = w, start = start)
  n_eff[i, ] &amp;lt;- precis(m_temp, 2)$n_eff
}
colnames(n_eff) &amp;lt;- rownames(precis(m_temp, 2))
rownames(n_eff) &amp;lt;- warm_list
n_eff # columns show parameters, rows show n_eff
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             a[1]        a[2]        b[1]        b[2]       sigma
## 5       2.314186    1.587251    2.713325    1.270369    1.776862
## 10   2243.084776 2157.086156  737.957589 1010.214712  953.010860
## 100  1725.334719 2294.576251  878.481785 1177.016946 1122.495229
## 500  2999.738299 3282.963810 2292.173710 2737.037252 2200.949134
## 1000 2485.029304 3406.341675 2372.274092 2772.175825 2607.552453
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we can see, past just 10 warmup samples, &lt;code&gt;n_eff&lt;/code&gt; does not change much (in terms of how useful our samples are). In this case, we could be quite happy with a warmup of 10.&lt;/p&gt;
&lt;h2 id=&#34;hard-exercises&#34;&gt;Hard Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-h1&#34;&gt;Practice H1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Run the model below and then inspect the posterior distribution and explain what it is accomplishing.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mp &amp;lt;- map2stan(
  alist(
    a ~ dnorm(0, 1),
    b ~ dcauchy(0, 1)
  ),
  data = list(y = 1),
  start = list(a = 0, b = 0),
  iter = 1e4,
  chains = 2, cores = 2,
  warmup = 100,
  WAIC = FALSE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Compare the samples for the parameters &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;. Can you explain the different trace plots, using what you know about the Cauchy distribution?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; First of all, let&amp;rsquo;s inspect the posterior:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(mp)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            mean         sd      5.5%    94.5%     n_eff    Rhat4
## a  0.0003388167  0.9988213 -1.601441 1.590561 12762.761 1.000120
## b -0.1918181852 13.8995715 -5.379742 5.346423  3892.011 1.000517
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Oof. Those uncertainties don&amp;rsquo;t look good at all! So what does the model even do? It simply just samples &lt;code&gt;a&lt;/code&gt; from a normal distribution with mean 0 and standard deviation 1. &lt;code&gt;b&lt;/code&gt; is sampled from a cauchy distribution. Let&amp;rsquo;s look at the traceplot for this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(mp, n_cols = 1, col = &amp;quot;royalblue4&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-02-25-statistical-rethinking-chapter-09_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we can see, there are quite some outliers in the sampling of the cauchy distribution (&lt;code&gt;b&lt;/code&gt;). Why is that? Because the cauchy distribution has very heavy tails thus making it more likely to jump to a value that is far out there in terms of posterior probability. Note that this also decreases &lt;code&gt;n_eff&lt;/code&gt;. &lt;code&gt;lp&lt;/code&gt; in the above is the log-posterior.&lt;/p&gt;
&lt;p&gt;Now let&amp;rsquo;s see how the samples we drew measure up against the underlying functions of &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;, respectively:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;post &amp;lt;- extract.samples(mp)
par(mfrow = c(1, 2))
dens(post$a)
curve(dnorm(x, 0, 1), from = -4, to = 4, add = T, lty = 2)
legend(&amp;quot;topright&amp;quot;, lty = c(1, 2), legend = c(&amp;quot;Sample&amp;quot;, &amp;quot;Exact density&amp;quot;), bty = &amp;quot;n&amp;quot;)
mtext(&amp;quot;Normal&amp;quot;)
dens(post$b, col = &amp;quot;royalblue4&amp;quot;, xlim = c(-10, 10))
curve(dcauchy(x, 0, 1),
  from = -10, to = 10, add = T, lty = 2,
  col = &amp;quot;royalblue4&amp;quot;
)
mtext(&amp;quot;Cauchy&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-02-25-statistical-rethinking-chapter-09_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the normal distribution has been reconstructed well. The cauchy distributions hasn&amp;rsquo;t.&lt;/p&gt;
&lt;h3 id=&#34;practice-h2&#34;&gt;Practice H2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Recall the divorce rate example from Chapter 5. Repeat that analysis, using &lt;code&gt;ulam()&lt;/code&gt; this time, fitting models &lt;code&gt;m5.1&lt;/code&gt;, &lt;code&gt;m5.2&lt;/code&gt;, and &lt;code&gt;m5.3&lt;/code&gt;. Use compare to compare the models on the basis of WAIC or PSIS. Explain the results.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; First, I need to load the data and prepare it for &lt;code&gt;ulam()&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(WaffleDivorce)
d &amp;lt;- WaffleDivorce
d$D &amp;lt;- standardize(d$Divorce)
d$M &amp;lt;- standardize(d$Marriage)
d$A &amp;lt;- standardize(d$MedianAgeMarriage)
d_trim &amp;lt;- list(D = d$D, M = d$M, A = d$A)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I fit the models with &lt;code&gt;ulam()&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m5.1_stan &amp;lt;- ulam(
  alist(
    D ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bA * A,
    a ~ dnorm(0, 0.2),
    bA ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d_trim,
  chains = 4, cores = 4,
  log_lik = TRUE # this is needed to get the terms for calculating PSIS or WAIC
)
m5.2_stan &amp;lt;- ulam(
  alist(
    D ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bM * M,
    a ~ dnorm(0, 0.2),
    bM ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d_trim,
  chains = 4, cores = 4,
  log_lik = TRUE # this is needed to get the terms for calculating PSIS or WAIC
)
m5.3_stan &amp;lt;- ulam(
  alist(
    D ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bA * A + bM * M,
    a ~ dnorm(0, 0.2),
    bA ~ dnorm(0, 0.5),
    bM ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d_trim,
  chains = 4, cores = 4,
  log_lik = TRUE # this is needed to get the terms for calculating PSIS or WAIC
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we compare the models:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;compare(m5.1_stan, m5.2_stan, m5.3_stan, func = PSIS)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               PSIS        SE     dPSIS       dSE    pPSIS       weight
## m5.1_stan 125.7210 12.708327  0.000000        NA 3.630705 0.7253039155
## m5.3_stan 127.6690 12.852350  1.947996 0.6705316 4.773054 0.2738533387
## m5.2_stan 139.2364  9.936093 13.515361 9.1363047 2.923975 0.0008427459
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;compare(m5.1_stan, m5.2_stan, m5.3_stan, func = WAIC)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               WAIC        SE     dWAIC       dSE    pWAIC       weight
## m5.1_stan 125.7778 12.641919  0.000000        NA 3.659072 0.6960655494
## m5.3_stan 127.4407 12.591741  1.662916 0.6770545 4.658881 0.3030766321
## m5.2_stan 139.1754  9.813604 13.397613 9.2109285 2.893468 0.0008578185
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;WAIC tells a similar story as PSIS, but the model only containing age (&lt;code&gt;m5.1_stan&lt;/code&gt;) wins. The model with both predictors (&lt;code&gt;m5.3_stan&lt;/code&gt;) does almost as well. However, their respective PSIS and WAIC values are nearly identical. Furthermore, both models get assigned all of the WAIC weight. Let&amp;rsquo;s call these equal in performance and investigate why:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m5.3_stan)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                mean         sd       5.5%      94.5%    n_eff     Rhat4
## a     -0.0001904293 0.10140928 -0.1591984  0.1619373 1877.251 1.0002239
## bA    -0.6023698429 0.16025804 -0.8510854 -0.3467602 1085.019 1.0007578
## bM    -0.0550634908 0.16034205 -0.3109204  0.2015101 1187.780 0.9998155
## sigma  0.8275838910 0.08826874  0.7028130  0.9779113 1474.265 1.0028212
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While &lt;code&gt;m5.3_stan&lt;/code&gt; contains the marriage predictor, it is very unsure of it&amp;rsquo;s influence. In practical terms, this means that &lt;code&gt;m5.1_stan&lt;/code&gt; and &lt;code&gt;m5.3_stan&lt;/code&gt; make basically the same predictions&lt;/p&gt;
&lt;h3 id=&#34;practice-h3&#34;&gt;Practice H3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Sometimes changing a prior for one parameter has unanticipated effects on other parameters. This is because when a parameter is highly correlated with another parameter in the posterior, the prior influences both parameters. Here’s an example to work and think through.&lt;br&gt;
Go back to the leg length example in Chapter 5. Here is the code again, which simulates height and leg lengths for 100 imagined individuals:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;N &amp;lt;- 100 # number of individuals
height &amp;lt;- rnorm(N, 10, 2) # sim total height of each
leg_prop &amp;lt;- runif(N, 0.4, 0.5) # leg as proportion of height
leg_left &amp;lt;- leg_prop * height + rnorm(N, 0, 0.02) # sim left leg as proportion + error
leg_right &amp;lt;- leg_prop * height + rnorm(N, 0, 0.02) # sim right leg as proportion + error
d &amp;lt;- data.frame(height, leg_left, leg_right) # combine into data frame
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And below is the model you fit before, resulting in a highly correlated posterior for the two beta parameters. This time, fit the model using &lt;code&gt;ulam()&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m5.8s &amp;lt;- ulam(
  alist(
    height ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bl * leg_left + br * leg_right,
    a ~ dnorm(10, 100),
    bl ~ dnorm(2, 10),
    br ~ dnorm(2, 10),
    sigma ~ dexp(1)
  ),
  data = d,
  chains = 4,
  cores = 4,
  start = list(
    a = 10,
    bl = 0,
    br = 0.1,
    sigma = 1
  )
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Compare the posterior distribution produced by the code above to the posterior distribution produced when you change the prior for &lt;code&gt;br&lt;/code&gt; so that it is strictly positive:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m5.8s2 &amp;lt;- ulam(
  alist(
    height ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bl * leg_left + br * leg_right,
    a ~ dnorm(10, 100),
    bl ~ dnorm(2, 10),
    br ~ dnorm(2, 10),
    sigma ~ dexp(1)
  ),
  data = d,
  chains = 4,
  cores = 4,
  constraints = list(br = &amp;quot;lower=0&amp;quot;),
  start = list(
    a = 10,
    bl = 0,
    br = 0.1,
    sigma = 1
  )
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note the constraints list. What this does is constrain the prior distribution of &lt;code&gt;br&lt;/code&gt; so that it has positive probability only above zero. In other words, that prior ensures that the posterior distribution for &lt;code&gt;br&lt;/code&gt; will have no probability mass below zero.&lt;br&gt;
Compare the two posterior distributions for &lt;code&gt;m5.8s&lt;/code&gt; and &lt;code&gt;m5.8s2&lt;/code&gt;. What has changed in the posterior distribution of both beta parameters? Can you explain the change induced by the change in prior?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; It&amp;rsquo;s probably easiest to just look at the posterior distributions of the beta prameters through the &lt;code&gt;pairs()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;pairs(m5.8s, main = &amp;quot;Model 1&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-02-25-statistical-rethinking-chapter-09_files/figure-html/unnamed-chunk-24-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;pairs(m5.8s2, main = &amp;quot;Model 2&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-02-25-statistical-rethinking-chapter-09_files/figure-html/unnamed-chunk-24-2.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the beta distributions have shifted drastically between the different models. Interestingly, &lt;code&gt;bl&lt;/code&gt; and &lt;code&gt;br&lt;/code&gt; were perfectly symmetric in &lt;code&gt;m5.8s&lt;/code&gt;, but are skewed in &lt;code&gt;m5.8s2&lt;/code&gt;. Given how the height of a person is approximated in both models (&lt;code&gt;a + bl*leg_left + br*leg_right&lt;/code&gt;), the distributions of leg lengths are necessarily negatively correlated (you can be of the same height with a short right leg and long left leg, long left leg and short right leg, or two medium-length legs). Thus, by setting &lt;code&gt;br&lt;/code&gt; to be strictly positive in &lt;code&gt;m5.8s2&lt;/code&gt; and made it skewed, we have forced &lt;code&gt;bl&lt;/code&gt; to be equally skewed in a mirror image of &lt;code&gt;br&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;practice-h4&#34;&gt;Practice H4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; For the two models fit in the previous problem, use WAIC or PSIS to compare the effective numbers of parameters for each model. You will need to use &lt;code&gt;log_lik=TRUE&lt;/code&gt; to instruct &lt;code&gt;ulam()&lt;/code&gt; to compute the terms that both WAIC and PSIS need. Which model has more effective parameters? Why?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Let&amp;rsquo;s run the models:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m.H4_1 &amp;lt;- ulam(
  alist(
    height ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bl * leg_left + br * leg_right,
    a ~ dnorm(10, 100),
    bl ~ dnorm(2, 10),
    br ~ dnorm(2, 10),
    sigma ~ dexp(1)
  ),
  data = d,
  chains = 4,
  cores = 4,
  start = list(
    a = 10,
    bl = 0,
    br = 0.1,
    sigma = 1
  ),
  log_lik = TRUE
)
m.H4_2 &amp;lt;- ulam(
  alist(
    height ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bl * leg_left + br * leg_right,
    a ~ dnorm(10, 100),
    bl ~ dnorm(2, 10),
    br ~ dnorm(2, 10),
    sigma ~ dexp(1)
  ),
  data = d,
  chains = 4,
  cores = 4,
  constraints = list(br = &amp;quot;lower=0&amp;quot;),
  start = list(
    a = 10,
    bl = 0,
    br = 0.1,
    sigma = 1
  ),
  log_lik = TRUE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we compare them with WAIC:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;compare(m.H4_1, m.H4_2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            WAIC       SE     dWAIC      dSE    pWAIC    weight
## m.H4_1 182.1474 10.21060 0.0000000       NA 2.961292 0.6063273
## m.H4_2 183.0112  9.88398 0.8638001 2.349502 2.382919 0.3936727
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The models are pretty much tied. The model with truncated priors (&lt;code&gt;m.H4_2&lt;/code&gt;) is less flexible as indicated by &lt;code&gt;pWAIC&lt;/code&gt;. This is because the prior is more informative and the variance in the posterior distribution is smaller as a result.&lt;/p&gt;
&lt;h3 id=&#34;practice-h5&#34;&gt;Practice H5&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Modify the Metropolis algorithm code from the chapter to handle the case that the island populations have a different distribution than the island labels. This means the island’s number will not be the same as its population.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; First of all, we need our 10 islands with population sizes of 1-10, but in random order:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;pop_size &amp;lt;- sample(1:10)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can use the code from the chapter almost unaltered safe for one exception - we need to use indexing to translate island location into population size:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;num_weeks &amp;lt;- 1e5
positions &amp;lt;- rep(NA, num_weeks)
current &amp;lt;- 10
for (i in 1:num_weeks) {
  positions[i] &amp;lt;- current # record current position
  proposal &amp;lt;- current + sample(c(-1, 1), size = 1) # flip coin to generate proposal
  # now make sure he loops around the archipelago
  if (proposal &amp;lt; 1) proposal &amp;lt;- 10
  if (proposal &amp;gt; 10) proposal &amp;lt;- 1
  prob_move &amp;lt;- pop_size[proposal] / pop_size[current] # move?
  current &amp;lt;- ifelse(runif(1) &amp;lt; prob_move, proposal, current)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To see if this works, we can plot population size against frequency of visit by the king:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;f &amp;lt;- table(positions) # compute frequencies
plot(as.vector(f), pop_size,
  type = &amp;quot;n&amp;quot;, # plot frequencies against relative population sizes
  xlab = &amp;quot;frequency&amp;quot;, ylab = &amp;quot;population size&amp;quot;
) # empty plot
text(x = f, y = pop_size, labels = names(f)) # add names of islands / their positions
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-02-25-statistical-rethinking-chapter-09_files/figure-html/unnamed-chunk-29-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;practice-h6&#34;&gt;Practice H6&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Modify the Metropolis algorithm code from the chapter to write your own simple MCMC estimator for globe tossing data and model from Chapter 2.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; We want to fit the following model:&lt;/p&gt;
&lt;p&gt;$$w∼Binom(θ,n)$$
$$θ∼Unif(0,1)$$
Our Metropolis algorithm looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(42)
# the globe tossing data
w &amp;lt;- 6
n &amp;lt;- 9
# prior on p
p_prior &amp;lt;- function(p) dunif(p, min = 0, max = 1)
# initializing MCMC
iter &amp;lt;- 1e4
p_sample &amp;lt;- rep(0, iter)
p_current &amp;lt;- 0.5 # start value
for (i in 1:iter) {
  p_sample[i] &amp;lt;- p_current # # record current p
  p_proposal &amp;lt;- runif(1, min = 0, max = 1) # generate proposal
  # compute likelihood for current and proposal
  lkhd_current &amp;lt;- dbinom(w, n, p_current)
  lkhd_proposal &amp;lt;- dbinom(w, n, p_proposal)
  prob_proposal &amp;lt;- lkhd_proposal * p_prior(p_proposal)
  prob_current &amp;lt;- lkhd_current * p_prior(p_current)
  prob_accept &amp;lt;- prob_proposal / prob_current
  p_current &amp;lt;- ifelse(runif(1) &amp;lt; prob_accept, p_proposal, p_current)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s visualise what happened here:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(p_sample, type = &amp;quot;l&amp;quot;, col = &amp;quot;royalblue4&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-02-25-statistical-rethinking-chapter-09_files/figure-html/unnamed-chunk-31-1.png&#34; width=&#34;1440&#34; /&gt;
Finally, let&amp;rsquo;s plot the posterior distribution:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dens(p_sample, col = &amp;quot;royalblue4&amp;quot;, adj = 1)
curve(dbeta(x, w + 1, n - w + 1), from = 0, to = 1, add = T, lty = 2)
abline(v = median(p_sample))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-02-25-statistical-rethinking-chapter-09_files/figure-html/unnamed-chunk-32-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.5 (2021-03-31)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19043)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.1252  LC_CTYPE=English_United Kingdom.1252    LC_MONETARY=English_United Kingdom.1252 LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.1252    
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] tidybayes_2.3.1      rethinking_2.13      rstan_2.21.2         ggplot2_3.3.6        StanHeaders_2.21.0-7
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.7           mvtnorm_1.1-1        lattice_0.20-41      tidyr_1.1.3          prettyunits_1.1.1    ps_1.6.0             assertthat_0.2.1     digest_0.6.27        utf8_1.2.1          
## [10] V8_3.4.1             plyr_1.8.6           R6_2.5.0             backports_1.2.1      stats4_4.0.5         evaluate_0.14        coda_0.19-4          highr_0.9            blogdown_1.3        
## [19] pillar_1.6.0         rlang_0.4.11         curl_4.3.2           callr_3.7.0          jquerylib_0.1.4      R.utils_2.10.1       R.oo_1.24.0          rmarkdown_2.7        styler_1.4.1        
## [28] labeling_0.4.2       stringr_1.4.0        loo_2.4.1            munsell_0.5.0        compiler_4.0.5       xfun_0.22            pkgconfig_2.0.3      pkgbuild_1.2.0       shape_1.4.5         
## [37] htmltools_0.5.1.1    tidyselect_1.1.0     tibble_3.1.1         gridExtra_2.3        bookdown_0.22        arrayhelpers_1.1-0   codetools_0.2-18     matrixStats_0.61.0   fansi_0.4.2         
## [46] crayon_1.4.1         dplyr_1.0.5          withr_2.4.2          MASS_7.3-53.1        R.methodsS3_1.8.1    distributional_0.2.2 ggdist_2.4.0         grid_4.0.5           jsonlite_1.7.2      
## [55] gtable_0.3.0         lifecycle_1.0.0      DBI_1.1.1            magrittr_2.0.1       scales_1.1.1         KernSmooth_2.23-18   RcppParallel_5.1.2   cli_3.0.0            stringi_1.5.3       
## [64] farver_2.1.0         bslib_0.2.4          ellipsis_0.3.2       generics_0.1.0       vctrs_0.3.7          rematch2_2.1.2       forcats_0.5.1        tools_4.0.5          svUnit_1.0.6        
## [73] R.cache_0.14.0       glue_1.4.2           purrr_0.3.4          processx_3.5.1       yaml_2.2.1           inline_0.3.17        colorspace_2.0-0     knitr_1.33           sass_0.3.1
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 10 &amp; 11</title>
      <link>https://www.erikkusch.com/courses/rethinking/chapter-11/</link>
      <pubDate>Thu, 18 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/rethinking/chapter-11/</guid>
      <description>&lt;h1 id=&#34;god-spiked-the-integers&#34;&gt;God Spiked The Integers&lt;/h1&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/rethinking/11__05-03-2021_SUMMARY_-Generalised-Linear-Models.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides Chapter 10&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/rethinking/13__19-03-2021_SUMMARY_-Discrete-Outcomes.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides Chapter 11&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to the exercises at the end of chapter 11 in 
&lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Satistical Rethinking 2&lt;/a&gt; by Richard McElreath. For anyone reading through these in order and wondering why I skipped chapter 10: chapter 10 did not contain any exercises (to my dismay, as you can imagine). I have created these notes as a part of my ongoing involvement in the 
&lt;a href=&#34;https://www.erikkusch.com/project/aubayes/&#34;&gt;AU Bayes Study Group&lt;/a&gt;. Much of my inspiration for these solutions, where necessary, has been obtained from 
&lt;a href=&#34;https://github.com/jffist/statistical-rethinking-solutions/blob/master/ch10_hw.R&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Taras Svirskyi&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/cavaunpeu/statistical-rethinking/blob/master/chapter-10/homework.R&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;William Wolf&lt;/a&gt;, and 
&lt;a href=&#34;https://www.samples-of-thoughts.com/projects/statistical-rethinking/chapter_10/chp10-ex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Corrie Bartelheimer&lt;/a&gt; as well as the solutions provided to instructors by Richard McElreath himself.&lt;/p&gt;
&lt;h2 id=&#34;r-environment&#34;&gt;&lt;code&gt;R&lt;/code&gt; Environment&lt;/h2&gt;
&lt;p&gt;For today&amp;rsquo;s exercise, I load the following packages:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rethinking)
library(rstan)
library(ggplot2)
library(tidybayes)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;easy-exercises&#34;&gt;Easy Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-e1&#34;&gt;Practice E1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  If an event has probability 0.35, what are the log-odds of this event?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  When $p = 0.35$ then the log-odds are $log\frac{0.35}{1-0.35}$, or in &lt;code&gt;R&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;log(0.35 / (1 - 0.35))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.6190392
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-e2&#34;&gt;Practice E2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  If an event has log-odds 3.2, what is the probability of this event?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; To transform log-odds into probability space, we want to use the &lt;code&gt;inv_logit()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;inv_logit(3.2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9608343
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-e3&#34;&gt;Practice E3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Suppose that a coefficient in a logistic regression has value 1.7. What does this imply about the proportional change in odds of the outcome?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;exp(1.7)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5.473947
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each one-unit increase in the predictor linked to this coefficient results in a multiplication of the odds of the event occurring by 5.47.&lt;/p&gt;
&lt;p&gt;The linear model behind the logistic regression simply represents the log-odds of an event happening. The odds of the events happening can thus be shown as $exp(odds)$. Comparing how the odds change when increasing the predictor variable by one unit comes down to solving this equation then:&lt;/p&gt;
&lt;p&gt;$$exp(α + βx)Z = exp(α + β(x + 1))$$
Solving this for $z$ results in:&lt;/p&gt;
&lt;p&gt;$$z = exp(\beta)$$&lt;/p&gt;
&lt;p&gt;which is how we derived the answer to this question.&lt;/p&gt;
&lt;h3 id=&#34;practice-e4&#34;&gt;Practice E4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Why do Poisson regressions sometimes require the use of an offset? Provide an example.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; When study regimes aren&amp;rsquo;t rigidly standardised, we may end up with count data collected over different time/distance intervals. Comparing these data without accounting for the difference in the underlying sampling frequency will inevitably lead to horribly inadequate predictions of our model(s).&lt;/p&gt;
&lt;p&gt;As an example, think of how many ants leave a hive in a certain interval. If we recorded numbers of ants leaving to forage on a minute-by-minute basis, we would obtain much smaller counts than if our sampling regime dictated hourly observation periods. Any poisson model we want to run between differing sampling regimes has to account for the heterogeneity in the observation period lengths. We do so as follows:&lt;/p&gt;
&lt;p&gt;$$Ants_i∼Poisson(λ)$$
$$log(λ)=log(period_i)+α+βHive_i$$&lt;/p&gt;
&lt;h2 id=&#34;medium-exercises&#34;&gt;Medium Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-m1&#34;&gt;Practice M1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  As explained in the chapter, binomial data can be organized in aggregated and disaggregated forms, without any impact on inference. But the likelihood of the data does change when the data are converted between the two formats. Can you explain why?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Think back to the 
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/rethinking/statistical-rethinking-chapter-02/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Card Drawing Example&lt;/a&gt; from chapter 2. We know a certain outcome. Let&amp;rsquo;s assume two black face, and one white face card are drawn.&lt;/p&gt;
&lt;p&gt;In the aggregated form of the data, we obtain the probability of our observation as $3p(1-p)$ (a binomial distribution with $3$ trials and a rate of black face cards of $p = \frac{2}{3}$). This tells us how many ways there are to get two black-face cards out of three pulls of cards. The order is irrelevant.&lt;/p&gt;
&lt;p&gt;With disaggregated data, we do not cope with any order, but simply predict the result of each draw of a card by itself and finally multiply our predictions together to form a joint probability according to $p(1-p)$.&lt;/p&gt;
&lt;p&gt;In conclusion, aggregated data is modelled with an extra constant to handle permutations. This does not change our inference, but merely changes the likelihood and log-likelihood.&lt;/p&gt;
&lt;h3 id=&#34;practice-m2&#34;&gt;Practice M2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  If a coefficient in a Poisson regression has value 1.7, what does this imply about the change in the outcome?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; A basic Poisson regression is expressed as such:
$$log(λ) = α + βx$$
$$λ = exp(α + βx)$$&lt;/p&gt;
&lt;p&gt;In this specific case $\beta = 1.7$. So what happens to $\lambda$ when $x$ increases by $1$? To solve this, we write a formula for the change in $\lambda$:&lt;/p&gt;
&lt;p&gt;$$Δλ = exp(α + β(x + 1)) − exp(α + βx)$$
$$Δλ = exp(α + βx)(exp(β) − 1)$$&lt;/p&gt;
&lt;p&gt;Unfortunately, this is about as far as we can take solving this formula. The change in $\lambda$ depends on all contents of the model. But about the ratio of $\lambda$ following a one-unit increase in $x$ compared to $\lambda$ a t base-level? We can compute this ratio as:&lt;/p&gt;
&lt;p&gt;$$\frac{λ_{x+1}}{λx} = \frac{exp(α + β(x + 1))}{exp(α + βx)} = exp(β)$$&lt;/p&gt;
&lt;p&gt;This is reminiscent of the proportional change in odds for logistic regressions. Conclusively, a coefficient of $\beta = 1.7$ in a Poisson model results in a proportional change in the expected count of exp(1.7) =  5.47 when the corresponding predictor variable increases by one unit.&lt;/p&gt;
&lt;h3 id=&#34;practice-m3&#34;&gt;Practice M3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Explain why the logit link is appropriate for a binomial generalized linear model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  With a binomial generalised linear model, we are interested in an outcome space between 0 and 1. With the outcome space denoting probabilities of an event transpiring. Our underlying linear model has no qualms about estimating parameter values outside of this interval. The logit link maps such probability space into $ℝ$ (linear model space).&lt;/p&gt;
&lt;h3 id=&#34;practice-m4&#34;&gt;Practice M4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Explain why the log link is appropriate for a Poisson generalized linear model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Poisson generalised linear models are producing strictly non-negative outputs (negative counts are impossible). As such, the underlying linear model space needs to be matched to the outcome space which is strictly non-negative. The log function maps positive value onto $ℝ$ and thus the function links count values (positive values) to a linear model.&lt;/p&gt;
&lt;h3 id=&#34;practice-m5&#34;&gt;Practice M5&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  What would it imply to use a logit link for the mean of a Poisson generalized linear model? Can you think of a real research problem for which this would make sense?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Using a logit link in a Poisson model implies that the mean of the Poisson distribution lies between 0 and 1:&lt;/p&gt;
&lt;p&gt;$$y_i ∼ Poisson(μ_i)$$
$$logit(μ_i) = α + βx_i$$
This would imply that there is at most one event per time interval. This might be the case for very rare or extremely cyclical events such as counting the number of El Niño events every four years or so.&lt;/p&gt;
&lt;h3 id=&#34;practice-m6&#34;&gt;Practice M6&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  State the constraints for which the binomial and Poisson distributions have maximum entropy. Are the constraints different at all for binomial and Poisson? Why or why not?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  For binomial and Poisson distributions to have maximum entropy, we need to meet the following assumptions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Discrete, binary outcomes&lt;/li&gt;
&lt;li&gt;Constant probability of event occurring across al trials (this is the same as a constant expected value)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Both distributions have the same constraints as Poisson is a simplified form of the binomial.&lt;/p&gt;
&lt;h2 id=&#34;hard-exercises&#34;&gt;Hard Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-h1&#34;&gt;Practice H1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Use &lt;code&gt;quap&lt;/code&gt; to construct a quadratic approximate posterior distribution for the chimpanzee model that includes a unique intercept for each actor, &lt;code&gt;m11.4&lt;/code&gt; (page 338). Compare the quadratic approximation to the posterior distribution produced instead from MCMC. Can you explain both the differences and the similarities between the approximate and the MCMC distributions?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Here are the models according to the book:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(chimpanzees)
d &amp;lt;- chimpanzees
d$treatment &amp;lt;- 1 + d$prosoc_left + 2 * d$condition
dat_list &amp;lt;- list(
  pulled_left = d$pulled_left,
  actor = d$actor,
  treatment = as.integer(d$treatment)
)
## MCMC model
m11.4 &amp;lt;- ulam(
  alist(
    pulled_left ~ dbinom(1, p),
    logit(p) &amp;lt;- a[actor] + b[treatment],
    a[actor] ~ dnorm(0, 1.5),
    b[treatment] ~ dnorm(0, 0.5)
  ),
  data = dat_list, chains = 4, log_lik = TRUE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &#39;80e2b6267e3dc4ff0c2916d0cf0879e8&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0.001 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 10 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 1: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 1: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 1: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 1: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 1.57 seconds (Warm-up)
## Chain 1:                1.629 seconds (Sampling)
## Chain 1:                3.199 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;80e2b6267e3dc4ff0c2916d0cf0879e8&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 0 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 2: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 2: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 2: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 2: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 2: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 2: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 2: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 2: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 2: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 2: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 2: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 1.667 seconds (Warm-up)
## Chain 2:                1.211 seconds (Sampling)
## Chain 2:                2.878 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &#39;80e2b6267e3dc4ff0c2916d0cf0879e8&#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 0 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 3: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 3: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 3: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 3: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 3: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 3: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 3: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 3: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 3: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 3: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 3: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 1.639 seconds (Warm-up)
## Chain 3:                2.718 seconds (Sampling)
## Chain 3:                4.357 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &#39;80e2b6267e3dc4ff0c2916d0cf0879e8&#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 0 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 4: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 4: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 4: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 4: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 4: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 4: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 4: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 4: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 4: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 4: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 4: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 2.031 seconds (Warm-up)
## Chain 4:                2.195 seconds (Sampling)
## Chain 4:                4.226 seconds (Total)
## Chain 4:
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m11.4, depth = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean        sd        5.5%       94.5%    n_eff    Rhat4
## a[1] -0.45075443 0.3331248 -0.98953991  0.06075382 698.8809 1.005361
## a[2]  3.91891754 0.8083257  2.74211339  5.24785152 901.5623 0.998913
## a[3] -0.75442240 0.3312761 -1.28541454 -0.24524706 699.9722 1.003839
## a[4] -0.76567679 0.3369675 -1.30127846 -0.23894487 857.8075 1.003696
## a[5] -0.44359742 0.3267862 -0.97201306  0.07594653 674.2582 1.005466
## a[6]  0.46443104 0.3317285 -0.05869988  0.99613901 810.7556 1.003976
## a[7]  1.96593450 0.4242551  1.31622570  2.65044970 899.8839 1.001162
## b[1] -0.03793442 0.2858597 -0.49368201  0.42539507 686.8362 1.005492
## b[2]  0.48704573 0.2866880  0.03296044  0.95020885 703.0115 1.004415
## b[3] -0.38805106 0.2815222 -0.82364579  0.06867557 639.3679 1.004886
## b[4]  0.37075102 0.2834843 -0.08070002  0.83024441 623.7355 1.006236
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Quap Model
m11.4quap &amp;lt;- quap(
  alist(
    pulled_left ~ dbinom(1, p),
    logit(p) &amp;lt;- a[actor] + b[treatment],
    a[actor] ~ dnorm(0, 1.5),
    b[treatment] ~ dnorm(0, 0.5)
  ),
  data = dat_list
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(coeftab(m11.4, m11.4quap),
  labels = paste(rep(rownames(coeftab(m11.4, m11.4quap)@coefs), each = 2),
    rep(c(&amp;quot;MCMC&amp;quot;, &amp;quot;quap&amp;quot;), nrow(coeftab(m11.4, m11.4quap)@coefs) * 2),
    sep = &amp;quot;-&amp;quot;
  )
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-18-statistical-rethinking-chapter-11_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looking at these parameter estimates, it is apparent that quadratic approximation is doing a good job in this case. The only noticeable difference lies with &lt;code&gt;a[2]&lt;/code&gt; which shows a higher estimate with the &lt;code&gt;ulam&lt;/code&gt; model. Let&amp;rsquo;s look at the densities of the estimates of this parameter:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;post &amp;lt;- extract.samples(m11.4)
postq &amp;lt;- extract.samples(m11.4quap)
dens(post$a[, 2], lwd = 2)
dens(postq$a[, 2], add = TRUE, lwd = 2, col = rangi2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-18-statistical-rethinking-chapter-11_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;ulam&lt;/code&gt; model (in black) placed more probability mass in the upper end of the tail which ends up pushing the mean of this posterior distribution further to the right when compared to that of the quadratic approximation model. This is because the quadratic approximation assumes the posterior distribution to be Gaussian thus producing a symmetric distribution with less probability mass in the upper tail.&lt;/p&gt;
&lt;h3 id=&#34;practice-h2&#34;&gt;Practice H2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Use &lt;code&gt;WAIC&lt;/code&gt; to compare the chimpanzee model that includes a unique intercept for each actor, &lt;code&gt;m11.4&lt;/code&gt; (page 338), to the simpler models fit in the same section.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  The models in question are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Intercept only&lt;/em&gt; model:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m11.1 &amp;lt;- quap(
  alist(
    pulled_left ~ dbinom(1, p),
    logit(p) &amp;lt;- a,
    a ~ dnorm(0, 10)
  ),
  data = d
)
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;em&gt;Intercept and Treatment&lt;/em&gt; model:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m11.3 &amp;lt;- quap(
  alist(
    pulled_left ~ dbinom(1, p),
    logit(p) &amp;lt;- a + b[treatment],
    a ~ dnorm(0, 1.5),
    b[treatment] ~ dnorm(0, 0.5)
  ),
  data = d
)
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;em&gt;Individual Intercept and Treatment&lt;/em&gt; model:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m11.4 &amp;lt;- ulam(
  alist(
    pulled_left ~ dbinom(1, p),
    logit(p) &amp;lt;- a[actor] + b[treatment],
    a[actor] ~ dnorm(0, 1.5),
    b[treatment] ~ dnorm(0, 0.5)
  ),
  data = dat_list, chains = 4, log_lik = TRUE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &#39;80e2b6267e3dc4ff0c2916d0cf0879e8&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 1: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 1: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 1: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 1: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.978 seconds (Warm-up)
## Chain 1:                0.775 seconds (Sampling)
## Chain 1:                1.753 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;80e2b6267e3dc4ff0c2916d0cf0879e8&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 0 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 2: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 2: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 2: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 2: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 2: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 2: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 2: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 2: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 2: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 2: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 2: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.865 seconds (Warm-up)
## Chain 2:                0.834 seconds (Sampling)
## Chain 2:                1.699 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &#39;80e2b6267e3dc4ff0c2916d0cf0879e8&#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 0 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 3: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 3: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 3: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 3: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 3: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 3: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 3: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 3: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 3: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 3: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 3: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.857 seconds (Warm-up)
## Chain 3:                0.612 seconds (Sampling)
## Chain 3:                1.469 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &#39;80e2b6267e3dc4ff0c2916d0cf0879e8&#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 0 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 4: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 4: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 4: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 4: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 4: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 4: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 4: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 4: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 4: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 4: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 4: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.919 seconds (Warm-up)
## Chain 4:                0.937 seconds (Sampling)
## Chain 4:                1.856 seconds (Total)
## Chain 4:
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To compare these, we can run:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(comp &amp;lt;- compare(m11.1, m11.3, m11.4))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           WAIC        SE    dWAIC      dSE    pWAIC       weight
## m11.4 532.4794 18.927161   0.0000       NA 8.572123 1.000000e+00
## m11.3 682.4152  8.973761 149.9358 18.37892 3.553310 2.765987e-33
## m11.1 687.9540  6.994012 155.4746 18.91781 1.004840 1.734267e-34
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(comp)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-18-statistical-rethinking-chapter-11_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This shows clearly that the model accounting for individual intercepts as well as treatment effects (&lt;code&gt;m11.4&lt;/code&gt;) outperforms the simpler models.&lt;/p&gt;
&lt;h3 id=&#34;practice-h3&#34;&gt;Practice H3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; The data contained in &lt;code&gt;library(MASS);data(eagles)&lt;/code&gt; are records of salmon pirating attempts by Bald Eagles in Washington State. See &lt;code&gt;?eagles&lt;/code&gt; for details. While one eagle feeds, sometimes another will swoop in and try to steal the salmon from it. Call the feeding eagle the “victim” and the thief the “pirate.” Use the available data to build a binomial GLM of successful pirating attempts.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(MASS)
data(eagles)
d &amp;lt;- eagles
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-a&#34;&gt;Part A&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Consider the following model:&lt;/p&gt;
&lt;p&gt;$$y_i ∼ Binomial(n_i, p_i)$$
$$log\frac{p_i}{1 − p_i} = α + β_PP_i + β_VV_i + β_AA_i $$
$$α ∼ Normal(0, 1.5)$$
$$β_P ∼ Normal(0, 0.5)$$ 
$$β_V ∼ Normal(0, 0.5)$$ 
$$β_A ∼ Normal(0, 0.5)$$
where $y$ is the number of successful attempts, $n$ is the total number of attempts, $P$ is a dummy variable indicating whether or not the pirate had large body size, $V$ is a dummy variable indicating whether or not the victim had large body size, and finally $A$ is a dummy variable indicating whether or not the pirate was an adult.&lt;/p&gt;
&lt;p&gt;Fit the model above to the eagles data, using both &lt;code&gt;quap&lt;/code&gt; and &lt;code&gt;ulam&lt;/code&gt;. Is the quadratic approximation okay?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; First, we have to make our dummy variables:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;d$pirateL &amp;lt;- ifelse(d$P == &amp;quot;L&amp;quot;, 1, 0)
d$victimL &amp;lt;- ifelse(d$V == &amp;quot;L&amp;quot;, 1, 0)
d$pirateA &amp;lt;- ifelse(d$A == &amp;quot;A&amp;quot;, 1, 0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Fitting the models is now trivial:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# define model list specification
f &amp;lt;- alist(
  y ~ dbinom(n, p),
  logit(p) &amp;lt;- a + bP * pirateL + bV * victimL + bA * pirateA,
  a ~ dnorm(0, 1.5),
  bP ~ dnorm(0, .5),
  bV ~ dnorm(0, .5),
  bA ~ dnorm(0, .5)
)
## quap model
mH3quap &amp;lt;- quap(f, data = d)
## ulam model
mH3ulam &amp;lt;- ulam(f, data = d, chains = 4, log_lik = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &#39;4eaf24dd51e5e9fce10e2cc7d32e0b01&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 1: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 1: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 1: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 1: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.07 seconds (Warm-up)
## Chain 1:                0.066 seconds (Sampling)
## Chain 1:                0.136 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;4eaf24dd51e5e9fce10e2cc7d32e0b01&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 0 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 2: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 2: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 2: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 2: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 2: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 2: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 2: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 2: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 2: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 2: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 2: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.08 seconds (Warm-up)
## Chain 2:                0.109 seconds (Sampling)
## Chain 2:                0.189 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &#39;4eaf24dd51e5e9fce10e2cc7d32e0b01&#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 0 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 3: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 3: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 3: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 3: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 3: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 3: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 3: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 3: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 3: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 3: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 3: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.114 seconds (Warm-up)
## Chain 3:                0.084 seconds (Sampling)
## Chain 3:                0.198 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &#39;4eaf24dd51e5e9fce10e2cc7d32e0b01&#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 0 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 4: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 4: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 4: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 4: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 4: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 4: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 4: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 4: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 4: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 4: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 4: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.101 seconds (Warm-up)
## Chain 4:                0.098 seconds (Sampling)
## Chain 4:                0.199 seconds (Total)
## Chain 4:
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, we visualise the parameter estimates&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(coeftab(mH3quap, mH3ulam),
  labels = paste(rep(rownames(coeftab(mH3quap, mH3ulam)@coefs), each = 2),
    rep(c(&amp;quot;MCMC&amp;quot;, &amp;quot;quap&amp;quot;), nrow(coeftab(mH3quap, mH3ulam)@coefs) * 2),
    sep = &amp;quot;-&amp;quot;
  )
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-18-statistical-rethinking-chapter-11_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;These are pretty similar looking to me.&lt;/p&gt;
&lt;h4 id=&#34;part-b&#34;&gt;Part B&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Now interpret the estimates. If the quadratic approximation turned out okay, then it’s okay
to use the &lt;code&gt;quap&lt;/code&gt; estimates. Otherwise stick to &lt;code&gt;ulam&lt;/code&gt; estimates. Then plot the posterior predictions. Compute and display both (1) the predicted &lt;strong&gt;probability&lt;/strong&gt; of success and its 89% interval for each row ($i$) in the data, as well as (2) the predicted success &lt;strong&gt;count&lt;/strong&gt; and its 89% interval. What different information does each type of posterior prediction provide?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Personally, I don&amp;rsquo;t think there&amp;rsquo;s much difference between the model estimates. Here, I am sticking to the &lt;code&gt;ulam&lt;/code&gt; model, because I feel like it. No other reason.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start by getting a baseline understanding of how often a non-adult, small-bodied pirate is able to fetch a salmon from a small-bodied victim(all dummy variables are at value 0) - this is our intercept &lt;code&gt;a&lt;/code&gt;. These are log-odds:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;post &amp;lt;- extract.samples(mH3ulam)
mean(logistic(post$a))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5695376
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We expect about 0.57% of all of our immature, small pirates to be successful when pirating on small victims.&lt;/p&gt;
&lt;p&gt;Now that we are armed with our baseline, we are ready to look at how our slope parameters affect what&amp;rsquo;s happening in our model.&lt;/p&gt;
&lt;p&gt;First, we start with the effect of pirate-body-size (&lt;code&gt;bP&lt;/code&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mean(logistic(post$a + post$bP))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8678798
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Damn. Large-bodied pirates win almost all of the time! We could repeat this for all slope parameters, but I find it prudent to move on to our actual task:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Probability&lt;/strong&gt; of success:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;d$psuccess &amp;lt;- d$y / d$n # successes divided by attempts
p &amp;lt;- link(mH3ulam) # success probability with inverse link
## Mean and Interval Calculation
p.mean &amp;lt;- apply(p, 2, mean)
p.PI &amp;lt;- apply(p, 2, PI)
# plot raw proportions success for each case
plot(d$psuccess,
  col = rangi2,
  ylab = &amp;quot;successful proportion&amp;quot;, xlab = &amp;quot;case&amp;quot;, xaxt = &amp;quot;n&amp;quot;,
  xlim = c(0.75, 8.25), pch = 16
)
# label cases on horizontal axis
axis(1,
  at = 1:8,
  labels = c(&amp;quot;LLA&amp;quot;, &amp;quot;LSA&amp;quot;, &amp;quot;LLI&amp;quot;, &amp;quot;LSI&amp;quot;, &amp;quot;SLA&amp;quot;, &amp;quot;SSA&amp;quot;, &amp;quot;SLI&amp;quot;, &amp;quot;SSI&amp;quot;) # same order as in data frame d
)
# display posterior predicted proportions successful
points(1:8, p.mean)
for (i in 1:8) lines(c(i, i), p.PI[, i])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-18-statistical-rethinking-chapter-11_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;Counts&lt;/strong&gt; of successes:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;y &amp;lt;- sim(mH3ulam) # simulate posterior for counts of successes
## Mean and Interval Calculation
y.mean &amp;lt;- apply(y, 2, mean)
y.PI &amp;lt;- apply(y, 2, PI)
# plot raw counts success for each case
plot(d$y,
  col = rangi2,
  ylab = &amp;quot;successful attempts&amp;quot;, xlab = &amp;quot;case&amp;quot;, xaxt = &amp;quot;n&amp;quot;,
  xlim = c(0.75, 8.25), pch = 16
)
# label cases on horizontal axis
axis(1,
  at = 1:8,
  labels = c(&amp;quot;LAL&amp;quot;, &amp;quot;LAS&amp;quot;, &amp;quot;LIL&amp;quot;, &amp;quot;LIS&amp;quot;, &amp;quot;SAL&amp;quot;, &amp;quot;SAS&amp;quot;, &amp;quot;SIL&amp;quot;, &amp;quot;SIS&amp;quot;)
)
# display posterior predicted successes
points(1:8, y.mean)
for (i in 1:8) lines(c(i, i), y.PI[, i])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-18-statistical-rethinking-chapter-11_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In conclusion, the probability plot makes the different settings of predictor variables more comparable because the number of piracy attempts are ignored in setting the y-axis. The count plot, however, shows the additional uncertainty stemming from the underlying sample size.&lt;/p&gt;
&lt;h4 id=&#34;part-c&#34;&gt;Part C&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Now try to improve the model. Consider an interaction between the pirate’s size and age(immature or adult). Compare this model to the previous one, using &lt;code&gt;WAIC&lt;/code&gt;. Interpret.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  Let&amp;rsquo;s fit a model with &lt;code&gt;ulam&lt;/code&gt; containing the interaction effect we were asked for:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mH3c &amp;lt;- ulam(
  alist(
    y ~ dbinom(n, p),
    logit(p) &amp;lt;- a + bP * pirateL + bV * victimL + bA * pirateA + bPA * pirateL * pirateA,
    a ~ dnorm(0, 1.5),
    bP ~ dnorm(0, .5),
    bV ~ dnorm(0, .5),
    bA ~ dnorm(0, .5),
    bPA ~ dnorm(0, .5)
  ),
  data = d, chains = 4, log_lik = TRUE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &#39;3f6607198507ea4881438baca721629d&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 1: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 1: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 1: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 1: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.162 seconds (Warm-up)
## Chain 1:                0.118 seconds (Sampling)
## Chain 1:                0.28 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;3f6607198507ea4881438baca721629d&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 0 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 2: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 2: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 2: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 2: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 2: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 2: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 2: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 2: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 2: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 2: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 2: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.142 seconds (Warm-up)
## Chain 2:                0.124 seconds (Sampling)
## Chain 2:                0.266 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &#39;3f6607198507ea4881438baca721629d&#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 0 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 3: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 3: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 3: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 3: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 3: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 3: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 3: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 3: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 3: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 3: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 3: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.141 seconds (Warm-up)
## Chain 3:                0.119 seconds (Sampling)
## Chain 3:                0.26 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &#39;3f6607198507ea4881438baca721629d&#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 0 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 4: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 4: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 4: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 4: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 4: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 4: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 4: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 4: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 4: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 4: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 4: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.107 seconds (Warm-up)
## Chain 4:                0.12 seconds (Sampling)
## Chain 4:                0.227 seconds (Total)
## Chain 4:
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;compare(mH3ulam, mH3c)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             WAIC       SE    dWAIC      dSE    pWAIC    weight
## mH3ulam 59.09875 11.34469 0.000000       NA 8.303613 0.6932173
## mH3c    60.72916 11.90457 1.630407 1.467142 9.165510 0.3067827
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is quite obviously a tie. So what about the model estimates?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(coeftab(mH3ulam, mH3c),
  labels = paste(rep(rownames(coeftab(mH3ulam, mH3c)@coefs), each = 2),
    rep(c(&amp;quot;Base&amp;quot;, &amp;quot;Interac&amp;quot;), nrow(coeftab(mH3ulam, mH3c)@coefs) * 2),
    sep = &amp;quot;-&amp;quot;
  )
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-18-statistical-rethinking-chapter-11_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;1440&#34; /&gt;
Jup, there&amp;rsquo;s not really much of a difference here. For the interaction model: the log-odds of successful piracy is just weakly bigger when the pirating individual is large and an adult. That is counter-intuitive, isn&amp;rsquo;t it? It is worth pointing out that the individual parameters for these conditions show the expected effects and the identified negative effect of their interaction may be down to the sparsity of the underlying data and we are also highly uncertain of it&amp;rsquo;s sign to begin with.&lt;/p&gt;
&lt;h3 id=&#34;practice-h4&#34;&gt;Practice H4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; The data contained in &lt;code&gt;data(salamanders)&lt;/code&gt; are counts of salamanders (&lt;em&gt;Plethodon elongatus&lt;/em&gt;) from 47 different 49$m^2$ plots in northern California. The column &lt;code&gt;SALAMAN&lt;/code&gt; is the count in each plot, and the columns &lt;code&gt;PCTCOVER&lt;/code&gt; and &lt;code&gt;FORESTAGE&lt;/code&gt; are percent of ground cover and age of trees in the plot, respectively. You will model &lt;code&gt;SALAMAN&lt;/code&gt; as a Poisson variable.&lt;/p&gt;
&lt;h4 id=&#34;part-a-1&#34;&gt;Part A&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Model the relationship between density and percent cover, using a log-link (same as the ex-
ample in the book and lecture). Use weakly informative priors of your choosing. Check the quadratic approximation again, by comparing &lt;code&gt;quap&lt;/code&gt; to &lt;code&gt;ulam&lt;/code&gt;. Then plot the expected counts and their 89% interval against percent cover. In which ways does the model do a good job? In which ways does it do a bad job?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  First, we load the data and standardise the predictors to get around their inconvenient scales which do not overlap well with each other:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(salamanders)
d &amp;lt;- salamanders
d$C &amp;lt;- standardize(d$PCTCOVER)
d$A &amp;lt;- standardize(d$FORESTAGE)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now it is time to write our Poisson model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;f &amp;lt;- alist(
  SALAMAN ~ dpois(lambda),
  log(lambda) &amp;lt;- a + bC * C,
  a ~ dnorm(0, 1),
  bC ~ dnorm(0, 1)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That was easy enough, but do those priors make sense? Let&amp;rsquo;s simulate:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;N &amp;lt;- 50 # 50 samples from prior
a &amp;lt;- rnorm(N, 0, 1)
bC &amp;lt;- rnorm(N, 0, 1)
C_seq &amp;lt;- seq(from = -2, to = 2, length.out = 30)
plot(NULL,
  xlim = c(-2, 2), ylim = c(0, 20),
  xlab = &amp;quot;cover(stanardized)&amp;quot;, ylab = &amp;quot;salamanders&amp;quot;
)
for (i in 1:N) {
  lines(C_seq, exp(a[i] + bC[i] * C_seq), col = grau(), lwd = 1.5)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-18-statistical-rethinking-chapter-11_files/figure-html/unnamed-chunk-24-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;While not terrible (the prior allows your some explosive trends, but mostly sticks to a reasonable count of individuals), we may want to consider making the prior a bit more informative:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bC &amp;lt;- rnorm(N, 0, 0.5)
plot(NULL,
  xlim = c(-2, 2), ylim = c(0, 20),
  xlab = &amp;quot;cover(stanardized)&amp;quot;, ylab = &amp;quot;salamanders&amp;quot;
)
for (i in 1:N) {
  lines(C_seq, exp(a[i] + bC[i] * C_seq), col = grau(), lwd = 1.5)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-18-statistical-rethinking-chapter-11_files/figure-html/unnamed-chunk-25-1.png&#34; width=&#34;1440&#34; /&gt;
Yup - I am happy with that.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s update the model specification and run it:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;f &amp;lt;- alist(
  SALAMAN ~ dpois(lambda),
  log(lambda) &amp;lt;- a + bC * C,
  a ~ dnorm(0, 1),
  bC ~ dnorm(0, 0.5)
)
mH4a &amp;lt;- ulam(f, data = d, chains = 4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &#39;ce27f50b1ba56f91eaeb68bb1bf4432c&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 1: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 1: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 1: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 1: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.045 seconds (Warm-up)
## Chain 1:                0.046 seconds (Sampling)
## Chain 1:                0.091 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;ce27f50b1ba56f91eaeb68bb1bf4432c&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 0 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 2: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 2: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 2: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 2: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 2: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 2: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 2: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 2: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 2: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 2: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 2: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.054 seconds (Warm-up)
## Chain 2:                0.062 seconds (Sampling)
## Chain 2:                0.116 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &#39;ce27f50b1ba56f91eaeb68bb1bf4432c&#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 0 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 3: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 3: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 3: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 3: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 3: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 3: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 3: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 3: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 3: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 3: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 3: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.052 seconds (Warm-up)
## Chain 3:                0.051 seconds (Sampling)
## Chain 3:                0.103 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &#39;ce27f50b1ba56f91eaeb68bb1bf4432c&#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 0 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 4: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 4: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 4: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 4: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 4: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 4: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 4: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 4: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 4: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 4: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 4: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.065 seconds (Warm-up)
## Chain 4:                0.058 seconds (Sampling)
## Chain 4:                0.123 seconds (Total)
## Chain 4:
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mH4aquap &amp;lt;- quap(f, data = d)
plot(coeftab(mH4a, mH4aquap),
  labels = paste(rep(rownames(coeftab(mH4a, mH4aquap)@coefs), each = 2),
    rep(c(&amp;quot;MCMC&amp;quot;, &amp;quot;quap&amp;quot;), nrow(coeftab(mH4a, mH4aquap)@coefs) * 2),
    sep = &amp;quot;-&amp;quot;
  )
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-18-statistical-rethinking-chapter-11_files/figure-html/unnamed-chunk-26-1.png&#34; width=&#34;1440&#34; /&gt;
Again, both models are doing fine and we continue to our plotting of expected counts and their interval with the &lt;code&gt;ulam&lt;/code&gt; model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(d$C, d$SALAMAN,
  col = rangi2, lwd = 2,
  xlab = &amp;quot;cover(standardized)&amp;quot;, ylab = &amp;quot;salamanders observed&amp;quot;
)
C_seq &amp;lt;- seq(from = -2, to = 2, length.out = 30)
l &amp;lt;- link(mH4a, data = list(C = C_seq))
lines(C_seq, colMeans(l))
shade(apply(l, 2, PI), C_seq)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-18-statistical-rethinking-chapter-11_files/figure-html/unnamed-chunk-27-1.png&#34; width=&#34;1440&#34; /&gt;
Well that model doesn&amp;rsquo;t fit all that nicely and the data seems over-dispersed to me.&lt;/p&gt;
&lt;h4 id=&#34;part-b-1&#34;&gt;Part B&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Can you improve the model by using the other predictor, &lt;code&gt;FORESTAGE&lt;/code&gt;? Try any models you think useful. Can you explain why &lt;code&gt;FORESTAGE&lt;/code&gt; helps or does not help with prediction?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Forest cover might be confounded by forest age. The older a forest, the bigger its coverage? A model to investigate this could look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;f2 &amp;lt;- alist(
  SALAMAN ~ dpois(lambda),
  log(lambda) &amp;lt;- a + bC * C + bA * A,
  a ~ dnorm(0, 1),
  c(bC, bA) ~ dnorm(0, 0.5)
)
mH4b &amp;lt;- ulam(f2, data = d, chains = 4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &#39;4850e2c86bda45f77f837aaee26a4da5&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 1: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 1: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 1: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 1: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.066 seconds (Warm-up)
## Chain 1:                0.067 seconds (Sampling)
## Chain 1:                0.133 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;4850e2c86bda45f77f837aaee26a4da5&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 0 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 2: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 2: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 2: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 2: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 2: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 2: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 2: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 2: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 2: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 2: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 2: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.068 seconds (Warm-up)
## Chain 2:                0.079 seconds (Sampling)
## Chain 2:                0.147 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &#39;4850e2c86bda45f77f837aaee26a4da5&#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 0 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 3: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 3: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 3: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 3: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 3: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 3: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 3: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 3: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 3: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 3: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 3: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.064 seconds (Warm-up)
## Chain 3:                0.062 seconds (Sampling)
## Chain 3:                0.126 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &#39;4850e2c86bda45f77f837aaee26a4da5&#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 0 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 4: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 4: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 4: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 4: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 4: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 4: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 4: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 4: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 4: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 4: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 4: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.076 seconds (Warm-up)
## Chain 4:                0.058 seconds (Sampling)
## Chain 4:                0.134 seconds (Total)
## Chain 4:
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(mH4b)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          mean         sd       5.5%     94.5%     n_eff    Rhat4
## a  0.48361398 0.13609701  0.2618982 0.6896444  873.3384 1.003115
## bA 0.01904618 0.09647959 -0.1361230 0.1679860 1102.3547 1.002465
## bC 1.04260846 0.17335950  0.7795899 1.3262340  919.5342 1.000832
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Fascinating! The estimate for &lt;code&gt;bA&lt;/code&gt; is nearly $0$ with a lot of certainty (i.e. a small interval) behind it. While conditioning on percent cover, forest age does not influence salamander count. This looks like a post-treatment effect to me.&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.5 (2021-03-31)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19043)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.1252  LC_CTYPE=English_United Kingdom.1252    LC_MONETARY=English_United Kingdom.1252 LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.1252    
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] MASS_7.3-53.1        tidybayes_2.3.1      rethinking_2.13      rstan_2.21.2         ggplot2_3.3.6        StanHeaders_2.21.0-7
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.7           mvtnorm_1.1-1        lattice_0.20-41      tidyr_1.1.3          prettyunits_1.1.1    ps_1.6.0             assertthat_0.2.1     digest_0.6.27        utf8_1.2.1          
## [10] V8_3.4.1             plyr_1.8.6           R6_2.5.0             backports_1.2.1      stats4_4.0.5         evaluate_0.14        coda_0.19-4          highr_0.9            blogdown_1.3        
## [19] pillar_1.6.0         rlang_0.4.11         curl_4.3.2           callr_3.7.0          jquerylib_0.1.4      R.utils_2.10.1       R.oo_1.24.0          rmarkdown_2.7        styler_1.4.1        
## [28] stringr_1.4.0        loo_2.4.1            munsell_0.5.0        compiler_4.0.5       xfun_0.22            pkgconfig_2.0.3      pkgbuild_1.2.0       shape_1.4.5          htmltools_0.5.1.1   
## [37] tidyselect_1.1.0     tibble_3.1.1         gridExtra_2.3        bookdown_0.22        arrayhelpers_1.1-0   codetools_0.2-18     matrixStats_0.61.0   fansi_0.4.2          crayon_1.4.1        
## [46] dplyr_1.0.5          withr_2.4.2          R.methodsS3_1.8.1    distributional_0.2.2 ggdist_2.4.0         grid_4.0.5           jsonlite_1.7.2       gtable_0.3.0         lifecycle_1.0.0     
## [55] DBI_1.1.1            magrittr_2.0.1       scales_1.1.1         RcppParallel_5.1.2   cli_3.0.0            stringi_1.5.3        farver_2.1.0         bslib_0.2.4          ellipsis_0.3.2      
## [64] generics_0.1.0       vctrs_0.3.7          rematch2_2.1.2       forcats_0.5.1        tools_4.0.5          svUnit_1.0.6         R.cache_0.14.0       glue_1.4.2           purrr_0.3.4         
## [73] processx_3.5.1       yaml_2.2.1           inline_0.3.17        colorspace_2.0-0     knitr_1.33           sass_0.3.1
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 12</title>
      <link>https://www.erikkusch.com/courses/rethinking/chapter-12/</link>
      <pubDate>Thu, 25 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/rethinking/chapter-12/</guid>
      <description>&lt;h1 id=&#34;monsters--mixtures&#34;&gt;Monsters &amp;amp; Mixtures&lt;/h1&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/rethinking/14__26-03-2021_SUMMARY_-Hybrid-_-Ordered-Models.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides Chapter 12&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to the exercises at the end of chapter 12 in 
&lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Satistical Rethinking 2&lt;/a&gt; by Richard McElreath. I have created these notes as a part of my ongoing involvement in the 
&lt;a href=&#34;https://www.erikkusch.com/project/aubayes/&#34;&gt;AU Bayes Study Group&lt;/a&gt;. Much of my inspiration for these solutions, where necessary, has been obtained from&lt;/p&gt;
&lt;!-- [Taras Svirskyi](https://github.com/jffist/statistical-rethinking-solutions/blob/master/ch10_hw.R), [William Wolf](https://github.com/cavaunpeu/statistical-rethinking/blob/master/chapter-10/homework.R), and [Corrie Bartelheimer](https://www.samples-of-thoughts.com/projects/statistical-rethinking/chapter_10/chp10-ex/) as well as  --&gt;
&lt;p&gt;the solutions provided to instructors by Richard McElreath himself.&lt;/p&gt;
&lt;h2 id=&#34;r-environment&#34;&gt;&lt;code&gt;R&lt;/code&gt; Environment&lt;/h2&gt;
&lt;p&gt;For today&amp;rsquo;s exercise, I load the following packages:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rethinking)
library(rstan)
library(ggplot2)
library(tidybayes)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;easy-exercises&#34;&gt;Easy Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-e1&#34;&gt;Practice E1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  What is the difference between an ordered categorical variable and an unordered one? Define and then give an example of each.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;Ordered&lt;/em&gt; categorical variables are those which are expressed on ordinal scales. Not very helpful, right? Well, these are variables which establish a pre-defined number of distinct outcomes. These may be expressed as numbers, but don&amp;rsquo;t have to be. What&amp;rsquo;s special about these variables is that their values (i.e. categories) can be ordered meaningfully from one extreme to the another without implying equal distances between the values. As an example, think of sparrow (&lt;em&gt;Passer domesticus&lt;/em&gt;) weight. We may have measured these in grams (as a continuous variable), but now want to model simply whether our sparrows are light-, medium-, or heavy-weights. This is an ordered categorical variable because we now how they can be ordered from one extreme to another, but we don&amp;rsquo;t assume that a sparrow has to become heavier by the same margin to classify as medium-weight instead of light-weight than to classify as heavy-weight instead of medium-weight.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Unordered&lt;/em&gt; categorical variables are much like their ordered counterpart, but come with an important distinction: we cannot order these in any meaningful way. Again, think of the common house sparrow (&lt;em&gt;Passer domesticus&lt;/em&gt;) and their colouration patterns. We may want to record them as overall black, brown, or grey. These are categories, but we certainly cannot order these from one extreme to another.&lt;/p&gt;
&lt;h3 id=&#34;practice-e2&#34;&gt;Practice E2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  What kind of link function does an ordered logistic regression employ? How does it differ from an ordinary logit link?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;br&gt;
Cumulative logit - $OrdLogit( = \phi, K)$. This link function defines a number of $K-1$ cumulative, proportional probabilities ($\phi$) of each outcome category. The $K-1$ $\phi_i$ sum up to 1 so that the $\phi_K = 1$ and can subsequently be dropped from the model. The link thus states that the linear model defines the log-cumulative odds of an event.&lt;/p&gt;
&lt;h3 id=&#34;practice-e3&#34;&gt;Practice E3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  When count data are zero-inflated, using a model that ignores zero-inflation will tend to induce which kind of inferential error?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;br&gt;
Under-prediction of the true rate of events. Zero-inflation means that counts of zero arise through more than one process at least one of which is not accounted for in our model. Subsequently our estimate of the true rate will be pushed closer to 0 than it truly is.&lt;/p&gt;
&lt;h3 id=&#34;practice-e4&#34;&gt;Practice E4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Over-dispersion is common in count data. Give an example of a natural process that might produce over-dispersed counts. Can you also give an example of a process that might produce under- dispersed counts?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;br&gt;
Over-dispersion often comes about as a result of heterogeneity in rates across different sampling units/systems. As an example, think of lizard counts in different patches of a dryland area over a given period of time. The resulting count data will likely be over-dispersed since the rate at which lizards are observed will vary strongly across the different study sites.&lt;/p&gt;
&lt;p&gt;Under-dispersion, on the other hand, shows less variation in the rates than would be expected. This is often the case when autocrrelation plays a role. For example, if we track our lizard abundances at each patch through time in half-day intervals, we are likely to end up with highly autocorrelated and under-dispersed counts/rates for each study site.&lt;/p&gt;
&lt;h2 id=&#34;medium-exercises&#34;&gt;Medium Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-m1&#34;&gt;Practice M1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  At a certain university, employees are annually rated from 1 to 4 on their productivity, with 1 being least productive and 4 most productive. In a certain department at this certain university in a certain year, the numbers of employees receiving each rating were (from 1 to 4): 12, 36, 7, 41. Compute the log cumulative odds of each rating.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;n &amp;lt;- c(12, 36, 7, 41) # assignment
q &amp;lt;- n / sum(n) # proportions
p &amp;lt;- cumsum(q) # cumulative proportions
o &amp;lt;- p / (1 - p) # cumulative odds
log(o) # log-cumulative odds
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -1.9459101  0.0000000  0.2937611        Inf
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-m2&#34;&gt;Practice M2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Make a version of Figure 12.5 for the employee ratings data given just above.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# plot raw proportions
plot(1:4, p,
  xlab = &amp;quot;rating&amp;quot;, ylab = &amp;quot;cumulative proportion&amp;quot;,
  xlim = c(0.7, 4.3), ylim = c(0, 1), xaxt = &amp;quot;n&amp;quot;, cex = 3
)
axis(1, at = 1:4, labels = 1:4)
# plot gray cumulative probability lines
for (x in 1:4) lines(c(x, x), c(0, p[x]), col = &amp;quot;gray&amp;quot;, lwd = 4)
# plot blue discrete probability segments
for (x in 1:4) lines(c(x, x) + 0.1, c(p[x] - q[x], p[x]), col = &amp;quot;slateblue&amp;quot;, lwd = 4)
# add number labels
text(1:4 + 0.2, p - q / 2, labels = 1:4, col = &amp;quot;slateblue&amp;quot;, cex = 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-25-statistical-rethinking-chapter-12_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;practice-m3&#34;&gt;Practice M3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Can you modify the derivation of the zero-inflated Poisson distribution (ZIPoisson) from the chapter to construct a zero-inflated binomial distribution?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;br&gt;
Let&amp;rsquo;s remind ourselves of the ZI-Poisson distribution from the chapter:&lt;/p&gt;
&lt;p&gt;$$ ZIPoisson(p, \lambda) $$&lt;/p&gt;
&lt;p&gt;with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;p = probability of no count generating process occurring&lt;/li&gt;
&lt;li&gt;λ = rate at which counts are produced when process occurs&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is an extension of the Poisson distribution which is in itself a special version of the Binomial distribution with many trials and a low success rate. The zero-inflated Poisson may also be expressed as ($F$ and $S$ indicate binomial process has succeeded or failed in prohibiting the poisson process from happening, respectively):&lt;/p&gt;
&lt;p&gt;$$Pr(0|p_0, λ) = Pr(F|p_0) + Pr(S|p_0)*Pr(0|λ) = p_0 + (1 − p_0) * exp(−λ)$$
For zero-observations.&lt;/p&gt;
&lt;p&gt;$$Pr(y|y&amp;gt;0,p_0,\lambda) = Pr(S|p_0)(0) + Pr(F|p_0)*Pr(y|\lambda) = (1-p_0)\frac{\lambda^yexp(-\lambda)}{y!}$$
For non zero-observations.&lt;/p&gt;
&lt;p&gt;So how do we now get to a binomial specification here? By changing the Poisson likelihood ($exp(−λ)$) to a Binomial likelihood ($(1 − q)^n$ with $q$ denoting the probability of success):&lt;/p&gt;
&lt;p&gt;$$Pr(0|p_0, q, n) = p_0 + (1 − p_0)(1 − q)^n$$
For zero-observations.&lt;/p&gt;
&lt;p&gt;$$Pr(y|p_0, q, n) = (1 − p_0) Binom(y, n, q) = (1 − p_0) \frac{n!}{y!(n − y)!}*q^y(1 − q)^{n−y}$$
For non zero-observations.&lt;/p&gt;
&lt;h2 id=&#34;hard-exercises&#34;&gt;Hard Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-h1&#34;&gt;Practice H1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  In 2014, a paper was published that was entitled “Female hurricanes are deadlier than male hurricanes.” As the title suggests, the paper claimed that hurricanes with female names have caused greater loss of life, and the explanation given is that people unconsciously rate female hurricanes as less dangerous and so are less likely to evacuate. Statisticians severely criticized the paper after publication. Here, you’ll explore the complete data
used in the paper and consider the hypothesis that hurricanes with female names are deadlier. Load the data with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rethinking)
data(Hurricanes)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Acquaint yourself with the columns by inspecting the help &lt;code&gt;?Hurricanes&lt;/code&gt;. In this problem, you’ll focus on predicting deaths using &lt;code&gt;femininity&lt;/code&gt; of each hurricane’s name.&lt;/p&gt;
&lt;p&gt;Fit and interpret the simplest possible model, a Poisson model of deaths using &lt;code&gt;femininity&lt;/code&gt; as a predictor. You can use &lt;code&gt;quap&lt;/code&gt; or &lt;code&gt;ulam&lt;/code&gt;. Compare the model to an intercept-only Poisson model of deaths. How strong is the association between femininity of name and deaths? Which storms does the model fit (&lt;code&gt;retrodict&lt;/code&gt;) well? Which storms does it fit poorly?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; First, let&amp;rsquo;s prepare the data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;d &amp;lt;- Hurricanes # load data on object called d
d$fem_std &amp;lt;- (d$femininity - mean(d$femininity)) / sd(d$femininity) # standardised femininity
dat &amp;lt;- list(D = d$deaths, F = d$fem_std)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have standardised data for the feminity of our hurricane names which makes priors easier to formulate, we can specify our initial model idea:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# model formula
f &amp;lt;- alist(
  D ~ dpois(lambda), # poisson outcome distribution
  log(lambda) &amp;lt;- a + bF * F, # log-link for lambda with linear model
  # priors in log-space, 0 corresponds to outcome of 1
  a ~ dnorm(1, 1),
  bF ~ dnorm(0, 1)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But are these priors any good? Let&amp;rsquo;s simulate them why don&amp;rsquo;t we:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;N &amp;lt;- 1e3
a &amp;lt;- rnorm(N, 1, 1)
bF &amp;lt;- rnorm(N, 0, 1)
F_seq &amp;lt;- seq(from = -2, to = 2, length.out = 30) # sequence from -2 to 2 because femininity data is standardised
plot(NULL,
  xlim = c(-2, 2), ylim = c(0, 500),
  xlab = &amp;quot;name femininity (std)&amp;quot;, ylab = &amp;quot;deaths&amp;quot;
)
for (i in 1:N) {
  lines(F_seq,
    exp(a[i] + bF[i] * F_seq), # inverse link to get outcome scale
    col = grau(), lwd = 1.5
  )
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-25-statistical-rethinking-chapter-12_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;d think that&amp;rsquo;s pretty alright. We allow for both positive and negative trends between death toll and femininity of hurricane name, but don&amp;rsquo;t have a lot of explosive trends in our priors. These strong trends are quite unintuitive. Our vast majority of trends however are very ambiguous and so I proceed with these priors and run the model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mH1 &amp;lt;- ulam(f, data = dat, chains = 4, cores = 4, log_lik = TRUE)
precis(mH1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         mean         sd      5.5%     94.5%    n_eff     Rhat4
## a  2.9994821 0.02344260 2.9612043 3.0356100 1145.726 0.9982237
## bF 0.2386957 0.02488145 0.1994521 0.2784917 1371.036 0.9996256
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So according to this, there is a positive relationship between hurricane name femininity and death toll. Which hurricanes do we actually retrodict well, though? Let&amp;rsquo;s plot, this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# plot raw data
plot(dat$F, dat$D,
  pch = 16, lwd = 2,
  col = rangi2, xlab = &amp;quot;femininity (std)&amp;quot;, ylab = &amp;quot;deaths&amp;quot;
)
# compute model-based trend
pred_dat &amp;lt;- list(F = seq(from = -2, to = 2, length.out = 1e2))
lambda &amp;lt;- link(mH1, data = pred_dat) # predict deaths
lambda.mu &amp;lt;- apply(lambda, 2, mean) # get mean prediction
lambda.PI &amp;lt;- apply(lambda, 2, PI) # get prediction interval
# superimpose trend
lines(pred_dat$F, lambda.mu)
shade(lambda.PI, pred_dat$F)
# compute sampling distribution
deaths_sim &amp;lt;- sim(mH1, data = pred_dat) # simulate posterior observations
deaths_sim.PI &amp;lt;- apply(deaths_sim, 2, PI) # get simulation interval
# superimpose sampling interval as dashed lines
lines(pred_dat$F, deaths_sim.PI[1, ], lty = 2)
lines(pred_dat$F, deaths_sim.PI[2, ], lty = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-25-statistical-rethinking-chapter-12_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Ok. There is quite a bit to unpack here. First of all, our model does not retrodict many of the hurricanes well even though it is quite certain of its predictions (grey shaded area which is hardly visible). Quite obviously, this model misses many of the hurricane death tolls to the right hand side of the above plot. This is a clear sign of over-dispersion which our model failed to account for. The weak, positive trend we are seeing here seems to be informed largely by these highly influential data points. We can assess whether and how influential some data points are with the Paraeto-K values (anything above 1 indicates an influential data point) following:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(as.data.frame(PSISk(mH1)), aes(x = PSISk(mH1))) +
  stat_halfeye() +
  theme_bw() +
  labs(title = &amp;quot;Paraeto-K values&amp;quot;, subtitle = &amp;quot;Values &amp;gt; 1 indicate highly influential data&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-25-statistical-rethinking-chapter-12_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Boy! Some hurricanes really do drive our model to a big extent!&lt;/p&gt;
&lt;h3 id=&#34;practice-h2&#34;&gt;Practice H2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Counts are nearly always over-dispersed relative to Poisson. So fit a gamma-Poisson (aka negative-binomial) model to predict deaths using &lt;code&gt;femininity&lt;/code&gt;. Show that the over-dispersed model no longer shows as precise a positive association between femininity and deaths, with an 89% interval that overlaps zero. Can you explain why the association diminished in strength?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; To start this off, I load the library and data again, so much of the exercise and my solutions can stand by itself:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rethinking)
data(Hurricanes)
d &amp;lt;- Hurricanes # load data on object called d
d$fem_std &amp;lt;- (d$femininity - mean(d$femininity)) / sd(d$femininity) # standardised femininity
dat &amp;lt;- list(D = d$deaths, F = d$fem_std)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, with the data prepared, we fit our model - the same model as before just with a different outcome distribution:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mH2 &amp;lt;- ulam(
  alist(
    D ~ dgampois(lambda, scale),
    log(lambda) &amp;lt;- a + bF * F,
    a ~ dnorm(1, 1),
    bF ~ dnorm(0, 1),
    scale ~ dexp(1) # strictly positive hence why exponential prior
  ),
  data = dat, chains = 4, cores = 4, log_lik = TRUE
)
precis(mH2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            mean         sd        5.5%     94.5%    n_eff     Rhat4
## a     2.9780290 0.15272205  2.73722408 3.2334449 1908.835 0.9989114
## bF    0.2107239 0.15442227 -0.04075382 0.4566408 1737.246 0.9993676
## scale 0.4532751 0.06226293  0.35889278 0.5579919 1897.214 1.0003374
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Cool. Our previously identified positive relationship between standardised femininity of hurricane name and death toll is still there albeit slightly diminished in magnitude. However, the credible interval around it has widened considerably and overlaps zero now.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s compare the estimates of our models side by side:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(coeftab(mH1, mH2))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-25-statistical-rethinking-chapter-12_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;These shows quite clearly how our new model is much more uncertain of the parameters.&lt;/p&gt;
&lt;p&gt;So what about the predictions of this new model? I plot them the exact same way as previously:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# plot raw data
plot(dat$F, dat$D,
  pch = 16, lwd = 2,
  col = rangi2, xlab = &amp;quot;femininity (std)&amp;quot;, ylab = &amp;quot;deaths&amp;quot;
)
# compute model-based trend
pred_dat &amp;lt;- list(F = seq(from = -2, to = 2, length.out = 1e2))
lambda &amp;lt;- link(mH2, data = pred_dat)
lambda.mu &amp;lt;- apply(lambda, 2, mean)
lambda.PI &amp;lt;- apply(lambda, 2, PI)
# superimpose trend
lines(pred_dat$F, lambda.mu)
shade(lambda.PI, pred_dat$F)
# compute sampling distribution
deaths_sim &amp;lt;- sim(mH2, data = pred_dat)
deaths_sim.PI &amp;lt;- apply(deaths_sim, 2, PI)
# superimpose sampling interval as dashed lines
lines(pred_dat$F, deaths_sim.PI[1, ], lty = 2)
lines(pred_dat$F, deaths_sim.PI[2, ], lty = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-25-statistical-rethinking-chapter-12_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;1440&#34; /&gt;
What&amp;rsquo;s there left to say other than: &amp;ldquo;Look at that increased uncertainty of our model&amp;rdquo; at this point? Well, we can talk about the accuracy of our predictions. They still blow. The uncertainty of our model is nice and all, but with a predictive accuracy like this why would we trust the model?&lt;/p&gt;
&lt;p&gt;For now, let&amp;rsquo;s turn to the conceptual part of this exercise: &amp;ldquo;Why has the association diminished with the new model?&amp;rdquo; The question comes down to understanding what the gamma distribution does to our model. The gamma distribution allows for a death rate to be calculated for each outcome individually rather than one overall death rate for all hurricanes. These individual rates are sampled from a common distribution which is a function of the femininity of hurricane names. As a matter of fact, we can plot this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;post &amp;lt;- extract.samples(mH2)
par(mfrow = c(1, 3))
for (fem in -1:1) {
  for (i in 1:1e2) {
    curve(dgamma2(
      x, # where to calculate density
      exp(post$a[i] + post$bF[i] * fem), # linear model with inverse link applied
      post$scale[i] # scale for gamma
    ),
    from = 0, to = 70, xlab = &amp;quot;mean deaths&amp;quot;, ylab = &amp;quot;Density&amp;quot;,
    ylim = c(0, 0.19), col = col.alpha(&amp;quot;black&amp;quot;, 0.2),
    add = ifelse(i == 1, FALSE, TRUE)
    )
  }
  mtext(concat(&amp;quot;femininity  =   &amp;quot;, fem))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-25-statistical-rethinking-chapter-12_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;These are the gamma distributions samples from the posterior distribution of death rates when assuming same femininity of name for all of them at three different levels of femininity. Yes, a distribution sampled from another distribution. The above plots simply show the uncertainty of which gamma distribution to settle on.&lt;/p&gt;
&lt;p&gt;Since our model and gamma distributions are informed by &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;bF&lt;/code&gt;, and the scale for the gamma distribution at the same time many combinations of &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;bF&lt;/code&gt; are consistent with the data which results in a wider posterior distribution.&lt;/p&gt;
&lt;p&gt;Finally, let&amp;rsquo;s look at Paraeto-K values and potentially influential data again:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(as.data.frame(PSISk(mH2)), aes(x = PSISk(mH2))) +
  stat_halfeye() +
  theme_bw() +
  labs(title = &amp;quot;Paraeto-K values&amp;quot;, subtitle = &amp;quot;Values &amp;gt; 1 indicate highly influential data&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-25-statistical-rethinking-chapter-12_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;MUCH BETTER than before!&lt;/p&gt;
&lt;h3 id=&#34;practice-h3&#34;&gt;Practice H3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  In order to infer a strong association between deaths and femininity, it’s necessary to include an interaction effect. In the data, there are two measures of a hurricane’s potential to cause death: &lt;code&gt;damage_norm&lt;/code&gt; and &lt;code&gt;min_pressure&lt;/code&gt;. Consult &lt;code&gt;?Hurricanes&lt;/code&gt; for their meanings. It makes some sense to imagine that femininity of a name matters more when the hurricane is itself deadly. This implies an interaction between &lt;code&gt;femininity&lt;/code&gt; and either or both of &lt;code&gt;damage_norm&lt;/code&gt; and &lt;code&gt;min_pressure&lt;/code&gt;. Fit a series of models evaluating these interactions. Interpret and compare the models. In interpreting the estimates, it may help to generate counterfactual predictions contrasting hurricanes with masculine and feminine names. Are the effect sizes plausible?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  To start this off, I load the library and data again, so much of the exercise and my solutions can stand by itself:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rethinking)
data(Hurricanes)
d &amp;lt;- Hurricanes # load data on object called d
d$fem_std &amp;lt;- (d$femininity - mean(d$femininity)) / sd(d$femininity) # standardised femininity
dat &amp;lt;- list(D = d$deaths, F = d$fem_std)
dat$P &amp;lt;- standardize(d$min_pressure)
dat$S &amp;lt;- standardize(d$damage_norm)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data is ready and I step into my model fitting procedure. Here, I start with a basic model which builds on the previous gamma-Poisson model by adding an interaction between &lt;code&gt;femininity&lt;/code&gt; and &lt;code&gt;min_pressure&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mH3a &amp;lt;- ulam(
  alist(
    D ~ dgampois(lambda, scale),
    log(lambda) &amp;lt;- a + bF * F + bP * P + bFP * F * P,
    a ~ dnorm(1, 1),
    c(bF, bP, bFP) ~ dnorm(0, 1),
    scale ~ dexp(1)
  ),
  data = dat, cores = 4, chains = 4, log_lik = TRUE
)
precis(mH3a)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean         sd        5.5%      94.5%    n_eff     Rhat4
## a      2.7499528 0.13739615  2.53479738  2.9731015 2057.085 0.9990744
## bFP    0.2991240 0.14883183  0.06678460  0.5242145 1849.512 1.0007366
## bP    -0.6715712 0.13597028 -0.88624645 -0.4539281 1894.311 1.0002580
## bF     0.3027293 0.14148646  0.08381879  0.5332170 1953.726 0.9999575
## scale  0.5523986 0.08078761  0.42818191  0.6867295 1985.855 0.9987496
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As minimum pressure gets lower, a storm grows stronger (I was confused by that myself when answering these exercises). Quite obviously, the lower the pressure in a storm, the more severe the storm, and the more people die which is reflected by the negative value in &lt;code&gt;bP&lt;/code&gt;. &lt;code&gt;bF&lt;/code&gt; is still estimated to be positive. This time, the interval doesn&amp;rsquo;t even overlap zero. Meanwhile, the interaction effect &lt;code&gt;bFP&lt;/code&gt; is positive. I find it hard to interpret this so I&amp;rsquo;d rather plot some predictions against real data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;P_seq &amp;lt;- seq(from = -3, to = 2, length.out = 1e2) # pressure sequence
# &#39;masculine&#39; storms
d_pred &amp;lt;- data.frame(F = -1, P = P_seq)
lambda_m &amp;lt;- link(mH3a, data = d_pred)
lambda_m.mu &amp;lt;- apply(lambda_m, 2, mean)
lambda_m.PI &amp;lt;- apply(lambda_m, 2, PI)
# &#39;feminine&#39; storms
d_pred &amp;lt;- data.frame(F = 1, P = P_seq)
lambda_f &amp;lt;- link(mH3a, data = d_pred)
lambda_f.mu &amp;lt;- apply(lambda_f, 2, mean)
lambda_f.PI &amp;lt;- apply(lambda_f, 2, PI)
# Plotting, sqrt() to make differences easier to spot, can&#39;t use log because there are storm with zero deaths
plot(dat$P, sqrt(dat$D),
  pch = 1, lwd = 2, col = ifelse(dat$F &amp;gt; 0, &amp;quot;red&amp;quot;, &amp;quot;dark gray&amp;quot;),
  xlab = &amp;quot;minimum pressure (std)&amp;quot;, ylab = &amp;quot;sqrt(deaths)&amp;quot;
)
lines(P_seq, sqrt(lambda_m.mu), lty = 2)
shade(sqrt(lambda_m.PI), P_seq)
lines(P_seq, sqrt(lambda_f.mu), lty = 1, col = &amp;quot;red&amp;quot;)
shade(sqrt(lambda_f.PI), P_seq, col = col.alpha(&amp;quot;red&amp;quot;, 0.2))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-25-statistical-rethinking-chapter-12_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Our model expects masculine (grey) storms to be less deadly, on average, than feminine (red) ones. As pressure drops (toward the rightward side of the plot above), these differences become smaller and smaller. Quite evidently, some of these storms are influencing what our model predicts much more so than others:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(as.data.frame(PSISk(mH3a)), aes(x = PSISk(mH3a))) +
  stat_halfeye() +
  theme_bw() +
  labs(title = &amp;quot;Paraeto-K values&amp;quot;, subtitle = &amp;quot;Values &amp;gt; 1 indicate highly influential data&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-25-statistical-rethinking-chapter-12_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s turn to the second variable we may want to add &lt;code&gt;damage_norm&lt;/code&gt; - the damage caused by each storm:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mH3b &amp;lt;- ulam(
  alist(
    D ~ dgampois(lambda, scale),
    log(lambda) &amp;lt;- a + bF * F + bS * S + bFS * F * S,
    a ~ dnorm(1, 1),
    c(bF, bS, bFS) ~ dnorm(0, 1),
    scale ~ dexp(1)
  ),
  data = dat, chains = 4, cores = 4, log_lik = TRUE
)
precis(mH3b)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean         sd        5.5%     94.5%    n_eff     Rhat4
## a     2.56677402 0.12946975  2.36261687 2.7748946 1903.061 0.9995145
## bFS   0.30853835 0.20499020 -0.04173134 0.6265386 2243.101 0.9994129
## bS    1.25058627 0.21057791  0.92538673 1.5951083 1956.116 1.0003102
## bF    0.08485749 0.12501328 -0.11660977 0.2820823 1963.489 1.0005148
## scale 0.68529101 0.09803179  0.53525802 0.8466209 2165.010 0.9990485
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That just eradicated the effect of femininity of hurricane name (&lt;code&gt;bF&lt;/code&gt;)! The newly added interaction parameter &lt;code&gt;bFS&lt;/code&gt; is incredibly strong and positive. Again, let&amp;rsquo;s visualise this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;S_seq &amp;lt;- seq(from = -1, to = 5.5, length.out = 1e2) # damage sequence
# &#39;masculine&#39; storms
d_pred &amp;lt;- data.frame(F = -1, S = S_seq)
lambda_m &amp;lt;- link(mH3b, data = d_pred)
lambda_m.mu &amp;lt;- apply(lambda_m, 2, mean)
lambda_m.PI &amp;lt;- apply(lambda_m, 2, PI)
# &#39;feminine&#39; storms
d_pred &amp;lt;- data.frame(F = 1, S = S_seq)
lambda_f &amp;lt;- link(mH3b, data = d_pred)
lambda_f.mu &amp;lt;- apply(lambda_f, 2, mean)
lambda_f.PI &amp;lt;- apply(lambda_f, 2, PI)
# plot
plot(dat$S, sqrt(dat$D),
  pch = 1, lwd = 2, col = ifelse(dat$F &amp;gt; 0, &amp;quot;red&amp;quot;, &amp;quot;dark gray&amp;quot;),
  xlab = &amp;quot;normalized damage (std)&amp;quot;, ylab = &amp;quot;sqrt(deaths)&amp;quot;
)
lines(S_seq, sqrt(lambda_m.mu), lty = 2)
shade(sqrt(lambda_m.PI), S_seq)
lines(S_seq, sqrt(lambda_f.mu), lty = 1, col = &amp;quot;red&amp;quot;)
shade(sqrt(lambda_f.PI), S_seq, col = col.alpha(&amp;quot;red&amp;quot;, 0.2))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-25-statistical-rethinking-chapter-12_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can clearly see how our model makes less of a distinction between masculine and feminine hurricanes overall at this point. Damage norm scales multiplicatively. The distances grow fast as we approach the rightward side of the plot. This is difficult for the model to account for. Hence why the model is underwhelming.&lt;/p&gt;
&lt;p&gt;So why is the interaction effect so strong? Probably because of those 3-4 highly influential feminine storms at the upper-righthand corner of our plot above which implies that feminine storms are especially deadly when they are damaging to begin with. Personally, I don&amp;rsquo;t trust this association and would argue that there is no logical reason for it and most likely an artefact of the limited data availability.&lt;/p&gt;
&lt;h3 id=&#34;practice-h4&#34;&gt;Practice H4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  In the original hurricanes paper, storm damage (&lt;code&gt;damage_norm&lt;/code&gt;) was used directly. This assumption implies that mortality increases exponentially with a linear increase in storm strength, because a Poisson regression uses a log link. So it’s worth exploring an alternative hypothesis: that the logarithm of storm strength is what matters. Explore this by using the logarithm of &lt;code&gt;damage_norm&lt;/code&gt; as a predictor. Using the best model structure from the previous problem, compare a model that uses &lt;code&gt;log(damage_norm)&lt;/code&gt; to a model that uses &lt;code&gt;damage_norm&lt;/code&gt; directly. Compare their DIC/WAIC values as well as their implied predictions. What do you conclude?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; To start this off, I load the library and data again, so much of the exercise and my solutions can stand by itself:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rethinking)
data(Hurricanes)
d &amp;lt;- Hurricanes # load data on object called d
d$fem_std &amp;lt;- (d$femininity - mean(d$femininity)) / sd(d$femininity) # standardised femininity
dat &amp;lt;- list(D = d$deaths, F = d$fem_std)
dat$S2 &amp;lt;- standardize(log(d$damage_norm))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s fit the model as before and compare it to the previously identified best model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mH4 &amp;lt;- ulam(
  alist(
    D ~ dgampois(lambda, scale),
    log(lambda) &amp;lt;- a + bF * F + bS * S2 + bFS * F * S2,
    a ~ dnorm(1, 1),
    c(bF, bS, bFS) ~ dnorm(0, 1),
    scale ~ dexp(1)
  ),
  data = dat, chains = 4, cores = 4, log_lik = TRUE
)
compare(mH3b, mH4, func = PSIS)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          PSIS       SE    dPSIS      dSE    pPSIS       weight
## mH4  630.7019 31.19102  0.00000       NA 5.376913 1.000000e+00
## mH3b 670.6361 34.19897 39.93425 13.67922 6.857192 2.130043e-09
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Model &lt;code&gt;mH4&lt;/code&gt; clearly outperforms the earlier (non-logarithmic) model &lt;code&gt;mH3b&lt;/code&gt;. How do the parameter estimates look in comparison?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(coeftab(mH3b, mH4),
  labels = paste(rep(rownames(coeftab(mH3b, mH4)@coefs), each = 2),
    rep(c(&amp;quot;Norm&amp;quot;, &amp;quot;Log&amp;quot;), nrow(coeftab(mH3b, mH4)@coefs) * 2),
    sep = &amp;quot;-&amp;quot;
  )
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-25-statistical-rethinking-chapter-12_files/figure-html/unnamed-chunk-25-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With the log-transformed input, &lt;code&gt;bFS&lt;/code&gt; has increased in magnitude. What do the resulting predictions look like?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;S2_seq &amp;lt;- seq(from = -3, to = 1.8, length.out = 1e2)
# &#39;masculine&#39; storms
d_pred &amp;lt;- data.frame(F = -1, S2 = S2_seq)
lambda_m &amp;lt;- link(mH4, data = d_pred)
lambda_m.mu &amp;lt;- apply(lambda_m, 2, mean)
lambda_m.PI &amp;lt;- apply(lambda_m, 2, PI)
# &#39;feminine&#39; storms
d_pred &amp;lt;- data.frame(F = 1, S2 = S2_seq)
lambda_f &amp;lt;- link(mH4, data = d_pred)
lambda_f.mu &amp;lt;- apply(lambda_f, 2, mean)
lambda_f.PI &amp;lt;- apply(lambda_f, 2, PI)
# plot
plot(dat$S2, sqrt(dat$D),
  pch = 1, lwd = 2, col = ifelse(dat$F &amp;gt; 0, &amp;quot;red&amp;quot;, &amp;quot;dark gray&amp;quot;),
  xlab = &amp;quot;normalized damage (std)&amp;quot;, ylab = &amp;quot;sqrt(deaths)&amp;quot;
)
lines(S2_seq, sqrt(lambda_m.mu), lty = 2)
shade(sqrt(lambda_m.PI), S2_seq)
lines(S2_seq, sqrt(lambda_f.mu), lty = 1, col = &amp;quot;red&amp;quot;)
shade(sqrt(lambda_f.PI), S2_seq, col = col.alpha(&amp;quot;red&amp;quot;, 0.2))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-25-statistical-rethinking-chapter-12_files/figure-html/unnamed-chunk-26-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now this model fits the data much better! Still not perfect, but much better.&lt;/p&gt;
&lt;h3 id=&#34;practice-h5&#34;&gt;Practice H5&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  One hypothesis from developmental psychology, usually attributed to Carol Gilligan, proposes that women and men have different average tendencies in moral reasoning. Like most hypotheses in social psychology, it is merely descriptive. The notion is that women are more concerned with care (avoiding harm), while men are more concerned with justice and rights. Culture-bound nonsense? Yes. Descriptively accurate? Maybe.&lt;/p&gt;
&lt;p&gt;Evaluate this hypothesis, using the &lt;code&gt;Trolley&lt;/code&gt; data, supposing that contact provides a proxy for physical harm. Are women more or less bothered by contact than are men, in these data? Figure out the model(s) that is needed to address this question.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Again, let&amp;rsquo;s start by preparing the data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rethinking)
data(Trolley)
d &amp;lt;- Trolley
dat &amp;lt;- list(
  R = d$response,
  A = d$action,
  I = d$intention,
  C = d$contact
)
dat$Gid &amp;lt;- ifelse(d$male == 1, 1L, 2L)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now for a model. We use the same model skeleton as provided in the book. However, this time around, we want cutpoints in our ordered, cumulative logit for both genders separately:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dat$F &amp;lt;- 1L - d$male # indicator of femaleness to turn intercepts on and off
mH5 &amp;lt;- ulam(
  alist(
    R ~ dordlogit(phi, cutpoints),
    phi &amp;lt;- a * F + bA[Gid] * A + bC[Gid] * C + BI * I,
    BI &amp;lt;- bI[Gid] + bIA[Gid] * A + bIC[Gid] * C,
    c(bA, bI, bC, bIA, bIC)[Gid] ~ dnorm(0, 0.5),
    a ~ dnorm(0, 0.5),
    cutpoints ~ dnorm(0, 1.5)
  ),
  data = dat, chains = 4, cores = 4
)
precis(mH5, depth = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                     mean         sd        5.5%       94.5%     n_eff    Rhat4
## bIC[1]       -1.29000860 0.13002100 -1.50109161 -1.07614615 1348.3365 1.000146
## bIC[2]       -1.14842366 0.13434342 -1.36319985 -0.93345115 1163.2537 1.006455
## bIA[1]       -0.43821462 0.10411766 -0.60427628 -0.27043071 1201.4372 1.003107
## bIA[2]       -0.42253949 0.11300405 -0.59891439 -0.23526558 1156.5531 1.005345
## bC[1]        -0.47691217 0.09253044 -0.62813726 -0.32838280 1294.9094 1.000744
## bC[2]        -0.20730657 0.09595489 -0.36193206 -0.06036975 1207.3517 1.004505
## bI[1]        -0.33520753 0.07772555 -0.46098056 -0.20740598 1075.2158 1.003142
## bI[2]        -0.25819201 0.08096287 -0.39036524 -0.12766901 1026.1192 1.006898
## bA[1]        -0.59363382 0.07036947 -0.70451058 -0.48492259 1075.2269 1.005641
## bA[2]        -0.33605523 0.07724757 -0.45859501 -0.21053675 1205.2000 1.005800
## a            -0.78364935 0.08006100 -0.90628106 -0.65659045  948.2152 1.003790
## cutpoints[1] -3.02124197 0.06328453 -3.11858791 -2.91868150 1181.9794 1.002080
## cutpoints[2] -2.32745650 0.06074323 -2.42260517 -2.23098876 1175.2672 1.001935
## cutpoints[3] -1.72766666 0.05885444 -1.82181068 -1.63431341 1123.8787 1.002648
## cutpoints[4] -0.67165728 0.05700586 -0.76381567 -0.58315767 1112.3527 1.003052
## cutpoints[5]  0.01889978 0.05633424 -0.07146169  0.10708376 1122.8518 1.003504
## cutpoints[6]  0.94926239 0.05853165  0.85602268  1.04283590 1166.1198 1.002682
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The parameter estimates of interest here are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;a&lt;/code&gt; (-0.78) - main effect of being female on cumulative log-odds. On average women, have more moral qualms about the trolley problem it seems.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bC[2]&lt;/code&gt; (-0.21) - the interaction effect of being both female and in a contact scenario as opposed to &lt;code&gt;bc[1]&lt;/code&gt; (-0.48) which is the same scenario but for men. Women in our set-up had less moral issues with contact events.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The latter goes against the previously stated hypothesis. Why is that? Because the people in our study are much more complex than just their genders.&lt;/p&gt;
&lt;h3 id=&#34;practice-h6&#34;&gt;Practice H6&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  The data in &lt;code&gt;data(Fish)&lt;/code&gt; are records of visits to a national park. See &lt;code&gt;?Fish&lt;/code&gt; for details. The question of interest is how many fish an average visitor takes per hour, when fishing. The problem is that not everyone tried to fish, so the &lt;code&gt;fish_caught&lt;/code&gt; numbers are zero-inflated. As with the monks example in the chapter, there is a process that determines who is fishing (working) and another process that determines fish per hour (manuscripts per day), conditional on fishing (working). We want to model both. Otherwise we’ll end up with an underestimate of rate of fish extraction from the park.&lt;/p&gt;
&lt;p&gt;You will model these data using zero-inflated Poisson GLMs. Predict &lt;code&gt;fish_caught&lt;/code&gt; as a function
of any of the other variables you think are relevant. One thing you must do, however, is use a proper Poisson offset/exposure in the Poisson portion of the zero-inflated model. Then use the hours variable to construct the offset. This will adjust the model for the differing amount of time individuals spent in the park.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; One last time, for this week, we prepare some data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rethinking)
data(Fish)
d &amp;lt;- Fish
str(d)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &#39;data.frame&#39;:	250 obs. of  6 variables:
##  $ fish_caught: int  0 0 0 0 1 0 0 0 0 1 ...
##  $ livebait   : int  0 1 1 1 1 1 1 1 0 1 ...
##  $ camper     : int  0 1 0 1 0 1 0 0 1 1 ...
##  $ persons    : int  1 1 1 2 1 4 3 4 3 1 ...
##  $ child      : int  0 0 0 1 0 2 1 3 2 0 ...
##  $ hours      : num  21.124 5.732 1.323 0.548 1.695 ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model I want to build will look at the following variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;fish_caught&lt;/code&gt;. This is our response variable.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;livebait&lt;/code&gt;. I suggest that using livebait increases number of fish caught.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;camper&lt;/code&gt;. I assume that being a camper increases the chances that one goes fishing, but not necessarily the number of fish caught.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;persons&lt;/code&gt;. I assume that the number of people in a group increases how many fish are caught.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;child&lt;/code&gt;. Being a child should reasonably determine both whether one fishes (I assume children fish less - call it personal bias) and also that they are less effective than adults.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hours&lt;/code&gt;. How long one fishes surely determines how many fish are caught. I want to include the base rate of these into my model of fish caught. Since said model will be log-linked, I log-transform them.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let me fit that model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;d$loghours &amp;lt;- log(d$hours)
mH6 &amp;lt;- ulam(
  alist(
    # outcome distribution
    fish_caught ~ dzipois(p, mu),
    # linear model for probability of fishing
    logit(p) &amp;lt;- a0 + bC0 * camper + bc0 * child,
    # linear model of catching a number of fish
    log(mu) &amp;lt;- a + bb * livebait + bp * persons + bc * child + bl * loghours,
    c(a0, a) ~ dnorm(0, 1),
    c(bC0, bc0, bb, bp, bc, bl) ~ dnorm(0, 0.5)
  ),
  data = d, chains = 4, cores = 4, log_lik = TRUE
)
precis(mH6)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           mean         sd       5.5%      94.5%     n_eff     Rhat4
## a   -2.0839466 0.24408479 -2.4814597 -1.6945336  899.9998 1.0016629
## a0  -0.4650216 0.28849629 -0.9436559 -0.0177434 1191.0483 1.0015268
## bl   0.1701858 0.03428906  0.1134792  0.2254237 1255.6829 0.9985511
## bc  -0.8280833 0.10732217 -0.9969186 -0.6569452 1218.3750 1.0024020
## bp   0.8625626 0.04553091  0.7919523  0.9330040 1479.7715 0.9990951
## bb   1.4076543 0.19237711  1.1106049  1.7217943 1169.0384 1.0032455
## bc0  0.9787908 0.23529584  0.6059568  1.3537809 1292.4802 0.9994360
## bC0 -0.6597597 0.29791643 -1.1313113 -0.1803763 1413.3364 1.0017523
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remember that $p$ stands for the probability of &lt;strong&gt;not&lt;/strong&gt; going to fish. Overall, park visitors are pretty likely to fish (&lt;code&gt;a0 =&lt;/code&gt;-0.47, logit scale). In line with my intuition, campers are more likely to fish (&lt;code&gt;bC0&lt;/code&gt;) while children are less likely to fish (&lt;code&gt;bc0&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Once one is actually fishing, one is not likely to catch much fish (&lt;code&gt;a=&lt;/code&gt;-2.08). The parameters pertaining to number of fish caught are expressed on log-scale and so transforming &lt;code&gt;a&lt;/code&gt; into the outcome scale (counts of fish) by exponentiating, an adult who fishes by themselves without livebait (this is what &lt;code&gt;a&lt;/code&gt; refers to) catches around 0.1249302 fish on average. More people catch more fish (&lt;code&gt;bp&lt;/code&gt;). Using livebait is effective (&lt;code&gt;bb&lt;/code&gt;). In line with my intuition, children catch less fish than adults (&lt;code&gt;bc&lt;/code&gt;). Lastly, the more time one spends fishing, the more fish one catches (&lt;code&gt;bl&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s turn to some actual model predictions of &lt;code&gt;p&lt;/code&gt; and &lt;code&gt;mu&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;zip_link &amp;lt;- link(mH6)
str(zip_link)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## List of 2
##  $ p : num [1:2000, 1:250] 0.403 0.388 0.469 0.382 0.443 ...
##  $ mu: num [1:2000, 1:250] 0.588 0.543 0.537 0.592 0.488 ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;p&lt;/code&gt; cases provide estimates for additional zeros not obtained through the actual Poisson-process behind fishing catches whose output lies with &lt;code&gt;mu&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For example, if $p =$0.39 (the inverse logit of &lt;code&gt;a0&lt;/code&gt; in the model above) and $μ = 1$ (much higher than &lt;code&gt;a&lt;/code&gt; in the model above for ease here), then the implied predictive distribution is:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;zeros &amp;lt;- rbinom(
  n = 1e4, # number of samples
  size = 1,
  prob = inv_logit(precis(mH6)[2, 1]) # probability of going fishing
)
obs_fish &amp;lt;- (1 - zeros) * rpois(1e4, 1)
simplehist(obs_fish)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-25-statistical-rethinking-chapter-12_files/figure-html/unnamed-chunk-32-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s see what our model would predict given the estimated parameters:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;fish_sim &amp;lt;- sim(mH6)
str(fish_sim)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  num [1:1000, 1:250] 0 0 1 1 0 0 0 2 0 1 ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This now contains a simulation output for each sample in our data! We could now plot ourselves into oblivion with counterfactual plots. We can also produce counterfactual posterior predictions. As an example, I assume a party of 1 adult spending 1 hour in the park without any use of livebait and not staying the night as a camper:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# new data
pred_dat &amp;lt;- list(
  loghours = log(1), # note that this is zero, the baseline rate
  persons = 1,
  child = 0,
  livebait = 0,
  camper = 0
)
# sim predictions - want expected number of fish, but must use both processes
fish_link &amp;lt;- link(mH6, data = pred_dat)
# summarize
p &amp;lt;- fish_link$p
mu &amp;lt;- fish_link$mu
(expected_fish_mean &amp;lt;- mean((1 - p) * mu))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1838057
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(expected_fish_PI &amp;lt;- PI((1 - p) * mu))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        5%       94% 
## 0.1261989 0.2553580
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This tells us that this hypothetical person is expected to catch 0.1838057 fish with the interval displayed above.&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.5 (2021-03-31)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19043)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.1252  LC_CTYPE=English_United Kingdom.1252    LC_MONETARY=English_United Kingdom.1252 LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.1252    
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] tidybayes_2.3.1      rethinking_2.13      rstan_2.21.2         ggplot2_3.3.6        StanHeaders_2.21.0-7
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.7           mvtnorm_1.1-1        lattice_0.20-41      tidyr_1.1.3          prettyunits_1.1.1    ps_1.6.0             assertthat_0.2.1     digest_0.6.27        utf8_1.2.1          
## [10] V8_3.4.1             plyr_1.8.6           R6_2.5.0             backports_1.2.1      stats4_4.0.5         evaluate_0.14        coda_0.19-4          highr_0.9            blogdown_1.3        
## [19] pillar_1.6.0         rlang_0.4.11         curl_4.3.2           callr_3.7.0          jquerylib_0.1.4      R.utils_2.10.1       R.oo_1.24.0          rmarkdown_2.7        styler_1.4.1        
## [28] labeling_0.4.2       stringr_1.4.0        loo_2.4.1            munsell_0.5.0        compiler_4.0.5       xfun_0.22            pkgconfig_2.0.3      pkgbuild_1.2.0       shape_1.4.5         
## [37] htmltools_0.5.1.1    tidyselect_1.1.0     tibble_3.1.1         gridExtra_2.3        bookdown_0.22        arrayhelpers_1.1-0   codetools_0.2-18     matrixStats_0.61.0   fansi_0.4.2         
## [46] crayon_1.4.1         dplyr_1.0.5          withr_2.4.2          MASS_7.3-53.1        R.methodsS3_1.8.1    distributional_0.2.2 ggdist_2.4.0         grid_4.0.5           jsonlite_1.7.2      
## [55] gtable_0.3.0         lifecycle_1.0.0      DBI_1.1.1            magrittr_2.0.1       scales_1.1.1         RcppParallel_5.1.2   cli_3.0.0            stringi_1.5.3        farver_2.1.0        
## [64] bslib_0.2.4          ellipsis_0.3.2       generics_0.1.0       vctrs_0.3.7          rematch2_2.1.2       forcats_0.5.1        tools_4.0.5          svUnit_1.0.6         R.cache_0.14.0      
## [73] glue_1.4.2           purrr_0.3.4          processx_3.5.1       yaml_2.2.1           inline_0.3.17        colorspace_2.0-0     knitr_1.33           sass_0.3.1
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 13</title>
      <link>https://www.erikkusch.com/courses/rethinking/chapter-13/</link>
      <pubDate>Wed, 07 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/rethinking/chapter-13/</guid>
      <description>&lt;h1 id=&#34;models-with-memory&#34;&gt;Models with Memory&lt;/h1&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/rethinking/15__09-04-2021_SUMMARY_-Multi-Level-Models.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides Chapter 13&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to the exercises at the end of chapter 13 in 
&lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Satistical Rethinking 2&lt;/a&gt; by Richard McElreath. I have created these notes as a part of my ongoing involvement in the 
&lt;a href=&#34;https://www.erikkusch.com/project/aubayes/&#34;&gt;AU Bayes Study Group&lt;/a&gt;. Much of my inspiration for these solutions, where necessary, has been obtained from&lt;/p&gt;
&lt;!-- [Taras Svirskyi](https://github.com/jffist/statistical-rethinking-solutions/blob/master/ch10_hw.R), [William Wolf](https://github.com/cavaunpeu/statistical-rethinking/blob/master/chapter-10/homework.R), and [Corrie Bartelheimer](https://www.samples-of-thoughts.com/projects/statistical-rethinking/chapter_10/chp10-ex/) as well as  --&gt;
&lt;p&gt;the solutions provided to instructors by Richard McElreath himself.&lt;/p&gt;
&lt;h2 id=&#34;r-environment&#34;&gt;&lt;code&gt;R&lt;/code&gt; Environment&lt;/h2&gt;
&lt;p&gt;For today&amp;rsquo;s exercise, I load the following packages:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rethinking)
library(rstan)
library(ggplot2)
library(tidybayes)
library(cowplot)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;easy-exercises&#34;&gt;Easy Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-e1&#34;&gt;Practice E1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Which of the following priors will produce more shrinkage in the estimates?&lt;/p&gt;
&lt;p&gt;(a) $α_{tank} ∼ Normal(0, 1)$&lt;br&gt;
(b) $α_{tank} ∼ Normal(0, 2)$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  Shrinkage is introduced by regularising/informative priors. This means that option (a) will introduce more shrinkage because it&amp;rsquo;s distribution is narrower than that of (b) and thus more informative/regularising.&lt;/p&gt;
&lt;h3 id=&#34;practice-e2&#34;&gt;Practice E2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Make the following model into a multilevel model.&lt;/p&gt;
&lt;p&gt;$$y_i ∼ Binomial(1, p_i)$$
$$logit(p_i) = α_{group[i]} + βx_i$$ 
$$α_{group} ∼ Normal(0, 10)$$ 
$$β ∼ Normal(0, 1)$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  To make the above model into a multi-level model, we need to assign some hyperpriors. These are priors on parameters of parameters. In this case, we express $\alpha_{group[i]}$ (a parameter) through a prior with another set of parameters ($\bar\alpha, \sigma_\alpha$). These parameters, in turn, require priors themselves - so called hyperpriors.&lt;/p&gt;
&lt;p&gt;$$y_i ∼ Binomial(1, p_i)$$
$$logit(p_i) = α_{group[i]} + βx_i$$ 
$$α_{group} ∼ Normal(\bar\alpha, \sigma_\alpha)$$ 
$$\bar\alpha \sim Normal(0, 2)$$
$$\sigma_\alpha \sim Exponential(1)$$
$$β ∼ Normal(0, 1)$$&lt;/p&gt;
&lt;p&gt;The numbers we feed into our hyperpriors here are difficult to assess for sensibility since we don&amp;rsquo;t have any data to test the performance of both models and their assumptions.&lt;/p&gt;
&lt;h3 id=&#34;practice-e3&#34;&gt;Practice E3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Make the following model into a multilevel model.&lt;/p&gt;
&lt;p&gt;$$y_i ∼ Normal(\mu, \sigma)$$
$$logit(p_i) = α_{group[i]} + βx_i$$ 
$$α_{group} ∼ Normal(0, 10)$$ 
$$\beta \sim Normal(0, 1)$$
$$\sigma ∼ HalfCauchy(0, 2)$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  Well this is just a repeat of the previous problem:&lt;/p&gt;
&lt;p&gt;$$y_i ∼ Normal(\mu_i, \sigma)$$
$$logit(p_i) = α_{group[i]} + βx_i$$ 
$$α_{group} ∼ Normal(\bar\alpha, \sigma_\alpha)$$ 
$$\bar\alpha \sim Normal(0, 2)$$
$$\sigma_\alpha \sim Exponential(1)$$
$$\beta \sim Normal(0, 1)$$
$$\sigma ∼ HalfCauchy(0, 2)$$&lt;/p&gt;
&lt;h3 id=&#34;practice-e4&#34;&gt;Practice E4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Write an example mathematical model formula for a Poisson regression with varying intercepts.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  This is simply just the solution to E2 with a change to the outcome distribution (now Poisson) and link function (now log):&lt;/p&gt;
&lt;p&gt;$$y_i ∼ Poisson(\lambda_i)$$
$$log(\lambda_i) = α_{group[i]} + βx_i$$ 
$$α_{group} ∼ Normal(\bar\alpha, \sigma_\alpha)$$ 
$$\bar\alpha \sim Normal(0, 2)$$
$$\sigma_\alpha \sim Exponential(1)$$
$$β ∼ Normal(0, 1)$$&lt;/p&gt;
&lt;p&gt;Again, I would like to highlight that I can&amp;rsquo;t set any meaningful priors here because I have no idea what we are analysing. These exercises are just about model structure, I wager.&lt;/p&gt;
&lt;h3 id=&#34;practice-e5&#34;&gt;Practice E5&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Write an example mathematical model formula for a Poisson regression with two different kinds of varying intercepts, a cross-classified model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  I start with the solution to E4 and add another intercept group ($\gamma$) which target a cluster of days like the example in the book:&lt;/p&gt;
&lt;p&gt;$$y_i ∼ Poisson(\lambda_i)$$
$$log(\lambda_i) = α_{group[i]}+ \gamma_{day[i]} + βx_i$$ 
$$α_{group} ∼ Normal(\bar\alpha, \sigma_\alpha)$$ 
$$\bar\alpha \sim Normal(0, 2)$$
$$\sigma_\alpha \sim Exponential(1)$$
$$\gamma_{day} ∼ Normal(\bar\gamma, \sigma_\gamma)$$ 
$$\bar\gamma \sim Normal(0, 2)$$
$$\sigma_\gamma \sim Exponential(1)$$&lt;/p&gt;
&lt;p&gt;$$β ∼ Normal(0, 1)$$&lt;/p&gt;
&lt;h2 id=&#34;medium-exercises&#34;&gt;Medium Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-m1&#34;&gt;Practice M1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Revisit the Reed frog survival data, &lt;code&gt;data(reedfrogs)&lt;/code&gt;, and add the predation and size treatment variables to the varying intercepts model. Consider models with either main effect alone, both main effects, as well as a model including both and their interaction. Instead of focusing on inferences about these two predictor variables, focus on the inferred variation across tanks. Explain why it changes as it does across models.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  This corresponds to the multi-level tadpole example in the book (starting in section 13.1 on page 415). First, I load the data and prepare the data list as was done in the book and add in the data about predation (binary - yes/no) and size treatment (binary - small/large):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(reedfrogs)
d &amp;lt;- reedfrogs
dat &amp;lt;- list(
  S = d$surv,
  n = d$density,
  tank = 1:nrow(d),
  pred = ifelse(d$pred == &amp;quot;no&amp;quot;, 0L, 1L),
  size_ = ifelse(d$size == &amp;quot;small&amp;quot;, 1L, 2L)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;ulam()&lt;/code&gt; doesn&amp;rsquo;t like when any data value is called &lt;code&gt;size&lt;/code&gt;. That&amp;rsquo;s why I call it &lt;code&gt;size_&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now, I can define the models. Note that I am running all of them with &lt;code&gt;log_lik = TRUE&lt;/code&gt; so I can compare them later:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Tank-only&lt;/strong&gt; model which will serve as our baseline.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_Tank &amp;lt;- ulam(
  alist(
    S ~ dbinom(n, p),
    logit(p) &amp;lt;- a[tank],
    a[tank] ~ dnorm(a_bar, sigma),
    a_bar ~ dnorm(0, 1.5),
    sigma ~ dexp(1)
  ),
  data = dat, chains = 4, cores = 4, log_lik = TRUE, iter = 2e3
)
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;Predation&lt;/strong&gt; model:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_Pred &amp;lt;- ulam(
  alist(
    S ~ dbinom(n, p),
    logit(p) &amp;lt;- a[tank] + bp * pred,
    a[tank] ~ dnorm(a_bar, sigma),
    bp ~ dnorm(-0.5, 1),
    a_bar ~ dnorm(0, 1.5),
    sigma ~ dexp(1)
  ),
  data = dat, chains = 4, cores = 4, log_lik = TRUE, iter = 2e3
)
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;strong&gt;Size&lt;/strong&gt; model:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_Size &amp;lt;- ulam(
  alist(
    S ~ dbinom(n, p),
    logit(p) &amp;lt;- a[tank] + s[size_],
    a[tank] ~ dnorm(a_bar, sigma),
    s[size_] ~ dnorm(0, 0.5),
    a_bar ~ dnorm(0, 1.5),
    sigma ~ dexp(1)
  ),
  data = dat, chains = 4, cores = 4, log_lik = TRUE, iter = 2e3
)
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;&lt;strong&gt;Predation + Size&lt;/strong&gt; model:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_Additive &amp;lt;- ulam(
  alist(
    S ~ dbinom(n, p),
    logit(p) &amp;lt;- a[tank] + bp * pred + s[size_],
    a[tank] ~ dnorm(a_bar, sigma),
    bp ~ dnorm(-0.5, 1),
    s[size_] ~ dnorm(0, 0.5),
    a_bar ~ dnorm(0, 1.5),
    sigma ~ dexp(1)
  ),
  data = dat, chains = 4, cores = 4, log_lik = TRUE, iter = 2e3
)
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;&lt;strong&gt;Predation-Size-Interaction&lt;/strong&gt; model:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# this is a con-centred parametrisation for giggles:
m_Interaction &amp;lt;- ulam(
  alist(
    S ~ dbinom(n, p),
    logit(p) &amp;lt;- a_bar + a[tank] * sigma + bp[size_] * pred + s[size_], # interaction comes in via bP[size_]
    a[tank] ~ dnorm(0, 1),
    bp[size_] ~ dnorm(-0.5, 1),
    s[size_] ~ dnorm(0, 0.5),
    a_bar ~ dnorm(0, 1.5),
    sigma ~ dexp(1)
  ),
  data = dat, chains = 4, cores = 4, log_lik = TRUE, iter = 2e3
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have all the models ready, we can assess the variation among tanks. This information is contained within the &lt;code&gt;sigma&lt;/code&gt; parameter in all of the models:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(coeftab(m_Tank, m_Pred, m_Size, m_Additive, m_Interaction),
  pars = &amp;quot;sigma&amp;quot;,
  labels = c(&amp;quot;Tank&amp;quot;, &amp;quot;Predation&amp;quot;, &amp;quot;Size&amp;quot;, &amp;quot;Additive&amp;quot;, &amp;quot;Interaction&amp;quot;)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-04-07-statistical-rethinking-chapter-13_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Quite evidently, omitting &lt;code&gt;pred&lt;/code&gt; (predation) from our models assigns a lot of variation to the &lt;code&gt;tank&lt;/code&gt; variable. Conclusively, we can say that predation explains a lot of the variation across tanks and helps to explain it. Omitting predation from our models simply assigns this variation to the tank intercepts without explaining it.&lt;/p&gt;
&lt;h3 id=&#34;practice-m2&#34;&gt;Practice M2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Compare the models you fit just above, using WAIC. Can you reconcile the differences in WAIC with the posterior distributions of the models?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;compare(m_Tank, m_Pred, m_Size, m_Additive, m_Interaction)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                   WAIC       SE     dWAIC      dSE    pWAIC    weight
## m_Interaction 199.0758 9.089561 0.0000000       NA 18.81776 0.2771353
## m_Pred        199.5219 8.995153 0.4460306 3.177855 19.44211 0.2217367
## m_Additive    199.9935 8.737508 0.9176872 2.209613 19.16555 0.1751534
## m_Tank        200.0751 7.259051 0.9992752 6.180324 20.94365 0.1681520
## m_Size        200.2019 7.140956 1.1260679 5.992834 20.97228 0.1578226
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Evidently, all of our models are expected to perform similarly in out-of-sample predictions. So how do the posterior samples look like? Here, I write a function to extract all parameter samples from the posterior given any of our models except the $\alpha$ parameters and feed them into a &lt;code&gt;ggplot&lt;/code&gt; using the beautiful &lt;code&gt;stat_halfeye()&lt;/code&gt; from the &lt;code&gt;tidybayes&lt;/code&gt; package:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;na.omit.list &amp;lt;- function(y) {
  return(y[!sapply(y, function(x) all(is.na(x)))])
}
Halfeyes_NoAs &amp;lt;- function(model = NULL, N = 1e4) {
  Samples &amp;lt;- extract.samples(model, n = N)
  list &amp;lt;- as.list(rep(NA, sum(!startsWith(names(model@coef), &amp;quot;a[&amp;quot;))))
  names(list) &amp;lt;- names(model@coef)[!startsWith(names(model@coef), &amp;quot;a[&amp;quot;)]
  for (i in names(Samples)) {
    if (i == &amp;quot;a&amp;quot;) {
      next
    } # skip all &amp;quot;a&amp;quot; parameters
    if (is.na(dim(Samples[[i]])[2])) {
      list[[i]] &amp;lt;- data.frame(
        Posterior = Samples[[i]],
        Parameter = rep(i, length(Samples[[i]]))
      )
    } else { # if there are multiple parameter levels
      list[[i]] &amp;lt;- data.frame(
        Posterior = Samples[[i]][, 1],
        Parameter = rep(paste(i, 1, sep = &amp;quot;_&amp;quot;), length(Samples[[i]]))
      )
      for (k in 2:dim(Samples[[i]])[2]) {
        list[[i]] &amp;lt;- rbind(
          list[[i]],
          data.frame(
            Posterior = Samples[[i]][, k],
            Parameter = rep(paste(i, k, sep = &amp;quot;_&amp;quot;), length(Samples[[i]]))
          )
        )
      }
    }
  } # Samples-loop
  Plot_df &amp;lt;- do.call(&amp;quot;rbind&amp;quot;, na.omit.list(list))
  Plot_gg &amp;lt;- ggplot(Plot_df, aes(y = Parameter, x = Posterior)) +
    stat_halfeye() +
    labs(x = &amp;quot;Parameter Estimate&amp;quot;, y = &amp;quot;Parameter&amp;quot;) +
    geom_vline(xintercept = 0, color = &amp;quot;red&amp;quot;) +
    theme_bw(base_size = 20)
  return(Plot_gg)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I don&amp;rsquo;t claim that this is beautiful code. There&amp;rsquo;s probably and easier way of doing this. Basically, this is a botch job. I am aware that it is, but it works for now.&lt;/p&gt;
&lt;p&gt;Let me apply this to our models and then show you the plots:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot_ls &amp;lt;- lapply(list(m_Tank, m_Pred, m_Size, m_Additive, m_Interaction), Halfeyes_NoAs, N = 1e4)
plot_grid(
  plotlist = plot_ls, labels = c(&amp;quot;Tank&amp;quot;, &amp;quot;Predation&amp;quot;, &amp;quot;Size&amp;quot;, &amp;quot;Additive&amp;quot;, &amp;quot;Interaction&amp;quot;),
  ncol = 1, vjust = 1.25, hjust = -0.1
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-04-07-statistical-rethinking-chapter-13_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;These plots only tell us what our models have sampled from the posterior in terms of parameter estimates. They do not tell us how accurate the models are when predicting data. However, they do tell us loads about what the models use to make their predictions.&lt;/p&gt;
&lt;p&gt;For now, I will focus on the posterior distributions of our predation parameter (&lt;code&gt;bp&lt;/code&gt;) and size parameter (&lt;code&gt;s&lt;/code&gt;). When inspecting these, it is apparent that the parameter estimates of &lt;code&gt;bp&lt;/code&gt; are much further from 0 than those for &lt;code&gt;s&lt;/code&gt;. This holds true across all models. In addition, anytime &lt;code&gt;bp&lt;/code&gt; is contained in a model, &lt;code&gt;sigma&lt;/code&gt; (the variation in tank intercepts) decreases drastically.&lt;/p&gt;
&lt;p&gt;This is consistent with the model rankings. The tank-only model does not because size and predation are meaningless predictors. The posterior distributions above show us that they do contain important information. The tank-only model does well because there exists variation among tanks for a multitude of reasons. Prediction and inference of causality are not the same thing, after all.&lt;/p&gt;
&lt;h3 id=&#34;practice-m3&#34;&gt;Practice M3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Re-estimate the basic Reed frog varying intercept model, but now using a Cauchy distribution in place of the Gaussian distribution for the varying intercepts. That is, fit this model:&lt;/p&gt;
&lt;p&gt;$$s_i ∼ Binomial(n_i, p_i)$$
$$logit(p_i) = α_{tank[i]}$$ 
$$α_{tank} ∼ Cauchy(\alpha, \sigma)$$ 
$$\alpha ∼ Normal(0, 1)$$
$$\sigma ∼ Exponential(1)$$&lt;/p&gt;
&lt;p&gt;Compare the posterior means of the intercepts, $α_{tank}$, to the posterior means produced in the chapter, using the customary Gaussian prior. Can you explain the pattern of differences?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  This is simply the &lt;code&gt;m_Tank&lt;/code&gt; model we ran previously, but with a &lt;code&gt;dcauchy()&lt;/code&gt; prior on $\alpha_{tank}$. Because the Cauchy distribution comes with very long tails, we run into a few issues of divergent transitions with default parameters and so I add &lt;code&gt;control=list(adapt_delta=0.99)&lt;/code&gt; to the call to &lt;code&gt;ulam()&lt;/code&gt; for more measured sampling of the posterior space:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_TankCauchy &amp;lt;- ulam(
  alist(
    S ~ dbinom(n, p),
    logit(p) &amp;lt;- a[tank],
    a[tank] ~ dcauchy(a_bar, sigma),
    a_bar ~ dnorm(0, 1.5),
    sigma ~ dexp(1)
  ),
  data = dat, chains = 4, cores = 4, log_lik = TRUE,
  iter = 2e3, control = list(adapt_delta = 0.99)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let&amp;rsquo;s compare the posterior means of &lt;code&gt;m_Tank&lt;/code&gt; and the new &lt;code&gt;m_TankCauchy&lt;/code&gt; for their estimates of the $\alpha_{tank}$ parameters:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;a_Tank &amp;lt;- apply(extract.samples(m_Tank)$a, 2, mean)
a_TankCauchy &amp;lt;- apply(extract.samples(m_TankCauchy)$a, 2, mean)
plot(a_Tank, a_TankCauchy,
  pch = 16, col = rangi2,
  xlab = &amp;quot;intercept (Gaussian prior)&amp;quot;, ylab = &amp;quot; intercept (Cauchy prior)&amp;quot;
)
abline(a = 0, b = 1, lty = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-04-07-statistical-rethinking-chapter-13_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;1440&#34; /&gt;
For most of our intercepts ($\alpha_{tank}$), both the Cauchy-prior model and the Gaussian-prior model are basically creating the same results (i.e. points on the dashed line). However, once we hit extreme $\alpha_{tank}$ under the Gaussian prior, the $\alpha_{tank}$ estimates of the Cauchy prior are even more extreme by comparison. This is because of how much adaptive shrinkage is going on. In the tanks on the right-hand side of the plot above, extreme proportions of tadpoles survived the experiment. These estimates are shrunk towards the population (i.e. all tanks) mean. Since the Gaussian distribution is more concentrated than the Cauchy distribution, the Gaussian estimates have more shrinkage applied to them and so fall to lower values.&lt;/p&gt;
&lt;h3 id=&#34;practice-m4&#34;&gt;Practice M4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Modify the cross-classified chimpanzees model &lt;code&gt;m13.4&lt;/code&gt; so that the adaptive prior for blocks contains a parameter $\bar\gamma$ for its mean:&lt;/p&gt;
&lt;p&gt;$$γ_i \sim Normal(\bar\gamma,  \sigma_γ)$$
$$\bar\gamma \sim Normal(0, 1.5)$$&lt;/p&gt;
&lt;p&gt;Compare this model to &lt;code&gt;m13.4&lt;/code&gt;. What has including $\bar\gamma$ done?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; First, I load the data again and prepare it like it was done in the book:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(chimpanzees)
d &amp;lt;- chimpanzees
d$treatment &amp;lt;- 1 + d$prosoc_left + 2 * d$condition
dat_list &amp;lt;- list(
  pulled_left = d$pulled_left,
  actor = d$actor,
  block_id = d$block,
  treatment = as.integer(d$treatment)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here&amp;rsquo;s model &lt;code&gt;m13.4&lt;/code&gt; from the book:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m13.4 &amp;lt;- ulam(
  alist(
    pulled_left ~ dbinom(1, p),
    logit(p) &amp;lt;- a[actor] + g[block_id] + b[treatment],
    b[treatment] ~ dnorm(0, 0.5),
    ## adaptive priors
    a[actor] ~ dnorm(a_bar, sigma_a),
    g[block_id] ~ dnorm(0, sigma_g),
    ## hyper-priors
    a_bar ~ dnorm(0, 1.5),
    sigma_a ~ dexp(1),
    sigma_g ~ dexp(1)
  ),
  data = dat_list, chains = 4, cores = 4, log_lik = TRUE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now for the modification with the adaptive prior on blocks with $\bar\gamma$ (&lt;code&gt;g_bar&lt;/code&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_M4 &amp;lt;- ulam(
  alist(
    pulled_left ~ dbinom(1, p),
    logit(p) &amp;lt;- a[actor] + g[block_id] + b[treatment],
    b[treatment] ~ dnorm(0, 0.5),
    ## adaptive priors
    a[actor] ~ dnorm(a_bar, sigma_a),
    g[block_id] ~ dnorm(g_bar, sigma_g),
    ## hyper-priors
    a_bar ~ dnorm(0, 1.5),
    g_bar ~ dnorm(0, 1.5),
    sigma_a ~ dexp(1),
    sigma_g ~ dexp(1)
  ),
  data = dat_list, chains = 4, cores = 4, log_lik = TRUE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, let&amp;rsquo;s compare these two models:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m13.4, 2, pars = c(&amp;quot;a_bar&amp;quot;, &amp;quot;b&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean        sd        5.5%      94.5%    n_eff    Rhat4
## a_bar  0.5572916 0.7400381 -0.61815978 1.71465486 846.1133 1.000954
## b[1]  -0.1117336 0.3049828 -0.59688685 0.38352742 537.8580 1.005063
## b[2]   0.4117441 0.3013615 -0.07603188 0.90041231 541.1696 1.004054
## b[3]  -0.4583805 0.3050711 -0.94912651 0.02402336 525.1723 1.003261
## b[4]   0.3022693 0.2992465 -0.19185582 0.76269842 496.3259 1.002969
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m_M4, 2, pars = c(&amp;quot;a_bar&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;g_bar&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean        sd        5.5%        94.5%    n_eff    Rhat4
## a_bar  0.4579123 1.1562369 -1.32920915  2.276252493 288.8963 1.000056
## b[1]  -0.1180502 0.2922779 -0.58177755  0.345807340 623.5734 1.004741
## b[2]   0.4021677 0.2935470 -0.08422107  0.874494410 653.4873 1.001191
## b[3]  -0.4628938 0.2963843 -0.93668728 -0.005844908 640.5164 1.004185
## b[4]   0.2897304 0.2867230 -0.16069746  0.758941565 611.7289 1.002156
## g_bar  0.1704292 1.1696618 -1.77806340  1.926636466 209.8574 1.001308
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Oof. That new model (&lt;code&gt;m_M4&lt;/code&gt;) did not work well. I gleam that its sampling was extremely inefficient from looking at the number of effective samples (&lt;code&gt;n_eff&lt;/code&gt;) and Gelman-Rubin statistic (&lt;code&gt;Rhat&lt;/code&gt;) above. The numbers of effective samples are much worse for all of our parameters in &lt;code&gt;m_M4&lt;/code&gt; when compared to the original model (&lt;code&gt;m13.4&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Why is that? Well, &lt;code&gt;m_M4&lt;/code&gt; is what is called &lt;em&gt;over-parameterised&lt;/em&gt;. Both means of our intercepts (&lt;code&gt;a[actor]&lt;/code&gt;, &lt;code&gt;g[block_id]&lt;/code&gt;) are defined via varying priors now. So since there are two parameters for our means, one inside each adaptive prior, we end up with an infinite number of combinations of values of $\bar\alpha$ and $\bar\gamma$ to produce the same sum. This makes the posterior poorly defined and hard to sample. It is worth pointing out, however, that the estimated parameters are almost exactly the same between the two models. Conclusively, over-parameterisation is inefficient in sampling, but will land on similar values if run long enough. We should still avoid coding our models this way in the first place, of course.&lt;/p&gt;
&lt;h2 id=&#34;hard-exercises&#34;&gt;Hard Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-h1&#34;&gt;Practice H1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; In 1980, a typical Bengali woman could have 5 or more children in her lifetime. By the year 2000, a typical Bengali woman had only 2 or 3. You’re going to look at a historical set of data, when contraception was widely available but many families chose not to use it. These data reside in &lt;code&gt;data(bangladesh)&lt;/code&gt; and come from the 1988 Bangladesh Fertility Survey. Each row is one of 1934 women. There are six variables, but you can focus on three of them for this practice problem:&lt;/p&gt;
&lt;p&gt;(1) &lt;code&gt;district&lt;/code&gt;: ID number of administrative district each woman resided in&lt;br&gt;
(2) &lt;code&gt;use.contraception&lt;/code&gt;: An indicator (0/1) of whether the woman was using contraception&lt;br&gt;
(3) &lt;code&gt;urban&lt;/code&gt;: An indicator (0/1) of whether the woman lived in a city, as opposed to living in a rural area&lt;/p&gt;
&lt;p&gt;The first thing to do is ensure that the cluster variable, district, is a contiguous set of integers. Recall that these values will be index values inside the model. If there are gaps, you’ll have parameters for which there is no data to inform them. Worse, the model probably won’t run. Look at the unique values of the district variable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(bangladesh)
d &amp;lt;- bangladesh
sort(unique(d$district))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 55 56 57 58 59 60 61
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;District 54 is absent. So district isn&amp;rsquo;t yet a good index variable, because it’s not contiguous. This is easy to fix. Just make a new variable that is contiguous. This is enough to do it:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;d$district_id &amp;lt;- as.integer(as.factor(d$district))
sort(unique(d$district_id))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now there are 60 values, contiguous integers 1 to 60. Now, focus on predicting &lt;code&gt;use.contraception&lt;/code&gt;, clustered by &lt;code&gt;district_id&lt;/code&gt;. Do not include &lt;code&gt;urban&lt;/code&gt; just yet. Fit both (1) a traditional fixed-effects model that uses dummy variables for district and (2) a multilevel model with varying intercepts for district. Plot the predicted proportions of women in each district using contraception, for both the fixed-effects model and the varying-effects model. That is, make a plot in which district ID is on the horizontal axis and expected proportion using contraception is on the vertical. Make one plot for each model, or layer them on the same plot, as you prefer. How do the models disagree? Can you explain the pattern of disagreement? In particular, can you explain the most extreme cases of disagreement, both why they happen where they do and why the models reach different inferences?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  First, I prep the data into a list:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dat_list &amp;lt;- list(
  C = d$use.contraception,
  D = d$district_id
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now for the models:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Fixed-Effect&lt;/strong&gt;:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_Fixed &amp;lt;- ulam(
  alist(
    C ~ bernoulli(p), # this is the same as dbinom(1, p)
    logit(p) &amp;lt;- a[D],
    a[D] ~ dnorm(0, 1.5)
  ),
  data = dat_list, chains = 4, cores = 4, log_lik = TRUE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;Varying-Intercept&lt;/strong&gt;:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_Varying &amp;lt;- ulam(
  alist(
    C ~ dbinom(1, p), # this is the same as bernoulli(p)
    logit(p) &amp;lt;- a[D],
    a[D] ~ normal(a_bar, sigma),
    a_bar ~ normal(0, 1.5),
    sigma ~ exponential(1)
  ),
  data = dat_list, chains = 4, cores = 4, log_lik = TRUE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now to make our predictions:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## compute posterior means
p_Fixed &amp;lt;- apply(inv_logit(extract.samples(m_Fixed)$a), 2, mean)
p_Varying &amp;lt;- apply(inv_logit(extract.samples(m_Varying)$a), 2, mean)
## compute raw estimate from data in each district
tab &amp;lt;- table(d$use.contraception, d$district_id) # contraception no and yes per district
n_per_district &amp;lt;- colSums(tab) # number of observations per district
p_raw &amp;lt;- as.numeric(tab[2, ] / n_per_district) # raw proportion per district
nd &amp;lt;- max(dat_list$D) # number of districts
plot(NULL, xlim = c(1, nd), ylim = c(0, 1), ylab = &amp;quot;prob use contraception&amp;quot;, xlab = &amp;quot;district&amp;quot;)
points(1:nd, p_Fixed, pch = 16, col = rangi2, cex = 3)
points(1:nd, p_Varying, cex = 3)
points(1:nd, p_raw, pch = 3, cex = 3)
abline(
  h = mean(inv_logit(extract.samples(m_Varying)$a_bar)), # population mean
  lty = 2
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-04-07-statistical-rethinking-chapter-13_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As expected, the varying-intercept-estimates (open circles) are shrunk towards the population mean (dashed line) when compared to the fixed-intercept-estimates (blue circles) and the raw proportion (cross symbols). Some are shrunk more than others. Those which are shrunk more have been shrunk because:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Their sample sizes were small&lt;/li&gt;
&lt;li&gt;Their raw proportions were far from the population mean&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Shrinkage is also introduced due to large variation in the values within each clustering variable which is much easier to demonstrate with continuous observations rather than a binary outcome (contraception used: yes/no).&lt;/p&gt;
&lt;h3 id=&#34;practice-h2&#34;&gt;Practice H2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Return to the Trolley data, &lt;code&gt;data(Trolley)&lt;/code&gt;, from Chapter 12. Define and fit a varying intercepts model for these data. Cluster intercepts on individual participants, as indicated by the unique values in the &lt;code&gt;id&lt;/code&gt; variable. Include &lt;code&gt;action&lt;/code&gt;, &lt;code&gt;intention&lt;/code&gt;, and &lt;code&gt;contact&lt;/code&gt; as ordinary terms. Compare the varying intercepts model and a model that ignores individuals, using both WAIC and posterior predictions. What is the impact of individual variation in these data?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  Again, let&amp;rsquo;s start with loading the data and preparing it into a list:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(Trolley)
d &amp;lt;- Trolley
dat &amp;lt;- list(
  R = d$response,
  A = d$action,
  I = d$intention,
  C = d$contact
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To run the varying intercept model, the &lt;code&gt;id&lt;/code&gt; variable needs to be a simple index variable. Currently that is not the case, so let fix that, too:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dat$id &amp;lt;- coerce_index(d$id)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here&amp;rsquo;s the model from chapter 12 (&lt;code&gt;m12.5&lt;/code&gt;) which will be our baseline model for comparison:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m12.5 &amp;lt;- ulam(
  alist(
    R ~ dordlogit(phi, cutpoints),
    phi &amp;lt;- bA * A + bC * C + BI * I,
    BI &amp;lt;- bI + bIA * A + bIC * C,
    c(bA, bI, bC, bIA, bIC) ~ dnorm(0, 0.5),
    cutpoints ~ dnorm(0, 1.5)
  ),
  data = dat, chains = 4, cores = 4, log_lik = TRUE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now for the varying intercepts model (I have added a new parameter: &lt;code&gt;a[id]&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_H2 &amp;lt;- ulam(
  alist(
    R ~ dordlogit(phi, cutpoints),
    phi &amp;lt;- a[id] + bA * A + bC * C + BI * I,
    BI &amp;lt;- bI + bIA * A + bIC * C,
    a[id] ~ normal(0, sigma),
    c(bA, bI, bC, bIA, bIC) ~ dnorm(0, 0.5),
    cutpoints ~ dnorm(0, 1.5),
    sigma ~ exponential(1)
  ),
  data = dat, chains = 4, cores = 4, log_lik = TRUE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s start comparing these two models by looking at their parameter estimates:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m12.5)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           mean         sd       5.5%      94.5%    n_eff    Rhat4
## bIC -1.2332275 0.09736975 -1.3882391 -1.0769963 999.3059 1.000883
## bIA -0.4325889 0.07986057 -0.5621930 -0.3043263 937.5476 1.000899
## bC  -0.3430801 0.06858197 -0.4535116 -0.2286056 999.4107 1.000974
## bI  -0.2919059 0.05770286 -0.3857312 -0.2020194 855.6081 1.001664
## bA  -0.4733247 0.05380776 -0.5606800 -0.3883240 892.1070 1.001730
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m_H2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean         sd       5.5%      94.5%    n_eff     Rhat4
## bIC   -1.6593276 0.10069140 -1.8242617 -1.4993492 1345.530 0.9989020
## bIA   -0.5524349 0.07997312 -0.6840528 -0.4238708 1185.324 0.9994280
## bC    -0.4584264 0.07028331 -0.5692494 -0.3468558 1299.616 0.9997609
## bI    -0.3892700 0.05888992 -0.4845553 -0.2932161 1044.587 0.9994699
## bA    -0.6511028 0.05543980 -0.7386650 -0.5631939 1256.216 0.9988687
## sigma  1.9176768 0.08248496  1.7902506  2.0537969 2291.638 0.9997130
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When moving to varying intercepts, in this case, all parameter estimates have become stronger in magnitude while remaining negative in sign. Why is that? Because there is a lot of variation among the individual intercepts. &lt;code&gt;sigma&lt;/code&gt; tells us that. Remember that is on the logit scale, so there is a lot of variation here in probability scale. Conclusively, the average formulation we explored in chapter 12 (&lt;code&gt;m12.5&lt;/code&gt;) hid a lot of the effect of the different treatments.&lt;/p&gt;
&lt;p&gt;Finally, let&amp;rsquo;s compare our models using WAIC:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;compare(m12.5, m_H2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           WAIC        SE    dWAIC      dSE     pWAIC weight
## m_H2  31057.47 179.39150    0.000       NA 355.68843      1
## m12.5 36929.69  80.66443 5872.216 173.5441  11.17174      0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now it&amp;rsquo;s official. Conditioning on the individual (&lt;code&gt;id&lt;/code&gt;) really made a massive difference here in understanding the assignments of morality among our data. Effectively, this tells us that our few variables which we used previously to understand how people of different backgrounds and genders perceive morality are not enough to fully understand the matter at hand.&lt;/p&gt;
&lt;h3 id=&#34;practice-h3&#34;&gt;Practice H3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; The Trolley data are also clustered by &lt;code&gt;story&lt;/code&gt;, which indicates a unique narrative for each vignette. Define and fit a cross-classified varying intercepts model with both &lt;code&gt;id&lt;/code&gt; and &lt;code&gt;story&lt;/code&gt;. Use the same ordinary terms as in the previous problem. Compare this model to the previous models. What do you infer about the impact of different stories on responses?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  I continue with the data as used before, but add the information about &lt;code&gt;story&lt;/code&gt; which needs to be coerced into a proper index, too:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dat$Sid &amp;lt;- coerce_index(d$story)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now for the cross-classified model. All I do here is just add a varying intercept for &lt;code&gt;story&lt;/code&gt;/&lt;code&gt;Sid&lt;/code&gt;. This is a non-centred parametrisation - it probably explores posterior space less efficiently than the centred counterpart, but I find it easier to write and am under a time crunch when writing these solutions:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_H3 &amp;lt;- ulam(
  alist(
    R ~ dordlogit(phi, cutpoints),
    phi &amp;lt;- z[id] * sigma + s[Sid] + bA * A + bC * C + BI * I,
    BI &amp;lt;- bI + bIA * A + bIC * C,
    z[id] ~ normal(0, 1),
    s[Sid] ~ normal(0, tau),
    c(bA, bI, bC, bIA, bIC) ~ dnorm(0, 0.5),
    cutpoints ~ dnorm(0, 1.5),
    sigma ~ exponential(1),
    tau ~ exponential(1)
  ),
  data = dat, chains = 4, cores = 4, log_lik = TRUE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m_H3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean         sd       5.5%      94.5%    n_eff     Rhat4
## bIC   -1.2842990 0.11302619 -1.4633731 -1.1053741 1298.018 1.0012462
## bIA   -0.5259545 0.08510626 -0.6602232 -0.3885573 1328.205 0.9999519
## bC    -1.0802958 0.09536571 -1.2312331 -0.9262213 1306.242 0.9997597
## bI    -0.4587315 0.06843039 -0.5681288 -0.3517746 1287.560 1.0026441
## bA    -0.8941359 0.06928219 -1.0008664 -0.7831957 1316.376 1.0001676
## sigma  1.9602712 0.08455294  1.8277702  2.0951087  170.422 1.0206646
## tau    0.5404331 0.13271393  0.3704167  0.7853622 1857.443 0.9987709
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The treatment variable estimates (&lt;code&gt;bIC&lt;/code&gt;, &lt;code&gt;bIA&lt;/code&gt;, etc.) are changed from the previous model (&lt;code&gt;m_H2&lt;/code&gt;). Interestingly, the estimate for &lt;code&gt;sigma&lt;/code&gt; (variation among individuals) has not changed much. The added variation among stories (&lt;code&gt;tau&lt;/code&gt;) is noticeable, albeit much smaller than &lt;code&gt;sigma&lt;/code&gt;. Let&amp;rsquo;s visualise this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(coeftab(m_H2, m_H3), pars = c(&amp;quot;bIC&amp;quot;, &amp;quot;bIA&amp;quot;, &amp;quot;bC&amp;quot;, &amp;quot;bI&amp;quot;, &amp;quot;bA&amp;quot;, &amp;quot;sigma&amp;quot;, &amp;quot;tau&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-04-07-statistical-rethinking-chapter-13_files/figure-html/unnamed-chunk-33-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This means that there is probably rather meaningful information contained within the story variable when trying to understand morality of decision in the trolley data. We cannot meaningfully compare these models using WAIC, however, and so this will remain a qualitative statement&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.5 (2021-03-31)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19043)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.1252  LC_CTYPE=English_United Kingdom.1252    LC_MONETARY=English_United Kingdom.1252 LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.1252    
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] cowplot_1.1.1        tidybayes_2.3.1      rethinking_2.13      rstan_2.21.2         ggplot2_3.3.6        StanHeaders_2.21.0-7
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.7           mvtnorm_1.1-1        lattice_0.20-41      tidyr_1.1.3          prettyunits_1.1.1    ps_1.6.0             assertthat_0.2.1     digest_0.6.27        utf8_1.2.1          
## [10] V8_3.4.1             plyr_1.8.6           R6_2.5.0             backports_1.2.1      stats4_4.0.5         evaluate_0.14        coda_0.19-4          highr_0.9            blogdown_1.3        
## [19] pillar_1.6.0         rlang_0.4.11         curl_4.3.2           callr_3.7.0          jquerylib_0.1.4      R.utils_2.10.1       R.oo_1.24.0          rmarkdown_2.7        styler_1.4.1        
## [28] labeling_0.4.2       stringr_1.4.0        loo_2.4.1            munsell_0.5.0        compiler_4.0.5       xfun_0.22            pkgconfig_2.0.3      pkgbuild_1.2.0       shape_1.4.5         
## [37] htmltools_0.5.1.1    tidyselect_1.1.0     tibble_3.1.1         gridExtra_2.3        bookdown_0.22        arrayhelpers_1.1-0   codetools_0.2-18     matrixStats_0.61.0   fansi_0.4.2         
## [46] crayon_1.4.1         dplyr_1.0.5          withr_2.4.2          MASS_7.3-53.1        R.methodsS3_1.8.1    distributional_0.2.2 ggdist_2.4.0         grid_4.0.5           jsonlite_1.7.2      
## [55] gtable_0.3.0         lifecycle_1.0.0      DBI_1.1.1            magrittr_2.0.1       scales_1.1.1         RcppParallel_5.1.2   cli_3.0.0            stringi_1.5.3        farver_2.1.0        
## [64] bslib_0.2.4          ellipsis_0.3.2       generics_0.1.0       vctrs_0.3.7          rematch2_2.1.2       forcats_0.5.1        tools_4.0.5          svUnit_1.0.6         R.cache_0.14.0      
## [73] glue_1.4.2           purrr_0.3.4          processx_3.5.1       yaml_2.2.1           inline_0.3.17        colorspace_2.0-0     knitr_1.33           sass_0.3.1
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 14</title>
      <link>https://www.erikkusch.com/courses/rethinking/chapter-14/</link>
      <pubDate>Fri, 16 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/rethinking/chapter-14/</guid>
      <description>&lt;h1 id=&#34;adventures-in-covariance&#34;&gt;Adventures in Covariance&lt;/h1&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/rethinking/16__16-04-2021_SUMMARY_-Multi-Level-Models-II.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides Chapter 14&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to the exercises at the end of chapter 14 in 
&lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Satistical Rethinking 2&lt;/a&gt; by Richard McElreath. I have created these notes as a part of my ongoing involvement in the 
&lt;a href=&#34;https://www.erikkusch.com/project/aubayes/&#34;&gt;AU Bayes Study Group&lt;/a&gt;. Much of my inspiration for these solutions, where necessary, has been obtained from&lt;/p&gt;
&lt;!-- [Taras Svirskyi](https://github.com/jffist/statistical-rethinking-solutions/blob/master/ch10_hw.R), [William Wolf](https://github.com/cavaunpeu/statistical-rethinking/blob/master/chapter-10/homework.R), and [Corrie Bartelheimer](https://www.samples-of-thoughts.com/projects/statistical-rethinking/chapter_10/chp10-ex/) as well as  --&gt;
&lt;p&gt;the solutions provided to instructors by Richard McElreath himself.&lt;/p&gt;
&lt;h2 id=&#34;r-environment&#34;&gt;&lt;code&gt;R&lt;/code&gt; Environment&lt;/h2&gt;
&lt;p&gt;For today&amp;rsquo;s exercise, I load the following packages:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rethinking)
library(rstan)
library(MASS)
library(ellipse)
library(ape)
library(ggplot2)
library(tidybayes)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;easy-exercises&#34;&gt;Easy Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-e1&#34;&gt;Practice E1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Add to the following model varying slopes on the predictor $x$.&lt;/p&gt;
&lt;p&gt;$$y_i ∼ Normal(µi, σ)$$ 
$$µ_i = α_{group[i]} + βx_i$$
$$α_{group} ∼ Normal(α, σ_α)$$ 
$$α ∼ Normal(0, 10)$$ 
$$β ∼ Normal(0, 1)$$ 
$$σ ∼ HalfCauchy(0, 2)$$ 
$$σ_α ∼ HalfCauchy(0, 2)$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; To do this, our outcome distribution does not change. So keep it as is:&lt;/p&gt;
&lt;p&gt;$$y_i ∼ Normal(μ_i, σ)$$&lt;/p&gt;
&lt;p&gt;Next, we come to the linear model. This needs changing. Since we are now interested in a varying slope for each group ($\beta_{group}$), we need to exchange the original $\beta$ with $\beta_{group}$:&lt;/p&gt;
&lt;p&gt;$$μ_i = α_{group[i]} + β_{group[i]}x_i$$&lt;/p&gt;
&lt;p&gt;Consequently, we also need to change our prior. Since $\alpha_{group}$ and $\beta_{group}$ now stem from a joint distribution, we need to express them as such. $\alpha$ is still the average intercept. However, $\beta$ now turns into the average slope. Both of these serve as the mean expectations for $\alpha_{group}$ and $\beta_{group}$ in a multivariate normal distribution ($MVNormal()$) with a covariance matrix ($S$) defining how they are linked.&lt;/p&gt;
&lt;p&gt;$$\begin{bmatrix} \alpha_{group} \ \beta_{group} \ \end{bmatrix}  \sim MVNormal \left(\begin{bmatrix} \alpha \ \beta \ \end{bmatrix}, S \right)$$&lt;/p&gt;
&lt;p&gt;Since we have just introduced the need for a covariance matrix, we now need to define it. A covariance matrix is the product of a variance matrix and a correlation matrix ($R$). What we can do when determining the covariance matrix ($S$) is setting our variances for $\alpha$ and $\beta$ - $\sigma_\alpha$ and $\sigma_\beta$, respectively - and subsequently multiplying this with the correlation matrix ($R$):&lt;/p&gt;
&lt;p&gt;$$S = \begin{pmatrix} \sigma_\alpha &amp;amp; 0 \ 0 &amp;amp; \sigma_\beta  \ \end{pmatrix} R \begin{pmatrix} \sigma_\alpha &amp;amp; 0 \ 0 &amp;amp; \sigma_\beta  \ \end{pmatrix} $$&lt;/p&gt;
&lt;p&gt;The variances and correlation matrix referenced above need priors of their own - so called hyperpriors. Let&amp;rsquo;s start with the priors of the variances:&lt;/p&gt;
&lt;p&gt;$$σ_α ∼ HalfCauchy(0, 2)$$
$$σ_\beta ∼ HalfCauchy(0, 2)$$&lt;/p&gt;
&lt;p&gt;And also add a somewhat regularising prior for $R$:&lt;/p&gt;
&lt;p&gt;$$R ∼ LKJcorr(2)$$&lt;/p&gt;
&lt;p&gt;Lastly, we simply keep the priors for $\alpha$, $\beta$, and $\sigma$ from the original model.
$$α ∼ Normal(0, 10)$$ 
$$β ∼ Normal(0, 1)$$ 
$$σ ∼ HalfCauchy(0, 2)$$&lt;/p&gt;
&lt;h3 id=&#34;practice-e2&#34;&gt;Practice E2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Think up a context in which varying intercepts will be positively correlated with varying slopes. Provide a mechanistic explanation for the correlation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; A setting within which there is positive correlation between varying intercepts and varying slopes can be put in laymen-terms as: &amp;ldquo;A setting within which high intercepts come with steep slopes&amp;rdquo;. With that in mind, what could be such a setting?&lt;/p&gt;
&lt;p&gt;There are many settings which would meet this criterion. I am a biologist by training and the first thing that came to mind was that of an ant colony. Let&amp;rsquo;s say we are interested studying ant hill size as a function of food availability. Ignoring the carrying capacity of a system, we can reasonably expect larger ant hills (higher intercepts) to benefit more strongly from increased food availability as their foraging will be much more efficient (steeper slope).&lt;/p&gt;
&lt;p&gt;Of course, I realise that this thought experiment ignores some crucial bits of biological reality such as diminishing returns and structural integrity of ant hills after a certain size is reached. For the sake of keeping this example simple, I neglect them.&lt;/p&gt;
&lt;h3 id=&#34;practice-e3&#34;&gt;Practice E3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  When is it possible for a varying slopes model to have fewer effective parameters (as estimated by WAIC or DIC) than the corresponding model with fixed (unpooled) slopes? Explain.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; When there is little or next-to-no variation among clusters. The absence of this among-cluster variation induces very strong shrinkage. As a result, albeit containing more actual parameters in the posterior distribution, the varying slopes model may end up less flexible in fitting to the data because of adaptive regularisation forcing strong shrinkage. Consequently, our number of effective parameters - a proxy of overfitting risk and posterior flexibility - decreases.&lt;/p&gt;
&lt;p&gt;For an example, consult the comparison of models &lt;code&gt;m13.1&lt;/code&gt; and &lt;code&gt;m13.2&lt;/code&gt; in R Code &lt;code&gt;13.4&lt;/code&gt; in the book.&lt;/p&gt;
&lt;h2 id=&#34;medium-exercises&#34;&gt;Medium Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-m1&#34;&gt;Practice M1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Repeat the café robot simulation from the beginning of the chapter. This time, set &lt;code&gt;rho&lt;/code&gt; to zero, so that there is no correlation between intercepts and slopes. How does the posterior distribution of the correlation reflect this change in the underlying simulation?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; This is what was done in the book. &lt;code&gt;rho&lt;/code&gt; has been adjusted to be $0$ now:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# set up parameters of population
a &amp;lt;- 3.5 # average morning wait time
b &amp;lt;- (-1) # average difference afternoon wait time
sigma_a &amp;lt;- 1 # std dev in intercepts
sigma_b &amp;lt;- 0.5 # std dev in slopes
rho &amp;lt;- 0 # correlation between intercepts and slopes
Mu &amp;lt;- c(a, b)
cov_ab &amp;lt;- sigma_a * sigma_b * rho
Sigma &amp;lt;- matrix(c(sigma_a^2, cov_ab, cov_ab, sigma_b^2), ncol = 2)
# simulate observations
N_cafes &amp;lt;- 20
set.seed(6)
vary_effects &amp;lt;- mvrnorm(N_cafes, Mu, Sigma)
a_cafe &amp;lt;- vary_effects[, 1]
b_cafe &amp;lt;- vary_effects[, 2]
N_visits &amp;lt;- 10
afternoon &amp;lt;- rep(0:1, N_visits * N_cafes / 2)
cafe_id &amp;lt;- rep(1:N_cafes, each = N_visits)
mu &amp;lt;- a_cafe[cafe_id] + b_cafe[cafe_id] * afternoon
sigma &amp;lt;- 0.5 # std dev within cafes
wait &amp;lt;- rnorm(N_visits * N_cafes, mu, sigma)
# package into  data frame
d &amp;lt;- data.frame(cafe = cafe_id, afternoon = afternoon, wait = wait)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now to run our model (&lt;code&gt;m14.1&lt;/code&gt;) with the exact same specification as in the book:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_M1 &amp;lt;- ulam(
  alist(
    wait ~ normal(mu, sigma),
    mu &amp;lt;- a_cafe[cafe] + b_cafe[cafe] * afternoon,
    c(a_cafe, b_cafe)[cafe] ~ multi_normal(c(a, b), Rho, sigma_cafe),
    a ~ normal(5, 2),
    b ~ normal(-1, 0.5),
    sigma_cafe ~ exponential(1),
    sigma ~ exponential(1),
    Rho ~ lkj_corr(2)
  ),
  data = d, chains = 6, cores = 6
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So what about that posterior distribution for &lt;code&gt;Rho&lt;/code&gt;?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;post &amp;lt;- extract.samples(m_M1)
ggplot() +
  stat_halfeye(aes(x = post$Rho[, 1, 2])) +
  theme_bw() +
  labs(x = &amp;quot;Rho&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-04-15-statistical-rethinking-chapter-14_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Jup. That accurately represents our underlying correlation of $0$. The &lt;code&gt;precis&lt;/code&gt; output agrees:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m_M1, pars = &amp;quot;Rho[1,2]&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              result
## mean     0.01110476
## sd       0.23468860
## 5.5%    -0.36400650
## 94.5%    0.38982346
## n_eff 2833.41771477
## Rhat     1.00023118
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-m2&#34;&gt;Practice M2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Fit this multilevel model to the simulated café data: 
$$W_i ∼ Normal(µ_i, σ)$$ 
$$µ_i = α_{café[i]} + β_{café[i]}A_i$$
$$α_{café} ∼ Normal(α, σ_α)$$ 
$$β_{café} ∼ Normal(β, σ_β)$$ 
$$α ∼ Normal(0, 10)$$ 
$$β ∼ Normal(0, 10)$$ 
$$σ ∼ HalfCauchy(0, 1)$$
$$σ_α ∼ HalfCauchy(0, 1)$$ 
$$σ_β ∼ HalfCauchy(0, 1)$$&lt;/p&gt;
&lt;p&gt;Use WAIC to compare this model to the model from the chapter, the one that uses a multi-variate Gaussian prior. Explain the result.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; I am strongly assuming that this question is targeting the simulated café data used in the book. I create that data again here:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# set up parameters of population
a &amp;lt;- 3.5 # average morning wait time
b &amp;lt;- -1 # average difference afternoon wait time
sigma_a &amp;lt;- 1 # std dev in intercepts
sigma_b &amp;lt;- 0.5 # std dev in slopes
rho &amp;lt;- -0.7 # correlation between intercepts and slopes
Mu &amp;lt;- c(a, b)
cov_ab &amp;lt;- sigma_a * sigma_b * rho
Sigma &amp;lt;- matrix(c(sigma_a^2, cov_ab, cov_ab, sigma_b^2), ncol = 2)
# simulate observations
N_cafes &amp;lt;- 20
set.seed(42)
vary_effects &amp;lt;- mvrnorm(N_cafes, Mu, Sigma)
a_cafe &amp;lt;- vary_effects[, 1]
b_cafe &amp;lt;- vary_effects[, 2]
N_visits &amp;lt;- 10
afternoon &amp;lt;- rep(0:1, N_visits * N_cafes / 2)
cafe_id &amp;lt;- rep(1:N_cafes, each = N_visits)
mu &amp;lt;- a_cafe[cafe_id] + b_cafe[cafe_id] * afternoon
sigma &amp;lt;- 0.5 # std dev within cafes
wait &amp;lt;- rnorm(N_visits * N_cafes, mu, sigma)
# package into  data frame
d &amp;lt;- data.frame(cafe = cafe_id, afternoon = afternoon, wait = wait)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the data at hand, I now run our baseline model which is, again, &lt;code&gt;m14.1&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_M2Baseline &amp;lt;- ulam(
  alist(
    wait ~ normal(mu, sigma),
    mu &amp;lt;- a_cafe[cafe] + b_cafe[cafe] * afternoon,
    c(a_cafe, b_cafe)[cafe] ~ multi_normal(c(a, b), Rho, sigma_cafe),
    a ~ normal(5, 2),
    b ~ normal(-1, 0.5),
    sigma_cafe ~ exponential(1),
    sigma ~ exponential(1),
    Rho ~ lkj_corr(2)
  ),
  data = d, chains = 4, cores = 4
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now onto our new model for this task. What is already striking is the use of independent intercepts and slopes. There is no correlation parameter between them so the assumed correlation, by the model, is $0$:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_M2 &amp;lt;- ulam(
  alist(
    wait ~ dnorm(mu, sigma),
    mu &amp;lt;- a_cafe[cafe] + b_cafe[cafe] * afternoon,
    a_cafe[cafe] ~ dnorm(a, sigma_alpha),
    b_cafe[cafe] ~ dnorm(b, sigma_beta),
    a ~ dnorm(0, 10),
    b ~ dnorm(0, 10),
    sigma ~ dexp(1),
    sigma_alpha ~ dexp(1),
    sigma_beta ~ dexp(1)
  ),
  data = d, chains = 4, cores = 4
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But what actually distinguishes the model outputs now? Let&amp;rsquo;s extract posterior samples for intercepts and slopes from both models and investigate visually:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;post_Base &amp;lt;- extract.samples(m_M2Baseline)
a_Base &amp;lt;- apply(post_Base$a_cafe, 2, mean)
b_Base &amp;lt;- apply(post_Base$b_cafe, 2, mean)
post_M2 &amp;lt;- extract.samples(m_M2)
a_M2 &amp;lt;- apply(post_M2$a_cafe, 2, mean)
b_M2 &amp;lt;- apply(post_M2$b_cafe, 2, mean)
plot(a_M2, b_M2,
  xlab = &amp;quot;intercept&amp;quot;, ylab = &amp;quot;slope&amp;quot;,
  pch = 16, col = rangi2, ylim = c(min(b_M2) - 0.05, max(b_M2) + 0.05),
  xlim = c(min(a_M2) - 0.1, max(a_M2) + 0.1), cex = 2
)
points(a_Base, b_Base, pch = 1, cex = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-04-15-statistical-rethinking-chapter-14_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I have stuck to McElreath&amp;rsquo;s colour scheme here once more. The filled circles represent samples from our new model, while the open circles represent samples from the posterior obtained via the model which accounts for correlation of slopes and intercepts. First and foremost, these are pretty similar I must say. This agreement is particularly pronounced towards the centre of the plot with increasing divergences of the posterior samples at the fringes of the intercept and slope ranges. This comes down to what the underlying models assume. Our baseline model assumes that slopes and intercepts are inherently related to one another and finds a negative correlation between them. This can be seen when looking at the lower right-hand and the upper left-hand corner of the plot above. Given the baseline model assumption, large intercepts are associated with strongly negative slopes and vice versa.&lt;/p&gt;
&lt;p&gt;The correlation-informed model does better here because it leverages more information from the entire population and just so happens to exactly mirror the data generation process.&lt;/p&gt;
&lt;h3 id=&#34;practice-m3&#34;&gt;Practice M3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Re-estimate the varying slopes model for the &lt;code&gt;UCBadmit&lt;/code&gt; data, now using a non-centered parametrization. Compare the efficiency of the forms of the model, using &lt;code&gt;n_eff&lt;/code&gt;. Which is better? Which chain sampled faster?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Ok&amp;hellip; This is a headache because there is no varying slopes model for the &lt;code&gt;UCBadmit&lt;/code&gt; data in the bookchapter. So let&amp;rsquo;s make one ourselves and then re-parameterise it.&lt;/p&gt;
&lt;p&gt;We start by loading and preparing the data. By defining an indicator variable for &lt;code&gt;male&lt;/code&gt; we make it easier to fit a varying slopes model based on gender of applicant:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(UCBadmit)
d &amp;lt;- UCBadmit
dat_list &amp;lt;- list(
  admit = d$admit,
  applications = d$applications,
  male = ifelse(d$applicant.gender == &amp;quot;male&amp;quot;, 1, 0),
  dept_id = rep(1:6, each = 2)
)
str(dat_list)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## List of 4
##  $ admit       : int [1:12] 512 89 353 17 120 202 138 131 53 94 ...
##  $ applications: int [1:12] 825 108 560 25 325 593 417 375 191 393 ...
##  $ male        : num [1:12] 1 0 1 0 1 0 1 0 1 0 ...
##  $ dept_id     : int [1:12] 1 1 2 2 3 3 4 4 5 5 ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now with the data in hand, we can fit our own model on varying slopes. Let&amp;rsquo;s think about this in theory first. What would this look like?&lt;/p&gt;
&lt;p&gt;We start out with a binomial outcome distribution for our admitted applications:&lt;/p&gt;
&lt;p&gt;$$admit_i ∼ Binomial(Applications, p_i)$$&lt;/p&gt;
&lt;p&gt;Our next line is the linear model again. This time, the admittance rate ($p_i$) is a product of a department-specific intercept ($\alpha_{deptId}$) and slope ($\beta_{deptId}$):&lt;/p&gt;
&lt;p&gt;$$p_i = \alpha_{deptID} + \beta_{deptID}*male$$&lt;/p&gt;
&lt;p&gt;Since the varying slopes and intercepts are certain to be correlated, we specify a multivariate normal prior again:&lt;/p&gt;
&lt;p&gt;$$\begin{bmatrix} \alpha_{deptID} \ \beta_{deptID} \ \end{bmatrix}  \sim MVNormal \left(\begin{bmatrix} \alpha \ \beta \ \end{bmatrix}, S \right)$$&lt;/p&gt;
&lt;p&gt;And now for the covariance matrix:&lt;/p&gt;
&lt;p&gt;$$S = \begin{pmatrix} \sigma_\alpha &amp;amp; 0 \ 0 &amp;amp; \sigma_\beta  \ \end{pmatrix} R \begin{pmatrix} \sigma_\alpha &amp;amp; 0 \ 0 &amp;amp; \sigma_\beta  \ \end{pmatrix} $$&lt;/p&gt;
&lt;p&gt;Finally, we just need some priors and hyperpriors:&lt;/p&gt;
&lt;p&gt;$$σ_α ∼ Exponential(1)$$
$$σ_\beta ∼ Exponential(1)$$&lt;/p&gt;
&lt;p&gt;And also add a somewhat regularising prior for $R$:&lt;/p&gt;
&lt;p&gt;$$R ∼ LKJcorr(2)$$&lt;/p&gt;
&lt;p&gt;Lastly, we simply keep the priors for $\alpha$, $\beta$:
$$α ∼ Normal(0, 1)$$ 
$$β ∼ Normal(0, 1)$$&lt;/p&gt;
&lt;p&gt;And now to do all of this in &lt;code&gt;R&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Begin_C &amp;lt;- Sys.time()
m_M3 &amp;lt;- ulam(
  alist(
    admit ~ dbinom(applications, p),
    logit(p) &amp;lt;- a[dept_id] + bm[dept_id] * male,
    c(a, bm)[dept_id] ~ multi_normal(c(a_bar, bm_bar), Rho, sigma_dept),
    a_bar ~ dnorm(0, 1),
    bm_bar ~ dnorm(0, 1),
    sigma_dept ~ dexp(1),
    Rho ~ dlkjcorr(2)
  ),
  data = dat_list, chains = 4, cores = 4
)
End_C &amp;lt;- Sys.time()
precis(m_M3, 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                      mean           sd       5.5%      94.5%     n_eff     Rhat4
## bm[1]         -0.75467135 2.642890e-01 -1.1974575 -0.3419394  762.8361 1.0035396
## bm[2]         -0.21546963 3.141762e-01 -0.7172490  0.2702785 1444.8377 1.0007292
## bm[3]          0.07788729 1.395628e-01 -0.1457046  0.3005848 1519.8738 1.0005234
## bm[4]         -0.09706014 1.389302e-01 -0.3171928  0.1276948 1760.1554 0.9999004
## bm[5]          0.11606582 1.793774e-01 -0.1610577  0.4022199 1726.0739 1.0005376
## bm[6]         -0.10986997 2.643276e-01 -0.5302442  0.3009293 1397.2613 1.0030231
## a[1]           1.27128456 2.477604e-01  0.8819218  1.6722364  778.7511 1.0032979
## a[2]           0.74963140 3.154061e-01  0.2626239  1.2484753 1436.8953 1.0004966
## a[3]          -0.64605733 8.582742e-02 -0.7808221 -0.5086328 1617.3253 0.9995321
## a[4]          -0.61501279 1.019817e-01 -0.7774726 -0.4515837 1708.0376 0.9992410
## a[5]          -1.13005620 1.096386e-01 -1.3054035 -0.9559542 1993.4173 1.0004230
## a[6]          -2.60481876 2.045530e-01 -2.9411431 -2.2759003 1799.7746 1.0015531
## a_bar         -0.39106882 5.307969e-01 -1.2265922  0.4709463 1698.0086 0.9994382
## bm_bar        -0.16176143 2.137141e-01 -0.4947019  0.1694293 1403.2870 1.0018595
## sigma_dept[1]  1.48653705 4.716297e-01  0.9088349  2.3531967 1292.0985 1.0008335
## sigma_dept[2]  0.44566910 2.144169e-01  0.1819589  0.8410725  988.9796 1.0015728
## Rho[1,1]       1.00000000 0.000000e+00  1.0000000  1.0000000       NaN       NaN
## Rho[1,2]      -0.32154165 3.408218e-01 -0.8124723  0.2811236 1522.8640 1.0015113
## Rho[2,1]      -0.32154165 3.408218e-01 -0.8124723  0.2811236 1522.8640 1.0015113
## Rho[2,2]       1.00000000 8.265587e-17  1.0000000  1.0000000 1811.6478 0.9979980
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s just acknowledge that the &lt;code&gt;precis()&lt;/code&gt; output is here, but move on for now to the re-parametrised model.&lt;/p&gt;
&lt;p&gt;I am not even going to attempt to come up with the mathematical notation of the non-centred version of the above model. Luckily, I don&amp;rsquo;t have to because &lt;code&gt;ulam()&lt;/code&gt; has helper functions which can do this for me:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Begin_NC &amp;lt;- Sys.time()
m_M3NonCent &amp;lt;- ulam(
  alist(
    admit ~ dbinom(applications, p),
    logit(p) &amp;lt;- a_bar + v[dept_id, 1] + (bm_bar + v[dept_id, 2]) * male,
    transpars &amp;gt; matrix[dept_id, 2]:v &amp;lt;- compose_noncentered(sigma_dept, L_Rho, z),
    matrix[2, dept_id]:z ~ dnorm(0, 1),
    a_bar ~ dnorm(0, 1.5),
    bm_bar ~ dnorm(0, 1),
    vector[2]:sigma_dept ~ dexp(1),
    cholesky_factor_corr[2]:L_Rho ~ lkj_corr_cholesky(2)
  ),
  data = dat_list, chains = 4, cores = 4
)
End_NC &amp;lt;- Sys.time()
precis(m_M3NonCent, 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                      mean        sd       5.5%       94.5%     n_eff     Rhat4
## z[1,1]         1.27847990 0.5262323  0.4705305  2.13326205  727.0227 1.0011087
## z[1,2]         0.88269092 0.4857079  0.1388867  1.68994558  719.7999 1.0031813
## z[1,3]        -0.12528283 0.3849925 -0.7106325  0.50669799  612.7119 1.0140975
## z[1,4]        -0.10433526 0.3866020 -0.6958663  0.51682268  601.9049 1.0154629
## z[1,5]        -0.48074322 0.4055438 -1.1153447  0.16675545  643.6182 1.0175388
## z[1,6]        -1.56200990 0.5688037 -2.4838992 -0.69872746  737.3167 1.0145782
## z[2,1]        -1.23644880 0.8164974 -2.6022559  0.05071129 1271.1801 1.0026798
## z[2,2]         0.18312727 0.8096899 -1.0820859  1.45719856 1573.6340 1.0003249
## z[2,3]         0.60192227 0.6276861 -0.3439741  1.64761142 1203.4125 1.0020469
## z[2,4]         0.10146041 0.5799030 -0.8100848  1.01529134 1291.1294 1.0015204
## z[2,5]         0.53484163 0.6684946 -0.4889643  1.64962250 1165.4873 1.0009753
## z[2,6]        -0.49127010 0.8038063 -1.8068248  0.74735922 1885.4880 1.0003286
## a_bar         -0.46792318 0.5611715 -1.3795380  0.39745615  588.0941 1.0121940
## bm_bar        -0.13969516 0.2136100 -0.4695941  0.18952741  799.7150 1.0034511
## sigma_dept[1]  1.46852953 0.4505571  0.9255641  2.31989495  833.1351 1.0030067
## sigma_dept[2]  0.44493698 0.2388977  0.1818530  0.86089979  757.1626 0.9991599
## L_Rho[1,1]     1.00000000 0.0000000  1.0000000  1.00000000       NaN       NaN
## L_Rho[1,2]     0.00000000 0.0000000  0.0000000  0.00000000       NaN       NaN
## L_Rho[2,1]    -0.31981947 0.3440124 -0.8022403  0.29104897 1687.4781 1.0001054
## L_Rho[2,2]     0.87223636 0.1365401  0.5970006  0.99912792 1190.3266 1.0002817
## v[1,1]         1.73536453 0.6011856  0.8128695  2.70174932  648.8099 1.0094272
## v[1,2]        -0.61370995 0.3128848 -1.1315494 -0.14674125 1028.2946 1.0005503
## v[2,1]         1.19934441 0.6279810  0.2169330  2.22040335  675.6765 1.0101770
## v[2,2]        -0.06176349 0.3328516 -0.5897528  0.46209909 1539.0956 1.0001901
## v[3,1]        -0.17733549 0.5673456 -1.0542767  0.75415175  593.0556 1.0109594
## v[3,2]         0.21846891 0.2395483 -0.1438607  0.60551012 1035.9533 1.0016130
## v[4,1]        -0.14832190 0.5675488 -1.0127159  0.76606136  575.1370 1.0125444
## v[4,2]         0.04602378 0.2363775 -0.3298587  0.41695086  890.5646 1.0022504
## v[5,1]        -0.65983984 0.5702765 -1.5558841  0.26053379  610.1289 1.0114797
## v[5,2]         0.25080258 0.2583262 -0.1081353  0.67319529 1159.0924 1.0024436
## v[6,1]        -2.12809664 0.5847887 -3.0186504 -1.19350046  633.0943 1.0111973
## v[6,2]         0.02683558 0.3048067 -0.4676814  0.50643307 1645.9028 1.0004084
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First of all, we can see that the number of effective samples (&lt;code&gt;n_eff&lt;/code&gt;) is higher for the centred model (&lt;code&gt;m_M3&lt;/code&gt;) which is surprising to me. I thought that non-centred models were supposed to sample more efficiently. Maybe the underlying data just doesn&amp;rsquo;t suffer from abrupt changes in posterior slope?&lt;/p&gt;
&lt;p&gt;So what about running time?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Centred
End_C - Begin_C
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Time difference of 55.36985 secs
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Non-Centred
End_NC - Begin_NC
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Time difference of 51.25241 secs
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ok. The non-centred model ran slightly faster.&lt;/p&gt;
&lt;h3 id=&#34;practice-m4&#34;&gt;Practice M4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Use WAIC to compare the Gaussian process model of Oceanic tools to the models fit to the same data in Chapter 11. Pay special attention to the effective numbers of parameters, as estimated by WAIC.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; So this needed some digging. The models in question are &lt;code&gt;m11.11&lt;/code&gt; for the simpler model of CHapter 11 and &lt;code&gt;m14.8&lt;/code&gt; from Chapter 14.&lt;/p&gt;
&lt;p&gt;Before we can do any modelling, we need to load and prepare the data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(Kline)
d &amp;lt;- Kline
# Chapter 11 stuff
d$P &amp;lt;- scale(log(d$population))
d$contact_id &amp;lt;- ifelse(d$contact == &amp;quot;high&amp;quot;, 2, 1)
# Chapter 14 stuff
d$society &amp;lt;- 1:10
data(islandsDistMatrix)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the data at hand, I simply use the exact same code as in the book to execute the respective models. Note that I have set the &lt;code&gt;ulam()&lt;/code&gt; argument &lt;code&gt;log_lik=TRUE&lt;/code&gt; for comparison with WAIC in the next step.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Chapter 11 Model
dat2 &amp;lt;- list(T = d$total_tools, P = d$population, cid = d$contact_id)
m11.11 &amp;lt;- ulam(
  alist(
    T ~ dpois(lambda),
    lambda &amp;lt;- exp(a[cid]) * P^b[cid] / g,
    a[cid] ~ dnorm(1, 1),
    b[cid] ~ dexp(1),
    g ~ dexp(1)
  ),
  data = dat2, chains = 4, cores = 4, log_lik = TRUE
)
## Chapter 14 Model
dat_list &amp;lt;- list(T = d$total_tools, P = d$population, society = d$society, Dmat = islandsDistMatrix)
m14.8 &amp;lt;- ulam(
  alist(
    T ~ dpois(lambda),
    lambda &amp;lt;- (a * P^b / g) * exp(k[society]),
    vector[10]:k ~ multi_normal(0, SIGMA),
    matrix[10, 10]:SIGMA &amp;lt;- cov_GPL2(Dmat, etasq, rhosq, 0.01), c(a, b, g) ~ dexp(1), etasq ~ dexp(2), rhosq ~ dexp(0.5)
  ),
  data = dat_list, chains = 4, cores = 4, iter = 2000, log_lik = TRUE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have both models at the ready, let&amp;rsquo;s do what the task asked of us and compare their out-of-sample accuracy predictions:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;compare(m11.11, m14.8)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            WAIC        SE    dWAIC      dSE    pWAIC      weight
## m14.8  67.87559  2.370998  0.00000       NA 4.196489 0.998336383
## m11.11 80.66978 11.103427 12.79419 10.95919 5.148678 0.001663617
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The more complex model taking into account spatial distances of societies - &lt;code&gt;m14.8&lt;/code&gt; - outperforms the previously held &amp;ldquo;best&amp;rdquo; model (&lt;code&gt;m11.11&lt;/code&gt;). We also see that the Gaussian process model has less effective parameters (&lt;code&gt;pWAIC&lt;/code&gt;) than the simpler model. This is a sign of intense regularisation on the part of the Gaussian Process model.&lt;/p&gt;
&lt;h3 id=&#34;practice-m5&#34;&gt;Practice M5&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Modify the phylogenetic distance example to use group size as the outcome and brain size as a predictor. Assuming brain size influences group size, what is your estimate of the effect? How does phylogeny influence the estimate?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; This is the example from the book, but simply just switching the positions of group size and brain size in the model specification. Coincidentally, this is the model shown in the 
&lt;a href=&#34;https://www.youtube.com/watch?v=pwMRbt2CbSU&amp;amp;list=PLDcUM9US4XdNM4Edgs7weiyIguLSToZRI&amp;amp;index=19&amp;amp;ab_channel=RichardMcElreath&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;YouTube Lecture series&lt;/a&gt; by Richard McElreath.&lt;/p&gt;
&lt;p&gt;For now, we start by loading the data as was done in the book and establish our first data list for subsequent modelling:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(Primates301)
d &amp;lt;- Primates301
d$name &amp;lt;- as.character(d$name)
dstan &amp;lt;- d[complete.cases(d$group_size, d$body, d$brain), ]
spp_obs &amp;lt;- dstan$name
dat_list &amp;lt;- list(
  N_spp = nrow(dstan),
  M = standardize(log(dstan$body)),
  B = standardize(log(dstan$brain)),
  G = standardize(log(dstan$group_size)),
  Imat = diag(nrow(dstan))
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this part of the observational data ready, I now turn to phylogenetic data which we can obtain and attach to our data list like so:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(Primates301_nex)
tree_trimmed &amp;lt;- keep.tip(Primates301_nex, spp_obs) # only keep tree that&#39;s relevant to our species
Rbm &amp;lt;- corBrownian(phy = tree_trimmed) # calculate expected covariance given a Brownian model
V &amp;lt;- vcv(Rbm) # compute expected variances and covariances
Dmat &amp;lt;- cophenetic(tree_trimmed) # cophenetic distance matrix
dat_list$V &amp;lt;- V[spp_obs, spp_obs] # covariances in speciesXspecies matrix
dat_list$R &amp;lt;- dat_list$V / max(V) # relative covariances of speciesXspecies matrix
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And we are ready to run our first model! Because they book went through multiple candidate models so do I.&lt;/p&gt;
&lt;p&gt;Here, I start off with the basic, ordinary regression:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_M5Ordi &amp;lt;- ulam(
  alist(
    G ~ multi_normal(mu, SIGMA),
    mu &amp;lt;- a + bM * M + bB * B,
    matrix[N_spp, N_spp]:SIGMA &amp;lt;- Imat * sigma_sq,
    a ~ normal(0, 1),
    c(bM, bB) ~ normal(0, 0.5),
    sigma_sq ~ exponential(1)
  ),
  data = dat_list, chains = 4, cores = 4
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, I run the Brownian motion model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_M5Brown &amp;lt;- ulam(
  alist(
    G ~ multi_normal(mu, SIGMA),
    mu &amp;lt;- a + bM * M + bB * B,
    matrix[N_spp, N_spp]:SIGMA &amp;lt;- R * sigma_sq,
    a ~ normal(0, 1),
    c(bM, bB) ~ normal(0, 0.5),
    sigma_sq ~ exponential(1)
  ),
  data = dat_list, chains = 4, cores = 4
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lastly, I execute a Gaussian Process model. To do so, we need to convert our phylogenetic distance matrix into a relative measure of distance among our species:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dat_list$Dmat &amp;lt;- Dmat[spp_obs, spp_obs] / max(Dmat)
m_M5GP &amp;lt;- ulam(
  alist(
    G ~ multi_normal(mu, SIGMA),
    mu &amp;lt;- a + bM * M + bB * B,
    matrix[N_spp, N_spp]:SIGMA &amp;lt;- cov_GPL1(Dmat, etasq, rhosq, 0.01),
    a ~ normal(0, 1),
    c(bM, bB) ~ normal(0, 0.5),
    etasq ~ half_normal(1, 0.25),
    rhosq ~ half_normal(3, 0.25)
  ),
  data = dat_list, chains = 4, cores = 4
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(coeftab(m_M5Ordi, m_M5Brown, m_M5GP), pars = c(&amp;quot;bM&amp;quot;, &amp;quot;bB&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-04-15-statistical-rethinking-chapter-14_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From the above, we clearly see that model which does not take into account our phylogeny - &lt;code&gt;m_M5Ordi&lt;/code&gt; - finds a clearly non-zero dependence of brain size on group size. However, both model which do include phylogenetic information - &lt;code&gt;m_M5Brown&lt;/code&gt; and &lt;code&gt;m_M5GP&lt;/code&gt; - do not show this relationship. Adding phylogenetic information seems to reduce the evidence for a causal link between brain size and group size.&lt;/p&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- post &lt;- extract.samples(m_M5GP) --&gt;
&lt;!-- plot(NULL, xlim=c(0,max(dat_list$Dmat)), ylim=c(0,1.5), --&gt;
&lt;!--     xlab=&#34;phylogenetic distance&#34;, ylab=&#34;covariance&#34;) --&gt;
&lt;!-- # posterior --&gt;
&lt;!-- for (i in 1:30) --&gt;
&lt;!--     curve(post$etasq[i]*exp(-post$rhosq[i]*x), add=TRUE, col=rangi2) --&gt;
&lt;!-- # prior mean and 89% interval --&gt;
&lt;!-- eta &lt;- abs(rnorm(1e3,1,0.25)) --&gt;
&lt;!-- rho &lt;- abs(rnorm(1e3,3,0.25)) --&gt;
&lt;!-- d_seq &lt;- seq(from=0,to=1,length.out=50) --&gt;
&lt;!-- K &lt;- sapply(d_seq, function(x) eta*exp(-rho*x)) --&gt;
&lt;!-- lines(d_seq, colMeans(K), lwd=2) --&gt;
&lt;!-- shade(apply(K,2,PI), d_seq) --&gt;
&lt;!-- text(0.5, 0.5, &#34;prior&#34;) --&gt;
&lt;!-- text(0.2, 0.1, &#34;posterior&#34;, col=rangi2) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- # OU model with different GP prior --&gt;
&lt;!-- m14M5.3 &lt;- ulam( --&gt;
&lt;!--     alist( --&gt;
&lt;!--         G ~ multi_normal(mu, SIGMA), --&gt;
&lt;!--         mu &lt;- a + bM*M + bB*B, --&gt;
&lt;!--         matrix[N_spp,N_spp]: SIGMA &lt;- cov_GPL1(Dmat, etasq, rhosq, 0.01), --&gt;
&lt;!--         a ~ normal(0,1), --&gt;
&lt;!--         c(bM,bB) ~ normal(0,0.5), --&gt;
&lt;!--         etasq ~ half_normal(0.25,0.25), --&gt;
&lt;!--         rhosq ~ half_normal(3,0.25) --&gt;
&lt;!--  ), data=dat_list, chains=4, cores=4) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- post &lt;- extract.samples(m14M5.3) --&gt;
&lt;!-- plot(NULL, xlim=c(0,max(dat_list$Dmat)), ylim=c(0,1.5), --&gt;
&lt;!--     xlab=&#34;phylogenetic distance&#34;, ylab=&#34;covariance&#34;) --&gt;
&lt;!-- # posterior --&gt;
&lt;!-- for (i in 1:30) --&gt;
&lt;!--     curve(post$etasq[i]*exp(-post$rhosq[i]*x), add=TRUE, col=rangi2) --&gt;
&lt;!-- # prior mean and 89% interval --&gt;
&lt;!-- eta &lt;- abs(rnorm(1e3,1,0.25)) --&gt;
&lt;!-- rho &lt;- abs(rnorm(1e3,3,0.25)) --&gt;
&lt;!-- d_seq &lt;- seq(from=0,to=1,length.out=50) --&gt;
&lt;!-- K &lt;- sapply(d_seq, function(x) eta*exp(-rho*x)) --&gt;
&lt;!-- lines(d_seq, colMeans(K), lwd=2) --&gt;
&lt;!-- shade(apply(K,2,PI), d_seq) --&gt;
&lt;!-- text(0.5, 0.5, &#34;prior&#34;) --&gt;
&lt;!-- text(0.2, 0.1, &#34;posterior&#34;, col=rangi2) --&gt;
&lt;!-- ``` --&gt;
&lt;h2 id=&#34;hard-exercises&#34;&gt;Hard Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-h1&#34;&gt;Practice H1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Let’s revisit the Bangladesh fertility data, &lt;code&gt;data(bangladesh)&lt;/code&gt;, from the practice problems for Chapter 13. Fit a model with both varying intercepts by &lt;code&gt;district_id&lt;/code&gt; and varying slopes of urban by &lt;code&gt;district_id&lt;/code&gt;. You are still predicting &lt;code&gt;use.contraception&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Inspect the correlation between the intercepts and slopes. Can you interpret this correlation, in terms of what it tells you about the pattern of contraceptive use in the sample? It might help to plot the mean (or median) varying effect estimates for both the intercepts and slopes, by district. Then you can visualize the correlation and maybe more easily think through what it means to have a particular correlation. Plotting predicted proportion of women using contraception, with urban women on one axis and rural on the other, might also help.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Once more, I start by loading the data and preparing it as was done in a previous chapter:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(bangladesh)
d &amp;lt;- bangladesh
dat_list &amp;lt;- list(
  C = d$use.contraception,
  did = as.integer(as.factor(d$district)),
  urban = d$urban
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I can run my model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_H1 &amp;lt;- ulam(
  alist(
    C ~ bernoulli(p),
    logit(p) &amp;lt;- a[did] + b[did] * urban,
    c(a, b)[did] ~ multi_normal(c(abar, bbar), Rho, Sigma),
    abar ~ normal(0, 1),
    bbar ~ normal(0, 0.5),
    Rho ~ lkj_corr(2),
    Sigma ~ exponential(1)
  ),
  data = dat_list, chains = 4, cores = 4, iter = 4000
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now look at the posterior estimates of average effects:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m_H1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            mean        sd       5.5%      94.5%    n_eff    Rhat4
## abar -0.6842219 0.1003302 -0.8475050 -0.5301731 6158.562 1.000395
## bbar  0.6369826 0.1590421  0.3866908  0.8900233 4474.285 1.000145
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unsurprisingly, I find a positive effect for &lt;code&gt;bbar&lt;/code&gt; which indicates that contraception is used more frequently in urban areas.&lt;/p&gt;
&lt;p&gt;Looking deeper into the posterior estimates:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m_H1, depth = 3, pars = c(&amp;quot;Rho&amp;quot;, &amp;quot;Sigma&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                mean           sd       5.5%      94.5%     n_eff     Rhat4
## Rho[1,1]  1.0000000 0.000000e+00  1.0000000  1.0000000       NaN       NaN
## Rho[1,2] -0.6512997 1.696069e-01 -0.8680436 -0.3443487 1431.4457 1.0003693
## Rho[2,1] -0.6512997 1.696069e-01 -0.8680436 -0.3443487 1431.4457 1.0003693
## Rho[2,2]  1.0000000 5.972661e-17  1.0000000  1.0000000 7485.2487 0.9994999
## Sigma[1]  0.5762512 9.710360e-02  0.4308600  0.7404430 1990.0617 1.0021542
## Sigma[2]  0.7731623 1.991282e-01  0.4617857  1.0991674  967.7497 1.0034399
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;shows a negative correlation between the intercepts and slopes (&lt;code&gt;Rho[1,2]&lt;/code&gt; or &lt;code&gt;Rho[2,1]&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s plot this relationship between the varying effects to get a better understanding of what is happening:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;post &amp;lt;- extract.samples(m_H1)
a &amp;lt;- apply(post$a, 2, mean)
b &amp;lt;- apply(post$b, 2, mean)
plot(a, b, xlab = &amp;quot;a (intercept)&amp;quot;, ylab = &amp;quot;b (urban slope)&amp;quot;)
abline(h = 0, lty = 2)
abline(v = 0, lty = 2)
R &amp;lt;- apply(post$Rho, 2:3, mean)
s &amp;lt;- apply(post$Sigma, 2, mean)
S &amp;lt;- diag(s) %*% R %*% diag(s)
ll &amp;lt;- c(0.5, 0.67, 0.89, 0.97)
for (l in ll) {
  el &amp;lt;- ellipse(S, centre = c(mean(post$abar), mean(post$bbar)), level = l)
  lines(el, col = &amp;quot;black&amp;quot;, lwd = 0.5)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-04-15-statistical-rethinking-chapter-14_files/figure-html/unnamed-chunk-27-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So districts with higher use of contraception outside of urban areas come with smaller slopes. Basically, what this means is that districts which boast a high use of contraception outside of urban areas do not have a marked shift in use of contraceptives when moving to urban areas.&lt;/p&gt;
&lt;p&gt;We can also show this in probability scale by applying inverse logit transformation to our estimates:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;u0 &amp;lt;- inv_logit(a)
u1 &amp;lt;- inv_logit(a + b)
plot(u0, u1, xlim = c(0, 1), ylim = c(0, 1), xlab = &amp;quot;urban = 0&amp;quot;, ylab = &amp;quot;urban = 1&amp;quot;)
abline(h = 0.5, lty = 2)
abline(v = 0.5, lty = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-04-15-statistical-rethinking-chapter-14_files/figure-html/unnamed-chunk-28-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;practice-h2&#34;&gt;Practice H2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Varying effects models are useful for modelling time series, as well as spatial clustering. In a time series, the observations cluster by entities that have continuity through time, such as individuals. Since observations within individuals are likely highly correlated, the multilevel structure can help quite a lot. You’ll use the data in &lt;code&gt;data(Oxboys)&lt;/code&gt;, which is 234 height measurements on 26 boys from an Oxford Boys Club (I think these were like youth athletic leagues?), at 9 different ages (centred and standardized) per boy.&lt;/p&gt;
&lt;p&gt;You’ll be interested in predicting &lt;code&gt;height&lt;/code&gt;, using &lt;code&gt;age&lt;/code&gt;, clustered by &lt;code&gt;Subject&lt;/code&gt; (individual boy). Fit a model with varying intercepts and slopes (on &lt;code&gt;age&lt;/code&gt;), clustered by &lt;code&gt;Subject&lt;/code&gt;. Present and interpret the parameter estimates. Which varying effect contributes more variation to the heights, the intercept or the slope?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; I start with loading the data, standardising the age data, and making the subject IDs into an index:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(Oxboys)
d &amp;lt;- Oxboys
d$A &amp;lt;- standardize(d$age)
d$id &amp;lt;- coerce_index(d$Subject)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Armed with my data, I can now turn to modelling:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_H2 &amp;lt;- ulam(
  alist(
    height ~ dnorm(mu, sigma),
    mu &amp;lt;- a_bar + a[id] + (b_bar + b[id]) * A,
    a_bar ~ dnorm(150, 10),
    b_bar ~ dnorm(0, 10),
    c(a, b)[id] ~ multi_normal(0, Rho_id, sigma_id),
    sigma_id ~ dexp(1),
    Rho_id ~ dlkjcorr(2),
    sigma ~ dexp(1)
  ),
  data = d, chains = 4, cores = 4, iter = 4000
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model has compiled and I am interested in the output it produced concerning average effects and variation:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m_H2, depth = 2, pars = c(&amp;quot;a_bar&amp;quot;, &amp;quot;b_bar&amp;quot;, &amp;quot;sigma_id&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                   mean        sd        5.5%      94.5%     n_eff    Rhat4
## a_bar       149.523993 1.3844230 147.2887666 151.690432  293.0144 1.011856
## b_bar         4.230315 0.2073667   3.8999943   4.554123  414.4092 1.003962
## sigma_id[1]   7.331832 0.8789976   6.0675622   8.840290 4173.7730 1.001038
## sigma_id[2]   1.065676 0.1525730   0.8508899   1.336301 4141.6398 1.000750
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since age is standardised, &lt;code&gt;a_bar&lt;/code&gt; represent the average height at average age in the data set. The average slope &lt;code&gt;b_bar&lt;/code&gt; represents change in height for a one-unit change in standardised age.&lt;/p&gt;
&lt;p&gt;I don&amp;rsquo;t like interpreting standardised data coefficients like that. Let&amp;rsquo;s rather plot it:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(height ~ age, type = &amp;quot;n&amp;quot;, data = d)
for (i in 1:26) {
  h &amp;lt;- d$height[d$Subject == i]
  a &amp;lt;- d$age[d$Subject == i]
  lines(a, h, col = col.alpha(&amp;quot;slateblue&amp;quot;, 0.5), lwd = 2)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-04-15-statistical-rethinking-chapter-14_files/figure-html/unnamed-chunk-32-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now the the task at hand. Which effect contributes more to the overall heights of our individuals? Given the plot above, I&amp;rsquo;d argue that it is the varying intercepts which provide us with most of the variation in heights. However, this is very much down to the data and should not be generalised beyond this data set. It might completely fall apart if we had longer time-series of data values.&lt;/p&gt;
&lt;h3 id=&#34;practice-h3&#34;&gt;Practice H3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Now consider the correlation between the varying intercepts and slopes. Can you explain its value? How would this estimated correlation influence your predictions about a new sample of boys?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; For this, we look at the correlation matrix &lt;code&gt;Rho_id&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m_H2, depth = 3, pars = &amp;quot;Rho_id&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                  mean           sd      5.5%     94.5%    n_eff     Rhat4
## Rho_id[1,1] 1.0000000 0.000000e+00 1.0000000 1.0000000      NaN       NaN
## Rho_id[1,2] 0.5307351 1.283373e-01 0.3046972 0.7170573 4627.842 1.0007918
## Rho_id[2,1] 0.5307351 1.283373e-01 0.3046972 0.7170573 4627.842 1.0007918
## Rho_id[2,2] 1.0000000 7.886203e-17 1.0000000 1.0000000 7829.277 0.9994999
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So there is a positive correlation. Let&amp;rsquo;s visualise that:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot() +
  stat_halfeye(aes(x = extract.samples(m_H2)$Rho_id[, 1, 2])) +
  theme_bw() +
  labs(x = &amp;quot;Rho&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-04-15-statistical-rethinking-chapter-14_files/figure-html/unnamed-chunk-34-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The positive correlation implies that larger intercepts are associated with steeper slopes. What this means is that taller boys grow faster.&lt;/p&gt;
&lt;h3 id=&#34;practice-h4&#34;&gt;Practice H4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Use &lt;code&gt;mvrnorm&lt;/code&gt; (in &lt;code&gt;library(MASS)&lt;/code&gt;) or &lt;code&gt;rmvnorm&lt;/code&gt; (in &lt;code&gt;library(mvtnorm)&lt;/code&gt;) to simulate a new sample of boys, based upon the posterior mean values of the parameters.&lt;/p&gt;
&lt;p&gt;That is, try to simulate varying intercepts and slopes, using the relevant parameter estimates, and then plot the predicted trends of height on age, one trend for each simulated boy you produce. A sample of 10 simulated boys is plenty, to illustrate the lesson. You can ignore uncertainty in the posterior, just to make the problem a little easier. But if you want to include the uncertainty about the parameters, go for it.&lt;/p&gt;
&lt;p&gt;Note that you can construct an arbitrary variance-covariance matrix to pass to either &lt;code&gt;mvrnorm&lt;/code&gt; or &lt;code&gt;rmvnorm&lt;/code&gt; with something like:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;S &amp;lt;- matrix(c(sa^2, sa * sb * rho, sa * sb * rho, sb^2), nrow = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where &lt;code&gt;sa&lt;/code&gt; is the standard deviation of the first variable, &lt;code&gt;sb&lt;/code&gt; is the standard deviation of the second variable, and &lt;code&gt;rho&lt;/code&gt; is the correlation between them.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; To simulate new observations we need to obtain the estimates of our model so far:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;post &amp;lt;- extract.samples(m_H2)
rho &amp;lt;- mean(post$Rho_id[, 1, 2])
sb &amp;lt;- mean(post$sigma_id[, 2])
sa &amp;lt;- mean(post$sigma_id[, 1])
sigma &amp;lt;- mean(post$sigma)
a &amp;lt;- mean(post$a_bar)
b &amp;lt;- mean(post$b_bar)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can define the variance-covariance matrix:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;S &amp;lt;- matrix(c(sa^2, sa * sb * rho, sa * sb * rho, sb^2), nrow = 2)
round(S, 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       [,1] [,2]
## [1,] 53.76 4.15
## [2,]  4.15 1.14
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Subsequently, we can sample from the multivariate normal distribution given our variance-covariance matrix to obtain a bivariate distribution of intercepts and slopes:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ve &amp;lt;- mvrnorm(10, c(0, 0), Sigma = S)
ve
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             [,1]       [,2]
##  [1,] -0.2914409 -0.8985609
##  [2,]  8.8137562  0.3436168
##  [3,] -1.5233131  1.5530926
##  [4,] -9.5179212 -0.6967038
##  [5,]  7.6547083 -0.3622056
##  [6,]  5.4710535 -0.3060008
##  [7,] -0.3548003  0.1445644
##  [8,]  7.2706590  3.0081540
##  [9,]  2.8143313  0.1653603
## [10,] -6.3582595 -1.0162374
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These are individual intercepts and slopes of 10 random boys have which we only need to add to the average intercept and slope values to generate predicted heights for them. Here, we simulate the trend for each boy and add it to a plot:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;age.seq &amp;lt;- seq(from = -1, to = 1, length.out = 9)
plot(0, 0, type = &amp;quot;n&amp;quot;, xlim = range(d$age), ylim = range(d$height), xlab = &amp;quot;age (centered)&amp;quot;, ylab = &amp;quot;height&amp;quot;)
for (i in 1:nrow(ve)) {
  h &amp;lt;- rnorm(9,
    mean = a + ve[i, 1] + (b + ve[i, 2]) * age.seq,
    sd = sigma
  )
  lines(age.seq, h, col = col.alpha(&amp;quot;slateblue&amp;quot;, 0.5))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-04-15-statistical-rethinking-chapter-14_files/figure-html/unnamed-chunk-39-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.5 (2021-03-31)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19043)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.1252  LC_CTYPE=English_United Kingdom.1252    LC_MONETARY=English_United Kingdom.1252 LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.1252    
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] tidybayes_2.3.1      ape_5.5              ellipse_0.4.2        MASS_7.3-53.1        rethinking_2.13      rstan_2.21.2         ggplot2_3.3.6        StanHeaders_2.21.0-7
## 
## loaded via a namespace (and not attached):
##  [1] sass_0.3.1           tidyr_1.1.3          jsonlite_1.7.2       R.utils_2.10.1       bslib_0.2.4          RcppParallel_5.1.2   assertthat_0.2.1     distributional_0.2.2 highr_0.9           
## [10] stats4_4.0.5         ggdist_2.4.0         yaml_2.2.1           pillar_1.6.0         backports_1.2.1      lattice_0.20-41      glue_1.4.2           arrayhelpers_1.1-0   digest_0.6.27       
## [19] colorspace_2.0-0     htmltools_0.5.1.1    R.oo_1.24.0          plyr_1.8.6           pkgconfig_2.0.3      svUnit_1.0.6         bookdown_0.22        purrr_0.3.4          mvtnorm_1.1-1       
## [28] scales_1.1.1         processx_3.5.1       tibble_3.1.1         styler_1.4.1         generics_0.1.0       farver_2.1.0         ellipsis_0.3.2       withr_2.4.2          cli_3.0.0           
## [37] magrittr_2.0.1       crayon_1.4.1         evaluate_0.14        ps_1.6.0             R.methodsS3_1.8.1    fansi_0.4.2          R.cache_0.14.0       nlme_3.1-152         forcats_0.5.1       
## [46] pkgbuild_1.2.0       blogdown_1.3         tools_4.0.5          loo_2.4.1            prettyunits_1.1.1    lifecycle_1.0.0      matrixStats_0.61.0   stringr_1.4.0        V8_3.4.1            
## [55] munsell_0.5.0        callr_3.7.0          compiler_4.0.5       jquerylib_0.1.4      rlang_0.4.11         grid_4.0.5           labeling_0.4.2       rmarkdown_2.7        gtable_0.3.0        
## [64] codetools_0.2-18     inline_0.3.17        DBI_1.1.1            curl_4.3.2           rematch2_2.1.2       R6_2.5.0             gridExtra_2.3        knitr_1.33           dplyr_1.0.5         
## [73] utf8_1.2.1           shape_1.4.5          stringi_1.5.3        Rcpp_1.0.7           vctrs_0.3.7          tidyselect_1.1.0     xfun_0.22            coda_0.19-4
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 15</title>
      <link>https://www.erikkusch.com/courses/rethinking/chapter-15/</link>
      <pubDate>Thu, 06 May 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/rethinking/chapter-15/</guid>
      <description>&lt;h1 id=&#34;missing-data-and-other-opportunities&#34;&gt;Missing Data and Other Opportunities&lt;/h1&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/rethinking/18__07-05-2021_SUMMARY_-Measurement-Error-and-Missing-Data.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides Chapter 15&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to the exercises at the end of chapter 15 in 
&lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Satistical Rethinking 2&lt;/a&gt; by Richard McElreath. I have created these notes as a part of my ongoing involvement in the 
&lt;a href=&#34;https://www.erikkusch.com/project/aubayes/&#34;&gt;AU Bayes Study Group&lt;/a&gt;. Much of my inspiration for these solutions, where necessary, has been obtained from&lt;/p&gt;
&lt;!-- [Taras Svirskyi](https://github.com/jffist/statistical-rethinking-solutions/blob/master/ch10_hw.R), [William Wolf](https://github.com/cavaunpeu/statistical-rethinking/blob/master/chapter-10/homework.R), and [Corrie Bartelheimer](https://www.samples-of-thoughts.com/projects/statistical-rethinking/chapter_10/chp10-ex/) as well as  --&gt;
&lt;p&gt;the solutions provided to instructors by Richard McElreath himself.&lt;/p&gt;
&lt;h2 id=&#34;r-environment&#34;&gt;&lt;code&gt;R&lt;/code&gt; Environment&lt;/h2&gt;
&lt;p&gt;For today&amp;rsquo;s exercise, I load the following packages:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rethinking)
library(gtools)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;easy-exercises&#34;&gt;Easy Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-e1&#34;&gt;Practice E1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Rewrite the Oceanic tools model (from Chapter 11) below so that it assumes measured error on the log population sizes of each society. You don’t need to fit the model to data. Just modify the mathematical formula below.
$$T_i ∼ Poisson(µ_i)$$
$$log(µ_i) = α + β*log(P_i)$$
$$α ∼ Normal(0, 1.5)$$ 
$$β ∼ Normal(0, 1)$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; The population variable ($P_i$) is a predictor in this model. In order to estimate/account for measurement error in a predictor variable, all we need to do is add a distribution to the observed values ($P^\star_i$) with a given error ($σ_P$):&lt;/p&gt;
&lt;p&gt;$$log(P_i) ∼ Normal(P^\star_i, σ_P)$$&lt;/p&gt;
&lt;p&gt;The final model specification combines the above line with the previous model specification and substitutes $P^\star_i$ in place of $P_i$:&lt;/p&gt;
&lt;p&gt;$$T_i ∼ Poisson(µ_i)$$&lt;/p&gt;
&lt;p&gt;$$log(µ_i) = α + β* P^\star_i $$&lt;/p&gt;
&lt;p&gt;$$log(P_i) ∼ Normal(P^\star_i, σ_P)$$&lt;/p&gt;
&lt;p&gt;$$α ∼ Normal(0, 1.5)$$&lt;/p&gt;
&lt;p&gt;$$β ∼ Normal(0, 1)$$&lt;/p&gt;
&lt;p&gt;$$σ_P \sim Exponential(1)$$&lt;/p&gt;
&lt;p&gt;Of course, we also need a prior for $σ_P$. I don&amp;rsquo;t know enough about the data to take a good educated guess for this parameter and so I just run the usual prior for standard deviations used in the book.&lt;/p&gt;
&lt;h3 id=&#34;practice-e2&#34;&gt;Practice E2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Rewrite the same model so that it allows imputation of missing values for log population. There aren’t any missing values in the variable, but you can still write down a model formula that would imply imputation, if any values were missing.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Imputation comes into play when measurement error is so intense that we have missing data - &amp;ldquo;missing data is grown-up measurement error&amp;rdquo;. The trick with missing data is to establish adaptive priors for the missing data which is informed by the observations for which we do have data:&lt;/p&gt;
&lt;p&gt;$$T_i ∼ Poisson(µ_i)$$&lt;/p&gt;
&lt;p&gt;$$log(µ_i) = α + β * P^\star_i$$&lt;/p&gt;
&lt;p&gt;$$P^\star_i ∼ Normal(\overline{ P^\star }, σ_P)$$&lt;/p&gt;
&lt;p&gt;$$α ∼ Normal(0, 1.5)$$ 
$$β ∼ Normal(0, 1)$$
$$P^\star \sim Normal(0, 1)$$
$$σ_P \sim Exponential(1)$$&lt;/p&gt;
&lt;p&gt;With the new specification, values of $P^\star_i$ (observed log-populations) are either assumed to be data or parameters according to whether data is present for observation $i$ or not.&lt;/p&gt;
&lt;h2 id=&#34;medium-exercises&#34;&gt;Medium Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-m1&#34;&gt;Practice M1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Using the mathematical form of the imputation model in the chapter, explain what is being assumed about how the missing values were generated.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; As a reminder, the mathematical form of the imputation model in the chapter is as follows:&lt;/p&gt;
&lt;p&gt;$$K_i ∼ Normal(µ_i, σ)$$
$$µ_i = α + β_BB_i + β_M * log(M_i)$$
$$B_i ∼ Normal(ν, σ_B)$$
$$α ∼ Normal(0, 0.5)$$ 
$$β_B ∼ Normal(0, 0.5)$$
$$β_M ∼ Normal(0, 0.5)$$ 
$$σ ∼ Exponential(1)$$ 
$$ν ∼ Normal(0.5, 1)$$ 
$$σ_B ∼ Exponential(1)$$&lt;/p&gt;
&lt;p&gt;The assumption about which distribution our predictor with missing data ($B$) does not contain any information about individual cases. It simply just assumes that missing values are randomly placed across the cases. As such, the model assumes that there is no causation at play for how the data came to be missing/not reported, but only states that information that is missing follows a certain distribution which is the same distribution against which to test the data which we do have.&lt;/p&gt;
&lt;h3 id=&#34;practice-m2&#34;&gt;Practice M2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  In earlier chapters, we threw away cases from the primate milk data, so we could use the neocortex variable. Now repeat the WAIC model comparison example from Chapter 6, but use imputation on the neocortex variable so that you can include all of the cases in the original data. The simplest form of imputation is acceptable. How are the model comparison results affected by being able to include all of the cases?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Unfortunately, chapter 6 does not include a neocortex model in the version of the book I am working with and pulling these exercises from. However, chapter 5 does. To begin with this exercise, I load the data and prepare it the same way we did back in chapter 5, by standardising our variables for energy content of milk (&lt;code&gt;K&lt;/code&gt;), and body mass (&lt;code&gt;M&lt;/code&gt;). Contrary to chapter 5, I do not standardise the neocortex portion (&lt;code&gt;P&lt;/code&gt;), but leave it as a proportion between 0 and 1:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(milk)
d &amp;lt;- milk
d$neocortex.prop &amp;lt;- d$neocortex.perc / 100
d$logmass &amp;lt;- log(d$mass)
## Incomplete cases allowed
dat_list &amp;lt;- list(
  K = standardize(d$kcal.per.g),
  P = d$neocortex.prop,
  M = standardize(d$logmass)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why did I set the neocortex variable (&lt;code&gt;P&lt;/code&gt;) to be non-standardised? So I could use priors more readily and make sure this proportion always stays between 0 and 1 - everything outside these bounds would be biological nonsense.&lt;/p&gt;
&lt;p&gt;With the data ready, we can now run our three models from chapter 5, but this time, in a way so as to account for missing data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Mass effect (not the video game franchise); no imputation needed here
m_M2_5.6 &amp;lt;- ulam(
  alist(
    K ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bM * M,
    a ~ dnorm(0, 0.2),
    bM ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = dat_list[-2], chains = 4, cores = 4, iter = 2000, log_lik = TRUE
)

## Neocortex effect
m_M2_5.5 &amp;lt;- ulam(
  alist(
    K ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bP * (P - 0.67), # 0.67 is the average value of P --&amp;gt; Intercept now represents K at average P
    P ~ dbeta2(nu, theta), # bound between 0 and 1, but wide
    nu ~ dbeta(2, 2), # bound between 0 and 1
    a ~ dnorm(0, 0.2), # same as before
    bP ~ dnorm(0, 10), # another wide prior, since there is little variation in values of P
    theta ~ dexp(1), # standard stdev prior
    sigma ~ dexp(1), # same as before
    vector[12]:P_impute ~ uniform(0, 1) # there are 12 NA-values for P, we bound them between 0 and 1
  ),
  data = dat_list, chains = 4, cores = 4, iter = 2000, log_lik = TRUE
)

## Both predictors
m_M2_5.7 &amp;lt;- ulam(
  alist(
    K ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bP * (P - 0.67) + bM * M, # 0.67 is the average value of P --&amp;gt; Intercept now represents K at average P
    P ~ dbeta2(nu, theta), # bound between 0 and 1, but wide
    nu ~ dbeta(2, 2), # bound between 0 and 1
    a ~ dnorm(0, 0.2), # same as before
    bM ~ dnorm(0, 0.5), # same as before
    bP ~ dnorm(0, 10), # another wide prior, since there is little variation in values of P
    theta ~ dexp(1), # standard stdev prior
    sigma ~ dexp(1), # same as before
    vector[12]:P_impute ~ uniform(0, 1) # there are 12 NA-values for P, we bound them between 0 and 1
  ),
  data = dat_list, chains = 4, cores = 4, iter = 2000, log_lik = TRUE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All three models are compiled. Time to compare how they perform:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;compare(m_M2_5.5, m_M2_5.6, m_M2_5.7)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              WAIC       SE    dWAIC      dSE    pWAIC     weight
## m_M2_5.7 79.50850 5.831506 0.000000       NA 4.565835 0.74491310
## m_M2_5.6 82.17410 5.870788 2.665598 1.455532 1.678415 0.19646189
## m_M2_5.5 84.59271 5.291591 5.084212 3.492726 2.344498 0.05862501
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unsurprisingly, the full model outperforms both one-effect models here. Interestingly, the mass-only model still pulls ahead of the (now imputation-driven) neocortex-only model.&lt;/p&gt;
&lt;p&gt;Visualising what our full imputation model sees, we obtain:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;post &amp;lt;- extract.samples(m_M2_5.7)
P_impute_mu &amp;lt;- apply(post$P_impute, 2, mean)
P_impute_ci &amp;lt;- apply(post$P_impute, 2, PI)
par(mfrow = c(1, 2))
# P vs K
plot(dat_list$P,
  dat_list$K,
  pch = 16, col = rangi2,
  xlab = &amp;quot;neocortex percent&amp;quot;, ylab = &amp;quot;kcal milk (std)&amp;quot;, xlim = c(0, 1)
)
miss_idx &amp;lt;- which(is.na(dat_list$P))
Ki &amp;lt;- dat_list$K[miss_idx]
points(P_impute_mu, Ki)
for (i in 1:12) lines(P_impute_ci[, i], rep(Ki[i], 2))
# M vs B
plot(dat_list$M, dat_list$P, pch = 16, col = rangi2, ylab = &amp;quot;neocortex percent (std)&amp;quot;, xlab = &amp;quot;log body mass (std)&amp;quot;, ylim = c(0, 1))
Mi &amp;lt;- dat_list$M[miss_idx]
points(Mi, P_impute_mu)
for (i in 1:12) lines(rep(Mi[i], 2), P_impute_ci[, i])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-07-statistical-rethinking-chapter-15_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;These are the same plots as in the book in chapter 15. The only difference is that our imputed neocortex percent values now fall into clearly readable (and sensible) ranges between 0 and 1.&lt;/p&gt;
&lt;h3 id=&#34;practice-m3&#34;&gt;Practice M3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Repeat the divorce data measurement error models, but this time double the standard errors. Can you explain how doubling the standard errors impacts inference?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Again, I prepare the data the same way as the book does it:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(WaffleDivorce)
d &amp;lt;- WaffleDivorce
dlist &amp;lt;- list(
  D_obs = standardize(d$Divorce),
  D_sd = d$Divorce.SE / sd(d$Divorce),
  M = standardize(d$Marriage),
  A = standardize(d$MedianAgeMarriage),
  N = nrow(d)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, I simply take the model from the book and run it:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m15.1 &amp;lt;- ulam(
  alist(
    D_obs ~ dnorm(D_true, D_sd),
    vector[N]:D_true ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bA * A + bM * M,
    a ~ dnorm(0, 0.2),
    bA ~ dnorm(0, 0.5),
    bM ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = dlist, chains = 4, cores = 4
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have our baseline model, it is time to double the standard error variable &lt;code&gt;D_sd&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_M3 &amp;lt;- ulam(
  alist(
    D_obs ~ dnorm(D_true, D_sd * 2.0),
    vector[N]:D_true ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bA * A + bM * M,
    a ~ dnorm(0, 0.2),
    bA ~ dnorm(0, 0.5),
    bM ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = dlist, chains = 4, cores = 4, iter = 4000
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s compare the two models for now and see what is happening:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m15.1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              mean         sd       5.5%      94.5%     n_eff    Rhat4
## a     -0.05218494 0.09615369 -0.2026636  0.1001795 1835.1415 1.001424
## bA    -0.61413500 0.16450611 -0.8828791 -0.3543928  916.8593 1.002451
## bM     0.05837404 0.16475940 -0.2017307  0.3267334  961.6816 1.002555
## sigma  0.58800945 0.10284935  0.4284475  0.7570348  784.5056 1.000690
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m_M3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean        sd        5.5%       94.5%     n_eff    Rhat4
## a     -0.1177004 0.1007595 -0.27064731  0.04738969 401.47170 1.008286
## bA    -0.6323619 0.1522011 -0.88505387 -0.38786285 510.81804 1.015609
## bM     0.2072101 0.1808927 -0.08675262  0.47906692 420.57867 1.022198
## sigma  0.1541612 0.1131281  0.02439099  0.36471832  97.19502 1.065839
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Oof. Without going into any detail on the parameter estimates, I have to point out that I don&amp;rsquo;t like the effective sample sizes (&lt;code&gt;n_eff&lt;/code&gt;) on our new model one bit. They are much, MUCH smaller than those of our baseline model. This highlights that out second model struggled with efficient exploration of posterior parameter space. I reckon this is a result of the increased standard deviation making the posterior landscape less easy to identify.&lt;/p&gt;
&lt;p&gt;One way to work around this issue is to rewrite the model in a non-centred parametrisation:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_M3B &amp;lt;- ulam(
  alist(
    D_obs ~ dnorm(mu + z_true * sigma, D_sd * 2.0),
    vector[N]:z_true ~ dnorm(0, 1), # gotten rid of the prior dependency here
    mu &amp;lt;- a + bA * A + bM * M,
    a ~ dnorm(0, 0.2),
    bA ~ dnorm(0, 0.5),
    bM ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = dlist, chains = 4, cores = 4, iter = 4000,
  control = list(max_treedepth = 14)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now, let&amp;rsquo;s compare these again:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m15.1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              mean         sd       5.5%      94.5%     n_eff    Rhat4
## a     -0.05218494 0.09615369 -0.2026636  0.1001795 1835.1415 1.001424
## bA    -0.61413500 0.16450611 -0.8828791 -0.3543928  916.8593 1.002451
## bM     0.05837404 0.16475940 -0.2017307  0.3267334  961.6816 1.002555
## sigma  0.58800945 0.10284935  0.4284475  0.7570348  784.5056 1.000690
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m_M3B)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean         sd       5.5%       94.5%     n_eff     Rhat4
## a     -0.1185044 0.09953924 -0.2778272  0.03741466 10901.282 0.9996237
## bA    -0.6444853 0.16468812 -0.9085568 -0.37653366  7171.900 0.9996660
## bM     0.1948245 0.19042617 -0.1056664  0.50037111  8322.124 1.0000969
## sigma  0.1431878 0.10844390  0.0117172  0.34538107  4706.223 0.9998725
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Nice. That got rid off our issues of non-effective sampling of posteriors. Now we can actually compare the model results. The biggest difference between these two models is found in the estimates for &lt;code&gt;bM&lt;/code&gt; (the effect of marriage rate on divorce rate) and &lt;code&gt;sigma&lt;/code&gt; (the standard deviation of the normal distribution from which the divorce rates are pulled). By increasing the standard error, we have effectively allowed individual states to exert much greater influence on the regression slope estimates thus shifting the result around.&lt;/p&gt;
&lt;p&gt;It is also worth pointing out right now that the non-centred model performs much more effective sampling, but the parameter estimates are ultimately the same irrespective of parametrisation in this example.&lt;/p&gt;
&lt;h2 id=&#34;hard-exercises&#34;&gt;Hard Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-h1&#34;&gt;Practice H1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  The data in &lt;code&gt;data(elephants)&lt;/code&gt; are counts of matings observed for bull elephants of differing ages. There is a strong positive relationship between age and matings. However, age is not always assessed accurately. First, fit a Poisson model predicting &lt;code&gt;MATINGS&lt;/code&gt; with &lt;code&gt;AGE&lt;/code&gt; as a predictor. Second, assume that the observed &lt;code&gt;AGE&lt;/code&gt; values are uncertain and have a standard error of $\pm$ 5 years. Re-estimate the relationship between &lt;code&gt;MATINGS&lt;/code&gt; and &lt;code&gt;AGE&lt;/code&gt;, incorporating this measurement error. Compare the inferences of the two models.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; First, I load the data and take a glance at its contents:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(elephants)
d &amp;lt;- elephants
str(d)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &#39;data.frame&#39;:	41 obs. of  2 variables:
##  $ AGE    : int  27 28 28 28 28 29 29 29 29 29 ...
##  $ MATINGS: int  0 1 1 1 3 0 0 0 2 2 ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can run some models. Before we get started, it is worth pointing out that there are a multitude of ways in which age could influence number of matings - exponential, logarithmic, poisson, etc. Here, I run with a poisson-approach. If this were a real-world research problem, I should probably test all three variations of the model. Alas, ain&amp;rsquo;t nobody got time fo&#39; that in an exercise.&lt;/p&gt;
&lt;p&gt;The data starts with &lt;code&gt;AGE&lt;/code&gt; values at 27. This suggests to me that this must be roughly around when elephants reach sexual maturity and will start to mate. Hence, I subtract 25 from all &lt;code&gt;AGE&lt;/code&gt; values in my model - just to be safe and interpret the number of matings as &amp;ldquo;number of matings since reaching sexual maturity&amp;rdquo;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Basic Model without uncertainty:
m_H1_A &amp;lt;- ulam(
  alist(
    MATINGS ~ dpois(lambda),
    lambda &amp;lt;- exp(a) * (AGE - 25)^bA,
    a ~ dnorm(0, 1),
    bA ~ dnorm(0, 1)
  ),
  data = d, chains = 4, cores = 4
)
precis(m_H1_A)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          mean        sd       5.5%      94.5%    n_eff     Rhat4
## a  -0.6922235 0.3429011 -1.2307922 -0.1347450 388.4313 0.9986860
## bA  0.7191531 0.1353220  0.5000447  0.9213017 379.1290 0.9988198
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, another not-so-efficient sampling model. How does it see the relationship between &lt;code&gt;AGE&lt;/code&gt; and &lt;code&gt;MATINGS&lt;/code&gt;?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# ages in the data range from 27 to 53
A_seq &amp;lt;- seq(from = 25, to = 55, length.out = 30)
lambda &amp;lt;- link(m_H1_A, data = list(AGE = A_seq))
lambda_mu &amp;lt;- apply(lambda, 2, mean)
lambda_PI &amp;lt;- apply(lambda, 2, PI)
plot(d$AGE, d$MATINGS,
  pch = 16, col = rangi2,
  xlab = &amp;quot;age&amp;quot;, ylab = &amp;quot;matings&amp;quot;
)
lines(A_seq, lambda_mu)
shade(lambda_PI, A_seq)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-07-statistical-rethinking-chapter-15_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s a pretty reliably positive relationship. Older elephants mate more.&lt;/p&gt;
&lt;p&gt;On to the measurement error model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;d$AGE0 &amp;lt;- d$AGE - 25 # add the sexual maturity consideration to the data
m_H1_B &amp;lt;- ulam(
  alist(
    MATINGS ~ dpois(lambda), # same outcome as before
    lambda &amp;lt;- exp(a) * AGE_est[i]^bA, # log-scale predictors
    AGE0 ~ dnorm(AGE_est, 5), # Gaussian distribution with error 5
    vector[41]:AGE_est ~ dunif(0, 50), # prior for individual observed ages
    a ~ dnorm(0, 1),
    bA ~ dnorm(0, 1)
  ),
  data = d, chains = 4, cores = 4
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m_H1_B)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          mean        sd       5.5%       94.5%    n_eff    Rhat4
## a  -0.7742834 0.4801451 -1.6114596 -0.04176099 1049.624 1.003248
## bA  0.7360628 0.1803691  0.4590526  1.05008454 1060.533 1.003109
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Interestingly enough, the estimate of &lt;code&gt;bA&lt;/code&gt; has not changed between these models. Why? Because we added completely symmetric measurement error that remains unchanged across all ages of our elephants. Hence, we don&amp;rsquo;t end up biasing our model because the error in the data is not biased (at least we assume so).&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s finish this off by looking at what our model expects the ages to be like for different matings:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;post &amp;lt;- extract.samples(m_H1_B) # extract samples
AGE_est &amp;lt;- apply(post$AGE_est, 2, mean) + 25 # add 25 back to ages
MATINGS_j &amp;lt;- jitter(d$MATINGS) # jitter MATINGS for better readability
plot(d$AGE, MATINGS_j, pch = 16, col = rangi2, xlab = &amp;quot;age&amp;quot;, ylab = &amp;quot;matings&amp;quot;, xlim = c(23, 55)) # observed ages
points(AGE_est, MATINGS_j) # estimated ages
for (i in 1:nrow(d)) lines(c(d$AGE[i], AGE_est[i]), rep(MATINGS_j[i], 2)) # shrinkage lines
lines(A_seq, lambda_mu) # linear regression from previous model
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-07-statistical-rethinking-chapter-15_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The blue dots represent the observed ages, while the open circles depict the estimated true ages from our model. We see some shrinkage. Fascinatingly, the shrinkage appears to switch direction around the regression line, however. Values above the regression line are shrunk to higher age ranges, while the reverse is true below the regression line. What this means is that the model assumed elephants with unexpectedly high mating numbers for their observed age to be older than our data implies and vice versa.&lt;/p&gt;
&lt;h3 id=&#34;practice-h2&#34;&gt;Practice H2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Repeat the model fitting problem above, now increasing the assumed standard error on &lt;code&gt;AGE&lt;/code&gt;. How large does the standard error have to get before the posterior mean for the coefficient on &lt;code&gt;AGE&lt;/code&gt; reaches zero?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; To solve this, I just run the model above again, but increase the standard error. I did several times with ever-increasing standard errors. Finally I landed on a standard error of &lt;code&gt;100&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_H2 &amp;lt;- ulam(
  alist(
    MATINGS ~ dpois(lambda),
    lambda &amp;lt;- exp(a) * AGE_est[i]^bA,
    AGE0 ~ dnorm(AGE_est, 100), # increase standard error here
    vector[41]:AGE_est ~ dunif(0, 50),
    a ~ dnorm(0, 1),
    bA ~ dnorm(0, 1)
  ),
  data = d, chains = 4, cores = 4
)
precis(m_H2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          mean        sd       5.5%     94.5%    n_eff    Rhat4
## a  -0.3487762 1.1358880 -1.7265552 1.9908366 6.393092 1.409652
## bA  0.4246873 0.3829174 -0.4010691 0.8592536 5.932438 1.454539
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Albeit not having reached 0, the mean estimate of &lt;code&gt;bA&lt;/code&gt; is closer to 0 now and the percentile interval around it is so large that we would not be able to identify the effect here.&lt;/p&gt;
&lt;h3 id=&#34;practice-h3&#34;&gt;Practice H3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  The fact that information flows in all directions among parameters sometimes leads to rather unintuitive conclusions. Here’s an example from missing data imputation, in which imputation of a single datum reverses the direction of an inferred relationship. Use these data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(100)
x &amp;lt;- c(rnorm(10), NA)
y &amp;lt;- c(rnorm(10, x), 100)
d &amp;lt;- list(x = x, y = y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These data comprise 11 cases, one of which has a missing predictor value. You can quickly confirm that a regression of $y$ on $x$ for only the complete cases indicates a strong positive relationship between the two variables. But now fit this model, imputing the one missing value for $x$:&lt;/p&gt;
&lt;p&gt;$$y_i ∼ Normal(µ_i, σ)$$ 
$$µ_i = α + βx_i$$ 
$$x_i ∼ Normal(0, 1)$$ 
$$α ∼ Normal(0, 100)$$ 
$$β ∼ Normal(0, 100)$$ 
$$σ ∼ HalfCauchy(0, 1)$$&lt;/p&gt;
&lt;p&gt;What has happened to the posterior distribution of $β$? Be sure to inspect the full density. Can you explain the change in inference?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Interestingly, the &lt;code&gt;rethinking&lt;/code&gt; functions also work on basic &lt;code&gt;lm&lt;/code&gt; objects:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(lm(y ~ x, d))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                  mean        sd       5.5%     94.5%
## (Intercept) 0.2412995 0.2774524 -0.2021231 0.6847221
## x           1.4236779 0.5209135  0.5911574 2.2561983
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On to the imputation model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_H3 &amp;lt;- ulam(
  alist(
    y ~ dnorm(mu, sigma),
    mu &amp;lt;- a + b * x,
    x ~ dnorm(0, 1),
    c(a, b) ~ dnorm(0, 100),
    sigma ~ dexp(1)
  ),
  data = d, chains = 4, cores = 4, iter = 4000,
  control = list(adapt_delta = 0.99)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m_H3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean        sd       5.5%     94.5%       n_eff    Rhat4
## b     -10.972553 19.300190 -27.944113 24.271345    2.066259 5.340370
## a       1.869767  3.346405  -3.361028  7.168704 4172.209841 1.003406
## sigma  10.294366  2.054437   7.383010 13.871396   75.915321 1.033777
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Well those percentile intervals look bad. The joint posterior distributions might help solve this mystery:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;pairs(m_H3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-07-statistical-rethinking-chapter-15_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We have a few bi-modal distributions which place the plausible values for &lt;code&gt;b&lt;/code&gt; and &lt;code&gt;x_impute&lt;/code&gt; either strongly in the negative or strongly in the positive realm. This feels like the issue of unidentifiable parameters all over again.&lt;/p&gt;
&lt;p&gt;The outcome variable value for which we are missing the predictor variable value is very extreme given the range of all other outcome variable values. This means, we can flip our predictor value to either extreme and still be consistent with the data and model thus forcing the regression line to be either positive or negative.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s extract positive and negative regression estimates and their positions in our extracted samples from the posterior:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;post &amp;lt;- extract.samples(m_H3)
post_pos &amp;lt;- post
post_neg &amp;lt;- post
for (i in 1:length(post)) {
  post_pos[[i]] &amp;lt;- post[[i]][post$b &amp;gt; 0]
  post_neg[[i]] &amp;lt;- post[[i]][post$b &amp;lt; 0]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this at hand, we can now compute the two regression lines and plot them:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;par(mfrow = c(1, 2))
## positive
x_seq &amp;lt;- seq(from = -2.6, to = 4, length.out = 30)
mu_link &amp;lt;- function(x, post) post$a + post$b * x
mu &amp;lt;- sapply(x_seq, mu_link, post = post_pos)
mu_mu &amp;lt;- apply(mu, 2, mean)
mu_PI &amp;lt;- apply(mu, 2, PI)
x_impute &amp;lt;- mean(post_pos$x_impute)
plot(y ~ x, d, pch = 16, col = rangi2, xlim = c(-0.85, x_impute))
points(x_impute, 100)
lines(x_seq, mu_mu)
shade(mu_PI, x_seq)
## negative
x_seq &amp;lt;- seq(from = -4, to = 4, length.out = 50)
mu &amp;lt;- sapply(x_seq, mu_link, post = post_neg)
mu_mu &amp;lt;- apply(mu, 2, mean)
mu_PI &amp;lt;- apply(mu, 2, PI)
x_impute &amp;lt;- mean(post_neg$x_impute)
plot(y ~ x, d, pch = 16, col = rangi2, xlim = c(-3.7, 0.9))
points(x_impute, 100)
lines(x_seq, mu_mu)
shade(mu_PI, x_seq)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-07-statistical-rethinking-chapter-15_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;1440&#34; /&gt;
This should make it obvious just how extreme the outcome variable value is and how our model could agree with either extreme imputed variable.&lt;/p&gt;
&lt;h3 id=&#34;practice-h4&#34;&gt;Practice H4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Some lad named Andrew made an eight-sided spinner. He wanted to know if it is fair. So he spun it a bunch of times, recording the counts of each value. Then he accidentally spilled coffee over the 4s and 5s. The surviving data are summarized below.&lt;/p&gt;
&lt;p&gt;| Value      |  1 |  2 |  3 |  4 |  5 |  6 |  7 |  8 |
| Frequency  | 18 | 19 | 22 |  ? |  ? | 19 | 20 | 22 |&lt;/p&gt;
&lt;p&gt;Your job is to impute the two missing values in the table above. Andrew doesn’t remember how many times he spun the spinner. So you will have to assign a prior distribution for the total number of spins and then marginalize over the unknown total. Andrew is not sure the spinner is fair (every value is equally likely), but he’s confident that none of the values is twice as likely as any other. Use a Dirichlet distribution to capture this prior belief. Plot the joint posterior distribution of 4s and 5s.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; First, I enter the data into &lt;code&gt;R&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;y &amp;lt;- c(18, 19, 22, NA, NA, 19, 20, 22)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What data do I need to somehow get to for my model?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;N&lt;/code&gt; - total number of spins&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For &lt;code&gt;N&lt;/code&gt;, we can say that is no smaller than 120 - the sum of all spins which we have observed outcomes for. The number of spins would be a count variable and so it would make sense to assign a Poisson distribution to them - especially seeing how we lack a sensible upper bound to the total number of spins. So what should our expected value be? Well, from the data above, it would be sensible to expect that the spins for sides 4 and 5 are 20 respectively - this is just a guess. As such, we could set a prior as:&lt;/p&gt;
&lt;p&gt;$$N \sim Poisson(40) + 120$$
Why 40 and why 120? 40 is the expected number of missing spins from our data table, 120 defines the lower bound of our total spins. We have data for 120 spins.&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;code&gt;Probs&lt;/code&gt; - vector of probabilities for each side of the spinner&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As for the vector of probabilities, we want to use the Dirichlet prior as outlined by the exercise text. The Dirichlet prior is used for categorical outcomes like these. We know that none of the outcomes is twice as likely as any other. Dirichlet doesn&amp;rsquo;t give us that control directly, unfortunately. What we can do is simulate:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;p &amp;lt;- rdirichlet(1e3, alpha = rep(4, 8))
plot(NULL, xlim = c(1, 8), ylim = c(0, 0.3), xlab = &amp;quot;outcome&amp;quot;, ylab = &amp;quot;probability&amp;quot;)
for (i in 1:10) lines(1:8, p[i, ], type = &amp;quot;b&amp;quot;, col = grau(), lwd = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-07-statistical-rethinking-chapter-15_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;1440&#34; /&gt;
It is difficult to judge from this what our prior is assuming and whether our assumption is met. We can identify this numerically though:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;twicer &amp;lt;- function(p) {
  o &amp;lt;- order(p)
  if (p[o][8] / p[o][1] &amp;gt; 2) {
    return(TRUE)
  } else {
    return(FALSE)
  }
}
sum(apply(p, 1, twicer))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 977
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our prior clearly needs to be tighter since our criterion of no category being twice as likely as any other category is being violated quite heavily.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;p &amp;lt;- rdirichlet(1e3, alpha = rep(50, 8))
sum(apply(p, 1, twicer))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 17
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That looks much better! Let&amp;rsquo;s plot that:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(NULL, xlim = c(1, 8), ylim = c(0, 0.3), xlab = &amp;quot;outcome&amp;quot;, ylab = &amp;quot;probability&amp;quot;)
for (i in 1:10) lines(1:8, p[i, ], type = &amp;quot;b&amp;quot;, col = grau(), lwd = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-07-statistical-rethinking-chapter-15_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;code&gt;N4&lt;/code&gt; and &lt;code&gt;N5&lt;/code&gt; - the counts of observations of the side 4 and 5, respectively&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This is what we want to get to to help Andrew get around his coffee-spillage mishap. What we need to do here is to marginalize over all combinations of 4s and 5s. I will freely admit that I was completely lost here and took the STAN code directly from the solutions by Richard McElreath. Looking at it, there are some loops in here, which I couldn&amp;rsquo;t have been able to code myself (yet). I have added some comments to indciate what I understood:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;code15H7 &amp;lt;- &amp;quot;
data{
    int N;
    int y[N];
    int y_max; // consider at most this many spins for y4 and y5
    int S_mean;
}
parameters{
    simplex[N] p;   // probabilities of each outcome
}
model{
    vector[(1+y_max)*(1+y_max)] terms; // all combinations of spins for 4 and 5
    int k = 1; // counter to index above vector of combinations

    p ~ dirichlet(rep_vector(50, N)); // Dirichlet prior

    // loop over possible values for unknown cells 4 and 5
    // this code updates posterior of p
    for(y4 in 0:y_max){
        for(y5 in 0:y_max){
            int Y[N] = y;  // probability of complete vector of individual spins
            Y[4] = y4;  // spins for 4s
            Y[5] = y5; // spins for 5s
            terms[k] = poisson_lpmf(y4+y5|S_mean-120) + multinomial_lpmf(Y|p);  // poisson prior for individual spins and multinomial prior for vector of counts conditional on number of spins n and prior p
            k = k + 1;
        }//y5
    }//y4
    target += log_sum_exp(terms);
}
generated quantities{  // repeates much of the above to compute posterior probability
    matrix[y_max+1, y_max+1] P45; // prob y4, y5 takes joint values
    // now compute Prob(y4, y5|p)
   {
        matrix[(1+y_max), (1+y_max)] terms;
        int k = 1;
        real Z;
        for(y4 in 0:y_max){
            for(y5 in 0:y_max){
              int Y[N] = y;
              Y[4] = y4;
              Y[5] = y5;
              terms[y4+1, y5+1] = poisson_lpmf(y4+y5|S_mean-120) + multinomial_lpmf(Y|p);
            }//y5
        }//y4
        Z = log_sum_exp(to_vector(terms));
        for(y4 in 0:y_max)
            for(y5 in 0:y_max)
                P45[y4+1, y5+1] = exp(terms[y4+1, y5+1] - Z);  //  make sure all probabilities sum to 1
    }
}
&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here&amp;rsquo;s the data that the model needs. STAN doesn&amp;rsquo;t accept &lt;code&gt;NA&lt;/code&gt;s, hence why the &lt;code&gt;NA&lt;/code&gt; values below are now encoded as -1:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;y &amp;lt;- c(18, 19, 22, -1, -1, 19, 20, 22)
dat &amp;lt;- list(
  N = length(y),
  y = y,
  S_mean = 160,
  y_max = 40
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, let&amp;rsquo;s run the model and plot some samples from it:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m15H7 &amp;lt;- stan(model_code = code15H7, data = dat, chains = 4, cores = 4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;post &amp;lt;- extract.samples(m15H7)
y_max &amp;lt;- dat$y_max
plot(NULL,
  xlim = c(10, y_max - 10), ylim = c(10, y_max - 10),
  xlab = &amp;quot;number of 4s&amp;quot;, ylab = &amp;quot;number of 5s&amp;quot;
)
mtext(&amp;quot;posterior distribution of 4s and 5s&amp;quot;)
for (y4 in 0:y_max) {
  for (y5 in 0:y_max) {
    k &amp;lt;- grau(mean(post$P45[, y4 + 1, y5 + 1]) / 0.01)
    points(y4, y5, col = k, pch = 16, cex = 1.5)
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-07-statistical-rethinking-chapter-15_files/figure-html/final plot-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From this, it is apparent that 20 spins for the 4s and 5s respectively is the most likely and that there is a negative correlation between these respective spins - more spins resulting in side 4 make less spins resulting in side 5 more likely.&lt;/p&gt;
&lt;p&gt;Andrew - don&amp;rsquo;t spill your coffee again.&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.5 (2021-03-31)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19043)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.1252  LC_CTYPE=English_United Kingdom.1252    LC_MONETARY=English_United Kingdom.1252 LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.1252    
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] gtools_3.8.2         rethinking_2.13      rstan_2.21.2         ggplot2_3.3.6        StanHeaders_2.21.0-7
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.7         mvtnorm_1.1-1      lattice_0.20-41    prettyunits_1.1.1  ps_1.6.0           assertthat_0.2.1   digest_0.6.27      utf8_1.2.1         V8_3.4.1           R6_2.5.0          
## [11] backports_1.2.1    stats4_4.0.5       evaluate_0.14      coda_0.19-4        highr_0.9          blogdown_1.3       pillar_1.6.0       rlang_0.4.11       curl_4.3.2         callr_3.7.0       
## [21] jquerylib_0.1.4    R.utils_2.10.1     R.oo_1.24.0        rmarkdown_2.7      styler_1.4.1       stringr_1.4.0      loo_2.4.1          munsell_0.5.0      compiler_4.0.5     xfun_0.22         
## [31] pkgconfig_2.0.3    pkgbuild_1.2.0     shape_1.4.5        htmltools_0.5.1.1  tidyselect_1.1.0   tibble_3.1.1       gridExtra_2.3      bookdown_0.22      codetools_0.2-18   matrixStats_0.61.0
## [41] fansi_0.4.2        crayon_1.4.1       dplyr_1.0.5        withr_2.4.2        MASS_7.3-53.1      R.methodsS3_1.8.1  grid_4.0.5         jsonlite_1.7.2     gtable_0.3.0       lifecycle_1.0.0   
## [51] DBI_1.1.1          magrittr_2.0.1     scales_1.1.1       KernSmooth_2.23-18 RcppParallel_5.1.2 cli_3.0.0          stringi_1.5.3      bslib_0.2.4        ellipsis_0.3.2     generics_0.1.0    
## [61] vctrs_0.3.7        rematch2_2.1.2     tools_4.0.5        R.cache_0.14.0     glue_1.4.2         purrr_0.3.4        processx_3.5.1     yaml_2.2.1         inline_0.3.17      colorspace_2.0-0  
## [71] knitr_1.33         sass_0.3.1
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 16</title>
      <link>https://www.erikkusch.com/courses/rethinking/chapter-16/</link>
      <pubDate>Thu, 13 May 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/rethinking/chapter-16/</guid>
      <description>&lt;h1 id=&#34;generalised-linear-madness&#34;&gt;Generalised Linear Madness&lt;/h1&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/rethinking/19__14-05-2021_SUMMARY_-GLM.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides Chapter 16&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to the exercises at the end of chapter 15 in 
&lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Satistical Rethinking 2&lt;/a&gt; by Richard McElreath. I have created these notes as a part of my ongoing involvement in the 
&lt;a href=&#34;https://www.erikkusch.com/project/aubayes/&#34;&gt;AU Bayes Study Group&lt;/a&gt;. Much of my inspiration for these solutions, where necessary, has been obtained from&lt;/p&gt;
&lt;!-- [Taras Svirskyi](https://github.com/jffist/statistical-rethinking-solutions/blob/master/ch10_hw.R), [William Wolf](https://github.com/cavaunpeu/statistical-rethinking/blob/master/chapter-10/homework.R), and [Corrie Bartelheimer](https://www.samples-of-thoughts.com/projects/statistical-rethinking/chapter_10/chp10-ex/) as well as  --&gt;
&lt;p&gt;the solutions provided to instructors by Richard McElreath himself.&lt;/p&gt;
&lt;h2 id=&#34;r-environment&#34;&gt;&lt;code&gt;R&lt;/code&gt; Environment&lt;/h2&gt;
&lt;p&gt;For today&amp;rsquo;s exercise, I load the following packages:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rethinking)
library(ggplot2)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;easy-exercises&#34;&gt;Easy Exercises&lt;/h2&gt;
&lt;p&gt;Unfortunately, the PDF version of 
&lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Satistical Rethinking 2&lt;/a&gt;, I am working with does not list any easy practice exercises for this chapter.&lt;/p&gt;
&lt;h2 id=&#34;medium-exercises&#34;&gt;Medium Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-m1&#34;&gt;Practice M1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Modify the cylinder height model, &lt;code&gt;m16.1&lt;/code&gt;, so that the exponent 3 on height is instead a free parameter. Do you recover the value of 3 or not? Plot the posterior predictions for the new model. How do they differ from those of &lt;code&gt;m16.1&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Before we move on, let me just remind all of us of the model itself:&lt;/p&gt;
&lt;p&gt;$W_i ∼ Log-Normal(µ_i, σ)$ [Distribution for weight]&lt;br&gt;
$exp(µ_i) = kπp^2h^3_i$ [expected median of weight]&lt;br&gt;
$k ∼ Beta(2, 18)$ [prior relation between weight and volume]&lt;br&gt;
$p ∼ Exponential(0.5)$ [prior proportionality of radius to height]&lt;br&gt;
$σ ∼ Exponential(1)$ [our old friend, sigma]&lt;/p&gt;
&lt;p&gt;As for the exercise, I start by loading the data and rescaling the &lt;code&gt;weight&lt;/code&gt; and &lt;code&gt;height&lt;/code&gt; variables as was done in the chapter:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(&amp;quot;Howell1&amp;quot;)
d &amp;lt;- Howell1
d$w &amp;lt;- d$weight / mean(d$weight)
d$h &amp;lt;- d$height / mean(d$height)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that the data is prepared, we can run the model. Before we run the model that we are asked for, however, I want to run the model from the chapter for later comparison:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m16.1 &amp;lt;- ulam(
  alist(
    w ~ dlnorm(mu, sigma),
    exp(mu) &amp;lt;- 3.141593 * k * p^2 * h^3,
    p ~ beta(2, 18),
    k ~ exponential(0.5),
    sigma ~ exponential(1)
  ),
  data = d, chains = 4, cores = 4, log_lik = TRUE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I run this model as well as the subsequent one with &lt;code&gt;log_lik = TRUE&lt;/code&gt; to allow for model comparison with WAIC.&lt;/p&gt;
&lt;p&gt;To assign a free parameter to the exponent 3 of the chapter, I simply substitute the value 3 in the model code with a letter (&lt;code&gt;e&lt;/code&gt;) to indicate a parameter. I also have to define a prior (I reckon the exponent should definitely be positive) for this new parameter, of course:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_M1 &amp;lt;- ulam(
  alist(
    w ~ dlnorm(mu, sigma),
    exp(mu) &amp;lt;- 3.141593 * k * p^2 * h^e,
    p ~ beta(2, 18),
    k ~ exponential(0.5),
    sigma ~ exponential(1),
    e ~ exponential(1)
  ),
  data = d, chains = 4, cores = 4, log_lik = TRUE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m_M1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            mean          sd      5.5%      94.5%     n_eff    Rhat4
## p     0.2440124 0.056092318 0.1699713  0.3450251  692.9784 1.002353
## k     5.7390657 2.482731435 2.5008291 10.2474462  812.8841 1.003970
## sigma 0.1263219 0.003660893 0.1206056  0.1321993 1004.6100 1.000673
## e     2.3234522 0.022774994 2.2877691  2.3598172 1108.6018 1.003564
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the new model, we obtain an estimates of 2.32 for the exponent rather than the value of 3 that was assumed in the chapter.&lt;/p&gt;
&lt;p&gt;So let&amp;rsquo;s get started with model comparison:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;compare(m16.1, m_M1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            WAIC       SE    dWAIC      dSE    pWAIC        weight
## m_M1  -845.6597 36.86898   0.0000       NA 3.441582  1.000000e+00
## m16.1 -310.4014 44.39907 535.2583 54.80355 3.775032 5.890284e-117
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Seems like model comparison strongly favours our new model &lt;code&gt;m_M1&lt;/code&gt;. What brings this difference about? Let&amp;rsquo;s look at the posterior predictions for the answer:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Getting the data
h_seq &amp;lt;- seq(from = 0, to = max(d$h), length.out = nrow(d))
# m_M1
w_sim &amp;lt;- sim(m_M1, data = list(h = h_seq))
m1_mean &amp;lt;- apply(w_sim, 2, mean)
m1_CI &amp;lt;- apply(w_sim, 2, PI)
# m16.1
w_sim &amp;lt;- sim(m16.1, data = list(h = h_seq))
m16.1_mean &amp;lt;- apply(w_sim, 2, mean)
m16.1_CI &amp;lt;- apply(w_sim, 2, PI)
## Making a data frame for plotting
plot_df &amp;lt;- data.frame(
  seq = rep(h_seq, 2),
  mean = c(m1_mean, m16.1_mean),
  CI_l = c(m1_CI[1, ], m16.1_CI[1, ]),
  CI_U = c(m1_CI[2, ], m16.1_CI[2, ]),
  y = rep(d$w, 2),
  x = rep(d$h, 2),
  model = rep(c(&amp;quot;m_M1&amp;quot;, &amp;quot;m16.1&amp;quot;), each = length(h_seq))
)
## Plotting posterior
ggplot(plot_df, aes(x = x, y = y)) +
  geom_point(col = &amp;quot;blue&amp;quot;) +
  geom_line(aes(x = seq, y = mean)) +
  geom_ribbon(aes(x = seq, ymin = CI_l, ymax = CI_U), alpha = 0.2) +
  labs(x = &amp;quot;height (scaled)&amp;quot;, y = &amp;quot;weight (scaled)&amp;quot;) +
  facet_wrap(~model) +
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-13-statistical-rethinking-chapter-16_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Compared to the original model &lt;code&gt;m16.1&lt;/code&gt;, the new model &lt;code&gt;m_M1&lt;/code&gt; fits shorter individuals much better than the original model which comes at the detriment of fitting taller individuals correctly. Overall, the posterior uncertainty is tighter for our new model &lt;code&gt;m_M1&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;practice-m2&#34;&gt;Practice M2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Conduct a prior predictive simulation for the cylinder height model. Begin with the priors in the chapter. Do these produce reasonable prior height distributions? If not, which modifications do you suggest?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Remember our priors from the chapter:&lt;/p&gt;
&lt;p&gt;$$p ∼ Beta(2, 18)$$
$$k ∼ Exponential(0.5)$$
$$\sigma \sim Exponential(1)$$
Now let&amp;rsquo;s simulate priors for a number of &lt;code&gt;N&lt;/code&gt; cases:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;N &amp;lt;- 1e2
p &amp;lt;- rbeta(N, 2, 18) # p ~ Beta(2, 18)
k &amp;lt;- rexp(N, 0.5) # k ~ Exponential(0.5)
sigma &amp;lt;- rexp(N, 1)
prior &amp;lt;- list(p = p, k = k, sigma = sigma)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The priors are all compiled into one list, now all we have to do is run the prior predictive check:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Making a data frame for plotting
plot_df &amp;lt;- with(prior, 3.141593 * k[1] * p[1]^2 * d$h^3)
for (i in 2:N) {
  plot_df &amp;lt;- c(plot_df, with(prior, 3.141593 * k[i] * p[i]^2 * d$h^3))
}
plot_df &amp;lt;- data.frame(
  w = plot_df,
  seq = rep(d$h, N),
  prior = rep(1:N, each = nrow(d))
)
## Plotting
ggplot() +
  geom_point(data = d, aes(x = h, y = w), col = &amp;quot;blue&amp;quot;) +
  geom_line(data = plot_df, aes(x = seq, y = w, group = prior)) +
  labs(x = &amp;quot;height (scaled)&amp;quot;, y = &amp;quot;weight (scaled)&amp;quot;) +
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-13-statistical-rethinking-chapter-16_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The combination of these priors seems to be too flat - weight is not increasing fast enough with height. Either $p$ or $k$ need to be larger on average:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## New priors
p &amp;lt;- rbeta(N, 4, 18)
k &amp;lt;- rexp(N, 1 / 4)
sigma &amp;lt;- rexp(N, 1)
prior &amp;lt;- list(p = p, k = k, sigma = sigma)
## Making a data frame for plotting
plot_df &amp;lt;- with(prior, 3.141593 * k[1] * p[1]^2 * d$h^3)
for (i in 2:N) {
  plot_df &amp;lt;- c(plot_df, with(prior, 3.141593 * k[i] * p[i]^2 * d$h^3))
}
plot_df &amp;lt;- data.frame(
  w = plot_df,
  seq = rep(d$h, N),
  prior = rep(1:N, each = nrow(d))
)
## Plotting
ggplot() +
  geom_point(data = d, aes(x = h, y = w), col = &amp;quot;blue&amp;quot;) +
  geom_line(data = plot_df, aes(x = seq, y = w, group = prior)) +
  labs(x = &amp;quot;height (scaled)&amp;quot;, y = &amp;quot;weight (scaled)&amp;quot;) +
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-13-statistical-rethinking-chapter-16_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;1440&#34; /&gt;
There are some prior combinations here that are definitely way too extreme, but most priors still bunch up too much along the x-axis. Let&amp;rsquo;s alter the prior for $k$ (the density) some more by making it log-Normal:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## New priors
N &amp;lt;- 1e2
p &amp;lt;- rbeta(N, 4, 18)
k &amp;lt;- rlnorm(N, log(7), 0.2) # median of log(7)
sigma &amp;lt;- rexp(N, 1)
prior &amp;lt;- list(p = p, k = k, sigma = sigma)
## Making a data frame for plotting
plot_df &amp;lt;- with(prior, 3.141593 * k[1] * p[1]^2 * d$h^3)
for (i in 2:N) {
  plot_df &amp;lt;- c(plot_df, with(prior, 3.141593 * k[i] * p[i]^2 * d$h^3))
}
plot_df &amp;lt;- data.frame(
  w = plot_df,
  seq = rep(d$h, N),
  prior = rep(1:N, each = nrow(d))
)
## Plotting
ggplot() +
  geom_point(data = d, aes(x = h, y = w), col = &amp;quot;blue&amp;quot;) +
  geom_line(data = plot_df, aes(x = seq, y = w, group = prior)) +
  labs(x = &amp;quot;height (scaled)&amp;quot;, y = &amp;quot;weight (scaled)&amp;quot;) +
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-13-statistical-rethinking-chapter-16_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is much better! Remember - priors are not about fitting the data exactly but informing the model about plausibility.&lt;/p&gt;
&lt;h3 id=&#34;practice-m3&#34;&gt;Practice M3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Use prior predictive simulations to investigate the Lynx-hare model. Begin with the priors in the chapter. Which population dynamics do these produce? Can you suggest any improvements to the priors, on the basis of your simulations?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Again, let me remind us of the model in the chapter:&lt;/p&gt;
&lt;p&gt;$h_t ∼ LogNormal(log(p_HH_t), σ_H)$ [Probability of observed hare pelts]&lt;br&gt;
$ℓ_t ∼ LogNormal(log(p_LL_t), σ_L)$ [Probability observed lynx pelts]&lt;br&gt;
$H_1 ∼ LogNormal(log(10), 1)$ [Prior for initial hare population]&lt;br&gt;
$L_1 ∼ LogNormal(log(10), 1)$ [Prior for initial lynx population]&lt;br&gt;
$H_{T&amp;gt;1} = H_1 + \int_1^TH_t(b_H −m_HL_t)d_t$ [Model for hare population]&lt;br&gt;
$L_{T&amp;gt;1} = L_1 + \int_1^T L_t(b_LH_t −m_L)d_t$ [Model for lynx population]&lt;br&gt;
$σ_H ∼ Exponential(1)$ [Prior for measurement dispersion]&lt;br&gt;
$σ_L ∼ Exponential(1)$ [Prior for measurement dispersion]&lt;br&gt;
$p_H ∼ Beta(α_H, β_H)$ [Prior for hare trap probability]&lt;br&gt;
$p_L ∼ Beta(α_L, β_L)$ [Prior for lynx trap probability]&lt;br&gt;
$b_H ∼ HalfNormal(1, 0.5)$ [Prior hare birth rate]&lt;br&gt;
$b_L ∼ HalfNormal(0.05, 0.05)$ [Prior lynx birth rate]&lt;br&gt;
$m_H ∼ HalfNormal(0.05, 0.05)$ [Prior hare mortality rate]&lt;br&gt;
$m_L ∼ HalfNormal(1, 0.5)$ [Prior lynx mortality rate]&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s get started on our exercise now by loading the data and preparing our priors. Here, we simply just draw theta parameters (these define halfnormal distributions) from normal distributions as defined above:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(Lynx_Hare)
N &amp;lt;- 12
theta &amp;lt;- matrix(NA, nrow = N, ncol = 4)
theta[, 1] &amp;lt;- rnorm(N, 1, 0.5) # b_H
theta[, 2] &amp;lt;- rnorm(N, 0.05, 0.05) # b_L
theta[, 3] &amp;lt;- rnorm(N, 1, 0.5) # m_L
theta[, 4] &amp;lt;- rnorm(N, 0.05, 0.05) # m_H
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now use these priors in combination with the &lt;code&gt;sim_lynx_hare()&lt;/code&gt; function from the book:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sim_lynx_hare &amp;lt;- function(n_steps, init, theta, dt = 0.002) {
  L &amp;lt;- rep(NA, n_steps)
  H &amp;lt;- rep(NA, n_steps)
  L[1] &amp;lt;- init[1]
  H[1] &amp;lt;- init[2]
  for (i in 2:n_steps) {
    H[i] &amp;lt;- H[i - 1] + dt * H[i - 1] * (theta[1] - theta[2] * L[i - 1])
    L[i] &amp;lt;- L[i - 1] + dt * L[i - 1] * (theta[3] * H[i - 1] - theta[4])
  }
  return(cbind(L, H))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the above function registered in our &lt;code&gt;R&lt;/code&gt; environment, we are ready to simulate with our priors and produce some plots:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Simulate for first prior
plot_df &amp;lt;- sim_lynx_hare(1e4, as.numeric(Lynx_Hare[1, 2:3]), theta[1, ])
plot_df &amp;lt;- data.frame(plot_df)
plot_df$prior &amp;lt;- rep(1, 1e4)
## simulate for all other priors
for (i in 2:N) {
  z &amp;lt;- sim_lynx_hare(1e4, as.numeric(Lynx_Hare[1, 2:3]), theta[i, ])
  z &amp;lt;- data.frame(z)
  z$prior &amp;lt;- rep(i, 1e4)
  plot_df &amp;lt;- rbind(plot_df, z)
}
plot_df$seq &amp;lt;- rep(1:1e4, N)
## Plotting
ggplot(plot_df, aes(x = seq)) +
  geom_line(aes(y = L), col = &amp;quot;brown&amp;quot;) +
  geom_line(aes(y = H), col = &amp;quot;blue&amp;quot;) +
  facet_wrap(~prior, scales = &amp;quot;free&amp;quot;) +
  theme_bw() +
  labs(x = &amp;quot;Time&amp;quot;, y = &amp;quot;Population&amp;quot;) +
  theme(axis.text.y = element_blank())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-13-statistical-rethinking-chapter-16_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Hare population estimates are shown in blue while Lynx population estimates are portrayed in brown. Nevermind that, however, these priors are clearly not good as far as building biological plausibility into our model. Why? There is no cycling in the blue trends depending on the brown trends (lynx eat hares and are thus coupled to them). In addition to that, although I have hidden the actual population estimates, I think it is evident from these plots that some of the prior estimates are just outlandish in terms of population size.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s see if we can do better. How would we do that? By making our priors more informative, of course. We should probably take this step-by-step:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Hare birth rate&lt;/em&gt; - $b_H$:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I want to make this more conservative by lowering the expected birth rate of hares. To do so, the theta parameter for my halfnormal distribution will now be drawn from $Normal(0.5, 0.1)$ as opposed to the previous $Normal(1, 0.5)$.&lt;/p&gt;
&lt;p&gt;$$b_H ∼ HalfNormal(0.5, 0.1)$$&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;em&gt;Lynx birth rate&lt;/em&gt; - $b_L$:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This one, I keep as it was previously. I strongly suspect that the base birth rate of lynx should be much smaller than that of hares. The new prior reflects that:&lt;/p&gt;
&lt;p&gt;$$b_L ∼ HalfNormal(0.05, 0.05)$$&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;em&gt;Lynx mortality rate&lt;/em&gt; - $m_L$:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I want to drastically decrease the estimated lynx mortality rate since lynx don&amp;rsquo;t die as much as hares do (longer life, no predators, etc.):&lt;/p&gt;
&lt;p&gt;$$m_H ∼ HalfNormal(0.025, 0.05)$$&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;&lt;em&gt;Hare mortality rate&lt;/em&gt; - $m_H$:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I increase the mortality rate of hares to reflect that they die much more frequently than lynx do for the aforementioned reasons:&lt;/p&gt;
&lt;p&gt;$$m_L ∼ HalfNormal(0.5, 0.1)$$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s simulate with these priors&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## New priors
N &amp;lt;- 12
theta &amp;lt;- matrix(NA, nrow = N, ncol = 4)
theta[, 1] &amp;lt;- rnorm(N, 0.5, 0.1) # b_H
theta[, 2] &amp;lt;- rnorm(N, 0.05, 0.05) # b_L
theta[, 3] &amp;lt;- rnorm(N, 0.025, 0.05) # m_L
theta[, 4] &amp;lt;- rnorm(N, 0.5, 0.1) # m_H
## Simulate for first prior
plot_df &amp;lt;- sim_lynx_hare(1e4, as.numeric(Lynx_Hare[1, 2:3]), theta[1, ])
plot_df &amp;lt;- data.frame(plot_df)
plot_df$prior &amp;lt;- rep(1, 1e4)
## simulate for all other priors
for (i in 2:N) {
  z &amp;lt;- sim_lynx_hare(1e4, as.numeric(Lynx_Hare[1, 2:3]), theta[i, ])
  z &amp;lt;- data.frame(z)
  z$prior &amp;lt;- rep(i, 1e4)
  plot_df &amp;lt;- rbind(plot_df, z)
}
plot_df$seq &amp;lt;- rep(1:1e4, N)
## Plotting
ggplot(plot_df, aes(x = seq)) +
  geom_line(aes(y = L), col = &amp;quot;brown&amp;quot;) +
  geom_line(aes(y = H), col = &amp;quot;blue&amp;quot;) +
  facet_wrap(~prior, scales = &amp;quot;free&amp;quot;) +
  theme_bw() +
  labs(x = &amp;quot;Time&amp;quot;, y = &amp;quot;Population&amp;quot;) +
  theme(axis.text.y = element_blank())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-13-statistical-rethinking-chapter-16_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Still, there are some populations here that experience explosive growth, but at least we now have properly cycling population trends for both species!&lt;/p&gt;
&lt;h2 id=&#34;hard-exercises&#34;&gt;Hard Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-h1&#34;&gt;Practice H1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Modify the Panda nut opening model so that male and female chimpanzees have different maximum adult body mass. The &lt;code&gt;sex&lt;/code&gt; variable in &lt;code&gt;data(Panda_nuts)&lt;/code&gt; provides the information you need. Be sure to incorporate the fact that you know, prior to seeing the data, that males are on average larger than females at maturity.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Once more, let me include the model specification from the chapter:&lt;/p&gt;
&lt;p&gt;$$n_i ∼ Poisson(λ_i)$$ 
$$λ_i = d_iϕ(1 − exp(−kt_i))^θ$$
$$ϕ ∼ LogNormal(log(1), 0.1)$$ 
$$k ∼ LogNormal(log(2), 0.25)$$ 
$$θ ∼ LogNormal(log(5), 0.25)$$&lt;/p&gt;
&lt;p&gt;Once more, we move on to loading the data as was done in the chapter and creating an index variable for male individuals:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(Panda_nuts)
dat_list &amp;lt;- list(
  n = as.integer(Panda_nuts$nuts_opened),
  age = Panda_nuts$age / max(Panda_nuts$age),
  seconds = Panda_nuts$seconds
)
dat_list$male_id &amp;lt;- ifelse(Panda_nuts$sex == &amp;quot;m&amp;quot;, 1L, 0L)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We need to alter the model above to allow for the effect of sex to take hold. How do we go about this? Well, the exercise states that the effect of sex is supposed to come about through the effect of body mass which, in turn, is included in the model through $\phi$ which handles the conversion of body mass into strength. We can add the effect of sex to the model as such:&lt;/p&gt;
&lt;p&gt;$$n_i ∼ Poisson(λ_i)$$ 
$$λ_i = d_i&lt;em&gt;p_mS&lt;/em&gt;ϕ(1 − exp(−kt_i))^θ$$
$$p_m ∼ Exponential(2)$$ 
$$ϕ ∼ LogNormal(log(1), 0.1)$$ 
$$k ∼ LogNormal(log(2), 0.25)$$ 
$$θ ∼ LogNormal(log(5), 0.25)$$&lt;/p&gt;
&lt;p&gt;where $S$ stands for the maleness indicator we built above:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_H1 &amp;lt;- ulam(
  alist(
    n ~ poisson(lambda),
    lambda &amp;lt;- seconds * (1 + pm * male_id) * phi * (1 - exp(-k * age))^theta, # 1+ addedd for baseline of effect of sex
    phi ~ lognormal(log(1), 0.1),
    pm ~ exponential(2),
    k ~ lognormal(log(2), 0.25),
    theta ~ lognormal(log(5), 0.25)
  ),
  data = dat_list, chains = 4, cores = 4
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m_H1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            mean         sd      5.5%      94.5%    n_eff     Rhat4
## phi   0.5996689 0.04786004 0.5284066  0.6779957 976.2037 1.0013342
## pm    0.6681319 0.13966484 0.4576800  0.9047816 991.7476 0.9993602
## k     5.1615999 0.66777159 4.0568926  6.2455157 743.6989 1.0060706
## theta 7.5940540 1.82343038 4.9655187 10.9163027 819.2285 1.0040457
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Due to how we built our model, the interpretation of $p_m$ is as follows: &amp;ldquo;Males are 0.67 times stronger than their female counterparts at maximum.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;How does this look when we plot it? I use the plotting scheme outlined by Richard McElreath in the chapter and modified in his solutions:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;post &amp;lt;- extract.samples(m_H1)
plot(NULL, xlim = c(0, 1), ylim = c(0, 1.5), xlab = &amp;quot;age&amp;quot;, ylab = &amp;quot;nuts per second&amp;quot;, xaxt = &amp;quot;n&amp;quot;)
at &amp;lt;- c(0, 0.25, 0.5, 0.75, 1, 1.25, 1.5)
axis(1, at = at, labels = round(at * max(Panda_nuts$age)))
pts &amp;lt;- dat_list$n / dat_list$seconds
point_size &amp;lt;- normalize(dat_list$seconds)
points(jitter(dat_list$age), pts,
  lwd = 2, cex = point_size * 3,
  col = ifelse(dat_list$male_id == 1, &amp;quot;black&amp;quot;, &amp;quot;red&amp;quot;)
)
# 10 female curves
for (i in 1:10) {
  with(
    post,
    curve(phi[i] * (1 - exp(-k[i] * x))^theta[i], add = TRUE, col = &amp;quot;red&amp;quot;)
  )
}
# 10 male curves
for (i in 1:10) {
  with(
    post,
    curve((1 + pm[i]) * phi[i] * (1 - exp(-k[i] * x))^theta[i], add = TRUE, col = grau())
  )
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-13-statistical-rethinking-chapter-16_files/figure-html/unnamed-chunk-25-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There is clearly quite the difference between males (black) and females (red) here. Males benefit more from age in opening nuts most likely due to their higher strength at maximum body mass. It is also worth pointing out that females have not been observed often or for long in this study as is apparent by the few, small circles in red.&lt;/p&gt;
&lt;h3 id=&#34;practice-h2&#34;&gt;Practice H2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Now return to the Panda nut model and try to incorporate individual differences. There are two parameters, $ϕ$ and $k$, which plausibly vary by individual. Pick one of these, allow it to vary by individual, and use partial pooling to avoid overfitting. The variable chimpanzee in &lt;code&gt;data(Panda_nuts)&lt;/code&gt; tells you which observations belong to which individuals.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; This works off of the same model as we just used:&lt;/p&gt;
&lt;p&gt;$$n_i ∼ Poisson(λ_i)$$ 
$$λ_i = d_iϕ(1 − exp(−kt_i))^θ$$
$$ϕ ∼ LogNormal(log(1), 0.1)$$ 
$$k ∼ LogNormal(log(2), 0.25)$$ 
$$θ ∼ LogNormal(log(5), 0.25)$$&lt;/p&gt;
&lt;p&gt;To incorporate individual effects here, we need to add our data about individuals into our data list:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dat_list$id &amp;lt;- Panda_nuts$chimpanzee
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To incorporate this ID variable into our model, we want to create a different mass-strength conversion parameter $\phi$ for each individual. Importantly, the average expected rate of opened nuts ($\lambda$) has to stay positive for each individual - otherwise, Poisson will fail us. For this reason, we will want to use a distribution for our individual, varying intercepts that is constrained to be positive. Here, I settle on the exponential. Consequently, I envision to alter the model like this:&lt;/p&gt;
&lt;p&gt;$$n_i ∼ Poisson(λ_i)$$ 
$$λ_i = d_i*(ϕz_{ID}*\tau)*(1 − exp(−kt_i))^θ$$
$$z_{ID} ~ Exponential(1)$$
$$\tau ~ Exponential(1)$$
$$ϕ ∼ LogNormal(log(1), 0.1)$$ 
$$k ∼ LogNormal(log(2), 0.25)$$ 
$$θ ∼ LogNormal(log(5), 0.25)$$&lt;/p&gt;
&lt;p&gt;Given our linear model and our constraint for positive values of $z_{ID}$ with a mean of 1, each value of $z_{ID}$ is a multiplicative effect with $\phi$. Due to the mean of 1, we expect on average no effect of individuals. The data may tell us otherwise.&lt;/p&gt;
&lt;p&gt;The model below bears two more important oddities:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It is parametrised in a &lt;strong&gt;non-centred&lt;/strong&gt; form to allow for more effective sampling of the posterior.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;gq&amp;gt;&lt;/code&gt; part rescales our non-centred estimates of &lt;code&gt;z&lt;/code&gt; and &lt;code&gt;tau&lt;/code&gt; back to our scale of origin for better interpretation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s run this model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_H2 &amp;lt;- ulam(
  alist(
    n ~ poisson(lambda),
    lambda &amp;lt;- seconds * (phi * z[id] * tau) * (1 - exp(-k * age))^theta,
    phi ~ lognormal(log(1), 0.1),
    z[id] ~ exponential(1),
    tau ~ exponential(1),
    k ~ lognormal(log(2), 0.25),
    theta ~ lognormal(log(5), 0.25),
    gq &amp;gt; vector[id]:zz &amp;lt;&amp;lt;- z * tau # rescaled
  ),
  data = dat_list, chains = 4, cores = 4,
  control = list(adapt_delta = 0.99), iter = 4000
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m_H2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            mean        sd      5.5%    94.5%    n_eff     Rhat4
## phi   1.0013518 0.1004075 0.8481803 1.169319 8877.555 0.9998477
## tau   0.6519607 0.2101021 0.3852493 1.030786 1702.305 1.0006857
## k     3.0816213 0.7406724 2.0076499 4.342410 3954.259 1.0007966
## theta 3.1730030 0.6639074 2.2740025 4.341821 5314.860 1.0009539
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;tau&lt;/code&gt; tells us whether there are individual differences or not and it firmly identifies that there are some. To understand these effects, it is easiest to use our rescaled estimates stored in &lt;code&gt;zz&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(precis(m_H2, 2, pars = &amp;quot;zz&amp;quot;))
abline(v = 1, lty = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-13-statistical-rethinking-chapter-16_files/figure-html/unnamed-chunk-31-1.png&#34; width=&#34;1440&#34; /&gt;
Above, we see the proportion of $\phi$ for each individual. Average values are found at 1. Values above 1 indicate stronger-than-average individuals.&lt;/p&gt;
&lt;h3 id=&#34;practice-h3&#34;&gt;Practice H3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  The chapter asserts that a typical, geocentric time series model might be one that uses lag variables. Here you’ll fit such a model and compare it to ODE model in the chapter. An autoregressive time series uses earlier values of the state variables to predict new values of the same variables. These earlier values are called &lt;em&gt;lag variables&lt;/em&gt;. You can construct the lag variables here with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(Lynx_Hare)
dat_ar1 &amp;lt;- list(
  L = Lynx_Hare$Lynx[2:21],
  L_lag1 = Lynx_Hare$Lynx[1:20],
  H = Lynx_Hare$Hare[2:21],
  H_lag1 = Lynx_Hare$Hare[1:20]
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you can use &lt;code&gt;L_lag1&lt;/code&gt; and &lt;code&gt;H_lag1&lt;/code&gt; as predictors of the outcomes &lt;code&gt;L&lt;/code&gt; and &lt;code&gt;H&lt;/code&gt;. Like this:&lt;/p&gt;
&lt;p&gt;$$L_t ∼ LogNormal(log (µ_{L,t}), σ_L)$$
$$µ_{L,t} = α_L + β_{LL}L_{t−1} + β_{LH}H_{t−1}$$ 
$$H_t ∼ LogNormal(log(µ_{H,t}), σ_H)$$ 
$$µ_{H,t} = α_H + β_{HH}H_{t−1} + β_{HL}L_{t−1}$$&lt;/p&gt;
&lt;p&gt;where $L_{t−1}$ and $H_{t−1}$ are the lag variables. Use &lt;code&gt;ulam()&lt;/code&gt; to fit this model. Be careful of the priors of the $α$ and $β$ parameters. Compare the posterior predictions of the autoregressive model to the ODE model in the chapter. How do the predictions differ? Can you explain why, using the structures of the models?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Let&amp;rsquo;s quickly complete the model above in mathematical notation before we code it:&lt;/p&gt;
&lt;p&gt;$$H_t ∼ LogNormal(log(µ_{H,t}), σ_H)$$ 
$$L_t ∼ LogNormal(log (µ_{L,t}), σ_L)$$
$$µ_{H,t} = α_H + β_{HH}H_{t−1} + β_{HL}L_{t−1}$$
$$µ_{L,t} = α_L + β_{LL}L_{t−1} + β_{LH}H_{t−1}$$&lt;/p&gt;
&lt;p&gt;$$H_{T&amp;gt;1} = H_1 + \int_1^TH_t(b_H −m_HL_t)d_t$$
$$L_{T&amp;gt;1} = L_1 + \int_1^T L_t(b_LH_t −m_L)d_t$$&lt;/p&gt;
&lt;p&gt;Now on to the priors:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Mean population size of hares&lt;/em&gt; - $\alpha_H$. I don&amp;rsquo;t have any strong idea about this one except for the fact that it has to be positive:&lt;br&gt;
$$\alpha_H ∼ Exponential(1)$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Mean population size of lynx&lt;/em&gt; - $\alpha_L$. Same as above - must be positive:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$\alpha_L ∼ Exponential(1)$$&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;em&gt;Effect of hares on hares through lag&lt;/em&gt; - $\beta_{HH}$. This one is probably rather positive than negative, but negative values are thinkable:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$\beta_{HH} \sim Normal(1, 0.5)$$&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;&lt;em&gt;Effect of lynx on hares&lt;/em&gt; - $\beta_{HL}$. Lynx eat hares, therefore I assume lynx have a negative effect on hare populations:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$\beta_{HL} \sim Normal(-1, 0.5)$$&lt;/p&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;&lt;em&gt;Effect of lynx on lynx through lag&lt;/em&gt; - $\beta_{LL}$. This one is probably rather positive than negative, but negative values are thinkable:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$\beta_{LL} \sim Normal(1, 0.5)$$&lt;/p&gt;
&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;&lt;em&gt;Effect of hares on lynx&lt;/em&gt; - $\beta_{HL}$. Lynx eat hares, therefore I assume lynx populations grow when hares are abundant:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$\beta_{LH} \sim Normal(1, 0.5)$$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s put this all into effect in a model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_H3_A &amp;lt;- ulam(
  alist(
    H ~ lognormal(log(muh), sigmah),
    L ~ lognormal(log(mul), sigmal),
    muh &amp;lt;- ah + b_hh * H_lag1 + b_hl * L_lag1,
    mul &amp;lt;- al + b_ll * L_lag1 + b_lh * H_lag1,
    c(ah, al) ~ normal(0, 1),
    b_hh ~ normal(1, 0.5),
    b_hl ~ normal(-1, 0.5),
    b_ll ~ normal(1, 0.5),
    b_lh ~ normal(1, 0.5),
    c(sigmah, sigmal) ~ exponential(1)
  ),
  data = dat_ar1, chains = 4, cores = 4
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m_H3_A)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              mean         sd       5.5%       94.5%     n_eff     Rhat4
## al     -0.9086472 0.92283343 -2.3478379  0.58408534 1766.8222 0.9994495
## ah      0.8030823 1.01577410 -0.7686795  2.39673796 1543.2587 1.0012806
## b_hh    1.1575260 0.15494168  0.9220842  1.41288452 1165.4372 0.9993601
## b_hl   -0.1898141 0.10424140 -0.3427948 -0.01818273  955.5289 1.0004735
## b_ll    0.5386323 0.09142832  0.3991584  0.68642887 1275.2896 1.0007976
## b_lh    0.2555668 0.05003324  0.1773263  0.33463902 1033.0136 1.0021123
## sigmal  0.3102309 0.06066444  0.2289059  0.42422181 1117.1826 1.0024006
## sigmah  0.4482276 0.08050156  0.3354154  0.58483481 1407.6199 1.0005659
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Turns out, the data agree with my prior intuition here. The implied time-series looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;post &amp;lt;- extract.samples(m_H3_A)
plot(dat_ar1$H, pch = 16, xlab = &amp;quot;Year&amp;quot;, ylab = &amp;quot;pelts (thousands)&amp;quot;, ylim = c(0, 100))
points(dat_ar1$L, pch = 16, col = rangi2)
mu &amp;lt;- link(m_H3_A)
for (s in 1:21) {
  lines(1:20, mu$muh[s, ], col = col.alpha(&amp;quot;black&amp;quot;, 0.2), lwd = 2) # hares
  lines(1:20, mu$mul[s, ], col = col.alpha(rangi2, 0.4), lwd = 2) # lynx
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-13-statistical-rethinking-chapter-16_files/figure-html/unnamed-chunk-37-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Lynx are portrayed in blue while hares are shown in black. The model in the chapter clearly does a better job at replicating these time-series, particularly that of lynx. Why is that? For starters, our model fails to appreciate measurement error on reported population sizes. Secondly, the effects are modelled as linear when we know them not to be.&lt;/p&gt;
&lt;p&gt;What about a lagged interaction model to resolve the issue of linear effects? Let&amp;rsquo;s try it by modelling as follows:&lt;/p&gt;
&lt;p&gt;$$µ_{H,t} = α_H + β_{HH}H_{t−1} + β_{HL}L_{t−1}H_{t−1}$$
$$µ_{L,t} = α_L + β_{LL}L_{t−1} + β_{LH}H_{t−1}L_{t−1}$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_H3_B &amp;lt;- ulam(
  alist(
    H ~ lognormal(log(muh), sigmah),
    L ~ lognormal(log(mul), sigmal),
    muh &amp;lt;- ah + b_hh * H_lag1 + b_hl * L_lag1 * H_lag1, # interaction here
    mul &amp;lt;- al + b_ll * L_lag1 + b_lh * H_lag1 * L_lag1, # interaction here
    c(ah, al) ~ normal(0, 1),
    b_hh ~ normal(1, 0.5),
    b_hl ~ normal(-1, 0.5),
    b_ll ~ normal(1, 0.5),
    b_lh ~ normal(1, 0.5),
    c(sigmah, sigmal) ~ exponential(1)
  ),
  data = dat_ar1, chains = 4, cores = 4
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;post &amp;lt;- extract.samples(m_H3_B)
plot(dat_ar1$H, pch = 16, xlab = &amp;quot;Year&amp;quot;, ylab = &amp;quot;pelts (thousands)&amp;quot;, ylim = c(0, 100))
points(dat_ar1$L, pch = 16, col = rangi2)
mu &amp;lt;- link(m_H3_B)
for (s in 1:21) {
  lines(1:20, mu$muh[s, ], col = col.alpha(&amp;quot;black&amp;quot;, 0.2), lwd = 2) # hares
  lines(1:20, mu$mul[s, ], col = col.alpha(rangi2, 0.4), lwd = 2) # lynx
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-13-statistical-rethinking-chapter-16_files/figure-html/unnamed-chunk-40-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s already a lot better, but our lines now overshoot the peaks of lynx populations. I will leave it at that for this exercise although this model is far from perfect. The better model is in the book.&lt;/p&gt;
&lt;h3 id=&#34;practice-h4&#34;&gt;Practice H4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Adapt the autoregressive model to use a two-step lag variable. This means that $L_{t−2}$ and $H_{t−2}$, in addition to $L_{t−1}$ and $H_{t−1}$, will appear in the equation for $µ$. This implies that prediction depends upon not only what happened just before now, but also on what happened two time steps ago. How does this model perform, compared to the ODE model?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Let&amp;rsquo;s prepare the data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dat_ar2 &amp;lt;- list(
  L = Lynx_Hare$Lynx[3:21],
  L_lag1 = Lynx_Hare$Lynx[2:20],
  L_lag2 = Lynx_Hare$Lynx[1:19],
  H = Lynx_Hare$Hare[3:21],
  H_lag1 = Lynx_Hare$Hare[2:20],
  H_lag2 = Lynx_Hare$Hare[1:19]
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Starting off with the basic, linear model we used above:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_H4_A &amp;lt;- ulam(
  alist(
    H ~ lognormal(log(muh), sigmah),
    L ~ lognormal(log(mul), sigmal),
    muh &amp;lt;- ah + phi_hh * H_lag1 + phi_hl * L_lag1 +
      phi2_hh * H_lag2 + phi2_hl * L_lag2,
    mul &amp;lt;- al + phi_ll * L_lag1 + phi_lh * H_lag1 +
      phi2_ll * L_lag2 + phi2_lh * H_lag2,
    c(ah, al) ~ normal(0, 1),
    phi_hh ~ normal(1, 0.5),
    phi_hl ~ normal(-1, 0.5),
    phi_ll ~ normal(1, 0.5),
    phi_lh ~ normal(1, 0.5),
    phi2_hh ~ normal(0, 0.5),
    phi2_hl ~ normal(0, 0.5),
    phi2_ll ~ normal(0, 0.5),
    phi2_lh ~ normal(0, 0.5),
    c(sigmah, sigmal) ~ exponential(1)
  ),
  data = dat_ar2, chains = 4, cores = 4
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m_H4_A)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               mean         sd       5.5%       94.5%     n_eff     Rhat4
## al      -0.4183636 0.91246723 -1.8486107  1.05960250 1606.2951 0.9992228
## ah       0.3719553 0.99475941 -1.1590825  1.95140989 1990.4075 1.0009188
## phi_hh   1.0099326 0.19248834  0.7119274  1.32914392 1045.2694 1.0034079
## phi_hl  -0.7399365 0.32827054 -1.2783735 -0.20488913  877.9196 1.0017976
## phi_ll   0.9271154 0.24347415  0.5527803  1.31868956  939.7483 1.0037273
## phi_lh   0.3897833 0.13124341  0.1862378  0.60413394  950.0423 1.0039032
## phi2_hh  0.1847126 0.27004715 -0.2318049  0.62006465  917.6192 1.0019516
## phi2_hl  0.3975250 0.15804542  0.1439605  0.64872790  998.1928 1.0016893
## phi2_ll -0.1939459 0.10830081 -0.3702380 -0.02048142 1180.9072 1.0017758
## phi2_lh -0.2402054 0.20086962 -0.5597479  0.06914398  876.4958 1.0041831
## sigmal   0.3020739 0.06006007  0.2193412  0.40635405 1238.3004 0.9997003
## sigmah   0.3949292 0.07801180  0.2906307  0.53267445 1440.4280 1.0019419
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All of these make sense still. As does the implied time-series:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(dat_ar2$H, pch = 16, xlab = &amp;quot;Year&amp;quot;, ylab = &amp;quot;pelts (thousands)&amp;quot;, ylim = c(0, 100))
points(dat_ar2$L, pch = 16, col = rangi2)
mu &amp;lt;- link(m_H4_A)
for (s in 1:21) {
  lines(1:19, mu$muh[s, ], col = col.alpha(&amp;quot;black&amp;quot;, 0.2), lwd = 2)
  lines(1:19, mu$mul[s, ], col = col.alpha(rangi2, 0.4), lwd = 2)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-13-statistical-rethinking-chapter-16_files/figure-html/unnamed-chunk-46-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This time-series hasn&amp;rsquo;t benefited much from including the second-order time-lag.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s try the interaction effect model with two lags:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_H4_B &amp;lt;- ulam(
  alist(
    H ~ lognormal(log(muh), sigmah),
    L ~ lognormal(log(mul), sigmal),
    muh &amp;lt;- ah + phi_hh * H_lag1 + phi_hl * L_lag1 * H_lag1 +
      phi2_hh * H_lag2 + phi2_hl * L_lag2 * H_lag2,
    mul &amp;lt;- al + phi_ll * L_lag1 + phi_lh * H_lag1 * L_lag1 +
      phi2_ll * L_lag2 + phi2_lh * H_lag2 * L_lag2,
    c(ah, al) ~ normal(0, 1),
    phi_hh ~ normal(1, 0.5),
    phi_hl ~ normal(-1, 0.5),
    phi_ll ~ normal(1, 0.5),
    phi_lh ~ normal(1, 0.5),
    phi2_hh ~ normal(0, 0.5),
    phi2_hl ~ normal(0, 0.5),
    phi2_ll ~ normal(0, 0.5),
    phi2_lh ~ normal(0, 0.5),
    c(sigmah, sigmal) ~ exponential(1)
  ),
  data = dat_ar2, chains = 4, cores = 4
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(dat_ar2$H, pch = 16, xlab = &amp;quot;Year&amp;quot;, ylab = &amp;quot;pelts (thousands)&amp;quot;, ylim = c(0, 100))
points(dat_ar2$L, pch = 16, col = rangi2)
mu &amp;lt;- link(m_H4_B)
for (s in 1:21) {
  lines(1:19, mu$muh[s, ], col = col.alpha(&amp;quot;black&amp;quot;, 0.2), lwd = 2)
  lines(1:19, mu$mul[s, ], col = col.alpha(rangi2, 0.4), lwd = 2)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-13-statistical-rethinking-chapter-16_files/figure-html/unnamed-chunk-50-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This time-series also didn&amp;rsquo;t gain anything from adding second-order lag effects, I&amp;rsquo;m afraid.&lt;/p&gt;
&lt;p&gt;I reckon this exercise was designed to highlight that higher-order lag effects don&amp;rsquo;t have any causal meaning.&lt;/p&gt;
&lt;h3 id=&#34;practice-h5&#34;&gt;Practice H5&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Population dynamic models are typically very difficult to fit to empirical data. The Lynx-hare example in the chapter was easy, partly because the data are unusually simple and partly because the chapter did the difficult prior selection for you. Here’s another data set that will impress upon you both how hard the task can be and how badly Lotka-Volterra fits empirical data in general. The data in &lt;code&gt;data(Mites)&lt;/code&gt; are numbers of predator and prey mites living on fruit. Model these data using the same Lotka-Volterra ODE system from the chapter. These data are actual counts of individuals, not just their pelts. You will need to adapt the Stan code in &lt;code&gt;data(Lynx_Hare_model)&lt;/code&gt;. Note that the priors will need to be rescaled, because the outcome variables are on a different scale. Prior predictive simulation will help. Keep in mind as well that the time variable and the birth and death parameters go together. If you rescale the time dimension, that implies you must also rescale the parameters.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; We have not worked with this data set before and so bet practise would have us load and plot it:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(Mites)
plot(Mites$day, Mites$prey)
points(Mites$day, Mites$predator, pch = 16)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-13-statistical-rethinking-chapter-16_files/figure-html/unnamed-chunk-52-1.png&#34; width=&#34;1440&#34; /&gt;
Open circles show prey. Closed circles show predators. One could definitely argue that there are cycles here.&lt;/p&gt;
&lt;p&gt;Luckily, so the solutions by McElreath tell me, there is no measurement error here. Thank the heavens!&lt;/p&gt;
&lt;p&gt;For prior predictive checks of our upcoming model and its priors we will want to repurpose the simulation function from the chapter that I used above:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sim_mites &amp;lt;- function(n_steps, init, theta, dt = 0.002) {
  L &amp;lt;- rep(NA, n_steps)
  H &amp;lt;- rep(NA, n_steps)
  L[1] &amp;lt;- init[1]
  H[1] &amp;lt;- init[2]
  for (i in 2:n_steps) {
    L[i] &amp;lt;- L[i - 1] + dt * L[i - 1] * (theta[3] * H[i - 1] - theta[4])
    H[i] &amp;lt;- H[i - 1] + dt * H[i - 1] * (theta[1] - theta[2] * L[i - 1])
  }
  return(cbind(L, H))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we need to define some priors for:  (1) prey birth rate &lt;code&gt;theta[1]&lt;/code&gt;, (2) prey mortality rate &lt;code&gt;theta[2]&lt;/code&gt;, (3) predator mortality rate &lt;code&gt;theta[3]&lt;/code&gt;, and (4) predator birth rate &lt;code&gt;theta[4]&lt;/code&gt;. Unfortunately, I lack a good understanding of mites and their prey to build intuitive priors.&lt;/p&gt;
&lt;p&gt;Playing around with the code below will lead you to identifying some priors that look right (the code below just report what we settle on for this exercise):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(41)
## Priors
N &amp;lt;- 16
theta &amp;lt;- matrix(NA, N, 4)
theta[, 1] &amp;lt;- rnorm(N, 1.5, 1) # prey birth rate
theta[, 2] &amp;lt;- rnorm(N, 0.005, 0.1) # prey mortality rate
theta[, 3] &amp;lt;- rnorm(N, 0.0005, 0.1) # predator mortality rate
theta[, 4] &amp;lt;- rnorm(N, 0.5, 1) # predator birth rate
## Simulate for first prior
plot_df &amp;lt;- sim_mites(1e4, as.numeric(Mites[1, 3:2]), theta[1, ])
plot_df &amp;lt;- data.frame(plot_df)
plot_df$prior &amp;lt;- rep(1, 1e4)
## simulate for all other priors
for (i in 2:N) {
  z &amp;lt;- sim_mites(1e4, as.numeric(Mites[1, 3:2]), theta[i, ])
  z &amp;lt;- data.frame(z)
  z$prior &amp;lt;- rep(i, 1e4)
  plot_df &amp;lt;- rbind(plot_df, z)
}
plot_df$seq &amp;lt;- rep(1:1e4, N)
## Plotting
ggplot(plot_df, aes(x = seq)) +
  geom_line(aes(y = L), col = &amp;quot;brown&amp;quot;) +
  geom_line(aes(y = H), col = &amp;quot;blue&amp;quot;) +
  facet_wrap(~prior, scales = &amp;quot;free&amp;quot;) +
  theme_bw() +
  labs(x = &amp;quot;Time&amp;quot;, y = &amp;quot;Population&amp;quot;) +
  theme(axis.text.y = element_blank())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-13-statistical-rethinking-chapter-16_files/figure-html/unnamed-chunk-55-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;These are decent enough, some show nice cycles for a few.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s run with these anyways and take them forward to a model. The model below is just a broken-back version of the STAN model in the chapter:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Mites_STAN &amp;lt;- &amp;quot;// Mites model, L is the predator, H is the prey
functions{
  real[] dpop_dt(real t,                    // time
                  real[] pop_init,          // initial state{lynx, hares}
                  real[] theta,             // parameters
                  real[] x_r, int[] x_i){   // unused
    real L = pop_init[1];                   // prey population initialisation
    real H = pop_init[2];                   // predator population initialisation
    real bh = theta[1];                     // prey birth rate
    real mh = theta[2];                     // prey mortality
    real ml = theta[3];                     // predator mortality
    real bl = theta[4];                     // predator birth rate
    // differential equations
    real dH_dt = (bh - mh * L) * H;
    real dL_dt = (bl * H - ml) * L;
    return{ dL_dt, dH_dt };
  }
}
data{
  int&amp;lt;lower=0&amp;gt; N;            // number of measurement times
  int&amp;lt;lower=0&amp;gt; mites[N,2];   // measured populations
  real&amp;lt;lower=0&amp;gt; days[N];     // days from start of experiment
}
parameters{
  real&amp;lt;lower=0&amp;gt; theta[4];     //{ bh, mh, ml, bl }
  real&amp;lt;lower=0&amp;gt; pop_init[2];  // initial population state
  real&amp;lt;lower=0&amp;gt; sigma[2];     // measurement errors
}
transformed parameters{
  real pop[N, 2];
  pop[1,1] = pop_init[1];     // prey population initialisation
  pop[1,2] = pop_init[2];     // predator population initialisation
  pop[2:N,1:2] = integrate_ode_rk45(
    dpop_dt, pop_init, 0, days[2:N], theta,
    rep_array(0.0, 0), rep_array(0, 0), 1e-5, 1e-3, 5e2);
}
model{
  // priors
  theta[1] ~ normal(1.5, 1);       // prey birth rate
  theta[2] ~ normal(0.005, 0.1);     // prey mortality
  theta[3] ~ normal(0.0005, 0.1);   // predator mortality
  theta[4] ~ normal(0.5, 1);       // predator birth rate
  sigma ~ exponential(1);
  pop_init[1] ~ normal(mites[1,1], 50);
  pop_init[2] ~ normal(mites[1,2], 50);
  // observation model
  // connect latent population state to observed pelts
  for (t in 1:N)
    for (k in 1:2)
      mites[t,k] ~ lognormal(log(pop[t,k]), sigma[k]);
  }
generated quantities{
  real mites_pred[N,2];
  for (t in 1:N)
    for (k in 1:2)
      mites_pred[t,k] = lognormal_rng(log(pop[t,k]), sigma[k]);
}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Preparing the data and running the model is quite straight-forward now:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dat_mites &amp;lt;- list(
  N = nrow(Mites),
  mites = as.matrix(Mites[, 3:2]),
  days = Mites[, 1] / 7
)
m_H5 &amp;lt;- stan(
  model_code = Mites_STAN, data = dat_mites, chains = 4, cores = 4, iter = 2000,
  control = list(adapt_delta = 0.99)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m_H5, 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                     mean           sd         5.5%        94.5%    n_eff     Rhat4
## theta[1]    1.288983e+00 3.126271e-01 9.175982e-01 1.852993e+00  984.656 1.0012770
## theta[2]    6.533764e-03 2.216549e-03 3.977740e-03 1.067107e-02 1095.219 1.0010345
## theta[3]    3.250613e-01 7.149778e-02 2.071048e-01 4.392285e-01 1218.141 1.0008058
## theta[4]    4.802551e-04 1.592542e-04 2.589479e-04 7.581328e-04 1473.000 1.0011075
## pop_init[1] 1.164473e+02 1.961122e+01 8.879640e+01 1.502520e+02 1822.214 1.0000536
## pop_init[2] 2.481791e+02 3.982614e+01 1.866272e+02 3.136870e+02 2611.777 0.9994916
## sigma[1]    7.293284e-01 1.209180e-01 5.645224e-01 9.408383e-01 1600.917 1.0004918
## sigma[2]    1.071276e+00 1.464701e-01 8.665494e-01 1.327090e+00 2048.149 1.0016011
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Without trying to interpret the parameters here, let&amp;rsquo;s just jump straight into the posterior predictions:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;post &amp;lt;- extract.samples(m_H5)
mites &amp;lt;- dat_mites$mites
plot(dat_mites$days, mites[, 2],
  pch = 16, ylim = c(0, 3000),
  xlab = &amp;quot;time (week)&amp;quot;, ylab = &amp;quot;mites&amp;quot;
)
points(dat_mites$days, mites[, 1], col = rangi2, pch = 16)
for (s in 1:21) {
  lines(dat_mites$days, post$pop[s, , 2], col = col.alpha(&amp;quot;black&amp;quot;, 0.2), lwd = 2)
  lines(dat_mites$days, post$pop[s, , 1], col = col.alpha(rangi2, 0.3), lwd = 2)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-13-statistical-rethinking-chapter-16_files/figure-html/unnamed-chunk-61-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Yet again, our model struggles to reconstruct the underlying time-series. This is certainly what McElreath referred to in the chapter when he said we would come face-to-face with the limitations of Lotka-Volterra models in the exercises. Why does the model do so baldy then? Well, it assumes equal cycle times which the data does not support. In addition our model is purely deterministic and lacks stochasticity which could help fit closer to the underlying cycles.&lt;/p&gt;
&lt;p&gt;I would have loved to end this series of blogposts on a more upbeat note, I must say. If you have found any use out of this series of posts and/or my summary slides linked at the top of these, then I am very happy. I must say I personally enjoyed working through this book a lot and hope my posts will come in handy for others looking to validate their solutions. Take care!&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.5 (2021-03-31)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19043)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.1252  LC_CTYPE=English_United Kingdom.1252    LC_MONETARY=English_United Kingdom.1252 LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.1252    
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] rethinking_2.13      rstan_2.21.2         ggplot2_3.3.6        StanHeaders_2.21.0-7
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.7         mvtnorm_1.1-1      lattice_0.20-41    prettyunits_1.1.1  ps_1.6.0           assertthat_0.2.1   digest_0.6.27      utf8_1.2.1         V8_3.4.1           R6_2.5.0          
## [11] backports_1.2.1    stats4_4.0.5       evaluate_0.14      coda_0.19-4        highr_0.9          blogdown_1.3       pillar_1.6.0       rlang_0.4.11       curl_4.3.2         callr_3.7.0       
## [21] jquerylib_0.1.4    R.utils_2.10.1     R.oo_1.24.0        rmarkdown_2.7      styler_1.4.1       labeling_0.4.2     stringr_1.4.0      loo_2.4.1          munsell_0.5.0      compiler_4.0.5    
## [31] xfun_0.22          pkgconfig_2.0.3    pkgbuild_1.2.0     shape_1.4.5        htmltools_0.5.1.1  tidyselect_1.1.0   tibble_3.1.1       gridExtra_2.3      bookdown_0.22      codetools_0.2-18  
## [41] matrixStats_0.61.0 fansi_0.4.2        crayon_1.4.1       dplyr_1.0.5        withr_2.4.2        MASS_7.3-53.1      R.methodsS3_1.8.1  grid_4.0.5         jsonlite_1.7.2     gtable_0.3.0      
## [51] lifecycle_1.0.0    DBI_1.1.1          magrittr_2.0.1     scales_1.1.1       RcppParallel_5.1.2 cli_3.0.0          stringi_1.5.3      farver_2.1.0       bslib_0.2.4        ellipsis_0.3.2    
## [61] generics_0.1.0     vctrs_0.3.7        rematch2_2.1.2     tools_4.0.5        R.cache_0.14.0     glue_1.4.2         purrr_0.3.4        processx_3.5.1     yaml_2.2.1         inline_0.3.17     
## [71] colorspace_2.0-0   knitr_1.33         sass_0.3.1
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian Networks</title>
      <link>https://www.erikkusch.com/courses/bayes-nets/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/bayes-nets/</guid>
      <description>&lt;p&gt;If you are seeing this page, something went awry in the build of the website. Please find the first course material for this course 
&lt;a href=&#34;https://www.erikkusch.com/courses/bayes-nets/introduction/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Statistical Rethinking</title>
      <link>https://www.erikkusch.com/courses/rethinking/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/rethinking/</guid>
      <description>&lt;p&gt;If you are seeing this page, something went awry in the build of the website. Please find the first course material for this course 
&lt;a href=&#34;https://www.erikkusch.com/courses/rethinking/chapter-02/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bayes Study Group</title>
      <link>https://www.erikkusch.com/project/aubayes/</link>
      <pubDate>Mon, 28 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/project/aubayes/</guid>
      <description>&lt;div class=&#34;alert alert-success&#34;&gt;
  &lt;div&gt;
    This study group has come to an end. All information about it can still be found below. We are now coordinating through Slack to get our fix of all things Bayes. Our Slack Workspace is open to anyone who wants to join us. To do so, simply let me know by &lt;a href=&#34;https://www.erikkusch.com/about#contact&#34;&gt;contacting me&lt;/a&gt; and shortly introduce yourself.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h1 id=&#34;motivation&#34;&gt;Motivation&lt;/h1&gt;
&lt;p&gt;Have you ever read a paper and thought: “Wow, this is really cool”, then read on only to find the word “Bayesian” in the analysis specification and promptly get lost at the terminology and approach to statistics?&lt;/p&gt;
&lt;p&gt;If you did/do, you are obviously not alone and I think it might be time we do something about it. To this end, I have created a Bayes Study Group (formerly AU Bayes Study Group) within which we work our way through (1) a 
&lt;a href=&#34;https://www.youtube.com/playlist?list=PLDcUM9US4XdNM4Edgs7weiyIguLSToZRI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;lecture series&lt;/a&gt; by Bayes Wunderkind Richard McElreath, (2) his corresponding book (“
&lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Statistical Rethinking&lt;/a&gt;”), and (3) a few practical examples in &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;h1 id=&#34;conduct--logistics&#34;&gt;Conduct &amp;amp; Logistics&lt;/h1&gt;
&lt;p&gt;Below, you will find a few key facts outlining how this Bayes Study Group sessions are run. You will find that what we do is quite a lot (I would like to think it quite exhaustive, actually). We are following the example set by some colleagues at the University of Oxford here and I have added some extra bits to the material and schedule. Go big or go home, right?&lt;/p&gt;
&lt;p&gt;That being said, I realise that what I propose is quite a big-time commitment – both in per-week hours and for how long this study group is supposed to run.&lt;/p&gt;
&lt;h2 id=&#34;meetings&#34;&gt;Meetings:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Scheduling:
&lt;ul&gt;
&lt;li&gt;Friday 1300-1600 GMT+1 (sessions end earlier if we are done earlier, of course)&lt;/li&gt;
&lt;li&gt;Weekly for 20 sessions including 4 input talks by practicioneers of Bayesian analyses&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;contents&#34;&gt;Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We follow the material noted further down under Group Material&lt;/li&gt;
&lt;li&gt;We prepare ourselves by working through the material noted for each session&lt;/li&gt;
&lt;li&gt;At the start of each session, either me or a volunteer quickly presents a summary of the preparation material. This does not mean a presenter should have mastered the material at all – simply be able to provide a red-thread through the contents and get a discussion going. We then go into questions/challenges concerning the material, practical examples, and personal implementations.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Lecture Recording “Statistical Rethinking” by Richard McElreath; available 
&lt;a href=&#34;https://www.youtube.com/playlist?list=PLDcUM9US4XdNM4Edgs7weiyIguLSToZRI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Book “Statistical Rethinking” by Richard McElreath; available 
&lt;a href=&#34;https://github.com/Booleans/statistical-rethinking/blob/master/Statistical%20Rethinking%202nd%20Edition.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Practical Examples:
&lt;ul&gt;
&lt;li&gt;Course Material 
&lt;a href=&#34;https://github.com/rmcelreath/stat_rethinking_2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Statistical Rethinking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Book “Introduction to WinBUGS for Ecologists” by Marc Kéry; available 
&lt;a href=&#34;https://www.sciencedirect.com/book/9780123786050/introduction-to-winbugs-for-ecologists&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Book “Bayesian Population Analysis using WinBUGS” by Marc Kéry and Michael Schaub; available 
&lt;a href=&#34;https://www.sciencedirect.com/book/9780123870209/bayesian-population-analysis-using-winbugs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Further Input (all prospective)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian Networks Study Group</title>
      <link>https://www.erikkusch.com/project/bayesnets/</link>
      <pubDate>Mon, 28 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/project/bayesnets/</guid>
      <description>&lt;div class=&#34;alert alert-success&#34;&gt;
  &lt;div&gt;
    This study group has come to an end. All information about it can still be found below. We are now coordinating through Slack to get our fix of all things Bayes. Our Slack Workspace is open to anyone who wants to join us. To do so, simply let me know by &lt;a href=&#34;https://www.erikkusch.com/about#contact&#34;&gt;contacting me&lt;/a&gt; and shortly introduce yourself.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h1 id=&#34;motivation&#34;&gt;Motivation&lt;/h1&gt;
&lt;p&gt;Building on the success of the 
&lt;a href=&#34;https://www.erikkusch.com/project/aubayes/&#34;&gt;Bayes Study Group&lt;/a&gt; I ran two years ago, I have decided to create another study group this time focusing on explicit inference and analysis of Bayesian Networks which I believe are a neat tool for hypothesis testing.&lt;/p&gt;
&lt;h1 id=&#34;conduct--logistics&#34;&gt;Conduct &amp;amp; Logistics&lt;/h1&gt;
&lt;p&gt;Below, you will find a few key facts outlining how this Bayesian Network Study Group sessions are run. You will find that what we do is quite a lot (I would like to think it quite exhaustive, actually).&lt;/p&gt;
&lt;p&gt;That being said, I realise that what I propose is quite a big-time commitment – both in per-week hours and for how long this study group is supposed to run.&lt;/p&gt;
&lt;h2 id=&#34;meetings&#34;&gt;Meetings:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Scheduling:
&lt;ul&gt;
&lt;li&gt;Tuesday 1500-1700 CEST (sessions end earlier if we are done earlier, of course)&lt;/li&gt;
&lt;li&gt;Weekly for 8 sessions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;contents&#34;&gt;Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We follow the material noted further down under Group Material&lt;/li&gt;
&lt;li&gt;We prepare ourselves by working through the material noted for each session the proposed 
&lt;a href=&#34;https://www.erikkusch.com/project/bayesnets/Schedule.pdf&#34;&gt;schedule&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;At the start of each session, either me or a volunteer quickly presents a summary of the preparation material. This does not mean a presenter should have mastered the material at all – simply be able to provide a red-thread through the contents and get a discussion going. We then go into questions/challenges concerning the material, practical examples, and personal implementations.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Book “Bayesian Networks With Examples in R” by Marco Scutari &amp;amp; Jean-Baptiste Denis; available 
&lt;a href=&#34;https://www.routledge.com/Bayesian-Networks-With-Examples-in-R/Scutari-Denis/p/book/9780367366513&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Book “Bayesian Networks in R with Applications in Systems Biology” by Radhakrishnan Nagarajan, Marco Scutari &amp;amp; Sophie Lèbre; available 
&lt;a href=&#34;https://link.springer.com/book/10.1007/978-1-4614-6446-4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
