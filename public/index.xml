<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Erik Kusch</title>
    <link>https://www.erikkusch.com/</link>
      <atom:link href="https://www.erikkusch.com/index.xml" rel="self" type="application/rss+xml" />
    <description>Erik Kusch</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-gb</language><copyright>Â© 2024</copyright><lastBuildDate>Tue, 29 Oct 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://www.erikkusch.com/img/%C3%A5motdalshytta.jpg</url>
      <title>Erik Kusch</title>
      <link>https://www.erikkusch.com/</link>
    </image>
    
    <item>
      <title>Working with the GBIF Backbone</title>
      <link>https://www.erikkusch.com/courses/gbif/backbone/</link>
      <pubDate>Sun, 21 May 2023 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/gbif/backbone/</guid>
      <description>&lt;div class=&#34;alert alert-normal&#34;&gt;
  &lt;div&gt;
    &lt;details&gt;
  &lt;summary&gt;Preamble, Package-Loading, and GBIF API Credential Registering (click here):&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Custom install &amp;amp; load function
install.load.package &amp;lt;- function(x) {
  if (!require(x, character.only = TRUE)) {
    install.packages(x, repos = &amp;quot;http://cran.us.r-project.org&amp;quot;)
  }
  require(x, character.only = TRUE)
}
## names of packages we want installed (if not installed yet) and loaded
package_vec &amp;lt;- c(
  &amp;quot;rgbif&amp;quot;,
  &amp;quot;knitr&amp;quot; # for rmarkdown table visualisations
)
## executing install &amp;amp; load for each package
sapply(package_vec, install.load.package)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## rgbif knitr 
##  TRUE  TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;options(gbif_user = &amp;quot;my gbif username&amp;quot;)
options(gbif_email = &amp;quot;my registred gbif e-mail&amp;quot;)
options(gbif_pwd = &amp;quot;my gbif password&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt; 
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The ways in which we record and report species identities is arguably more varied than recorded species identities themselves. For example, while the binomial nomenclature is widely adopted across scientific research, the same species may be still be referred to via different binomial names with descriptor or subspecies suffixes. In addition, particularly when dealing with citizen science data, species names may not always be recorded according to the binomial nomenclature but rather via vernacular names.&lt;/p&gt;
&lt;p&gt;The 
&lt;a href=&#34;https://www.gbif.org/dataset/d7dddbf4-2cf0-4f39-9b2a-bb099caae36c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GBIF Backbone Taxonomy&lt;/a&gt; circumvents these issues on the data-management side as it assigns unambiguous keys to taxonomic units of interest - these are known as &lt;strong&gt;taxonKeys&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;alert alert-danger&#34;&gt;
  &lt;div&gt;
    GBIF recognises taxonomic units via unique identifiers which are linked to more commonly used names and descriptors.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Matching between what you require and how GBIF indexes its data is therefore vital to ensure you retrieve the data you need accurately and in full. To discover data themselves, we first need to discover their corresponding relevant identifiers.&lt;/p&gt;
&lt;h2 id=&#34;finding-the-taxonkeys&#34;&gt;Finding the &lt;strong&gt;taxonKeys&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;To identify the relevant taxonKeys for our 
&lt;a href=&#34;https://www.erikkusch.com/courses/gbif/#study-organism&#34;&gt;study organism&lt;/a&gt; (&lt;em&gt;Lagopus muta&lt;/em&gt;), we will use the &lt;code&gt;name_backbone(...)&lt;/code&gt; function to match our binomial species name to the GBIF backbone as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sp_name &amp;lt;- &amp;quot;Lagopus muta&amp;quot;
sp_backbone &amp;lt;- name_backbone(name = sp_name)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s look at the output of this function call:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;knitr::kable(sp_backbone)
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right&#34;&gt;usageKey&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;scientificName&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;canonicalName&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;rank&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;status&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;confidence&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;matchType&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;kingdom&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;phylum&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;order&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;family&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;genus&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;species&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;kingdomKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;phylumKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;classKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;orderKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;familyKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;genusKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;speciesKey&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;synonym&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;class&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;verbatim_name&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta (Montin, 1781)&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SPECIES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;ACCEPTED&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;99&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EXACT&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Animalia&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Chordata&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Galliformes&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Phasianidae&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;44&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;212&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;723&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;9331&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2473369&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Aves&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The data frame / &lt;code&gt;tibble&lt;/code&gt; returned by the &lt;code&gt;name_backbone(...)&lt;/code&gt; function contains important information regarding the confidence and type of match achieved between the input species name and the GBIF backbone. In addition, it lists all relevant taxonKeys. Of particular to most use-cases are the following columns:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;usageKey&lt;/code&gt;: The taxonKey by which this species is indexed in the GBIF backbone.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;matchType&lt;/code&gt;: This can be either:
&lt;ul&gt;
&lt;li&gt;EXACT: binomial input matched 1-1 to backbone&lt;/li&gt;
&lt;li&gt;FUZZY: binomial input was matched to backbone assuming misspelling&lt;/li&gt;
&lt;li&gt;HIGHERRANK: binomial input is not a species-level name, but indexes a higher-rank taxonomic group&lt;/li&gt;
&lt;li&gt;NONE: no match could be made&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s extract the &lt;code&gt;usageKey&lt;/code&gt; of &lt;em&gt;Lagopus muta&lt;/em&gt; in the GBIF backbone for later use in this workshop.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sp_key &amp;lt;- sp_backbone$usageKey
sp_key
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5227679
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-success&#34;&gt;
  &lt;div&gt;
    We now have a unique identifier for &lt;em&gt;Lagpus muta&lt;/em&gt; which we can use to query GBIF for data.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;resolving-taxonomic-names&#34;&gt;Resolving Taxonomic Names&lt;/h2&gt;
&lt;p&gt;Not all species identities are as straightforwardly matched to the GBIF backbone and there is more information stored in the GBIF backbone which may be relevant to users. Here, I would like to spend some time delving further into these considerations.&lt;/p&gt;
&lt;h3 id=&#34;matching-input-to-backbone&#34;&gt;Matching Input to Backbone&lt;/h3&gt;
&lt;p&gt;To widen the backbone matching, we can set &lt;code&gt;verbose = TRUE&lt;/code&gt; in the &lt;code&gt;name_backbone(...)&lt;/code&gt; function. Doing so for &lt;em&gt;Lagopus muta&lt;/em&gt;, we obtain the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sp_backbone &amp;lt;- name_backbone(name = sp_name, verbose = TRUE)
knitr::kable(sp_backbone)
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right&#34;&gt;usageKey&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;scientificName&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;canonicalName&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;rank&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;status&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;confidence&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;matchType&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;kingdom&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;phylum&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;order&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;family&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;genus&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;species&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;kingdomKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;phylumKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;classKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;orderKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;familyKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;genusKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;speciesKey&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;synonym&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;class&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;verbatim_name&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta (Montin, 1781)&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SPECIES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;ACCEPTED&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;99&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EXACT&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Animalia&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Chordata&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Galliformes&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Phasianidae&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;44&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;212&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;723&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;9331&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2473369&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Aves&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Seems like, even with widened backbone matching, &lt;em&gt;Lagopus muta&lt;/em&gt; is precise enough of a specification for there to be one direct match.&lt;/p&gt;
&lt;p&gt;To demonstrate how this widened backbone matching can result in multiple matches, let&amp;rsquo;s consider &lt;em&gt;Calluna vulgaris&lt;/em&gt; - the common heather and my favourite plant:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sp_backbone2 &amp;lt;- name_backbone(name = &amp;quot;Calluna vulgaris&amp;quot;, verbose = TRUE)
knitr::kable(t(sp_backbone2))
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;1&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;2&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;3&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;4&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;usageKey&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2882482&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;8208549&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;3105380&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;7918820&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;scientificName&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Calluna vulgaris (L.) Hull&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Calluna vulgaris Salisb., 1802&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Carlina vulgaris L.&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Carlina vulgaris Schur&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;canonicalName&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Calluna vulgaris&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Calluna vulgaris&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Carlina vulgaris&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Carlina vulgaris&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;rank&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SPECIES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SPECIES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SPECIES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SPECIES&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;status&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;ACCEPTED&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SYNONYM&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;ACCEPTED&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;DOUBTFUL&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;confidence&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;97&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;97&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;70&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;64&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;matchType&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EXACT&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EXACT&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FUZZY&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FUZZY&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;kingdom&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Plantae&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Plantae&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Plantae&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Plantae&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;phylum&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Tracheophyta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Tracheophyta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Tracheophyta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Tracheophyta&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;order&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Ericales&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Ericales&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Asterales&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Asterales&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;family&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Ericaceae&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Ericaceae&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Asteraceae&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Asteraceae&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;genus&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Calluna&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Calluna&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Carlina&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Carlina&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;species&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Calluna vulgaris&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Calluna vulgaris&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Carlina vulgaris&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Carlina vulgaris&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;kingdomKey&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;6&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;6&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;6&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;phylumKey&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;7707728&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;7707728&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;7707728&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;7707728&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;classKey&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;220&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;220&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;220&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;220&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;orderKey&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1353&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1353&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;414&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;414&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;familyKey&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2505&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2505&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;3065&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;3065&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;genusKey&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2882481&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2882481&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;3105349&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;3105349&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;speciesKey&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2882482&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2882482&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;3105380&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;7918820&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;synonym&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;TRUE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;class&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Magnoliopsida&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Magnoliopsida&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Magnoliopsida&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Magnoliopsida&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;acceptedUsageKey&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2882482&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;verbatim_name&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Calluna vulgaris&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Calluna vulgaris&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Calluna vulgaris&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Calluna vulgaris&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Here, you can see how fuzzy matching has resulted in an erroneous match with a different plant: &lt;em&gt;Carlina vulgaris&lt;/em&gt; - the thistle - also a neat plant, but not the one I was after here.&lt;/p&gt;
&lt;h3 id=&#34;competing-name-matches&#34;&gt;Competing Name Matches&lt;/h3&gt;
&lt;p&gt;By horribly misspelling our binomial input, we can coerce an output of match type FUZZY (a match achieved with deviations to the supplied string) or HIGHERRANK  (a match indexing the Genus itself):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;knitr::kable(name_backbone(&amp;quot;Lagopus mut&amp;quot;, verbose = TRUE))
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right&#34;&gt;confidence&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;matchType&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;synonym&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;usageKey&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;scientificName&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;canonicalName&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;rank&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;status&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;kingdom&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;phylum&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;order&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;family&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;genus&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;kingdomKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;phylumKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;classKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;orderKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;familyKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;genusKey&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;class&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;acceptedUsageKey&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;verbatim_name&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;100&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NONE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus mut&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;69&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EXACT&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2473369&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus Brisson, 1760&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;GENUS&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;ACCEPTED&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Animalia&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Chordata&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Galliformes&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Phasianidae&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;44&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;212&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;723&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;9331&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2473369&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Aves&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus mut&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;68&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EXACT&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;TRUE&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3233696&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus (Gren. &amp;amp; Godr.) Fourr.&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;GENUS&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SYNONYM&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Plantae&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Tracheophyta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lamiales&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Plantaginaceae&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Plantago&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;6&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;7707728&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;220&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;408&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2420&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3189695&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Magnoliopsida&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3189695&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus mut&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;68&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EXACT&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;TRUE&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3233247&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus Bernh.&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;GENUS&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SYNONYM&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Plantae&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Tracheophyta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Fabales&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Fabaceae&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Trifolium&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;6&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;7707728&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;220&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1370&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5386&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2973363&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Magnoliopsida&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2973363&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus mut&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;68&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EXACT&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;TRUE&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;8132572&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus Hill&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;GENUS&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SYNONYM&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Plantae&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Tracheophyta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Fabales&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Fabaceae&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Trifolium&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;6&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;7707728&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;220&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1370&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5386&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2973363&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Magnoliopsida&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2973363&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus mut&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;68&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EXACT&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;TRUE&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;6007644&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus Reichenbach, 1817&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;GENUS&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SYNONYM&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Animalia&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Arthropoda&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lepidoptera&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Noctuidae&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Callopistria&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;54&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;216&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;797&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;7015&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;8875134&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Insecta&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;8875134&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus mut&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;43&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FUZZY&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;TRUE&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4825684&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagomus McEnery, 1859&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagomus&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;GENUS&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SYNONYM&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Animalia&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Chordata&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagomorpha&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Prolagidae&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Prolagus&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;44&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;359&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;785&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5468&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2436678&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Mammalia&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2436678&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus mut&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;38&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FUZZY&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4834660&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagodus Pomel, 1852&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagodus&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;GENUS&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;DOUBTFUL&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Animalia&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Chordata&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagodus&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;44&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;359&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4834660&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Mammalia&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus mut&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;To truly see how competing name identifiers can cause us to struggle identifying the correct &lt;code&gt;usageKey&lt;/code&gt; we must turn away from &lt;em&gt;Lagopus muta&lt;/em&gt;. Instead, let us look at &lt;em&gt;Glocianus punctiger&lt;/em&gt; - a species of weevil:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;knitr::kable(name_backbone(&amp;quot;Glocianus punctiger&amp;quot;, verbose = TRUE))
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right&#34;&gt;usageKey&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;scientificName&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;canonicalName&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;rank&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;status&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;confidence&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;matchType&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;kingdom&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;phylum&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;order&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;family&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;kingdomKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;phylumKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;classKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;orderKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;familyKey&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;synonym&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;class&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;acceptedUsageKey&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;genus&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;species&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;genusKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;speciesKey&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;verbatim_name&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4239&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Curculionidae&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Curculionidae&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FAMILY&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;ACCEPTED&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;97&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;HIGHERRANK&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Animalia&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Arthropoda&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Coleoptera&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Curculionidae&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;54&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;216&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1470&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4239&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Insecta&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Glocianus punctiger&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;11356251&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Glocianus punctiger (C.R.Sahlberg, 1835)&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Glocianus punctiger&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SPECIES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SYNONYM&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;97&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EXACT&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Animalia&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Arthropoda&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Coleoptera&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Curculionidae&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;54&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;216&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1470&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4239&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;TRUE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Insecta&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1187423&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Rhynchaenus&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Rhynchaenus punctiger&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1187150&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1187423&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Glocianus punctiger&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4464480&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Glocianus punctiger (Gyllenhal, 1837)&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Glocianus punctiger&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SPECIES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SYNONYM&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;97&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EXACT&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Animalia&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Arthropoda&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Coleoptera&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Curculionidae&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;54&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;216&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1470&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4239&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;TRUE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Insecta&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1178810&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Ceuthorhynchus&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Ceuthorhynchus punctiger&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;8265946&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1178810&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Glocianus punctiger&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Here, we find that there exist two competing identifiers for &lt;em&gt;Glocianus punctiger&lt;/em&gt; in the GBIF backbone in accordance with their competing descriptors. To query data for all &lt;em&gt;Glocianus punctiger&lt;/em&gt; records, we should thus always use the keys 11356251 and 4464480.&lt;/p&gt;
&lt;h3 id=&#34;matching-names-and-backbone-for-several-species&#34;&gt;Matching Names and Backbone for several Species&lt;/h3&gt;
&lt;p&gt;The above use of &lt;code&gt;name_backbone(...)&lt;/code&gt; can be executed for multiple species at once using instead the &lt;code&gt;name_backbone_checklist(...)&lt;/code&gt; function. So let&amp;rsquo;s do so for our target species as well as my favourite plant:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;checklist_df &amp;lt;- name_backbone_checklist(c(sp_name, &amp;quot;Calluna vulgaris&amp;quot;))
knitr::kable(checklist_df)
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right&#34;&gt;usageKey&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;scientificName&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;canonicalName&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;rank&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;status&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;confidence&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;matchType&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;kingdom&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;phylum&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;order&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;family&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;genus&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;species&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;kingdomKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;phylumKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;classKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;orderKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;familyKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;genusKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;speciesKey&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;synonym&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;class&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;verbatim_name&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;verbatim_index&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta (Montin, 1781)&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SPECIES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;ACCEPTED&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;99&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EXACT&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Animalia&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Chordata&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Galliformes&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Phasianidae&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;44&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;212&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;723&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;9331&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2473369&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Aves&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2882482&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Calluna vulgaris (L.) Hull&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Calluna vulgaris&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SPECIES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;ACCEPTED&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;97&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EXACT&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Plantae&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Tracheophyta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Ericales&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Ericaceae&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Calluna&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Calluna vulgaris&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;6&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;7707728&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;220&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1353&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2505&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2882481&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2882482&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Magnoliopsida&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Calluna vulgaris&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div class=&#34;alert alert-success&#34;&gt;
  &lt;div&gt;
    It is best practise to carefully investigate the match between your binomial input and the GBIF backbone.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;name-suggestions-and-lookup&#34;&gt;Name Suggestions and Lookup&lt;/h2&gt;
&lt;p&gt;To catch other commonly used or relevant names for a species of interest, you can use the &lt;code&gt;name_suggest(...)&lt;/code&gt; function. This is particularly useful when data mining publications or data sets for records which can be grouped to the same species although they might be recorded with different names:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sp_suggest &amp;lt;- name_suggest(sp_name)$data
knitr::kable(t(head(sp_suggest)))
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;key&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;5227684&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;5227686&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;5227713&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;5227710&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;7397817&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;canonicalName&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta rupestris&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta townsendi&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta welchi&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta pyrenaica&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta macruros&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;rank&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SPECIES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SUBSPECIES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SUBSPECIES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SUBSPECIES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SUBSPECIES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SUBSPECIES&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;To trawl GBIF mediated data sets for records of a specific species, one may use the &lt;code&gt;name_lookup(...)&lt;/code&gt; function:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sp_lookup &amp;lt;- name_lookup(sp_name)$data
knitr::kable(head(sp_lookup))
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right&#34;&gt;key&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;scientificName&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;nameKey&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;datasetKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;nubKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;parentKey&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;parent&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;kingdom&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;order&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;family&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;species&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;kingdomKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;orderKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;familyKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;speciesKey&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;canonicalName&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;authorship&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;nameType&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;taxonomicStatus&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;rank&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;origin&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;numDescendants&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;numOccurrences&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;habitats&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;nomenclaturalStatus&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;threatStatuses&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;synonym&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;classKey&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;taxonID&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;extinct&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;class&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;phylum&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;genus&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;phylumKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;genusKey&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;constituentKey&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;publishedIn&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;basionymKey&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;basionym&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;accordingTo&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;acceptedKey&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;accepted&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;123212008&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5972798&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;a5dd063e-f45b-4a54-8b94-8fa3adf7f1e1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;167183824&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Phasianidae&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Animalia&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Galliformes&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Phasianidae&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;167183684&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;167183822&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;167183824&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;123212008&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SCIENTIFIC&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;ACCEPTED&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SPECIES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SOURCE&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;133167086&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5972798&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;47f16512-bf31-410f-b272-d151c996b2f6&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;135274878&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Phasianidae&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Animalia&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Galliformes&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Phasianidae&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;135274602&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;135274874&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;135274878&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;133167086&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SCIENTIFIC&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;ACCEPTED&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SPECIES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SOURCE&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;135274603&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1613&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Aves&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;119341248&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5972798&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;4f1047ac-a19d-41a8-98eb-d968b2548b53&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SCIENTIFIC&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;ACCEPTED&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SOURCE&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NEAR_THREATENED&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;104151733&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;fab88965-e69d-4491-a04d-e3198b626e52&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;104151714&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Metazoa&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Galliformes&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Phasianidae&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;103832354&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;104149839&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;104150497&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;104151733&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SCIENTIFIC&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;ACCEPTED&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SPECIES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SOURCE&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;104106614&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;64668&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Aves&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Chordata&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;103882489&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;104151714&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;177659687&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;6b6b2923-0a10-4708-b170-5b7c611aceef&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;177659682&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Metazoa&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Galliformes&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Phasianidae&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;177651702&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;177659367&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;177659587&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;177659687&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SCIENTIFIC&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;ACCEPTED&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SPECIES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SOURCE&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;177656782&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;64668&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Aves&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Chordata&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;177654008&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;177659682&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;100160233&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5972798&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;d7435f14-dfc9-4aaa-bef3-5d1ed22d65bf&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;128727594&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Animalia&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Phasianidae&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;128725468&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;128727593&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;100160233&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SCIENTIFIC&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;ACCEPTED&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SPECIES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SOURCE&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;128725469&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Aves&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;128727594&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Here, we see clearly that &lt;em&gt;Lagopus muta&lt;/em&gt; is recorded slightly differently in the datasets mediated by GBIF, but are indexed just fine for GBIF to find them for us.&lt;/p&gt;
&lt;p&gt;Lastly, to gain a better understanding of the variety of vernacular names by which our species is know, we can use the &lt;code&gt;name_usage(..., data = &amp;quot;vernacularnames&amp;quot;)&lt;/code&gt; function as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sp_usage &amp;lt;- name_usage(key = sp_key, data = &amp;quot;vernacularNames&amp;quot;)$data
knitr::kable(head(sp_usage))
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right&#34;&gt;taxonKey&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;vernacularName&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;language&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;source&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;sourceTaxonKey&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;country&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;area&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;preferred&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Alpenschneehuhn&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;deu&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Multilingual IOC World Bird List, v11.2&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;123212008&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Alpenschneehuhn&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;deu&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Taxon list of animals with German names (worldwide) compiled at the SMNS&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;116803956&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;DE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Alpenschneehuhn&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;deu&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EUNIS Biodiversity Database&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;101137652&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Alpensneeuwhoen&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;nld&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EUNIS Biodiversity Database&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;101137652&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Alpensneeuwhoen&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;nld&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Multilingual IOC World Bird List, v11.2&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;123212008&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Fjeldrype&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;dan&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Multilingual IOC World Bird List, v11.2&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;123212008&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;code&gt;name_usage(...)&lt;/code&gt; can be tuned to output different information and its documentation gives a good overview of this. Simply call &lt;code&gt;?name_usage&lt;/code&gt; to look for yourself.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian Network Inference</title>
      <link>https://www.erikkusch.com/courses/bayes-nets/inference/</link>
      <pubDate>Tue, 08 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/bayes-nets/inference/</guid>
      <description>&lt;style&gt;

blockquote{
color:#633a00;
}

&lt;/style&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;p&gt;Most of the material in these chapters has already been covered in previous material, so the following summary is rather brief:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/content/courses/bayes-nets/8-Bayesian-Network-Inference_08-11-22.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bayesian Network Inference&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Please refer to earlier material for introductions of queries, structure learning, and parameter learning in theory and in &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;exercises&#34;&gt;Exercises&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to the exercises at the end of chapter 4 in 
&lt;a href=&#34;https://link.springer.com/book/10.1007/978-1-4614-6446-4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bayesian Networks in R with Applications in Systems Biology&lt;/a&gt; by by Radhakrishnan Nagarajan, Marco Scutari &amp;amp; Sophie LÃ¨bre and Part 4 in 
&lt;a href=&#34;https://www.bnlearn.com/book-crc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bayesian Networks with Examples in R&lt;/a&gt; by M. Scutari and J.-B. Denis. Much of my inspiration for these solutions, where necessary, by consulting the solutions provided by the authors themselves as in the appendix.&lt;/p&gt;
&lt;h3 id=&#34;r-environment&#34;&gt;&lt;code&gt;R&lt;/code&gt; Environment&lt;/h3&gt;
&lt;p&gt;For today&amp;rsquo;s exercise, I load the following packages:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(bnlearn)
library(gRain)
library(GeneNet)
library(penalized)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;nagarajan-41&#34;&gt;Nagarajan 4.1&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Apply the junction tree algorithm to the validated network structure from Sachs et al. (2005), and draw the resulting undirected triangulated graph.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Taken directly from the solutions:&lt;/p&gt;
&lt;img src=&#34;https://www.erikkusch.com/courses/bayes-nets/Nagarajan4_1.JPG&#34; width=&#34;900&#34;/&gt;
&lt;h3 id=&#34;nagarajan-42&#34;&gt;Nagarajan 4.2&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Consider the Sachs et al. (2005) data used in Sect. 4.2.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;First, let&amp;rsquo;s read the data in like it was done in the book:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;isachs &amp;lt;- read.table(&amp;quot;sachs.interventional.txt&amp;quot;, header = TRUE, colClasses = &amp;quot;factor&amp;quot;)
isachs &amp;lt;- isachs[, 1:11]
for (i in names(isachs)) {
  levels(isachs[, i]) &amp;lt;- c(&amp;quot;LOW&amp;quot;, &amp;quot;AVG&amp;quot;, &amp;quot;HIGH&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This .txt file can be downloaded from 
&lt;a href=&#34;https://www.bnlearn.com/book-useR/code/sachs.interventional.txt.gz&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h4 id=&#34;part-a&#34;&gt;Part A&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Perform parameter learning with the &lt;code&gt;bn.fit&lt;/code&gt; function from &lt;code&gt;bnlearn&lt;/code&gt; and the validated network structure. How do the maximum likelihood estimates differ from the Bayesian ones, and how do the latter vary as the imaginary sample size increases?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sachs_DAG &amp;lt;- model2network(paste0(
  &amp;quot;[PKC][PKA|PKC][praf|PKC:PKA]&amp;quot;,
  &amp;quot;[pmek|PKC:PKA:praf][p44.42|pmek:PKA]&amp;quot;,
  &amp;quot;[pakts473|p44.42:PKA][P38|PKC:PKA]&amp;quot;,
  &amp;quot;[pjnk|PKC:PKA][plcg][PIP3|plcg]&amp;quot;,
  &amp;quot;[PIP2|plcg:PIP3]&amp;quot;
))
f4.1_mle &amp;lt;- bn.fit(sachs_DAG, isachs, method = &amp;quot;mle&amp;quot;)
f4.1_bayes1 &amp;lt;- bn.fit(sachs_DAG, isachs, method = &amp;quot;bayes&amp;quot;, iss = 1)
f4.1_bayes10 &amp;lt;- bn.fit(sachs_DAG, isachs, method = &amp;quot;bayes&amp;quot;, iss = 10)
f4.1_bayes100 &amp;lt;- bn.fit(sachs_DAG, isachs, method = &amp;quot;bayes&amp;quot;, iss = 100)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I omit the outputs of the individual objects created above here for space.&lt;/p&gt;
&lt;p&gt;From a theoretical standpoint mle estimates may contain NA values while bayes-inferred estimates do not. That being said, I did not see any NA outputs in the maximum likelihood estimates here.&lt;/p&gt;
&lt;p&gt;As far as iss is concerned, higher iss values result in smoother estimates.&lt;/p&gt;
&lt;h4 id=&#34;part-b&#34;&gt;Part B&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Node &lt;code&gt;PKA&lt;/code&gt; is parent of all the nodes in the &lt;code&gt;praf â pmek â p44.42 â pakts473&lt;/code&gt; chain. Use the junction tree algorithm to explore how our beliefs on those nodes change when we have evidence that &lt;code&gt;PKA&lt;/code&gt; is &lt;code&gt;âLOWâ&lt;/code&gt;, and when &lt;code&gt;PKA&lt;/code&gt; is &lt;code&gt;âHIGHâ&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mle_jtree &amp;lt;- compile(as.grain(f4.1_mle))
query &amp;lt;- c(&amp;quot;praf&amp;quot;, &amp;quot;pmek&amp;quot;, &amp;quot;p44.42&amp;quot;, &amp;quot;pakts473&amp;quot;)

## baseline query
querygrain(mle_jtree, nodes = query)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $pmek
## pmek
##       LOW       AVG      HIGH 
## 0.5798148 0.3066667 0.1135185 
## 
## $praf
## praf
##       LOW       AVG      HIGH 
## 0.5112963 0.2835185 0.2051852 
## 
## $p44.42
## p44.42
##       LOW       AVG      HIGH 
## 0.1361111 0.6062963 0.2575926 
## 
## $pakts473
## pakts473
##        LOW        AVG       HIGH 
## 0.60944444 0.31037037 0.08018519
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## low evidence
mle_jprop &amp;lt;- setFinding(mle_jtree, nodes = &amp;quot;PKA&amp;quot;, states = &amp;quot;LOW&amp;quot;)
querygrain(mle_jprop, nodes = query)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $pmek
## pmek
##        LOW        AVG       HIGH 
## 0.35782443 0.08874046 0.55343511 
## 
## $praf
## praf
##       LOW       AVG      HIGH 
## 0.1145038 0.1746183 0.7108779 
## 
## $p44.42
## p44.42
##       LOW       AVG      HIGH 
## 0.3435115 0.1965649 0.4599237 
## 
## $pakts473
## pakts473
##       LOW       AVG      HIGH 
## 0.2967557 0.2977099 0.4055344
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## high evidence
mle_jprop &amp;lt;- setFinding(mle_jtree, nodes = &amp;quot;PKA&amp;quot;, states = &amp;quot;HIGH&amp;quot;)
querygrain(mle_jprop, nodes = query)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $pmek
## pmek
##         LOW         AVG        HIGH 
## 0.981418919 0.016891892 0.001689189 
## 
## $praf
## praf
##        LOW        AVG       HIGH 
## 0.83614865 0.13006757 0.03378378 
## 
## $p44.42
## p44.42
##        LOW        AVG       HIGH 
## 0.07263514 0.68918919 0.23817568 
## 
## $pakts473
## pakts473
##       LOW       AVG      HIGH 
## 0.7652027 0.2347973 0.0000000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;PKA&lt;/code&gt; inhibits all other nodes. When &lt;code&gt;PKA&lt;/code&gt; is &lt;code&gt;HIGH&lt;/code&gt; then the &lt;code&gt;LOW&lt;/code&gt; probability of all other nodes increases.&lt;/p&gt;
&lt;p&gt;When &lt;code&gt;PKA&lt;/code&gt; is &lt;code&gt;HIGH&lt;/code&gt;, the activity of all the proteins corresponding to the query nodes is inhibited (the &lt;code&gt;LOW&lt;/code&gt; probability increases and the &lt;code&gt;HIGH&lt;/code&gt; decreases). When &lt;code&gt;PKA&lt;/code&gt; is &lt;code&gt;LOW&lt;/code&gt;, the opposite is true (the LOW probability decreases and the &lt;code&gt;HIGH&lt;/code&gt; increases).&lt;/p&gt;
&lt;h4 id=&#34;part-c&#34;&gt;Part C&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Similarly, explore the effects on &lt;code&gt;pjnk&lt;/code&gt; of evidence on &lt;code&gt;PIP2&lt;/code&gt;, &lt;code&gt;PIP3&lt;/code&gt;, and &lt;code&gt;plcg&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mle_jprop &amp;lt;- setFinding(mle_jtree,
  nodes = c(&amp;quot;PIP2&amp;quot;, &amp;quot;PIP3&amp;quot;, &amp;quot;plcg&amp;quot;),
  states = rep(&amp;quot;LOW&amp;quot;, 3)
)

## baseline query
querygrain(mle_jtree, nodes = &amp;quot;pjnk&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $pjnk
## pjnk
##        LOW        AVG       HIGH 
## 0.53944444 0.38277778 0.07777778
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## low evidence
querygrain(mle_jprop, nodes = &amp;quot;pjnk&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $pjnk
## pjnk
##        LOW        AVG       HIGH 
## 0.53944444 0.38277778 0.07777778
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Turns out &lt;code&gt;pjnk&lt;/code&gt; is unaffected by the others. The DAG shown in the answers to exercise Nagarajan 4.1 supports this.&lt;/p&gt;
&lt;h3 id=&#34;nagarajan-43&#34;&gt;Nagarajan 4.3&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Consider the marks data set analyzed in Sect. 2.3.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(marks)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-a-1&#34;&gt;Part A&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Learn both the network structure and the parameters with likelihood based approaches, i.e., BIC or AIC, for structure learning and maximum likelihood estimates for the parameters.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;f4.3_dag &amp;lt;- hc(marks, score = &amp;quot;bic-g&amp;quot;)
f4.3_dag
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   Bayesian network learned via Score-based methods
## 
##   model:
##    [MECH][VECT|MECH][ALG|MECH:VECT][ANL|ALG][STAT|ALG:ANL] 
##   nodes:                                 5 
##   arcs:                                  6 
##     undirected arcs:                     0 
##     directed arcs:                       6 
##   average markov blanket size:           2.40 
##   average neighbourhood size:            2.40 
##   average branching factor:              1.20 
## 
##   learning algorithm:                    Hill-Climbing 
##   score:                                 BIC (Gauss.) 
##   penalization coefficient:              2.238668 
##   tests used in the learning procedure:  34 
##   optimized:                             TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;f4.3_bn &amp;lt;- bn.fit(f4.3_dag, marks)
f4.3_bn
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   Bayesian network parameters
## 
##   Parameters of node MECH (Gaussian distribution)
## 
## Conditional density: MECH
## Coefficients:
## (Intercept)  
##    38.95455  
## Standard deviation of the residuals: 17.48622 
## 
##   Parameters of node VECT (Gaussian distribution)
## 
## Conditional density: VECT | MECH
## Coefficients:
## (Intercept)         MECH  
##  34.3828788    0.4160755  
## Standard deviation of the residuals: 11.01373 
## 
##   Parameters of node ALG (Gaussian distribution)
## 
## Conditional density: ALG | MECH + VECT
## Coefficients:
## (Intercept)         MECH         VECT  
##  25.3619809    0.1833755    0.3577122  
## Standard deviation of the residuals: 8.080725 
## 
##   Parameters of node ANL (Gaussian distribution)
## 
## Conditional density: ANL | ALG
## Coefficients:
## (Intercept)          ALG  
##   -3.574130     0.993156  
## Standard deviation of the residuals: 10.50248 
## 
##   Parameters of node STAT (Gaussian distribution)
## 
## Conditional density: STAT | ALG + ANL
## Coefficients:
## (Intercept)          ALG          ANL  
## -11.1920114    0.7653499    0.3164056  
## Standard deviation of the residuals: 12.60646
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-b-1&#34;&gt;Part B&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Query the network learned in the previous point for the probability to have the marks for both &lt;code&gt;STAT&lt;/code&gt; and &lt;code&gt;MECH&lt;/code&gt; above 60, given evidence that the mark for &lt;code&gt;ALG&lt;/code&gt; is at most 60. Are the two variables independent given the evidence on &lt;code&gt;ALG&lt;/code&gt;?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cpquery(f4.3_bn, event = (STAT &amp;gt; 60) &amp;amp; (MECH &amp;gt; 60), evidence = (ALG &amp;lt;= 60), n = 1e7)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.009562571
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cpquery(f4.3_bn, event = (STAT &amp;gt; 60), evidence = (ALG &amp;lt;= 60), n = 1e7)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.08289571
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cpquery(f4.3_bn, event = (MECH &amp;gt; 60), evidence = (ALG &amp;lt;= 60), n = 1e7)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0683385
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The conditional probability of the two outcomes (0.0095912) is not the same as the product of their corresponding marginal probabilities (0.0056668). Conclusively, we can say that &lt;code&gt;STAT&lt;/code&gt; and &lt;code&gt;MECH&lt;/code&gt; are not independent conditional on &lt;code&gt;ALG&lt;/code&gt;.&lt;/p&gt;
&lt;h4 id=&#34;part-c-1&#34;&gt;Part C&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;What is the (conditional) probability of having an average vote (in the [60,70] range) in both &lt;code&gt;VECT&lt;/code&gt; and &lt;code&gt;MECH&lt;/code&gt; while having an outstanding vote in &lt;code&gt;ALG&lt;/code&gt; (at least 90)?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cpquery(f4.3_bn,
  event = ((MECH &amp;gt;= 60) &amp;amp; (MECH &amp;lt;= 70)) | ((VECT &amp;gt;= 60) &amp;amp; (VECT &amp;lt;= 70)),
  evidence = (ALG &amp;gt;= 90),
  n = 1e7
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2872254
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;nagarajan-44&#34;&gt;Nagarajan 4.4&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Using the dynamic Bayesian network &lt;code&gt;dbn2&lt;/code&gt; from Sect. 4.3, investigate the effects of genes &lt;code&gt;257710_at&lt;/code&gt; and &lt;code&gt;255070_at&lt;/code&gt; observed at time t-2 on gene &lt;code&gt;265768_at&lt;/code&gt; at time t.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is the network in the chapter according to the errata corrige 
&lt;a href=&#34;https://www.bnlearn.com/book-useR/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(arth800)
subset &amp;lt;- c(60, 141, 260, 333, 365, 424, 441, 512, 521, 578, 789, 799)
arth12 &amp;lt;- arth800.expr[, subset]
x &amp;lt;- arth12[1:(nrow(arth12) - 2), ]
y &amp;lt;- arth12[-(1:2), &amp;quot;265768_at&amp;quot;]
lambda &amp;lt;- optL1(response = y, penalized = x, trace = FALSE)$lambda
lasso.t &amp;lt;- penalized(response = y, penalized = x, lambda1 = lambda, trace = FALSE)
y &amp;lt;- arth12[-(1:2), &amp;quot;245094_at&amp;quot;]
colnames(x)[12] &amp;lt;- &amp;quot;245094_at1&amp;quot;
lambda &amp;lt;- optL1(response = y, penalized = x, trace = FALSE)$lambda
lasso.s &amp;lt;- penalized(response = y, penalized = x, lambda1 = lambda, trace = FALSE)
## errate comes in here
dbn2 &amp;lt;- empty.graph(c(
  &amp;quot;265768_at&amp;quot;, &amp;quot;245094_at1&amp;quot;,
  &amp;quot;258736_at&amp;quot;, &amp;quot;257710_at&amp;quot;, &amp;quot;255070_at&amp;quot;,
  &amp;quot;245319_at&amp;quot;, &amp;quot;245094_at&amp;quot;
))
dbn2 &amp;lt;- set.arc(dbn2, &amp;quot;245094_at&amp;quot;, &amp;quot;265768_at&amp;quot;)
for (node in names(coef(lasso.s))[-c(1, 6)]) {
  dbn2 &amp;lt;- set.arc(dbn2, node, &amp;quot;245094_at&amp;quot;)
}
dbn2 &amp;lt;- set.arc(dbn2, &amp;quot;245094_at1&amp;quot;, &amp;quot;245094_at&amp;quot;)
dbn2.data &amp;lt;- as.data.frame(x[, nodes(dbn2)[1:6]])
dbn2.data[, &amp;quot;245094_at&amp;quot;] &amp;lt;- y
dbn2.data[, &amp;quot;245094_at1&amp;quot;] &amp;lt;- arth12[2:(nrow(arth12) - 1), &amp;quot;245094_at&amp;quot;]
dbn2.fit &amp;lt;- bn.fit(dbn2, dbn2.data)
## errata stops here
dbn2.fit[[&amp;quot;265768_at&amp;quot;]] &amp;lt;- lasso.t
dbn2.fit[[&amp;quot;245094_at&amp;quot;]] &amp;lt;- lasso.s
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is the solution to the exercise:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(42)
cpquery(dbn2.fit, event = (`265768_at` &amp;gt; 8), evidence = (`257710_at` &amp;gt; 8))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3590734
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cpquery(dbn2.fit, event = (`265768_at` &amp;gt; 8), evidence = (`255070_at` &amp;gt; 8))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5753049
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cpquery(dbn2.fit, event = (`265768_at` &amp;gt; 8), evidence = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.4396
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;High expression levels of gene 257710_at at time t â2 reduce the probability of high expression levels of gene 265768_at at time t; the opposite is true for gene 255070_at.&lt;/p&gt;
&lt;h3 id=&#34;scutari-41&#34;&gt;Scutari 4.1&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Consider the survey data set from Chapter 1.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The data can be obtained from 
&lt;a href=&#34;https://www.bnlearn.com/book-crc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;survey &amp;lt;- read.table(&amp;quot;survey.txt&amp;quot;, header = TRUE, colClasses = &amp;quot;factor&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remember, this is the corresponding DAG we know to be true:&lt;/p&gt;
&lt;img src=&#34;https://www.erikkusch.com/courses/networksscutari/Fig1.1.JPG&#34; width=&#34;900&#34;/&gt;
&lt;h4 id=&#34;part-a-2&#34;&gt;Part A&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Learn a BN with the IAMB algorithm and the asymptotic mutual information test.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;s4.1_dag &amp;lt;- iamb(survey, test = &amp;quot;mi&amp;quot;)
s4.1_dag
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   Bayesian network learned via Constraint-based methods
## 
##   model:
##     [undirected graph]
##   nodes:                                 6 
##   arcs:                                  4 
##     undirected arcs:                     4 
##     directed arcs:                       0 
##   average markov blanket size:           1.33 
##   average neighbourhood size:            1.33 
##   average branching factor:              0.00 
## 
##   learning algorithm:                    IAMB 
##   conditional independence test:         Mutual Information (disc.) 
##   alpha threshold:                       0.05 
##   tests used in the learning procedure:  85
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-b-2&#34;&gt;Part B&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Learn a second BN with IAMB but using only the first 100 observations of the data set. Is there a significant loss of information in the resulting BN compared to the BN learned from the whole data set?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;s4.1_dagB &amp;lt;- iamb(survey[1:1e2, ], test = &amp;quot;mi&amp;quot;)
s4.1_dagB
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   Bayesian network learned via Constraint-based methods
## 
##   model:
##     [undirected graph]
##   nodes:                                 6 
##   arcs:                                  1 
##     undirected arcs:                     1 
##     directed arcs:                       0 
##   average markov blanket size:           0.33 
##   average neighbourhood size:            0.33 
##   average branching factor:              0.00 
## 
##   learning algorithm:                    IAMB 
##   conditional independence test:         Mutual Information (disc.) 
##   alpha threshold:                       0.05 
##   tests used in the learning procedure:  42
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We discover far fewer arcs!&lt;/p&gt;
&lt;h4 id=&#34;part-c-2&#34;&gt;Part C&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Repeat the structure learning in the previous point with IAMB and the Monte Carlo and sequential Monte Carlo mutual information tests. How do the resulting networks compare with the BN learned with the asymptotic test? Is the increased execution time justified?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;s4.1_dagC_mcmc &amp;lt;- iamb(survey[1:1e2, ], test = &amp;quot;mc-mi&amp;quot;)
s4.1_dagC_mcmc
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   Bayesian network learned via Constraint-based methods
## 
##   model:
##     [undirected graph]
##   nodes:                                 6 
##   arcs:                                  1 
##     undirected arcs:                     1 
##     directed arcs:                       0 
##   average markov blanket size:           0.33 
##   average neighbourhood size:            0.33 
##   average branching factor:              0.00 
## 
##   learning algorithm:                    IAMB 
##   conditional independence test:         Mutual Information (disc., MC) 
##   alpha threshold:                       0.05 
##   permutations:                          5000 
##   tests used in the learning procedure:  38
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;s4.1_dagC_smc &amp;lt;- iamb(survey[1:1e2, ], test = &amp;quot;smc-mi&amp;quot;)
s4.1_dagC_smc
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   Bayesian network learned via Constraint-based methods
## 
##   model:
##     [undirected graph]
##   nodes:                                 6 
##   arcs:                                  1 
##     undirected arcs:                     1 
##     directed arcs:                       0 
##   average markov blanket size:           0.33 
##   average neighbourhood size:            0.33 
##   average branching factor:              0.00 
## 
##   learning algorithm:                    IAMB 
##   conditional independence test:         Mutual Information (disc., Seq. MC) 
##   alpha threshold:                       0.05 
##   permutations:                          5000 
##   tests used in the learning procedure:  38
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We do not discover more arcs, and the outputs of the two asymptotic tests are equal for this case:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;all.equal(s4.1_dagC_mcmc, s4.1_dagC_smc, s4.1_dagB)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;scutari-42&#34;&gt;Scutari 4.2&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Consider again the survey data set from Chapter 1.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The data can be obtained from 
&lt;a href=&#34;https://www.bnlearn.com/book-crc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;survey &amp;lt;- read.table(&amp;quot;survey.txt&amp;quot;, header = TRUE, colClasses = &amp;quot;factor&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-a-3&#34;&gt;Part A&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Learn a BN using Bayesian posteriors for both structure and parameter learning, in both cases with &lt;code&gt;iss = 5&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;s4.2_dag &amp;lt;- hc(survey, score = &amp;quot;bde&amp;quot;, iss = 5)
s4.2_bn &amp;lt;- bn.fit(s4.2_dag, survey, method = &amp;quot;bayes&amp;quot;, iss = 5)
modelstring(s4.2_bn)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;[R][E|R][T|R][A|E][O|E][S|E]&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-b-3&#34;&gt;Part B&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Repeat structure learning with hc and 3 random restarts and with tabu. How do the BNs differ? Is there any evidence of numerical or convergence problems?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;s4.2_hc &amp;lt;- hc(survey, score = &amp;quot;bde&amp;quot;, iss = 5, restart = 3)
modelstring(s4.2_hc)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;[T][R|T][E|R][A|E][O|E][S|E]&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;s4.2_tabu &amp;lt;- tabu(survey, score = &amp;quot;bde&amp;quot;, iss = 5)
modelstring(s4.2_tabu)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;[O][S][E|O:S][A|E][R|E][T|R]&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Bayesian networks inferred here differ quite substantially in their DAG structures.&lt;/p&gt;
&lt;p&gt;The random-start hill-climbing algorithm builds a DAG structure closer to the validated structure which is supported by the &lt;code&gt;score&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;score(s4.2_hc, survey)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -1998.432
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;score(s4.2_tabu, survey)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -1999.733
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-c-3&#34;&gt;Part C&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Use increasingly large subsets of the survey data to check empirically that BIC and BDe are asymptotically equivalent.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(42)
breaks &amp;lt;- seq(from = 10, to = 100, by = 10) # percentage of data
analysis_df &amp;lt;- data.frame(
  bde = NA,
  bic = NA,
  breaks = NA
)
for (k in 1:1e3) {
  bde_vec &amp;lt;- c()
  bic_vec &amp;lt;- c()
  for (i in breaks) {
    samp &amp;lt;- sample(1:nrow(survey), nrow(survey) / i)
    samp &amp;lt;- survey[samp, ]
    s4.2_bde &amp;lt;- hc(samp, score = &amp;quot;bde&amp;quot;, iss = 5)
    s4.2_bic &amp;lt;- hc(samp, score = &amp;quot;bic&amp;quot;)
    bde_vec &amp;lt;- c(bde_vec, score(s4.2_bde, survey))
    bic_vec &amp;lt;- c(bic_vec, score(s4.2_bic, survey))
  }
  analysis_df &amp;lt;- rbind(
    analysis_df,
    data.frame(
      bde = bde_vec,
      bic = bic_vec,
      breaks = breaks
    )
  )
}

analysis_df &amp;lt;- na.omit(analysis_df)

plot(
  x = analysis_df$breaks,
  y = abs(analysis_df$bde - analysis_df$bic)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-11-08-network-inference_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;scutari-43&#34;&gt;Scutari 4.3&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Consider the marks data set from Section 4.7.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(marks)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-a-4&#34;&gt;Part A&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Create a bn object describing the graph in the bottom right panel of Figure 4.5 and call it &lt;code&gt;mdag&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mdag &amp;lt;- model2network(paste0(
  &amp;quot;[ANL][MECH][LAT|ANL:MECH]&amp;quot;,
  &amp;quot;[VECT|LAT][ALG|LAT][STAT|LAT]&amp;quot;
))
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-b-4&#34;&gt;Part B&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Construct the skeleton, the CPDAG and the moral graph of &lt;code&gt;mdag&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mdag_skel &amp;lt;- skeleton(mdag)
mdag_cpdag &amp;lt;- cpdag(mdag)
mdag_moral &amp;lt;- moral(mdag)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-c-4&#34;&gt;Part C&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Discretise the marks data using &amp;ldquo;interval&amp;rdquo; discretisation with 2, 3 and 4 intervals.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dmarks_2 &amp;lt;- discretize(marks, &amp;quot;interval&amp;quot;, breaks = 2)
dmarks_3 &amp;lt;- discretize(marks, &amp;quot;interval&amp;quot;, breaks = 3)
dmarks_4 &amp;lt;- discretize(marks, &amp;quot;interval&amp;quot;, breaks = 4)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-d&#34;&gt;Part D&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Perform structure learning with hc on each of the discretised data sets; how do the resulting DAGs differ?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;hc_2 &amp;lt;- hc(dmarks_2)
modelstring(hc_2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;[MECH][VECT|MECH][ALG|VECT][ANL|ALG][STAT|ALG]&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;hc_3 &amp;lt;- hc(dmarks_3)
modelstring(hc_3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;[MECH][ALG|MECH][ANL|ALG][STAT|ALG][VECT|ANL]&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;hc_4 &amp;lt;- hc(dmarks_4)
modelstring(hc_4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;[MECH][VECT][ALG][ANL|ALG][STAT|ANL]&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Quite evidently, as we increase the number of intervals, we break conditional relationships so much so that fewer arcs are identified.&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.2.1 (2022-06-23 ucrt)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19044)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_Germany.utf8  LC_CTYPE=English_Germany.utf8    LC_MONETARY=English_Germany.utf8 LC_NUMERIC=C                     LC_TIME=English_Germany.utf8    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] penalized_0.9-52    survival_3.4-0      GeneNet_1.2.16      fdrtool_1.2.17      longitudinal_1.1.13 corpcor_1.6.10      gRain_1.3.11        gRbase_1.8.7        bnlearn_4.8.1      
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.9          highr_0.9           bslib_0.4.0         compiler_4.2.1      BiocManager_1.30.18 jquerylib_0.1.4     R.methodsS3_1.8.2   R.utils_2.12.0      tools_4.2.1        
## [10] digest_0.6.29       jsonlite_1.8.0      evaluate_0.16       R.cache_0.16.0      lattice_0.20-45     pkgconfig_2.0.3     rlang_1.0.5         igraph_1.3.4        Matrix_1.5-1       
## [19] graph_1.74.0        cli_3.3.0           rstudioapi_0.14     Rgraphviz_2.40.0    yaml_2.3.5          parallel_4.2.1      blogdown_1.13       xfun_0.33           fastmap_1.1.0      
## [28] styler_1.8.0        stringr_1.4.1       knitr_1.40          vctrs_0.4.1         sass_0.4.2          stats4_4.2.1        grid_4.2.1          R6_2.5.1            RBGL_1.72.0        
## [37] rmarkdown_2.16      bookdown_0.29       purrr_0.3.4         magrittr_2.0.3      splines_4.2.1       BiocGenerics_0.42.0 htmltools_0.5.3     stringi_1.7.8       cachem_1.0.6       
## [46] R.oo_1.25.0
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Dynamic Bayesian Networks</title>
      <link>https://www.erikkusch.com/courses/bayes-nets/dynamic/</link>
      <pubDate>Tue, 01 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/bayes-nets/dynamic/</guid>
      <description>&lt;style&gt;

blockquote{
color:#633a00;
}

&lt;/style&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/content/courses/bayes-nets/Dynamic-BNs.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dynamic Bayesian Networks&lt;/a&gt; by 
&lt;a href=&#34;https://gregor-mathes.netlify.app/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gregor Mathes&lt;/a&gt; (one of our study group members)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;exercises&#34;&gt;Exercises&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to the exercises at the end of chapter 3 in 
&lt;a href=&#34;https://link.springer.com/book/10.1007/978-1-4614-6446-4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bayesian Networks in R with Applications in Systems Biology&lt;/a&gt; by by Radhakrishnan Nagarajan, Marco Scutari &amp;amp; Sophie LÃ¨bre. Much of my inspiration for these solutions, where necessary, by consulting the solutions provided by the authors themselves as in the appendix.&lt;/p&gt;
&lt;h3 id=&#34;r-environment&#34;&gt;&lt;code&gt;R&lt;/code&gt; Environment&lt;/h3&gt;
&lt;p&gt;For today&amp;rsquo;s exercise, I load the following packages:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(vars)
library(lars)
library(GeneNet)
library(G1DBN) # might have to run remotes::install_version(&amp;quot;G1DBN&amp;quot;, &amp;quot;3.1.1&amp;quot;) first
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;nagarajan-31&#34;&gt;Nagarajan 3.1&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Consider the &lt;code&gt;Canada&lt;/code&gt; data set from the vars package, which we analyzed in Sect. 3.5.1.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(Canada)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-a&#34;&gt;Part A&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Load the data set from the &lt;code&gt;vars&lt;/code&gt; package and investigate its properties using the exploratory analysis techniques covered in Chap. 1.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(Canada)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Time-Series [1:84, 1:4] from 1980 to 2001: 930 930 930 931 933 ...
##  - attr(*, &amp;quot;dimnames&amp;quot;)=List of 2
##   ..$ : NULL
##   ..$ : chr [1:4] &amp;quot;e&amp;quot; &amp;quot;prod&amp;quot; &amp;quot;rw&amp;quot; &amp;quot;U&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Canada)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        e              prod             rw              U         
##  Min.   :928.6   Min.   :401.3   Min.   :386.1   Min.   : 6.700  
##  1st Qu.:935.4   1st Qu.:404.8   1st Qu.:423.9   1st Qu.: 7.782  
##  Median :946.0   Median :406.5   Median :444.4   Median : 9.450  
##  Mean   :944.3   Mean   :407.8   Mean   :440.8   Mean   : 9.321  
##  3rd Qu.:950.0   3rd Qu.:410.7   3rd Qu.:461.1   3rd Qu.:10.607  
##  Max.   :961.8   Max.   :418.0   Max.   :470.0   Max.   :12.770
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-b&#34;&gt;Part B&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Estimate a VAR(1) process for this data set.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(var1 &amp;lt;- VAR(Canada, p = 1, type = &amp;quot;const&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## VAR Estimation Results:
## ======================= 
## 
## Estimated coefficients for equation e: 
## ====================================== 
## Call:
## e = e.l1 + prod.l1 + rw.l1 + U.l1 + const 
## 
##          e.l1       prod.l1         rw.l1          U.l1         const 
##    1.17353629    0.14479389   -0.07904568    0.52438144 -192.56360758 
## 
## 
## Estimated coefficients for equation prod: 
## ========================================= 
## Call:
## prod = e.l1 + prod.l1 + rw.l1 + U.l1 + const 
## 
##         e.l1      prod.l1        rw.l1         U.l1        const 
##   0.08709510   1.01970070  -0.02629309   0.32299246 -81.55109611 
## 
## 
## Estimated coefficients for equation rw: 
## ======================================= 
## Call:
## rw = e.l1 + prod.l1 + rw.l1 + U.l1 + const 
## 
##        e.l1     prod.l1       rw.l1        U.l1       const 
##  0.06381103 -0.13551199  0.96872851 -0.19538479 11.61375726 
## 
## 
## Estimated coefficients for equation U: 
## ====================================== 
## Call:
## U = e.l1 + prod.l1 + rw.l1 + U.l1 + const 
## 
##         e.l1      prod.l1        rw.l1         U.l1        const 
##  -0.19293575  -0.08086896   0.07538624   0.47530976 186.80892410
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-c&#34;&gt;Part C&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Build the auto-regressive matrix $A$ and the constant matrix $B$ defining the VAR(1) model.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## base object creation
base_mat &amp;lt;- matrix(0, 4, 5)
colnames(base_mat) &amp;lt;- c(&amp;quot;e&amp;quot;, &amp;quot;prod&amp;quot;, &amp;quot;rw&amp;quot;, &amp;quot;U&amp;quot;, &amp;quot;constant&amp;quot;)
p &amp;lt;- 0.05
## object filling
pos &amp;lt;- which(coef(var1)$e[, &amp;quot;Pr(&amp;gt;|t|)&amp;quot;] &amp;lt; p)
base_mat[1, pos] &amp;lt;- coef(var1)$e[pos, &amp;quot;Estimate&amp;quot;]
pos &amp;lt;- which(coef(var1)$prod[, &amp;quot;Pr(&amp;gt;|t|)&amp;quot;] &amp;lt; p)
base_mat[2, pos] &amp;lt;- coef(var1)$prod[pos, &amp;quot;Estimate&amp;quot;]
pos &amp;lt;- which(coef(var1)$rw[, &amp;quot;Pr(&amp;gt;|t|)&amp;quot;] &amp;lt; p)
base_mat[3, pos] &amp;lt;- coef(var1)$rw[pos, &amp;quot;Estimate&amp;quot;]
pos &amp;lt;- which(coef(var1)$U[, &amp;quot;Pr(&amp;gt;|t|)&amp;quot;] &amp;lt; p)
base_mat[4, pos] &amp;lt;- coef(var1)$U[pos, &amp;quot;Estimate&amp;quot;]
## final objects
(A &amp;lt;- base_mat[, 1:4])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               e        prod          rw         U
## [1,]  1.1735363  0.14479389 -0.07904568 0.5243814
## [2,]  0.0000000  1.01970070  0.00000000 0.0000000
## [3,]  0.0000000 -0.13551199  0.96872851 0.0000000
## [4,] -0.1929358 -0.08086896  0.07538624 0.4753098
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(B &amp;lt;- base_mat[, 5])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -192.5636    0.0000    0.0000  186.8089
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-d&#34;&gt;Part D&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Compare the results with the LASSO matrix when estimating the L1-penalty with cross-validation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## data preparation
data_df &amp;lt;- Canada[-nrow(Canada), ] # remove last row of data
## Lasso
Lasso_ls &amp;lt;- lapply(colnames(Canada), function(gene) {
  y &amp;lt;- Canada[-1, gene] # remove first row of data, and select only target gene
  lars(y = y, x = data_df, type = &amp;quot;lasso&amp;quot;) # LASSO matrix
})
## Cross-validation
CV_ls &amp;lt;- lapply(1:ncol(Canada), function(gene) {
  y &amp;lt;- Canada[-1, gene] # remove first row of data, and select only target gene
  lasso.cv &amp;lt;- cv.lars(y = y, x = data_df, mode = &amp;quot;fraction&amp;quot;)
  frac &amp;lt;- lasso.cv$index[which.min(lasso.cv$cv)]
  predict(Lasso_ls[[gene]], s = frac, type = &amp;quot;coef&amp;quot;, mode = &amp;quot;fraction&amp;quot;)
})
## output
rbind(
  CV_ls[[1]]$coefficients,
  CV_ls[[2]]$coefficients,
  CV_ls[[3]]$coefficients,
  CV_ls[[4]]$coefficients
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                e        prod           rw          U
## [1,]  1.17353629  0.14479389 -0.079045685  0.5243814
## [2,]  0.02570001  1.02314558 -0.004878295  0.1994059
## [3,]  0.09749788 -0.11991692  0.954389035 -0.1023845
## [4,] -0.17604953 -0.08192783  0.069502065  0.5086115
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And for comparison the previously identified $A$:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;A
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               e        prod          rw         U
## [1,]  1.1735363  0.14479389 -0.07904568 0.5243814
## [2,]  0.0000000  1.01970070  0.00000000 0.0000000
## [3,]  0.0000000 -0.13551199  0.96872851 0.0000000
## [4,] -0.1929358 -0.08086896  0.07538624 0.4753098
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-e&#34;&gt;Part E&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;What can you conclude?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The whole point of LASSO, as far as I understand it, is to shrink parameter estimates towards 0 often times reaching 0 exactly. In the above this has not happened for many parameters, but is the case with the estimation provided by &lt;code&gt;vars&lt;/code&gt;. I assume this might be because there just aren&amp;rsquo;t enough variables and/or observations in time.&lt;/p&gt;
&lt;h3 id=&#34;nagarajan-32&#34;&gt;Nagarajan 3.2&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Consider the &lt;code&gt;arth800&lt;/code&gt; data set from the GeneNet package, which we analyzed in Sects. 3.5.2 and 3.5.3.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(arth800)
data(arth800.expr)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-a-1&#34;&gt;Part A&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Load the data set from the &lt;code&gt;GeneNet&lt;/code&gt; package. The time series expression of the 800 genes is included in a data set called &lt;code&gt;arth800.expr&lt;/code&gt;. Investigate its properties using the exploratory analysis techniques covered in Chap. 1.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(arth800.expr)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  &#39;longitudinal&#39; num [1:22, 1:800] 10.04 10.11 9.77 10.06 10.02 ...
##  - attr(*, &amp;quot;dimnames&amp;quot;)=List of 2
##   ..$ : chr [1:22] &amp;quot;0-1&amp;quot; &amp;quot;0-2&amp;quot; &amp;quot;1-1&amp;quot; &amp;quot;1-2&amp;quot; ...
##   ..$ : chr [1:800] &amp;quot;AFFX-Athal-GAPDH_3_s_at&amp;quot; &amp;quot;AFFX-Athal-Actin_3_f_at&amp;quot; &amp;quot;267612_at&amp;quot; &amp;quot;267520_at&amp;quot; ...
##  - attr(*, &amp;quot;time&amp;quot;)= num [1:11] 0 1 2 4 8 12 13 14 16 20 ...
##  - attr(*, &amp;quot;repeats&amp;quot;)= num [1:11] 2 2 2 2 2 2 2 2 2 2 ...
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(arth800.expr)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Longitudinal data:
##  800 variables measured at 11 different time points
##  Total number of measurements per variable: 22 
##  Repeated measurements: yes 
## 
##  To obtain the measurement design call &#39;get.time.repeats()&#39;.
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-b-1&#34;&gt;Part B&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;For this practical exercise, we will work on a subset of variables (one for each gene) having a large variance. Compute the variance of each of the 800 variables, plot the various variance values in decreasing order, and create a data set with the variables greater than 2.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## variance calculation
variance &amp;lt;- diag(var(arth800.expr))
## plotting
plot(sort(variance, decreasing = TRUE), type = &amp;quot;l&amp;quot;, ylab = &amp;quot;Variance&amp;quot;)
abline(h = 2, lty = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-11-01-nagara-dynamic_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## variables with variances greater than 2
dataVar2 &amp;lt;- arth800.expr[, which(variance &amp;gt; 2)]
dim(dataVar2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 22 49
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-c-1&#34;&gt;Part C&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Can you fit a VAR process with a usual approach from this data set?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I don&amp;rsquo;t think so. There are more variables (genes) than there are samples (time steps):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dim(dataVar2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 22 49
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-d-1&#34;&gt;Part D&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Which alternative approaches can be used to fit a VAR process from this data set?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The chapter discusses these alternatives:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LASSO&lt;/li&gt;
&lt;li&gt;James-Stein Shrinkage&lt;/li&gt;
&lt;li&gt;Low-order conditional dependency approximation&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;part-e-1&#34;&gt;Part E&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Estimate a dynamic Bayesian network with each of the alternative approaches presented in this chapter.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;First, I prepare the data by re-ordering them:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## make the data sequential for both repetitions
dataVar2seq &amp;lt;- dataVar2[c(seq(1, 22, by = 2), seq(2, 22, by = 2)), ]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;LASSO&lt;/em&gt; with the &lt;code&gt;lars&lt;/code&gt; package:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x &amp;lt;- dataVar2seq[-c(21:22), ] # remove final rows (end of sequences)
Lasso_ls &amp;lt;- lapply(colnames(dataVar2seq), function(gene) {
  y &amp;lt;- dataVar2seq[-(1:2), gene]
  lars(y = y, x = x, type = &amp;quot;lasso&amp;quot;)
})
CV_ls &amp;lt;- lapply(1:ncol(dataVar2seq), function(gene) {
  y &amp;lt;- dataVar2seq[-(1:2), gene]
  lasso.cv &amp;lt;- cv.lars(y = y, x = x, mode = &amp;quot;fraction&amp;quot;, plot.it = FALSE)
  frac &amp;lt;- lasso.cv$index[which.min(lasso.cv$cv)]
  predict(Lasso_ls[[gene]], s = frac, type = &amp;quot;coef&amp;quot;, mode = &amp;quot;fraction&amp;quot;)
})
Lasso_mat &amp;lt;- matrix(0, dim(dataVar2seq)[2], dim(dataVar2seq)[2])
for (i in 1:dim(Lasso_mat)[1]) {
  Lasso_mat[i, ] &amp;lt;- CV_ls[i][[1]]$coefficients
}
sum(Lasso_mat != 0) # number of arcs
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 456
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(sort(abs(Lasso_mat), decr = TRUE)[1:500], type = &amp;quot;l&amp;quot;, ylab = &amp;quot;Absolute coefficients&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-11-01-nagara-dynamic_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;James-Stein shrinkage&lt;/em&gt; with the &lt;code&gt;GeneNet&lt;/code&gt; package:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;DBNGeneNet &amp;lt;- ggm.estimate.pcor(dataVar2, method = &amp;quot;dynamic&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Estimating optimal shrinkage intensity lambda (correlation matrix): 0.0539
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;DBNGeneNet.edges &amp;lt;- network.test.edges(DBNGeneNet) # p-values, q-values and posterior probabilities for each potential arc
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Estimate (local) false discovery rates (partial correlations):
## Step 1... determine cutoff point
## Step 2... estimate parameters of null distribution and eta0
## Step 3... compute p-values and estimate empirical PDF/CDF
## Step 4... compute q-values and local fdr
## Step 5... prepare for plotting
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-11-01-nagara-dynamic_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(DBNGeneNet.edges[, &amp;quot;prob&amp;quot;], type = &amp;quot;l&amp;quot;) # arcs probability by decreasing order
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-11-01-nagara-dynamic_files/figure-html/unnamed-chunk-14-2.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sum(DBNGeneNet.edges$prob &amp;gt; 0.95) # arcs with prob &amp;gt; 0.95
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;First-order conditional dependency&lt;/em&gt; with the &lt;code&gt;G1DBN&lt;/code&gt; package:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;G1DB_BN &amp;lt;- DBNScoreStep1(dataVar2seq, method = &amp;quot;ls&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Treating 49 vertices:
## 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;G1DB_BN &amp;lt;- DBNScoreStep2(G1DB_BN$S1ls, dataVar2seq, method = &amp;quot;ls&amp;quot;, alpha1 = 0.5)
plot(sort(G1DB_BN, decreasing = TRUE), type = &amp;quot;l&amp;quot;, ylab = &amp;quot;Arcsâ p-values&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-11-01-nagara-dynamic_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;nagarajan-33&#34;&gt;Nagarajan 3.3&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Consider the dimension reduction approaches used in the previous exercise and the &lt;code&gt;arth800&lt;/code&gt; data set from the &lt;code&gt;GeneNet&lt;/code&gt; package.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(arth800)
data(arth800.expr)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-a-2&#34;&gt;Part A&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;For a comparative analysis of the different approaches, select the top 50 arcs for each approach (function &lt;code&gt;BuildEdges&lt;/code&gt; from the &lt;code&gt;G1DBN&lt;/code&gt; package can be used to that end).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;LASSO&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;lasso_tresh &amp;lt;- mean(sort(abs(Lasso_mat), decreasing = TRUE)[50:51]) # Lasso_mat from exercise 3.2
lasso_50 &amp;lt;- BuildEdges(score = -abs(Lasso_mat), threshold = -lasso_tresh)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;James-Stein shrinkage&lt;/em&gt; with the &lt;code&gt;GeneNet&lt;/code&gt; package:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;DBNGeneNet_50 &amp;lt;- cbind(DBNGeneNet.edges[1:50, &amp;quot;node1&amp;quot;], DBNGeneNet.edges[1:50, &amp;quot;node2&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;First-order conditional dependency&lt;/em&gt; with the &lt;code&gt;G1DBN&lt;/code&gt; package:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;G1DBN_tresh &amp;lt;- mean(sort(G1DB_BN)[50:51])
G1DBN.edges &amp;lt;- BuildEdges(score = G1DB_BN, threshold = G1DBN_tresh, prec = 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-b-2&#34;&gt;Part B&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Plot the four inferred networks with the function plot from package &lt;code&gt;G1DBN&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Four inferred networks? I assume the exercise so far wanted me to also analyse the data using the LASSO approach with the SIMoNe (&lt;code&gt;simone&lt;/code&gt;) package. I will skip over that one and continue with the three I have:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;par(mfrow = c(1, 3))

## LASSO
LASSO_plot &amp;lt;- graph.edgelist(cbind(lasso_50[, 1], lasso_50[, 2]))
Lasso_layout &amp;lt;- layout.fruchterman.reingold(LASSO_plot)
plot(LASSO_plot,
  layout = Lasso_layout,
  edge.arrow.size = 0.5, vertex.size = 10,
  main = &amp;quot;LASSO&amp;quot;
)

## James-Stein
DBN_plot &amp;lt;- graph.edgelist(DBNGeneNet_50)
# DBN_layout &amp;lt;- layout.fruchterman.reingold(DBN_plot)
plot(DBN_plot,
  layout = Lasso_layout,
  edge.arrow.size = 0.5, vertex.size = 10,
  main = &amp;quot;GeneNet&amp;quot;
)

## First-order conditional
G1DBN_plot &amp;lt;- graph.edgelist(cbind(G1DBN.edges[, 1], G1DBN.edges[, 2]))
# G1DBN_layout = layout.fruchterman.reingold(G1DBN_plot)
plot(G1DBN_plot,
  layout = Lasso_layout,
  edge.arrow.size = 0.5, vertex.size = 10,
  main = &amp;quot;G1DBN&amp;quot;
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-11-01-nagara-dynamic_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h4 id=&#34;part-c-2&#34;&gt;Part C&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;How many arcs are common to the four inferred networks?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## extract edges
LASSO_el &amp;lt;- as_edgelist(LASSO_plot)
DBN_el &amp;lt;- as_edgelist(DBN_plot)
G1DBN_el &amp;lt;- as_edgelist(G1DBN_plot)

## number of repeated edges in pairwise comparisons
sum(duplicated(rbind(LASSO_el, DBN_el)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sum(duplicated(rbind(LASSO_el, G1DBN_el)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 6
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sum(duplicated(rbind(DBN_el, G1DBN_el)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;### all at once
sum(duplicated(rbind(LASSO_el, DBN_el, G1DBN_el)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 7
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-d-2&#34;&gt;Part D&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Are the top 50 arcs of each inferred network similar? What can you conclude?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;No, they are not. I can conclude that different dimension reductions produce different DAG structures.&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.2.1 (2022-06-23 ucrt)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19044)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_Germany.utf8  LC_CTYPE=English_Germany.utf8    LC_MONETARY=English_Germany.utf8 LC_NUMERIC=C                     LC_TIME=English_Germany.utf8    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] G1DBN_3.1.1         igraph_1.3.4        GeneNet_1.2.16      fdrtool_1.2.17      longitudinal_1.1.13 corpcor_1.6.10      lars_1.3            vars_1.5-6          lmtest_0.9-40      
## [10] urca_1.3-3          strucchange_1.5-3   sandwich_3.0-2      zoo_1.8-10          MASS_7.3-58.1      
## 
## loaded via a namespace (and not attached):
##  [1] highr_0.9         bslib_0.4.0       compiler_4.2.1    jquerylib_0.1.4   R.methodsS3_1.8.2 R.utils_2.12.0    tools_4.2.1       digest_0.6.29     jsonlite_1.8.0    evaluate_0.16    
## [11] nlme_3.1-159      R.cache_0.16.0    lattice_0.20-45   pkgconfig_2.0.3   rlang_1.0.5       cli_3.3.0         rstudioapi_0.14   yaml_2.3.5        blogdown_1.13     xfun_0.33        
## [21] fastmap_1.1.0     styler_1.8.0      stringr_1.4.1     knitr_1.40        vctrs_0.4.1       sass_0.4.2        grid_4.2.1        R6_2.5.1          rmarkdown_2.16    bookdown_0.29    
## [31] purrr_0.3.4       magrittr_2.0.3    htmltools_0.5.3   stringi_1.7.8     cachem_1.0.6      R.oo_1.25.0
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Graph Theory &amp; Bayes</title>
      <link>https://www.erikkusch.com/courses/bayes-nets/introduction/</link>
      <pubDate>Tue, 13 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/bayes-nets/introduction/</guid>
      <description>&lt;style&gt;

blockquote{
color:#633a00;
}


&lt;/style&gt; 
&lt;p&gt;This session of our study group did not include any practical material. For the summary of the theory discussed in this session, please refer to the slides linked below.&lt;/p&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/content/courses/bayes-nets/1-Bayes-_-Graph-Theory_13-09-22.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>The Theory Behind KrigR</title>
      <link>https://www.erikkusch.com/courses/krigr/background/</link>
      <pubDate>Thu, 26 May 2022 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/krigr/background/</guid>
      <description>&lt;h2 id=&#34;climate-reanalyses&#34;&gt;Climate Reanalyses&lt;/h2&gt;
&lt;p&gt;Climate reanalyses are the go-to data products for climate scientists and represent the best-in-class approximations of climate characteristics across the Earth.&lt;/p&gt;
&lt;p&gt;The accuracy of climate reanalyses is largely owed to the number of observations assimilated, the underlying dynamical model, and the data assimilation methodology. Furthermore, climate reanalyses offer access to a vast array of Essential Climate Variables (ECVs) at unparalleled temporal resolutions. Lastly, as reanalyses are created from multiple models (i.e. ensembles), we can obtain data uncertainty for each data record.&lt;/p&gt;
&lt;p&gt;To our mind, this makes climate reanalyses the gold standard in climate data products for use in macroecological analyses.&lt;/p&gt;
&lt;p&gt;Please have a look at this presentation for an introduction to climate science:
&lt;a href=&#34;http://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/content/courses/krigr/KrigRClimate.html&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://www.erikkusch.com/courses/krigr/climrean.png&#34; width=&#34;900&#34; margin-top = &#34;0&#34;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;the-krigr-package&#34;&gt;The &lt;code&gt;KrigR&lt;/code&gt; Package&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;KrigR&lt;/code&gt; package has been designed to overcome the major stop-gaps in integrating climate reanalyses data into our research frameworks:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Accessing, downloading, and processing of climate reanalysis data&lt;/li&gt;
&lt;li&gt;Matching spatial resolutions which downstream applications have become used to&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For an introduction to &lt;code&gt;KrigR&lt;/code&gt; in presentation form, please have a look at this material:
&lt;a href=&#34;http://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/content/courses/krigr/KrigRDemo.html&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://www.erikkusch.com/courses/krigr/climrean.png&#34; width=&#34;900&#34; border=&#34;-20&#34;/&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multinomial Bayesian Networks</title>
      <link>https://www.erikkusch.com/courses/bayes-nets/part-1/</link>
      <pubDate>Tue, 13 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/bayes-nets/part-1/</guid>
      <description>&lt;style&gt;

blockquote{
color:#633a00;
}

&lt;/style&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/content/courses/bayes-nets/3-Multinomial-Networks_03-10-22.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Multinomial Bayesian Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;exercises&#34;&gt;Exercises&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to the exercises at the end of Part 1 in 
&lt;a href=&#34;https://www.bnlearn.com/book-crc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bayesian Networks with Examples in R&lt;/a&gt; by M. Scutari and J.-B. Denis. I have created these notes as a part of a study-partnership with 
&lt;a href=&#34;https://www.linkedin.com/in/frederik-have-kallesoe-0584889b/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Frederik KallesÃ¸e&lt;/a&gt;. Much of my inspiration for these solutions, where necessary, has been obtained either from chatting with Frederik or by consulting the solutions provided by the authors themselves as in the appendix.&lt;/p&gt;
&lt;h3 id=&#34;r-environment&#34;&gt;&lt;code&gt;R&lt;/code&gt; Environment&lt;/h3&gt;
&lt;p&gt;For today&amp;rsquo;s exercise, I load the following packages:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(bnlearn)
library(gRain)
library(ggplot2)
library(lattice)
library(gridExtra)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;scutari-11&#34;&gt;Scutari 1.1&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Consider the DAG for the survey studied in this chapter and shown in Figure 1.1.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here&amp;rsquo;s the DAG in question:&lt;/p&gt;
&lt;img src=&#34;https://www.erikkusch.com/courses/networksscutari/Fig1.1.JPG&#34; width=&#34;900&#34;/&gt;
&lt;h4 id=&#34;part-1&#34;&gt;Part 1.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;List the parents and the children of each node.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Node&lt;/th&gt;
&lt;th&gt;Parent(s)&lt;/th&gt;
&lt;th&gt;Child(ren)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Age (A)&lt;/td&gt;
&lt;td&gt;{ }&lt;/td&gt;
&lt;td&gt;E&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Sex (S)&lt;/td&gt;
&lt;td&gt;{ }&lt;/td&gt;
&lt;td&gt;E&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Education (E)&lt;/td&gt;
&lt;td&gt;A, S&lt;/td&gt;
&lt;td&gt;O, R&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Occupation (O)&lt;/td&gt;
&lt;td&gt;E&lt;/td&gt;
&lt;td&gt;E&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Residence (R)&lt;/td&gt;
&lt;td&gt;E&lt;/td&gt;
&lt;td&gt;E&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Travel (T)&lt;/td&gt;
&lt;td&gt;O, R&lt;/td&gt;
&lt;td&gt;{ }&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;part-2&#34;&gt;Part 2.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;List all the fundamental connections present in the DAG, and classify them as either serial, divergent or convergent.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Fundamental connections are those paths who contain three vertices/nodes. In directed graphs, they can be classified into three different categories depending on flow of dependencies.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Path&lt;/th&gt;
&lt;th&gt;Classification&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;A â E â S&lt;/td&gt;
&lt;td&gt;Convergent&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;A â E â O&lt;/td&gt;
&lt;td&gt;Serial&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;A â E â R&lt;/td&gt;
&lt;td&gt;Serial&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;S â E â O&lt;/td&gt;
&lt;td&gt;Serial&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;S â E â R&lt;/td&gt;
&lt;td&gt;Serial&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;O â E â R&lt;/td&gt;
&lt;td&gt;Divergent&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;E â O â T&lt;/td&gt;
&lt;td&gt;Serial&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;E â R â T&lt;/td&gt;
&lt;td&gt;Serial&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;O â T â R&lt;/td&gt;
&lt;td&gt;Convergent&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;part-3&#34;&gt;Part 3.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Add an arc from Age to Occupation, and another arc from Travel to Education. Is the resulting graph still a valid BN? If not, why?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let&amp;rsquo;s take this one arc at a time:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A â O. Adding this arc does not lead to the introduction of any cycles and so the Bayesian Network (BN) remains valid. I have added this graph to the figure from the book and highlighted it in green just below.&lt;/li&gt;
&lt;li&gt;T â E. Adding this arc does introduce cyclic paths along T â E â R â T and T â E â O â T thus resulting in a non-valid BN. I have highlighted the added arc in red and shaded the cyclic paths in orange below.&lt;/li&gt;
&lt;/ol&gt;
&lt;img src=&#34;https://www.erikkusch.com/courses/networksscutari/Fig1.1_Arcs.JPG&#34; width=&#34;900&#34;/&gt;
&lt;h3 id=&#34;scutari-12&#34;&gt;Scutari 1.2&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Consider the probability distribution from the survey in Section 1.3.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The data can be obtained from 
&lt;a href=&#34;https://www.bnlearn.com/book-crc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;survey &amp;lt;- read.table(&amp;quot;survey.txt&amp;quot;, header = TRUE, colClasses = &amp;quot;factor&amp;quot;)
A.lv &amp;lt;- c(&amp;quot;young&amp;quot;, &amp;quot;adult&amp;quot;, &amp;quot;old&amp;quot;)
S.lv &amp;lt;- c(&amp;quot;M&amp;quot;, &amp;quot;F&amp;quot;)
E.lv &amp;lt;- c(&amp;quot;high&amp;quot;, &amp;quot;uni&amp;quot;)
O.lv &amp;lt;- c(&amp;quot;emp&amp;quot;, &amp;quot;self&amp;quot;)
R.lv &amp;lt;- c(&amp;quot;small&amp;quot;, &amp;quot;big&amp;quot;)
T.lv &amp;lt;- c(&amp;quot;car&amp;quot;, &amp;quot;train&amp;quot;, &amp;quot;other&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-1-1&#34;&gt;Part 1.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Compute the number of configurations of the parents of each node.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;code&gt;A&lt;/code&gt; and &lt;code&gt;S&lt;/code&gt; have no parents (refer back to the DAG in exercise 1.1). Therefore, we are only interested in the configurations of parental nodes for &lt;code&gt;E&lt;/code&gt;, &lt;code&gt;O&lt;/code&gt;, &lt;code&gt;R&lt;/code&gt;, and &lt;code&gt;T&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(E &amp;lt;- length(A.lv) * length(S.lv))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 6
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(O &amp;lt;- length(E.lv))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(R &amp;lt;- length(E.lv))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(T &amp;lt;- length(O.lv) * length(R.lv))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a simple exercise of combinatorics. The number of parental configurations is simply the number of states each parental node can be in multiplied by the same for all other parental nodes.&lt;/p&gt;
&lt;h4 id=&#34;part-2-1&#34;&gt;Part 2.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Compute the number of parameters of the local distributions.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;All of this comes down to how many parameters we need to estimate to accurately represent the probability distributions belonging to each node in our DAG. Since all probabilities per node sum up to 1, we effectively only ever need to estimate a number $n-1$ parameters for each node with $n$ being the number of states said node can be in. Let&amp;rsquo;s walk through this.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;A&lt;/code&gt; has 3 states, so need to estimate 2 parameters ($p_A = 2$). &lt;code&gt;S&lt;/code&gt; has 2 states hence we need 1 parameter for this node ($p_S = 1$).&lt;/p&gt;
&lt;p&gt;Now we arrive at &lt;code&gt;E&lt;/code&gt; and things get more complicated. The probability distribution for &lt;code&gt;E&lt;/code&gt; comes in two parts - one for &lt;code&gt;&amp;quot;high&amp;quot;&lt;/code&gt; and one for &lt;code&gt;&amp;quot;low&amp;quot;&lt;/code&gt; education level. Both of these contain additional probability distributions of combinations of the levels of &lt;code&gt;S&lt;/code&gt; and &lt;code&gt;A&lt;/code&gt;. To obtain the number of parameters needed to describe this 3-dimensional distribution, we can simply calculate $p_E = n_S * n_A * (n_E-1) = 2 * 3 * 1 = 6$.&lt;/p&gt;
&lt;p&gt;Moving on to &lt;code&gt;O&lt;/code&gt; and &lt;code&gt;R&lt;/code&gt;. Both of these need 2 parameters ($p_O = p_r = 2$) because of their two-dimensional distributions being made up of two levels of education and two levels occupation and residency respectively ($2 * (2-1) = 2$).&lt;/p&gt;
&lt;p&gt;Lastly, we arrive at &lt;code&gt;T&lt;/code&gt; which we need 8 parameters for ($p_T = 8$). Holy smokes. Why? Basically, this is a repeat of what we did for &lt;code&gt;E&lt;/code&gt;. We have a three-dimensional distribution with three levels in T-Space, two levels in-space, and two more levels in O-Space. To arrive at the number of parameters we simply do $p_T = (n_T-1) * n_o * n_R = 2 * 2 * 2 = 8$.&lt;/p&gt;
&lt;h4 id=&#34;part-3-1&#34;&gt;Part 3.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Compute the number of parameters of the global distribution.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We can sum all of the local parameters up to arrive at $p_{total} = p_A + p_S + p_E + p_O + p_R + p_T = 2+1+6+2+2+8 = 21$.&lt;/p&gt;
&lt;p&gt;And in &lt;code&gt;R&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# define DAG structure
dag &amp;lt;- model2network(&amp;quot;[A][S][E|A:S][O|E][R|E][T|O:R]&amp;quot;)
# define local distributions
A.prob &amp;lt;- array(c(0.30, 0.50, 0.20), dim = 3, dimnames = list(A = A.lv))
S.prob &amp;lt;- array(c(0.60, 0.40), dim = 2, dimnames = list(S = S.lv))
E.prob &amp;lt;- array(
  c(
    0.75, 0.25, 0.72, 0.28, 0.88, 0.12, 0.64, 0.36, 0.70,
    0.30, 0.90, 0.10
  ),
  dim = c(2, 3, 2),
  dimnames = list(E = E.lv, A = A.lv, S = S.lv)
)
O.prob &amp;lt;- array(c(0.96, 0.04, 0.92, 0.08),
  dim = c(2, 2),
  dimnames = list(O = O.lv, E = E.lv)
)
R.prob &amp;lt;- array(c(0.25, 0.75, 0.20, 0.80),
  dim = c(2, 2),
  dimnames = list(R = R.lv, E = E.lv)
)
T.prob &amp;lt;- array(
  c(
    0.48, 0.42, 0.10, 0.56, 0.36, 0.08, 0.58, 0.24, 0.18,
    0.70, 0.21, 0.09
  ),
  dim = c(3, 2, 2),
  dimnames = list(T = T.lv, O = O.lv, R = R.lv)
)
# define set of local distributions
cpt &amp;lt;- list(
  A = A.prob, S = S.prob, E = E.prob, O = O.prob,
  R = R.prob, T = T.prob
)
# create BN
bn &amp;lt;- custom.fit(dag, cpt)
# obtain parameters
nparams(bn)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 21
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that I pulled the probabilities for the distributions from the book and their values are irrelevant to the number of parameters.&lt;/p&gt;
&lt;h4 id=&#34;part-4&#34;&gt;Part 4.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Add an arc from Education to Travel. Recompute the factorisation into local distributions shown in Equation (1.1). How does the number of parameters of each local distribution change?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Adding E â T to Equation (1.1) results in:
$$P(A, S, E, O, R, T) = P(A) P(S) P(E | A, S) P(O | E) P(R | E) P(T | E, O, R)$$&lt;/p&gt;
&lt;p&gt;Now that &lt;code&gt;T&lt;/code&gt; is dependant on &lt;code&gt;E&lt;/code&gt; as well as the previous parents, the number of free parameters of the local distribution of &lt;code&gt;T&lt;/code&gt; increases
to 16 ($p_E = 16$). This is because our local distribution of &lt;code&gt;T&lt;/code&gt; is now four-dimensional resulting in $p_T = (n_T-1) * n_o * n_R * n_E = 2 * 2 * 2 * 2 = 16$.&lt;/p&gt;
&lt;p&gt;All other local distributions remain the same.&lt;/p&gt;
&lt;h3 id=&#34;scutari-13&#34;&gt;Scutari 1.3&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Consider again the DAG for the survey.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;part-1-2&#34;&gt;Part 1.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Create an object of class &lt;code&gt;bn&lt;/code&gt; for the DAG.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here&amp;rsquo;s the simplest way of doing this by specifying the model string:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# define DAG structure
bn &amp;lt;- model2network(&amp;quot;[A][S][E|A:S][O|E][R|E][T|O:R]&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-2-2&#34;&gt;Part 2.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Use the functions in &lt;code&gt;bnlearn&lt;/code&gt; and the R object created in the previous point to extract the nodes and the arcs of the DAG. Also extract the parents and the children of each node.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here we go:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;nodes(bn)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;A&amp;quot; &amp;quot;E&amp;quot; &amp;quot;O&amp;quot; &amp;quot;R&amp;quot; &amp;quot;S&amp;quot; &amp;quot;T&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;arcs(bn)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      from to 
## [1,] &amp;quot;A&amp;quot;  &amp;quot;E&amp;quot;
## [2,] &amp;quot;S&amp;quot;  &amp;quot;E&amp;quot;
## [3,] &amp;quot;E&amp;quot;  &amp;quot;O&amp;quot;
## [4,] &amp;quot;E&amp;quot;  &amp;quot;R&amp;quot;
## [5,] &amp;quot;O&amp;quot;  &amp;quot;T&amp;quot;
## [6,] &amp;quot;R&amp;quot;  &amp;quot;T&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sapply(X = nodes(bn), FUN = bnlearn::parents, x = bn)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $A
## character(0)
## 
## $E
## [1] &amp;quot;A&amp;quot; &amp;quot;S&amp;quot;
## 
## $O
## [1] &amp;quot;E&amp;quot;
## 
## $R
## [1] &amp;quot;E&amp;quot;
## 
## $S
## character(0)
## 
## $T
## [1] &amp;quot;O&amp;quot; &amp;quot;R&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sapply(X = nodes(bn), FUN = bnlearn::children, x = bn)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $A
## [1] &amp;quot;E&amp;quot;
## 
## $E
## [1] &amp;quot;O&amp;quot; &amp;quot;R&amp;quot;
## 
## $O
## [1] &amp;quot;T&amp;quot;
## 
## $R
## [1] &amp;quot;T&amp;quot;
## 
## $S
## [1] &amp;quot;E&amp;quot;
## 
## $T
## character(0)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-3-2&#34;&gt;Part 3.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Print the model formula from &lt;code&gt;bn&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;modelstring(bn)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;[A][S][E|A:S][O|E][R|E][T|O:R]&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-4-1&#34;&gt;Part 4.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Fit the parameters of the network from the data stored in survey.txt using their Bayesian estimators and save the result into an object of class &lt;code&gt;bn.fit&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bn_full &amp;lt;- bn.fit(bn, data = survey, method = &amp;quot;bayes&amp;quot;, iss = 10)
class(bn_full)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;bn.fit&amp;quot;      &amp;quot;bn.fit.dnet&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-5&#34;&gt;Part 5.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Remove the arc from Education to Occupation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bn_sparse &amp;lt;- drop.arc(bn, from = &amp;quot;E&amp;quot;, to = &amp;quot;O&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-6&#34;&gt;Part 6.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Fit the parameters of the modified network. Which local distributions change, and how?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bn_sparse &amp;lt;- bn.fit(bn_sparse, data = survey, method = &amp;quot;bayes&amp;quot;, iss = 10)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We already now that the only local distribution which should change is that of &lt;code&gt;O&lt;/code&gt;. Let&amp;rsquo;s check that:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dim(coef(bn_full$O))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2 2
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dim(coef(bn_sparse$O))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Quite evidently, the local distribution of &lt;code&gt;O&lt;/code&gt; has become much simpler in our sparse Bayesian Network. Why? Because it has no parent node now which would parse additional information and complexity onto it.&lt;/p&gt;
&lt;h3 id=&#34;scutari-14&#34;&gt;Scutari 1.4&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Re-create the &lt;code&gt;bn.mle&lt;/code&gt; object used in Section 1.4.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bn.mle &amp;lt;- bn.fit(dag, data = survey, method = &amp;quot;mle&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-1-3&#34;&gt;Part 1.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Compare the distribution of Occupation conditional on Age with the corresponding marginal distribution using &lt;code&gt;querygrain&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## creating object ready for gRain functions
junction &amp;lt;- compile(as.grain(bn.mle))
## Overall query
query_over &amp;lt;- querygrain(junction, nodes = &amp;quot;O&amp;quot;)$O
## Marginal query when A is young
jage &amp;lt;- setEvidence(junction, &amp;quot;A&amp;quot;, states = &amp;quot;young&amp;quot;)
query_young &amp;lt;- querygrain(jage, nodes = &amp;quot;O&amp;quot;)$O
## Marginal query when A is adult
jage &amp;lt;- setEvidence(junction, &amp;quot;A&amp;quot;, states = &amp;quot;adult&amp;quot;)
query_adult &amp;lt;- querygrain(jage, nodes = &amp;quot;O&amp;quot;)$O
## Marginal query when A is old
jage &amp;lt;- setEvidence(junction, &amp;quot;A&amp;quot;, states = &amp;quot;old&amp;quot;)
query_old &amp;lt;- querygrain(jage, nodes = &amp;quot;O&amp;quot;)$O
## Combining queries
queries_df &amp;lt;- rbind(query_over, query_young, query_adult, query_old)
rownames(queries_df) &amp;lt;- c(&amp;quot;Overall&amp;quot;, &amp;quot;Young&amp;quot;, &amp;quot;Adult&amp;quot;, &amp;quot;Old&amp;quot;)
queries_df
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               emp       self
## Overall 0.9660248 0.03397517
## Young   0.9644166 0.03558340
## Adult   0.9636485 0.03635151
## Old     0.9738915 0.02610849
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we can see, conditioning on &lt;code&gt;A&lt;/code&gt; does not influence the distribution of &lt;code&gt;O&lt;/code&gt; that much.&lt;/p&gt;
&lt;h4 id=&#34;part-2-3&#34;&gt;Part 2.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;How many random observations are needed for &lt;code&gt;cpquery&lt;/code&gt; to produce estimates of the parameters of these two distributions with a precision of Â±0.01?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I find this question to be difficult to understand. What I assume I am tasked with is to compare the distribution of Occupation conditional on Age (&lt;code&gt;query_over&lt;/code&gt;: 0.97, 0.03) with the estimates produced by &lt;code&gt;cpquery&lt;/code&gt; given some evidence (i.e. parental node configuration). This would mean comparing each query EXCEPT for query_over to it&amp;rsquo;s counterpart with &lt;code&gt;cpquery()&lt;/code&gt;. That&amp;rsquo;s a tad excessive, and so I only compare &lt;code&gt;query_young&lt;/code&gt; (0.96, 0.04) from above with the results obtained by &lt;code&gt;cpquery()&lt;/code&gt;. What I am looking at is: &amp;ldquo;How high do my sample sizes have to be in &lt;code&gt;cpquery()&lt;/code&gt; to be within a Â±0.01 margin of &lt;code&gt;query_young&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Luckily, &lt;code&gt;query_young&lt;/code&gt; only has two values and so I can tell &lt;code&gt;cpquery()&lt;/code&gt; to only compute one of them as the other follows logically by subtracting the former from 1.&lt;/p&gt;
&lt;p&gt;Here, I want to test this for likelihood weighting (&lt;code&gt;lw&lt;/code&gt;) and logic sampling (&lt;code&gt;ls&lt;/code&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# create test list and test sequence
precis_ls &amp;lt;- as.list(c(0, 0))
names(precis_ls) &amp;lt;- c(&amp;quot;LW&amp;quot;, &amp;quot;LS&amp;quot;)
n_seq &amp;lt;- as.integer(seq(from = 1e2, to = 1e5, length.out = 1e2))
# iterate over our sample sizes
for (i in n_seq) {
  precis_ls$LW &amp;lt;- c(
    precis_ls$LW,
    cpquery(bn.mle, event = (O == &amp;quot;emp&amp;quot;), evidence = list(A = &amp;quot;young&amp;quot;), method = &amp;quot;lw&amp;quot;, n = i)
  )
  precis_ls$LS &amp;lt;- c(
    precis_ls$LS,
    cpquery(bn.mle, event = (O == &amp;quot;emp&amp;quot;), evidence = (A == &amp;quot;young&amp;quot;), method = &amp;quot;ls&amp;quot;, n = i)
  )
}
# remove first positions which were blanks
precis_ls$LW &amp;lt;- precis_ls$LW[-1]
precis_ls$LS &amp;lt;- precis_ls$LS[-1]
# plotting the results
plot_df &amp;lt;- data.frame(
  N = c(n_seq, n_seq),
  Precision = c(
    query_young[1] - precis_ls$LW,
    query_young[1] - precis_ls$LS
  ),
  Method = rep(c(&amp;quot;Likelihood Weighting&amp;quot;, &amp;quot;Logical Sampling&amp;quot;), each = length(n_seq))
)
ggplot(data = plot_df, aes(x = N, y = Precision, col = Method)) +
  geom_line(size = 1.5) +
  geom_hline(yintercept = 0.01) +
  geom_hline(yintercept = -0.01) +
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-04-13-crc_part1_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As is evident from this plot, we do not need much in terms of sample to arrive at highly precise results using &lt;code&gt;cpquery()&lt;/code&gt; with either method. Still, to be safe, I would probably always run with &lt;code&gt;n = 1e3&lt;/code&gt; at least.&lt;/p&gt;
&lt;h4 id=&#34;part-3-3&#34;&gt;Part 3.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Use the functions in &lt;code&gt;bnlearn&lt;/code&gt; to extract the DAG from &lt;code&gt;bn.mle&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dag &amp;lt;- bn.net(bn.mle)
dag
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   Random/Generated Bayesian network
## 
##   model:
##    [A][S][E|A:S][O|E][R|E][T|O:R] 
##   nodes:                                 6 
##   arcs:                                  6 
##     undirected arcs:                     0 
##     directed arcs:                       6 
##   average markov blanket size:           2.67 
##   average neighbourhood size:            2.00 
##   average branching factor:              1.00 
## 
##   generation algorithm:                  Empty
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-4-2&#34;&gt;Part 4.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Which nodes d-separate Age and Occupation?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sapply(nodes(dag), function(z) dsep(dag, &amp;quot;A&amp;quot;, &amp;quot;O&amp;quot;, z))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     A     E     O     R     S     T 
##  TRUE  TRUE  TRUE FALSE FALSE FALSE
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;scutari-15&#34;&gt;Scutari 1.5&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Implement an R function for BN inference via rejection sampling using the description provided in Section 1.4 as a reference.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;From the book:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;In rejection sampling, we generate random independent observations from the BN. Then we count how many match the evidence we are conditioning on and how many of those observations also match the event whose probability we are computing; the estimated conditional probability is the ratio between the latter and the former.&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rejection.sampling &amp;lt;- function(bn, nsim, event.node, event.value, evidence.node, evidence.value) {
  sims &amp;lt;- rbn(x = bn, n = nsim) # random samples for each node from a Bayesian network
  m1 &amp;lt;- sims[sims[, evidence.node] == evidence.value, ] # retain only those samples where our evidence node matches the evidence condition
  m2 &amp;lt;- m1[m1[, event.node] == event.value, ] # retain only those samples where our event node matches the event condition
  return(nrow(m2) / nrow(m1)) # how many percent of the evidence samples also return the event state?
}
rejection.sampling(
  bn = bn.mle, nsim = 10^4,
  event.node = &amp;quot;O&amp;quot;, event.value = &amp;quot;emp&amp;quot;,
  evidence.node = &amp;quot;A&amp;quot;, evidence.value = &amp;quot;young&amp;quot;
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9640978
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;scutari-16&#34;&gt;Scutari 1.6&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Using the &lt;code&gt;dag&lt;/code&gt; and &lt;code&gt;bn&lt;/code&gt; objects from Sections 1.2 and 1.3:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The DAG in question is the same as &lt;code&gt;dag&lt;/code&gt; in these solutions. The BN is the same as &lt;code&gt;bn.mle&lt;/code&gt; or &lt;code&gt;bn_full&lt;/code&gt;. Since I do this for the Bayesian part of Bayesian networks, I use the latter.&lt;/p&gt;
&lt;h4 id=&#34;part-1-4&#34;&gt;Part 1.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Plot the DAG using &lt;code&gt;graphviz.plot&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Easy enough:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;graphviz.plot(dag)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-04-13-crc_part1_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h4 id=&#34;part-2-4&#34;&gt;Part 2.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Plot the DAG again, highlighting the nodes and the arcs that are part of one or more v-structures.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A bit meaningless of a plot because all of these nodes are involved in v-structures. However, the paths E â O, and E â R are not highlighted as v-structures. Why? Because they are sequential paths rather than convergent or divergent.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;vs &amp;lt;- vstructs(dag, arcs = TRUE)
hl &amp;lt;- list(nodes = unique(as.character(vs)), arcs = vs)
graphviz.plot(dag, highlight = hl)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-04-13-crc_part1_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h4 id=&#34;part-3-4&#34;&gt;Part 3.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Plot the DAG one more time, highlighting the path leading from Age to Occupation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;All we need to do is highlight the paths A â E and E â O:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;hl &amp;lt;- matrix(c(&amp;quot;A&amp;quot;, &amp;quot;E&amp;quot;, &amp;quot;E&amp;quot;, &amp;quot;O&amp;quot;), nc = 2, byrow = TRUE)
graphviz.plot(bn.mle, highlight = list(arcs = hl))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-04-13-crc_part1_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h4 id=&#34;part-4-3&#34;&gt;Part 4.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Plot the conditional probability table of Education.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There is a ready-made function for that!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bn.fit.barchart(bn_full$E)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-04-13-crc_part1_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;1440&#34; /&gt;
Across all age ranges, women are much higher educated (on average) than men.&lt;/p&gt;
&lt;h4 id=&#34;part-5-1&#34;&gt;Part 5.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Compare graphically the distributions of Education for male and female interviewees.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here, we simply need to extract the relevant proportions and need them into a barchart.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;junction &amp;lt;- compile(as.grain(bn.mle))
jmale &amp;lt;- setEvidence(junction, &amp;quot;S&amp;quot;, states = &amp;quot;M&amp;quot;)
jfemale &amp;lt;- setEvidence(junction, &amp;quot;S&amp;quot;, states = &amp;quot;F&amp;quot;)
p1 &amp;lt;- barchart(querygrain(jmale, nodes = &amp;quot;E&amp;quot;)$E, main = &amp;quot;Male&amp;quot;, xlim = c(0, 1))
p2 &amp;lt;- barchart(querygrain(jfemale, nodes = &amp;quot;E&amp;quot;)$E, main = &amp;quot;Female&amp;quot;, xlim = c(0, 1))
grid.arrange(p1, p2, ncol = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-04-13-crc_part1_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.2.1 (2022-06-23 ucrt)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19044)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_Germany.utf8  LC_CTYPE=English_Germany.utf8    LC_MONETARY=English_Germany.utf8 LC_NUMERIC=C                     LC_TIME=English_Germany.utf8    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] gridExtra_2.3   lattice_0.20-45 ggplot2_3.3.6   gRain_1.3.11    gRbase_1.8.7    bnlearn_4.8.1  
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.9          assertthat_0.2.1    digest_0.6.29       utf8_1.2.2          R6_2.5.1            stats4_4.2.1        evaluate_0.16       highr_0.9           blogdown_1.13      
## [10] pillar_1.8.1        rlang_1.0.5         rstudioapi_0.14     Rgraphviz_2.40.0    jquerylib_0.1.4     R.utils_2.12.0      R.oo_1.25.0         Matrix_1.5-1        rmarkdown_2.16     
## [19] styler_1.8.0        labeling_0.4.2      stringr_1.4.1       igraph_1.3.4        munsell_0.5.0       compiler_4.2.1      xfun_0.33           pkgconfig_2.0.3     BiocGenerics_0.42.0
## [28] htmltools_0.5.3     tidyselect_1.1.2    tibble_3.1.8        bookdown_0.29       fansi_1.0.3         dplyr_1.0.9         withr_2.5.0         R.methodsS3_1.8.2   grid_4.2.1         
## [37] RBGL_1.72.0         jsonlite_1.8.0      gtable_0.3.1        lifecycle_1.0.2     DBI_1.1.3           magrittr_2.0.3      scales_1.2.1        graph_1.74.0        cli_3.3.0          
## [46] stringi_1.7.8       cachem_1.0.6        farver_2.1.1        bslib_0.4.0         vctrs_0.4.1         generics_0.1.3      tools_4.2.1         R.cache_0.16.0      glue_1.6.2         
## [55] purrr_0.3.4         parallel_4.2.1      fastmap_1.1.0       yaml_2.3.5          colorspace_2.0-3    BiocManager_1.30.18 knitr_1.40          sass_0.4.2
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Biostatistics - Wait... What?!</title>
      <link>https://www.erikkusch.com/courses/excursions-into-biostatistics/1_biostatistics-wait.-what/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/excursions-into-biostatistics/1_biostatistics-wait.-what/</guid>
      <description>&lt;p&gt;I have prepared some &lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/Excursions-into-Biostatistics/Biostatistics---Wait.-What!.html&#34; target=&#34;_blank&#34;&gt; Lecture Slides &lt;/a&gt; for this session.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://www.erikkusch.com/courses/bftp-biome-detection/data-allocation/</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/bftp-biome-detection/data-allocation/</guid>
      <description>&lt;h2 id=&#34;preparing-the-work&#34;&gt;Preparing The Work&lt;/h2&gt;
&lt;p&gt;First of all, most &lt;code&gt;.R&lt;/code&gt; scripts will follow the same kind of structure:&lt;/p&gt;
&lt;h3 id=&#34;head&#34;&gt;Head&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;Head&lt;/strong&gt; is used as an information statement at the top of your code document that informs the user of the contents, author, and (sometimes) date of the last edit on said document. This is highly useful when you are intending to give your document to other people at some point. The head for our analysis might look something like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# ####################################################################### #
# PROJECT: [BFTP] Identifying Biomes And Their Shifts Using Remote Sensing
# CONTENTS: Functionality to download, aggregate, and crop/mask NDVI data
# AUTHOR: Erik Kusch
# EDIT: 09/03/20
# ####################################################################### #
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;preamble&#34;&gt;Preamble&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;Preamble&lt;/strong&gt; is where you set up the most important parameters/guidelines for your coding script. Personally, I &lt;em&gt;highly&lt;/em&gt; recommend to make your first line in the preamble read &lt;code&gt;rm(list=ls())&lt;/code&gt;. This nifty line of code clears your entire working environment in &lt;code&gt;R&lt;/code&gt; meaning that you work from a clean slate thus eliminating all possible interference of previous work. If your code works as intended after this line, it means that your project is &lt;em&gt;internally consistent&lt;/em&gt; and &lt;em&gt;self-contained&lt;/em&gt; which makes your analysis &lt;strong&gt;reproducible&lt;/strong&gt; (we want that!).&lt;/p&gt;
&lt;p&gt;Afterwards, I like to establish a &lt;em&gt;directory&lt;/em&gt; (i.e. &amp;ldquo;folder&amp;rdquo;) structure. After all, no one likes a cluttered folder on their hard drive.Therefore, we identify our current working directory (wd) with &lt;code&gt;getwd()&lt;/code&gt; and save it as an object in &lt;code&gt;R&lt;/code&gt; which we call &lt;code&gt;Dir.Base&lt;/code&gt;. This is the folder in which our document is located and where &lt;code&gt;R&lt;/code&gt; is looking for and saving files to. We don&amp;rsquo;t want to dump everything there. Conclusively, we need to create our own folders within our project folder. We would like to call these folders &amp;ldquo;Data&amp;rdquo; and &amp;ldquo;Plots&amp;rdquo; (the purpose of these folders should be obvious from their names). To actually create these folders on your hard drive, we must first establish the folder paths. We do so by using the &lt;code&gt;paste()&lt;/code&gt; command in &lt;code&gt;R&lt;/code&gt; which combines objects and writes the &lt;code&gt;sep&lt;/code&gt; argument between the combined objects. Here, we take the path to our project folder (&lt;code&gt;Dir.Base &lt;/code&gt;) and combine it with the name of the folder we want (e.g. &amp;ldquo;Data&amp;rdquo;) while using the backslash (&amp;quot;/&amp;quot;) between these two objects as it indicates the jump in a folder hierarchy. The folder is then created using the &lt;code&gt;dir.create()&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;Our preamble then looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rm(list=ls()) # clearing the entire environment
Dir.Base &amp;lt;- getwd() # identifying the current directory
Dir.Data &amp;lt;- paste(Dir.Base, &amp;quot;Data&amp;quot;, sep=&amp;quot;/&amp;quot;) # generating the folder path for data folder
dir.create(Dir.Data) # creating the data folder
Dir.Plots &amp;lt;- paste(Dir.Base, &amp;quot;Plots&amp;quot;, sep=&amp;quot;/&amp;quot;) # generating the folder path for figures folder
dir.create(Dir.Plots) # creating the figures folder
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Usually, this is also where we load &lt;em&gt;packages&lt;/em&gt; for more functionality of our analyses. However, this time, we will do so when they are necessary to give you a better overview and explanation what they do.&lt;/p&gt;
&lt;h3 id=&#34;coding&#34;&gt;Coding&lt;/h3&gt;
&lt;p&gt;All of the important &lt;strong&gt;Coding&lt;/strong&gt; happens after the head and the preamble are written and run in &lt;code&gt;R&lt;/code&gt;. Basically, this is the rest of this document.&lt;/p&gt;
&lt;h2 id=&#34;downloading-ndvi-data&#34;&gt;Downloading NDVI Data&lt;/h2&gt;
&lt;p&gt;First of all, we need to download the NDVI data that we are interested in. One particularly useful repository for this is the Global Inventory Modelling and Mapping Studies (GIMMS) 3g v.1 data set obtained via the Advanced Very High Resolution Radiometer (AVHRR). This time series goes back all the way to January 1982 and contains bi-weekly, global projects of NDVI values.&lt;/p&gt;
&lt;h3 id=&#34;packages&#34;&gt;Packages&lt;/h3&gt;
&lt;p&gt;Firstly, we need some packages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;gimms&lt;/code&gt; is a package which enables us to download the GIMMS 3g v.1 data set directly through &lt;code&gt;R&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;raster&lt;/code&gt; is a package which allows us to establish, handle, and save spatial gridded products of any variable we want (NDVI in this case)&lt;br&gt;
We install and load our packages as follows:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&amp;quot;gimms&amp;quot;) # for GIMMS NDVI data download
library(gimms)
install.packages(&amp;quot;raster&amp;quot;) # for spatial raster format
library(raster)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;downloading&#34;&gt;Downloading&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s download the GIMMS 3g v.1 NDVI data for the entire year of 1982:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gimms_files &amp;lt;- downloadGimms(x = as.Date(&amp;quot;1982-01-01&amp;quot;), # download from January 1982
                             y = as.Date(&amp;quot;1982-12-31&amp;quot;), # download to December 1982
                             dsn = Dir.Data, # save downloads in data folder
                             quiet = FALSE # show download progress
                             )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we want to turn our downloaded data into a raster so we can do spatial analyses with it:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gimms_raster &amp;lt;- rasterizeGimms(x = gimms_files, # the data we rasterize
                               remove_header = TRUE # we don&#39;t need the header of the data
                               )
gimms_raster # some information about the raster stack we created here
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## class      : RasterStack 
## dimensions : 2160, 4320, 9331200, 24  (nrow, ncol, ncell, nlayers)
## resolution : 0.083, 0.083  (x, y)
## extent     : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)
## crs        : +proj=longlat +datum=WGS84 +no_defs 
## names      : ndvi.1.1, ndvi.2.1, ndvi.3.1, ndvi.4.1, ndvi.5.1, ndvi.6.1, ndvi.7.1, ndvi.8.1, ndvi.9.1, ndvi.10.1, ndvi.11.1, ndvi.12.1, ndvi.1.2, ndvi.2.2, ndvi.3.2, ... 
## min values :     -0.3,     -0.3,     -0.3,     -0.3,     -0.3,     -0.3,     -0.3,     -0.3,     -0.3,      -0.3,      -0.3,      -0.3,     -0.3,     -0.3,     -0.3, ... 
## max values :     0.99,     0.99,     1.00,     0.99,     0.99,     0.99,     1.00,     1.00,     0.99,      0.99,      1.00,      1.00,     1.00,     1.00,     1.00, ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, you can see that we have successfully created a &lt;code&gt;RasterStack&lt;/code&gt; with 24 layers (two for each month because measurements were bi-weekly), for the entire earth (extent of -180, 180, -90, 90). We can also see that there are some values below 0 which we don&amp;rsquo;t expect for the NDVI and we will fix this in a second. For now, just notice that we have successfully downloaded the data.&lt;/p&gt;
&lt;h2 id=&#34;aggregating-ndvi-data&#34;&gt;Aggregating NDVI Data&lt;/h2&gt;
&lt;p&gt;With our data successfully downloaded, it is now time to prepare the data further for our analysis.&lt;/p&gt;
&lt;h3 id=&#34;composites&#34;&gt;Composites&lt;/h3&gt;
&lt;p&gt;Firstly, we want to deal with monthly NDVI values. To do so, we want to build monthly maximum composites. Luckily, the &lt;code&gt;gimms&lt;/code&gt; package has just the right option for us:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;indices &amp;lt;- monthlyIndices(gimms_files) # generate month indices from the data
gimms_raster_mvc &amp;lt;- monthlyComposite(gimms_raster, # the data
                                     indices = indices # the indices
                                     )
gimms_raster_mvc # some information about our monthly composite raster stack
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## class      : RasterStack 
## dimensions : 2160, 4320, 9331200, 12  (nrow, ncol, ncell, nlayers)
## resolution : 0.083, 0.083  (x, y)
## extent     : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)
## crs        : +proj=longlat +datum=WGS84 +no_defs 
## names      : index_1.1, index_1.2, index_1.3, index_1.4, index_1.5, index_1.6, index_1.7, index_1.8, index_1.9, index_1.10, index_1.11, index_1.12 
## min values :     -0.30,     -0.30,     -0.30,     -0.29,     -0.30,     -0.30,     -0.30,     -0.30,     -0.30,      -0.30,      -0.30,      -0.30 
## max values :      0.99,      1.00,      0.99,      1.00,      0.99,      1.00,      1.00,      1.00,      1.00,       1.00,       0.99,       0.99
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see above, our new raster stack has the same dimensions, resolution, coordinate reference system (crs), and extent as the previous one. However, we have reduced the number of layers to 12 (one for each month).&lt;/p&gt;
&lt;p&gt;Since there are still negative values present (an artifact of how NASA stores the data or cloud cover), we simply set these to 0:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Negatives &amp;lt;- which(values(gimms_raster_mvc) &amp;lt; 0) # identify all negative values
values(gimms_raster_mvc)[Negatives] &amp;lt;- 0 # set threshold for barren land (NDVI&amp;lt;0)
gimms_raster_mvc
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## class      : RasterBrick 
## dimensions : 2160, 4320, 9331200, 12  (nrow, ncol, ncell, nlayers)
## resolution : 0.083, 0.083  (x, y)
## extent     : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)
## crs        : +proj=longlat +datum=WGS84 +no_defs 
## source     : memory
## names      : index_1.1, index_1.2, index_1.3, index_1.4, index_1.5, index_1.6, index_1.7, index_1.8, index_1.9, index_1.10, index_1.11, index_1.12 
## min values :         0,         0,         0,         0,         0,         0,         0,         0,         0,          0,          0,          0 
## max values :      0.99,      1.00,      0.99,      1.00,      0.99,      1.00,      1.00,      1.00,      1.00,       1.00,       0.99,       0.99
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;See how all of the &lt;code&gt;min values&lt;/code&gt; are now on 0!&lt;/p&gt;
&lt;p&gt;Lastly, we want to see what our data looks like (visual inspection is an important step to sanity check your work). We do so as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(gimms_raster_mvc)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;01---data_allocation_files/figure-html/Aggr3-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What a beautiful seasonal trend of greening we can observe (I&amp;rsquo;ll stop nerding out here before it get&amp;rsquo;s out of hand, don&amp;rsquo;t worry)!&lt;/p&gt;
&lt;h3 id=&#34;annual-values&#34;&gt;Annual Values&lt;/h3&gt;
&lt;p&gt;Lastly, we may wish (and in fact, you will have to) aggregate our data to annual and even more-than-annual means and seasonality measures.&lt;/p&gt;
&lt;h4 id=&#34;mean-values&#34;&gt;Mean Values&lt;/h4&gt;
&lt;p&gt;To establish annual mean values, we simply take the mean for each cell in our raster stack for all the layers as such:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gimms_annual &amp;lt;- calc(gimms_raster_mvc, # data from which to calculate
                     fun=mean, # function which to calculate with
                     na.rm = TRUE # ignore NAs
                     )
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;seasonality-values&#34;&gt;Seasonality Values&lt;/h4&gt;
&lt;p&gt;Measures of seasonality are defined as the span between the maximum value of a cell and the minimum value of the same cell. So, we calculate a maximum raster and a minimum raster and then simply subtract the minimum from the maximum as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;maxi &amp;lt;- calc(gimms_raster_mvc, fun=max)
mini &amp;lt;- calc(gimms_raster_mvc, fun=min)
gimms_seasonality &amp;lt;- maxi-mini
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;\pagebreak&lt;/p&gt;
&lt;h4 id=&#34;plots&#34;&gt;Plots&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at our annual mean and seasonality:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(gimms_annual, main = &amp;quot;Mean&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;01---data_allocation_files/figure-html/Aggr6-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(gimms_seasonality, main = &amp;quot;Seasonality&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;01---data_allocation_files/figure-html/Aggr6-2.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;cropping-ndvi-data&#34;&gt;Cropping NDVI Data&lt;/h2&gt;
&lt;p&gt;Our data is still on a global scale. We are only interested in data across Alaska, though. Let&amp;rsquo;s deal with that.&lt;/p&gt;
&lt;h3 id=&#34;packages--data&#34;&gt;Packages &amp;amp; Data&lt;/h3&gt;
&lt;p&gt;Again, we have to install some packages and load them.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&amp;quot;sp&amp;quot;) # for spatialpolygons format
library(sp)
install.packages(&amp;quot;rgdal&amp;quot;) # for shapefiles
library(rgdal)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Secondly, we require the actual shape files. Personally, I am a big fan of the Natural Earth Shape files (\url{http://www.naturalearthdata.com/downloads/10m-cultural-vectors/}) because of all the different shape files I can get there. Here, we are interested in states/provinces and so want to download the data from here: \url{https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_admin_1_states_provinces.zip}. Thankfully, &lt;code&gt;R&lt;/code&gt;let&amp;rsquo;s us do the downloading as well as the unpacking of archived (.zip) data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Downloading
download.file(&amp;quot;https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_admin_1_states_provinces.zip&amp;quot;,
              destfile = paste(Dir.Data, &amp;quot;Shapes.zip&amp;quot;, sep=&amp;quot;/&amp;quot;)) # destination file
# Unzipping
unzip(paste(Dir.Data, &amp;quot;Shapes.zip&amp;quot;, sep=&amp;quot;/&amp;quot;), # which file to unzip
      exdir = Dir.Data) # where to unzip to
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we want to load our shape files into &lt;code&gt;R&lt;/code&gt;. We do this using the &lt;code&gt;readOGR()&lt;/code&gt; function in &lt;code&gt;R&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Shapes &amp;lt;- readOGR(Dir.Data, # where to look for the file
                  &amp;quot;ne_10m_admin_1_states_provinces&amp;quot;, # the file name
                  verbose = FALSE) # we don&#39;t want an overview of the loaded data
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;crop--mask&#34;&gt;Crop &amp;amp; Mask&lt;/h3&gt;
&lt;p&gt;Now, we are ready to use our shape file for Alaska. First, we have to find out which of our shape files is for Alaska:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Position &amp;lt;- which(Shapes$name_en == &amp;quot;Alaska&amp;quot;) # find the english name that&#39;s &amp;quot;Alaska&amp;quot; in our shapefiles
Alaska_Shp &amp;lt;- Shapes[Position,] # extract the Alaska shapefile
plot(Alaska_Shp) # plot it for inspection
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;01---data_allocation_files/figure-html/Crop4-1.png&#34; width=&#34;1152&#34; /&gt;
We really don&amp;rsquo;t care much about that island chain all the way to the right in our plot.&lt;/p&gt;
&lt;p&gt;This is likely to be an extent-caused issue and we should crop our shape file extent to the easternmost point of Alaska on the continent:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;extent(Alaska_Shp) # extent clearly shows the super-eastern coordinates
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## class      : Extent 
## xmin       : -179 
## xmax       : 180 
## ymin       : 51 
## ymax       : 71
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Crop
Alaska_Shp &amp;lt;- crop(Alaska_Shp, # what to crop
                   extent(-190, -130, 51, 71)) # which extent to crop to
plot(Alaska_Shp) # visualising the cropped product
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;01---data_allocation_files/figure-html/Crop5-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Lovely. That resolved the issue. We are ready for final cropping of our data and saving of the cropped data.&lt;/p&gt;
&lt;h4 id=&#34;mean-values-1&#34;&gt;Mean Values&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s deal with the annual mean for 1982:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Crop
gimms_annual &amp;lt;- crop(gimms_annual, # mean annual data
                     extent(Alaska_Shp)) # cropped Alaska extent
# Mask (this keeps only cells that fall into our shapefile)
gimms_annual &amp;lt;- mask(gimms_annual, # cropped annual means
                     Alaska_Shp) # cropped Alaska shapefile
plot(gimms_annual, main =&amp;quot;Annual Mean 1982&amp;quot;) # inspection time!
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;01---data_allocation_files/figure-html/Crop6-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Save data
writeRaster(x = gimms_annual, # which raster to save
            file = paste(Dir.Data, &amp;quot;1982Mean&amp;quot;, sep=&amp;quot;/&amp;quot;), # which file to save to
            format = &amp;quot;CDF&amp;quot;, overwrite = TRUE) # which format to use and whether to overwrite
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## class      : RasterLayer 
## dimensions : 237, 590, 139830  (nrow, ncol, ncell)
## resolution : 0.083, 0.083  (x, y)
## extent     : -179, -130, 51, 71  (xmin, xmax, ymin, ymax)
## crs        : +proj=longlat +datum=WGS84 +no_defs 
## source     : 1982Mean.nc 
## names      : layer 
## values     : 0, 0.84  (min, max)
## zvar       : layer
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;seasonality-values-1&#34;&gt;Seasonality Values&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Crop
gimms_seasonality &amp;lt;- crop(gimms_seasonality, # mean seasonality data
                     extent(Alaska_Shp)) # cropped Alaska extent
# Mask (this keeps only cells that fall into our shapefile)
gimms_seasonality &amp;lt;- mask(gimms_seasonality, # cropped seasonality data
                     Alaska_Shp) # cropped Alaska shapefile
plot(gimms_seasonality, main = &amp;quot;Seasonality 1982&amp;quot;) # inspection time!
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;01---data_allocation_files/figure-html/Crop7-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Save data
writeRaster(x = gimms_seasonality, # which raster to save
            file = paste(Dir.Data, &amp;quot;1982Season&amp;quot;, sep=&amp;quot;/&amp;quot;), # which file to save to
            format = &amp;quot;CDF&amp;quot;, overwrite = TRUE) # which format to use and whether to overwrite
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## class      : RasterLayer 
## dimensions : 237, 590, 139830  (nrow, ncol, ncell)
## resolution : 0.083, 0.083  (x, y)
## extent     : -179, -130, 51, 71  (xmin, xmax, ymin, ymax)
## crs        : +proj=longlat +datum=WGS84 +no_defs 
## source     : 1982Season.nc 
## names      : layer 
## values     : 0, 1  (min, max)
## zvar       : layer
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Basic Statistics for Biologists</title>
      <link>https://www.erikkusch.com/courses/an-introduction-to-biostatistics/01-an-introduction-to-basic-statistics-for-biologists/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/an-introduction-to-biostatistics/01-an-introduction-to-basic-statistics-for-biologists/</guid>
      <description>&lt;p&gt;I have prepared some &lt;a href=&#34;https://www.erikkusch.com/courses/an-introduction-to-biostatistics/01---An-Introduction-to-Basic-Statistics-for-Biologists_Handout.html&#34; target=&#34;_blank&#34;&gt; Lecture Slides &lt;/a&gt; for this session.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Basic Statistics for Biologists</title>
      <link>https://www.erikkusch.com/courses/biostat101/01-an-introduction-to-basic-statistics-for-biologists/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/biostat101/01-an-introduction-to-basic-statistics-for-biologists/</guid>
      <description>&lt;div class=&#34;alert alert-success&#34;&gt;
  &lt;div&gt;
    &lt;details&gt;
  &lt;summary&gt;Theory slides for this session.&lt;/summary&gt;
  Click the outline of the presentation below to get to the HTML version of the slides for this session.
    &lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/biostat101/01---An-Introduction-to-Basic-Statistics-for-Biologists_Handout.html&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://www.erikkusch.com/courses/BioStat101/01---BioStat101_featured.png&#34; width=&#34;900&#34; margin-top = &#34;0&#34;/&gt;&lt;/a&gt; 
&lt;/details&gt; 
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Finding &amp; Downloading Data with rgbif</title>
      <link>https://www.erikkusch.com/courses/gbif/nfdi-download/</link>
      <pubDate>Tue, 14 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/gbif/nfdi-download/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Custom install &amp;amp; load function
install.load.package &amp;lt;- function(x) {
  if (!require(x, character.only = TRUE)) {
    install.packages(x, repos = &amp;quot;http://cran.us.r-project.org&amp;quot;)
  }
  require(x, character.only = TRUE)
}
## names of packages we want installed (if not installed yet) and loaded
package_vec &amp;lt;- c(
  &amp;quot;rgbif&amp;quot;,
  &amp;quot;knitr&amp;quot;, # for rmarkdown table visualisations
  &amp;quot;ggplot2&amp;quot;, # for plotting
  &amp;quot;sf&amp;quot; # for spatial operations
  # &amp;quot;rio&amp;quot; # for dwc import
)
## executing install &amp;amp; load for each package
sapply(package_vec, install.load.package)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   rgbif   knitr ggplot2      sf 
##    TRUE    TRUE    TRUE    TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;options(gbif_user = &amp;quot;my gbif username&amp;quot;)
options(gbif_email = &amp;quot;my registred gbif e-mail&amp;quot;)
options(gbif_pwd = &amp;quot;my gbif password&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(Pinaceae_backbone &amp;lt;- name_backbone(name = &amp;quot;Pinaceae&amp;quot;))
Pinaceae_key &amp;lt;- Pinaceae_backbone$familyKey

(Fagaceae_backbone &amp;lt;- name_backbone(name = &amp;quot;Fagaceae&amp;quot;))
Fagaceae_key &amp;lt;- Fagaceae_backbone$familyKey




if (file.exists(file.path(getwd(), &amp;quot;registereddownloadSeasonalSchool.RData&amp;quot;))) {
  load(file.path(getwd(), &amp;quot;registereddownloadSeasonalSchool.RData&amp;quot;))
} else {
  res &amp;lt;- occ_download(
    pred_or(
      pred(&amp;quot;taxonKey&amp;quot;, Pinaceae_key),
      pred(&amp;quot;taxonKey&amp;quot;, Fagaceae_key)
    ),
    pred(&amp;quot;basisOfRecord&amp;quot;, &amp;quot;HUMAN_OBSERVATION&amp;quot;),
    pred(&amp;quot;country&amp;quot;, &amp;quot;DE&amp;quot;),
    pred_gte(&amp;quot;year&amp;quot;, 1970),
    pred_lte(&amp;quot;year&amp;quot;, 2020)
  )
  save(res, file = &amp;quot;registereddownload.RData&amp;quot;)
}

## 1. Check GBIF whether data is ready, this function will finish running when done and return metadata
res_meta &amp;lt;- occ_download_wait(res, status_ping = 10, quiet = FALSE)
## 2. Download the data as .zip (can specify a path)
res_get &amp;lt;- occ_download_get(res)
## 3. Load the data into R
res_data &amp;lt;- occ_download_import(res_get)


# remove non-coordinates
sum(!res_data$hasCoordinate)
res_data &amp;lt;- res_data[res_data$hasCoordinate, ]

ggplot(res_data, aes(x = coordinateUncertaintyInMeters)) +
  geom_histogram(bins = 1e2) +
  theme_bw() +
  scale_y_continuous(trans = &amp;quot;log10&amp;quot;)

res_data$hasGeospatialIssues
sum(is.na(res_data$coordinateUncertaintyInMeters))

preci_data &amp;lt;- res_data[which(!(res_data$coordinateUncertaintyInMeters &amp;gt; 1000)), ]
dim(preci_data)

data_subset &amp;lt;- preci_data[
  ,
  c(&amp;quot;scientificName&amp;quot;, &amp;quot;decimalLongitude&amp;quot;, &amp;quot;decimalLatitude&amp;quot;, &amp;quot;basisOfRecord&amp;quot;, &amp;quot;year&amp;quot;, &amp;quot;month&amp;quot;, &amp;quot;day&amp;quot;, &amp;quot;eventDate&amp;quot;, &amp;quot;countryCode&amp;quot;, &amp;quot;municipality&amp;quot;, &amp;quot;stateProvince&amp;quot;, &amp;quot;taxonKey&amp;quot;, &amp;quot;familyKey&amp;quot;, &amp;quot;species&amp;quot;, &amp;quot;catalogNumber&amp;quot;, &amp;quot;hasGeospatialIssues&amp;quot;, &amp;quot;hasCoordinate&amp;quot;, &amp;quot;mediaType&amp;quot;, &amp;quot;datasetKey&amp;quot;)
]
knitr::kable(head(data_subset))

table(data_subset$year)
table(data_subset$stateProvince)

options(digits = 8) ## set 8 digits (ie. all digits, not decimals) for the type cast as.double to keep decimals
data_subset &amp;lt;- as.data.frame(data_subset)
data_subset$lon &amp;lt;- as.double(data_subset$decimalLongitude) ## cast lon from char to double
data_subset$lat &amp;lt;- as.double(data_subset$decimalLatitude) ## cast lat from char to double
data_sf &amp;lt;- st_as_sf(data_subset, coords = c(&amp;quot;lon&amp;quot;, &amp;quot;lat&amp;quot;), remove = FALSE)

## get background map
DE_shp &amp;lt;- rnaturalearth::ne_countries(country = &amp;quot;Germany&amp;quot;, scale = 10, returnclass = &amp;quot;sf&amp;quot;)[, 1]
st_crs(data_sf) &amp;lt;- st_crs(DE_shp)
## make plot
ggplot() +
  geom_sf(data = DE_shp) +
  geom_sf(data = data_sf[, 1], aes(col = factor(data_sf$familyKey))) +
  theme_bw() +
  labs(title = &amp;quot;Occurrences of Pinaceae and Fagaceae recorded by human observations between 1970 and 1999&amp;quot;)








paste(&amp;quot;GBIF Occurrence Download&amp;quot;, occ_download_meta(res)$doi, &amp;quot;accessed via GBIF.org on&amp;quot;, Sys.Date())


ggplot(data.frame(
  Records = c(nrow(data_subset), nrow(res_data)),
  Data = c(&amp;quot;Subset&amp;quot;, &amp;quot;Original&amp;quot;)
), aes(y = Data, x = Records)) +
  geom_bar(stat = &amp;quot;identity&amp;quot;, fill = &amp;quot;#4f004e&amp;quot;) +
  theme_bw(base_size = 12)

## Original
length(unique(res_data$datasetKey))
## Subsetted
length(unique(data_subset$datasetKey))

## Originally downloaded abundances per dataset
plot_data &amp;lt;- data.frame(table(res_data$datasetKey))
ggplot(
  plot_data,
  aes(
    x = factor(Var1, levels = plot_data$Var1[order(plot_data$Freq, decreasing = TRUE)]),
    y = Freq
  )
) +
  geom_col(color = &amp;quot;black&amp;quot;, fill = &amp;quot;forestgreen&amp;quot;) +
  labs(
    y = &amp;quot;Abundance&amp;quot;, x = &amp;quot;Dataset&amp;quot;,
    title = paste0(&amp;quot;Originally downloaded abundances per dataset (&amp;quot;, occ_download_meta(res)$doi, &amp;quot;)&amp;quot;)
  ) +
  theme_bw(base_size = 21) +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())
## Subsetted abundances per dataset
plot_subset &amp;lt;- data.frame(table(data_subset$datasetKey))
ggplot(
  plot_subset,
  aes(
    x = factor(Var1, levels = plot_subset$Var1[order(plot_subset$Freq, decreasing = TRUE)]),
    y = Freq
  )
) +
  geom_col(color = &amp;quot;black&amp;quot;, fill = &amp;quot;darkred&amp;quot;) +
  labs(y = &amp;quot;Abundance&amp;quot;, x = &amp;quot;Dataset&amp;quot;, title = &amp;quot;Subsetted abundances per dataset&amp;quot;) +
  theme_bw(base_size = 21) +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())


DE_shp &amp;lt;- rnaturalearth::ne_countries(country = &amp;quot;Germany&amp;quot;, scale = 10, returnclass = &amp;quot;sf&amp;quot;)[, 1]

BioClimTest &amp;lt;- BioClim(
  Temperature_Var = &amp;quot;2m_temperature&amp;quot;,
  Temperature_DataSet = &amp;quot;reanalysis-era5-land&amp;quot;,
  Temperature_Type = NA,
  Water_Var = &amp;quot;total_precipitation&amp;quot;,
  Water_DataSet = &amp;quot;reanalysis-era5-land-monthly-means&amp;quot;,
  Water_Type = &amp;quot;monthly_averaged_reanalysis&amp;quot;,
  Y_start = 1970,
  Y_end = 2020,
  TZone = &amp;quot;CET&amp;quot;,
  Extent = DE_shp,
  Buffer = 0.5,
  Dir = getwd(),
  FileName = &amp;quot;Germany&amp;quot;,
  FileExtension = &amp;quot;.nc&amp;quot;,
  Compression = 9,
  API_User = API_User,
  API_Key = API_Key,
  TChunkSize = 6000,
  TryDown = 10,
  TimeOut = 36000,
  Cores = 1,
  verbose = TRUE,
  Keep_Raw = FALSE,
  Keep_Monthly = TRUE
)

Plot.BioClim(BioClims = BioClimTest, SF = DE_shp, Water_Var = &amp;quot;Total Precipitation [mm]&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;session-info&#34;&gt;Session Info&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.4.0 (2024-04-24 ucrt)
## Platform: x86_64-w64-mingw32/x64
## Running under: Windows 11 x64 (build 22631)
## 
## Matrix products: default
## 
## 
## locale:
## [1] LC_COLLATE=Norwegian BokmÃ¥l_Norway.utf8  LC_CTYPE=Norwegian BokmÃ¥l_Norway.utf8   
## [3] LC_MONETARY=Norwegian BokmÃ¥l_Norway.utf8 LC_NUMERIC=C                            
## [5] LC_TIME=Norwegian BokmÃ¥l_Norway.utf8    
## 
## time zone: Europe/Oslo
## tzcode source: internal
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] sf_1.0-17     ggplot2_3.5.1 knitr_1.48    rgbif_3.8.1  
## 
## loaded via a namespace (and not attached):
##  [1] styler_1.10.3      sass_0.4.9         utf8_1.2.4         generics_0.1.3    
##  [5] class_7.3-22       xml2_1.3.6         KernSmooth_2.23-22 blogdown_1.19     
##  [9] stringi_1.8.4      digest_0.6.37      magrittr_2.0.3     evaluate_0.24.0   
## [13] grid_4.4.0         bookdown_0.40      fastmap_1.2.0      R.oo_1.26.0       
## [17] R.cache_0.16.0     plyr_1.8.9         jsonlite_1.8.8     R.utils_2.12.3    
## [21] whisker_0.4.1      e1071_1.7-16       DBI_1.2.3          httr_1.4.7        
## [25] purrr_1.0.2        fansi_1.0.6        scales_1.3.0       oai_0.4.0         
## [29] lazyeval_0.2.2     jquerylib_0.1.4    cli_3.6.3          rlang_1.1.4       
## [33] units_0.8-5        R.methodsS3_1.8.2  munsell_0.5.1      withr_3.0.2       
## [37] cachem_1.1.0       yaml_2.3.10        tools_4.4.0        dplyr_1.1.4       
## [41] colorspace_2.1-1   vctrs_0.6.5        R6_2.5.1           proxy_0.4-27      
## [45] classInt_0.4-10    lifecycle_1.0.4    stringr_1.5.1      pkgconfig_2.0.3   
## [49] pillar_1.9.0       bslib_0.8.0        gtable_0.3.6       data.table_1.16.0 
## [53] glue_1.7.0         Rcpp_1.0.13        xfun_0.47          tibble_3.2.1      
## [57] tidyselect_1.2.1   htmltools_0.5.8.1  rmarkdown_2.28     compiler_4.4.0
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>(Bayesian) Networks &amp; R</title>
      <link>https://www.erikkusch.com/courses/bayes-nets/networksr/</link>
      <pubDate>Tue, 27 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/bayes-nets/networksr/</guid>
      <description>&lt;style&gt;

blockquote{
color:#633a00;
}

&lt;/style&gt; 
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/content/courses/bayes-nets/2-Networks-in-R_27-09-22.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Networks in &lt;code&gt;R&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;exercises&#34;&gt;Exercises&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to the exercises at the end of chapter 1 in 
&lt;a href=&#34;https://link.springer.com/book/10.1007/978-1-4614-6446-4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bayesian Networks in R with Applications in Systems Biology&lt;/a&gt;  by Radhakrishnan Nagarajan, Marco Scutari &amp;amp; Sophie LÃ¨bre. Much of my inspiration for these solutions, where necessary, by consulting the solutions provided by the authors themselves as in the appendix.&lt;/p&gt;
&lt;h3 id=&#34;nagarajan-11&#34;&gt;Nagarajan 1.1&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;1.1. Consider a directed acyclic graph with n nodes.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;(a) Show that at least one node must not have any incoming arc, i.e., the graph must contain at least one root node.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A graph without a root node violates the premise of acyclicity inherent to Bayesian Networks.&lt;/p&gt;
&lt;p&gt;Assuming our graph is directed with G = (&lt;strong&gt;V&lt;/strong&gt;, A), and assuming that there is no root node present, for any vertex $V_i$ that we chose of the set &lt;strong&gt;V&lt;/strong&gt;, we can find a path that $V_i \rightarrow &amp;hellip; \rightarrow V_n$ which spans all nodes in &lt;strong&gt;V&lt;/strong&gt;. However, $V_n$ must have an outgoing arc ($V_n \rightarrow &amp;hellip;$) which must connect to any of the nodes already on the path between $V_i$ and $V_n$ thereby incurring a loop or cycle.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;(b) Show that such a graph can have at most n(nâ1) arcs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This can be explained as an iterative process. Starting with any node $V_1 \in \boldsymbol V$, not violating the prerequisite for acyclicity, $V_1$ may only contain $n-1$ outgoing arcs (an arc linking to itself would create a cycle). Continuing now to another node $V_2 \in \boldsymbol V$ and $V_2 \neq V_1$, this node may only contain $n-2$ outgoing arcs as any arc linking to itself or $V_1$ would introduce a cycle or loop.&lt;/p&gt;
&lt;p&gt;Continuing this process to its logical conclusion of $V_n$, we can summarise the number of possible outgoing arcs thusly:&lt;/p&gt;
&lt;p&gt;\begin{equation}
A \leq (n-1) + (n-2) + &amp;hellip; + (1) = \left( n\atop{2} \right) = \frac{n(n-1)}{2}
\end{equation}&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;(c) Show that a path can span at most nâ1 arcs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Any arc contains two nodes - one tail and one head. Therefore, a path spanning n arcs would have to contain n+1 vertices, thus passing trough one vertex twice and introducing a cycle.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;(d) Describe an algorithm to determine the topological ordering of the graph.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Two prominent examples are 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Breadth-first_search&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;breadth-first&lt;/a&gt; (BF) and 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Depth-first_search&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;depth-first&lt;/a&gt; (DF) algorithms.&lt;/p&gt;
&lt;p&gt;The former (BF) attempts to locate a node that satisfies a certain query conditions by exploring a graph from a root node and subsequently evaluating nodes which are equidistant to the root (at a distance of one arc). If none of these nodes satisfies the search criteria, the distance to root node is increased by an additional arc distance.&lt;/p&gt;
&lt;p&gt;DF, on the other hand, starts at a root node and fully explores a randomly chosen path until either a node satisfying the search criteria is reached or the path has ended in which case a new path is explored starting at the root node.&lt;/p&gt;
&lt;h3 id=&#34;nagarajan-12&#34;&gt;Nagarajan 1.2&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;1.2. Consider the graphs shown in Fig. 1.1.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;!-- Figure 1.1 is the following: --&gt;
&lt;img src=&#34;https://www.erikkusch.com/courses/bayes-nets/Nagara1.1.JPG&#34; width=&#34;900&#34;/&gt;
&lt;blockquote&gt;
&lt;p&gt;(a) Obtain the skeleton of the partially directed and directed graphs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I start by loading the &lt;code&gt;visNetwork&lt;/code&gt; &lt;code&gt;R&lt;/code&gt; package. I chose this package simply because I like how it creates interactive visualisations - try it out! Click some of the vertices below or try and drag them around.&lt;/p&gt;
&lt;p&gt;I also load additional html libraries with which I can include the thml outputs produced by &lt;code&gt;visNetwork&lt;/code&gt; in this blog:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(visNetwork)
library(htmlwidgets)
library(htmltools)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, I register the node set as well as the two arc sets we are working with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;V &amp;lt;- data.frame(
  id = c(&amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;C&amp;quot;, &amp;quot;D&amp;quot;, &amp;quot;E&amp;quot;),
  label = c(&amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;C&amp;quot;, &amp;quot;D&amp;quot;, &amp;quot;E&amp;quot;)
)
A_directed &amp;lt;- data.frame(
  from = c(&amp;quot;C&amp;quot;, &amp;quot;C&amp;quot;, &amp;quot;C&amp;quot;, &amp;quot;D&amp;quot;, &amp;quot;A&amp;quot;),
  to = c(&amp;quot;E&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;D&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;B&amp;quot;)
)
A_partial &amp;lt;- data.frame(
  from = c(&amp;quot;E&amp;quot;, &amp;quot;D&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;C&amp;quot;, &amp;quot;C&amp;quot;),
  to = c(&amp;quot;B&amp;quot;, &amp;quot;E&amp;quot;, &amp;quot;D&amp;quot;, &amp;quot;D&amp;quot;, &amp;quot;A&amp;quot;)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that I still have to register a direction even for undirected edges.&lt;/p&gt;
&lt;p&gt;Now, to visualise the skeletons, we simply plot the graphs without any edge directionality:&lt;/p&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for the plotting call of the skeleton of the directed graph:&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## skeleton of directed graph
Nagara1.2aD &amp;lt;- visNetwork(
  nodes = V,
  edges = A_directed
) %&amp;gt;%
  visNodes(
    shape = &amp;quot;circle&amp;quot;,
    font = list(size = 40, color = &amp;quot;white&amp;quot;),
    color = list(
      background = &amp;quot;darkgrey&amp;quot;,
      border = &amp;quot;black&amp;quot;,
      highlight = &amp;quot;orange&amp;quot;
    ),
    shadow = list(enabled = TRUE, size = 10)
  ) %&amp;gt;%
  visEdges(color = list(
    color = &amp;quot;green&amp;quot;,
    highlight = &amp;quot;red&amp;quot;
  )) %&amp;gt;%
  visLayout(randomSeed = 42)
saveWidget(Nagara1.2aD, &amp;quot;Nagara1.2aD.html&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;iframe seamless src=&#34;https://www.erikkusch.com/courses/bayes-nets/Nagara1.2aD.html&#34; width=&#34;900&#34; height=&#34;500&#34;&gt;&lt;/iframe&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for the plotting call of the skeleton of the partially directed graph:&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## skeleton of partially directed graph
Nagara1.2aP &amp;lt;- visNetwork(
  nodes = V,
  edges = A_partial
) %&amp;gt;%
  visNodes(
    shape = &amp;quot;circle&amp;quot;,
    font = list(size = 40, color = &amp;quot;white&amp;quot;),
    color = list(
      background = &amp;quot;darkgrey&amp;quot;,
      border = &amp;quot;black&amp;quot;,
      highlight = &amp;quot;orange&amp;quot;
    ),
    shadow = list(enabled = TRUE, size = 10)
  ) %&amp;gt;%
  visEdges(color = list(
    color = &amp;quot;green&amp;quot;,
    highlight = &amp;quot;red&amp;quot;
  )) %&amp;gt;%
  visLayout(randomSeed = 42)

saveWidget(Nagara1.2aP, &amp;quot;Nagara1.2aP.html&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;iframe seamless src=&#34;https://www.erikkusch.com/courses/bayes-nets/Nagara1.2aP.html&#34; width=&#34;900&#34; height=&#34;500&#34;&gt;&lt;/iframe&gt;
&lt;blockquote&gt;
&lt;p&gt;(b) Enumerate the acyclic graphs that can be obtained by orienting the undirected arcs of the partially directed graph.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Right. Sorting this out using &lt;code&gt;visNetwork&lt;/code&gt; would be a royal pain. So I instead opt for using &lt;code&gt;igraph&lt;/code&gt; for this exercise.&lt;/p&gt;
&lt;p&gt;First, I load the necessary &lt;code&gt;R&lt;/code&gt; package:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(igraph)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, I consider the edge and node set of the partially directed graph:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;V &amp;lt;- data.frame(id = c(&amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;C&amp;quot;, &amp;quot;D&amp;quot;, &amp;quot;E&amp;quot;))
A_partial &amp;lt;- data.frame(
  from = c(&amp;quot;E&amp;quot;, &amp;quot;D&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;C&amp;quot;, &amp;quot;C&amp;quot;),
  to = c(&amp;quot;B&amp;quot;, &amp;quot;E&amp;quot;, &amp;quot;D&amp;quot;, &amp;quot;D&amp;quot;, &amp;quot;A&amp;quot;)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To enumerate all DAGs from these sets, I may only change the directions of the last three edges listed in my edge set.&lt;/p&gt;
&lt;p&gt;Objectively, this exercise could be solved by simply &lt;em&gt;thinking&lt;/em&gt; about the different constellations. &lt;strong&gt;However&lt;/strong&gt;, I code for a living. One may argue that thinking is not necessarily in my wheelhouse (on may also claim the opposite). Hence, I will automate the procedure.&lt;/p&gt;
&lt;p&gt;To do so, I will need to create all possible combinations of directions of currently undirected edges in the graph in question and check whether they create cycles or not.&lt;/p&gt;
&lt;p&gt;To do so, I load the &lt;code&gt;bnlearn&lt;/code&gt; package as it comes with an error message when trying to assign an arc set that introduces cycles. I also register a base graph consisting of only our vertex set:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(bnlearn)
base_bn &amp;lt;- empty.graph(nodes = V$id)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, I simply loop over my three arcs in question and alternate which direction they are pointing. At the deepest level of this monstrosity, I am then assingning the final arcs to the base graph and only retain it if the cyclicity error has not been thrown:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;A_iter &amp;lt;- A_partial
A_ls &amp;lt;- list()
counter &amp;lt;- 1
for (AD in 1:2) {
  A_iter[3, ] &amp;lt;- if (AD == 1) {
    A_partial[3, ]
  } else {
    rev(A_partial[3, ])
  }
  for (CD in 1:2) {
    A_iter[4, ] &amp;lt;- if (CD == 1) {
      A_partial[4, ]
    } else {
      rev(A_partial[4, ])
    }
    for (CA in 1:2) {
      A_iter[5, ] &amp;lt;- if (CA == 1) {
        A_partial[5, ]
      } else {
        rev(A_partial[5, ])
      }
      A_check &amp;lt;- tryCatch(
        {
          arcs(base_bn) &amp;lt;- A_iter
        },
        error = function(e) {
          &amp;quot;error&amp;quot;
        }
      )
      if (all(A_check != &amp;quot;error&amp;quot;)) {
        A_ls[[counter]] &amp;lt;- base_bn
        counter &amp;lt;- counter + 1
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How many valid DAGs did this result in?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;length(A_ls)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 6
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s plot these out and be done with it:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;par(mfrow = c(2, 3))
for (plot_iter in A_ls) {
  dag_igraph &amp;lt;- graph_from_edgelist(arcs(plot_iter))
  plot(dag_igraph,
    layout = layout.circle,
    vertex.size = 30
  )
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-09-27-practice_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;(c) List the arcs that can be reversed (i.e., turned in the opposite direction), one at a time, without introducing cycles in the directed graph.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;All arcs of the directed graph can be reversed without introducing cycles.&lt;/p&gt;
&lt;h3 id=&#34;nagarajan-13&#34;&gt;Nagarajan 1.3&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;1.3. The (famous) iris data set reports the measurements in centimeters of the sepal length and width and the petal length and width for 50 flowers from each of 3 species of iris (âsetosa,â âversicolor,â and âvirginicaâ).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;(a) Load the iris data set (it is included in the datasets package, which is part of the base R distribution and does not need to be loaded explicitly) and read its manual page.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(iris)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;?iris
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;(b) Investigate the structure of the data set.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(iris)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &#39;data.frame&#39;:	150 obs. of  5 variables:
##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...
##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...
##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...
##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...
##  $ Species     : Factor w/ 3 levels &amp;quot;setosa&amp;quot;,&amp;quot;versicolor&amp;quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;(c) Compare the sepal length among the three species by plotting histograms side by side.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(ggplot2)
ggplot(iris, aes(x = Sepal.Length)) +
  geom_histogram() +
  facet_wrap(~Species) +
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-09-27-practice_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;(d) Repeat the previous point using boxplots.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(ggplot2)
ggplot(iris, aes(x = Species, y = Sepal.Length)) +
  geom_boxplot() +
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-09-27-practice_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;nagarajan-14&#34;&gt;Nagarajan 1.4&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;1.4. Consider again the iris data set from Exercise 1.3.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;(a) Write the data frame holding iris data frame into a space-separated text file named âiris.txt,â and read it back into a second data frame called iris2.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;write.table(iris, file = &amp;quot;iris.txt&amp;quot;, row.names = FALSE)
iris2 &amp;lt;- read.table(&amp;quot;iris.txt&amp;quot;, header = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;(b) Check that iris and iris2 are identical.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;identical(iris, iris2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why are they not identical? That&amp;rsquo;s because &lt;code&gt;iris&lt;/code&gt; stores &lt;code&gt;Species&lt;/code&gt; as a &lt;code&gt;factor&lt;/code&gt;. Information which is lost when writing to a .txt file. Let&amp;rsquo;s reformat this and check again:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;iris2$Species &amp;lt;- factor(iris2$Species)
identical(iris, iris2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;(c) Repeat the previous two steps with a file compressed with bzip2 named âiris.txt.bz2.â&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bzfd &amp;lt;- bzfile(&amp;quot;iris.txt.bz2&amp;quot;, open = &amp;quot;w&amp;quot;)
write.table(iris, file = bzfd, row.names = FALSE)
close(bzfd)
bzfd &amp;lt;- bzfile(&amp;quot;iris.txt.bz2&amp;quot;, open = &amp;quot;r&amp;quot;)
iris2 &amp;lt;- read.table(bzfd, header = TRUE)
close(bzfd)
iris2$Species &amp;lt;- factor(iris2$Species)
identical(iris, iris2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;(d) Save iris directly (e.g., without converting it to a text table) into a file called âiris.rda,â and read it back.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;save(iris, file = &amp;quot;iris.rda&amp;quot;)
load(&amp;quot;iris.rda&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;(e) List all R objects in the global environment and remove all of them apart from iris.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ls()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;A_check&amp;quot;     &amp;quot;A_directed&amp;quot;  &amp;quot;A_iter&amp;quot;      &amp;quot;A_ls&amp;quot;        &amp;quot;A_partial&amp;quot;   &amp;quot;AD&amp;quot;          &amp;quot;base_bn&amp;quot;     &amp;quot;bzfd&amp;quot;        &amp;quot;CA&amp;quot;          &amp;quot;CD&amp;quot;          &amp;quot;counter&amp;quot;     &amp;quot;dag_igraph&amp;quot;  &amp;quot;iris&amp;quot;        &amp;quot;iris2&amp;quot;      
## [15] &amp;quot;Nagara1.2aD&amp;quot; &amp;quot;Nagara1.2aP&amp;quot; &amp;quot;plot_iter&amp;quot;   &amp;quot;V&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;l &amp;lt;- ls()
rm(list = l[l != &amp;quot;iris&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;(f) Exit the R saving the contents of the current session.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;quit(save = &amp;quot;yes&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;nagarajan-15&#34;&gt;Nagarajan 1.5&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;1.5. Consider the gaussian.test data set included in bnlearn.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;(a) Print the column names.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;colnames(gaussian.test)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;C&amp;quot; &amp;quot;D&amp;quot; &amp;quot;E&amp;quot; &amp;quot;F&amp;quot; &amp;quot;G&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;(b) Print the range and the quartiles of each variable.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## range
for (var in names(gaussian.test)) {
  print(range(gaussian.test[, var]))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -2.246520  4.847388
## [1] -10.09456  14.21538
## [1] -15.80996  32.44077
## [1] -9.043796 26.977326
## [1] -3.558768 11.494383
## [1] -1.170247 45.849594
## [1] -1.365823 12.409607
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## quantiles
for (var in names(gaussian.test)) {
  print(quantile(gaussian.test[, var]))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         0%        25%        50%        75%       100% 
## -2.2465197  0.3240041  0.9836491  1.6896519  4.8473877 
##           0%          25%          50%          75%         100% 
## -10.09455807  -0.01751825   2.00025495   4.07692065  14.21537969 
##         0%        25%        50%        75%       100% 
## -15.809961   3.718150   8.056369  12.373614  32.440769 
##        0%       25%       50%       75%      100% 
## -9.043796  5.984274  8.994232 12.164417 26.977326 
##        0%       25%       50%       75%      100% 
## -3.558768  2.095676  3.508567  4.873497 11.494383 
##        0%       25%       50%       75%      100% 
## -1.170247 17.916175 21.982997 26.330886 45.849594 
##        0%       25%       50%       75%      100% 
## -1.365823  3.738940  5.028420  6.344179 12.409607
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;(c) Print all the observations for which A falls in the interval [3,4] and B in (ââ,â5]âª[10,â).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;TestA &amp;lt;- (gaussian.test[, &amp;quot;A&amp;quot;] &amp;gt;= 3) &amp;amp; (gaussian.test[, &amp;quot;A&amp;quot;] &amp;lt;= 4)
TestB &amp;lt;- (gaussian.test[, &amp;quot;B&amp;quot;] &amp;lt;= -4) | (gaussian.test[, &amp;quot;B&amp;quot;] &amp;gt;= 4)
gaussian.test[TestA &amp;amp; TestB, ]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             A         B          C           D         E        F         G
## 134  3.000115  5.677259 19.6165914 13.90710134 1.2399546 29.66208 6.1184117
## 171  3.912097  5.000325 19.5261459 13.31373543 0.2807556 32.51754 7.3699037
## 954  3.735995  4.052434 18.1856087 12.09431457 3.7080579 26.97241 3.0517690
## 1034 3.317637  4.837303 18.1044125 13.32279487 2.9563555 28.40041 3.9876290
## 1042 3.372036  4.197395 17.6533233 12.52777893 4.8983597 31.08633 3.7283984
## 1078 3.157623  5.184670 19.0430676 13.89072391 3.2572347 37.08646 8.9429136
## 1127 3.141251  4.269507 17.1939411 12.42629251 2.6622288 29.53266 4.4132469
## 1237 3.031727  4.341881 17.6020776 12.43625964 2.7949498 27.74622 5.4084005
## 1755 3.052266  4.612071 18.1180989 13.19968750 5.6471997 27.78189 1.5129846
## 1819 3.290631  7.143942 22.6867359 16.14048867 4.8648686 37.51501 8.0241134
## 2030 3.289162  5.787095 20.0194963 14.52479092 5.2423368 37.46786 7.6643575
## 2153 3.037955  4.797399 17.1331929 13.58727751 2.7524554 31.84176 5.5202624
## 2179 3.114916  5.414628 19.3677155 14.14501162 3.4438786 31.06106 3.8799064
## 2576 3.458761  4.471637 18.1722539 13.36940122 2.3688318 23.60165 0.7689322
## 2865 3.140513  7.269383 22.5686681 16.86773531 4.0260061 36.17848 6.1028714
## 3035 3.476832  4.109519 17.0599625 11.55995675 4.4027366 29.83397 3.6381844
## 3133 3.667252  4.953129 19.6575484 13.31833594 5.0080665 31.78321 3.9031780
## 3434 3.418895  6.412021 21.8072576 15.48090391 5.6847825 29.41806 1.3941551
## 3481 3.050811  6.203747 21.1427355 15.39309314 4.2068982 33.79386 5.1348785
## 3573 3.612315  4.741220 18.4841065 13.08992732 5.4532191 31.22224 3.3986084
## 3695 3.284053  4.899003 17.7691812 13.73467842 4.5814578 39.96773 9.5808916
## 3893 3.070645  6.111989 20.6963754 15.34110860 2.0329921 29.92680 4.1352188
## 3999 3.493238  5.307218 19.1502279 14.10123450 4.8567953 35.69970 5.8485995
## 4144 3.045976  4.925688 18.8388604 13.28690773 7.3473994 34.12657 4.0527848
## 4164 3.624343  5.411443 20.3400016 13.35879870 7.4107565 32.20646 2.5717899
## 4220 3.133246  4.950543 17.9604182 13.91087282 4.2105519 30.01146 3.7960603
## 4229 3.119493  7.255816 22.4329800 16.59886045 2.4893039 33.62491 5.0819631
## 4258 3.777205  7.189762 23.9019969 16.75321069 4.0727603 37.66095 6.6653238
## 4671 3.455920 -4.198865  0.1452861 -0.27503006 1.9954874 16.60534 4.7261660
## 4703 3.301100 -4.109750  0.2244686 -0.07441052 6.2531813 23.58530 6.4627941
## 4739 3.010097  9.775164 28.1861733 20.54659992 5.1594216 40.50032 5.8467280
## 4779 3.215547  6.393758 20.7043512 15.59370075 3.2628127 33.35600 5.8151547
## 4866 3.873728 -4.257339  0.8213114 -0.31665717 0.2758219 14.94325 5.4011586
## 4987 3.058566  8.128704 24.9419446 18.56396890 5.8402279 35.29171 4.4032448
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;(d) Sample 50 rows without replacement.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I&amp;rsquo;m leaving out the output to save some space.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(42)
gaussian.test[sample(50, replace = FALSE), ]
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;(e) Draw a bootstrap sample (e.g., sample 5,000 observations with replacement) and compute the mean of each variable.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;colMeans(gaussian.test[sample(5000, replace = TRUE), ])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         A         B         C         D         E         F         G 
##  1.007428  2.076592  8.157574  9.101292  3.532690 22.160643  4.998093
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;(f) Standardize each variable.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I&amp;rsquo;m leaving out the oputput to save some space.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;scale(gaussian.test)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;nagarajan-16&#34;&gt;Nagarajan 1.6&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;1.6. Generate a data frame with 100 observations for the following variables:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;(a) A categorical variable with two levels, low and high. The first 50 observations should be set to low, the others to high.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;A &amp;lt;- factor(c(rep(&amp;quot;low&amp;quot;, 50), rep(&amp;quot;high&amp;quot;, 50)), levels = c(&amp;quot;low&amp;quot;, &amp;quot;high&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;(b) A categorical variable with two levels, good and bad, nested within the first variable, i.e., the first 25 observations should be set to good, the second 25 to bad, and so on.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;nesting &amp;lt;- c(rep(&amp;quot;good&amp;quot;, 25), rep(&amp;quot;bad&amp;quot;, 25))
B &amp;lt;- factor(rep(nesting, 2), levels = c(&amp;quot;good&amp;quot;, &amp;quot;bad&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;(c) A continuous, numerical variable following a Gaussian distribution with mean 2 and variance 4 when the first variable is equal to low and with mean 4 and variance 1 if the first variable is equal to high. In addition, compute the standard deviation of the last variable for each configuration of the first two variables.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(42)
C &amp;lt;- c(rnorm(50, mean = 2, sd = 2), rnorm(50, mean = 4, sd = 1))
data &amp;lt;- data.frame(A = A, B = B, C = C)
aggregate(C ~ A + B, data = data, FUN = sd)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      A    B         C
## 1  low good 2.6127294
## 2 high good 0.9712271
## 3  low  bad 1.8938398
## 4 high  bad 0.8943556
&lt;/code&gt;&lt;/pre&gt;
&lt;!-- ### Scutari Exercises --&gt;
&lt;!-- These are answers and solutions to the exercises at the end of Part 5 in [Bayesian Networks with Examples in R](https://www.bnlearn.com/book-crc/) by M. Scutari and J.-B. Denis. Much of my inspiration for these solutions, where necessary, by consulting the solutions provided by the authors themselves as in the appendix. --&gt;
&lt;!-- #### Scutari 5.1 --&gt;
&lt;!-- &gt; One essential task in any analysis is to import and export the R objects describing models from different packages. This is all the more true in the case of BN modelling, as no package implements all of structure learning, parameter learning and inference. --&gt;
&lt;!-- &gt; 1. Create the dag.bnlearn object from Section 5.1.1. --&gt;
&lt;!-- &gt; 2. Export it to deal. --&gt;
&lt;!-- &gt;3. Import the result back into bnlearn. --&gt;
&lt;!-- &gt; 4. Export dag.bnlearn to catnet and import it back in bnlearn. --&gt;
&lt;!-- &gt; 5. Perform parameter learning using the discretised dmarks and dag.bnlearn and export it to a DSC file, which can be read in Hugin and GeNIe. --&gt;
&lt;!-- #### Scutari 5.2 --&gt;
&lt;!-- &gt; Learn a GBN from the marks data (without the LAT variable) using pcalg and a custom test that defines dependence as significant if the corresponding partial correlation is greater than 0.50. --&gt;
&lt;!-- #### Scutari 5.3 --&gt;
&lt;!-- &gt; Reproduce the example of structure learning from Section 5.1.1 using deal, but set the imaginary sample size to 20. How does the resulting network change? --&gt;
</description>
    </item>
    
    <item>
      <title>Setting up KrigR</title>
      <link>https://www.erikkusch.com/courses/krigr/setup/</link>
      <pubDate>Thu, 26 May 2022 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/krigr/setup/</guid>
      <description>&lt;!-- # Setting Things Up &amp; Preparing the Workshop --&gt;
&lt;h2 id=&#34;installing-krigr&#34;&gt;Installing &lt;code&gt;KrigR&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;First of all, we need to install &lt;code&gt;KrigR&lt;/code&gt;.&lt;/p&gt;
&lt;!-- &lt;div class=&#34;alert alert-info&#34;&gt;
  &lt;div&gt;
    &amp;ndash;&amp;gt;
Until we have implemented our &lt;a href=&#34;https://github.com/ErikKusch/KrigR/issues?q=is%3Aopen+is%3Aissue+label%3Aenhancement&#34;&gt;development ideas and goals&lt;/a&gt;, &lt;code&gt;KrigR&lt;/code&gt; will not be submitted to &lt;a href=&#34;https://cran.r-project.org/&#34;&gt;CRAN&lt;/a&gt; to avoid the hassle of updating an already accepted package. For the time being, &lt;code&gt;KrigR&lt;/code&gt; is only available through the associated &lt;a href=&#34;https://github.com/ErikKusch/KrigR&#34;&gt;GitHub&lt;/a&gt; repository.&lt;/p&gt;
&lt;!--
  &lt;/div&gt;
&lt;/div&gt;
 --&gt;
&lt;p&gt;Installation of an &lt;code&gt;R&lt;/code&gt;-Package from GitHub is facilitated via the &lt;code&gt;devtools&lt;/code&gt; package and its &lt;code&gt;install_github()&lt;/code&gt; function. Thus, &lt;code&gt;KrigR&lt;/code&gt; can be installed and loaded as follows:&lt;/p&gt;
&lt;div class=&#34;alert alert-danger&#34;&gt;
  &lt;div&gt;
    As &lt;code&gt;KrigR&lt;/code&gt; is currently undergoing substantial development in response to the establishment of a new CDS and deprecation of previous dependency packages, you have to install the latest development version.
  &lt;/div&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;devtools::install_github(&amp;quot;https://github.com/ErikKusch/KrigR&amp;quot;, ref = &amp;quot;Development&amp;quot;)
library(KrigR)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;cds-api-access&#34;&gt;CDS API Access&lt;/h2&gt;
&lt;div class=&#34;alert alert-danger&#34;&gt;
  &lt;div&gt;
    If you have used &lt;code&gt;KrigR&lt;/code&gt; or CDS services before 2024-09-16, please be advised that a new CDS has been established and you will need to create a new account and register your new user credentials with &lt;code&gt;KrigR&lt;/code&gt; to ensure continued use of it.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Before you can access 
&lt;a href=&#34;https://cds.climate.copernicus.eu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Climate Data Store&lt;/a&gt; (CDS) products through &lt;code&gt;KrigR&lt;/code&gt;, you need to 
&lt;a href=&#34;https://accounts.ecmwf.int/auth/realms/ecmwf/login-actions/registration?client_id=cds&amp;amp;tab_id=13GUMw5dlhU&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;register an account at CDS&lt;/a&gt;. Don&amp;rsquo;t forget to accept the terms and conditions:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;TermsConditions.png&#34; alt=&#34;&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Once you have created your account, you can find your API credentials on your personal page (which you access by clicking your name in the top-right corner of the webpage) on the CDS webpage:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;APIKEY.png&#34; alt=&#34;&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Once you have done so, we recommend you register the user ID and API Key as characters as seen below (this will match the naming scheme in our workshop material):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;API_User &amp;lt;- &amp;quot;youremail@somethingortheother&amp;quot;
API_Key &amp;lt;- &amp;quot;YourApiKeyGoesHereAsACharacterString&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-success&#34;&gt;
  &lt;div&gt;
    You are now &lt;strong&gt;ready&lt;/strong&gt; for &lt;code&gt;KrigR&lt;/code&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;If this is your &lt;strong&gt;first contact&lt;/strong&gt; with &lt;code&gt;KrigR&lt;/code&gt;, we recommend strongly you &lt;strong&gt;follow the workshop material in order&lt;/strong&gt;. If you &lt;strong&gt;return to&lt;/strong&gt; &lt;code&gt;KrigR&lt;/code&gt; with specific questions or ideas, we recommend you make use of the &lt;strong&gt;search function&lt;/strong&gt; at the top left of this page. If you are short on time, the 
&lt;a href=&#34;https://www.erikkusch.com/courses/krigr/quickstart/&#34;&gt;quick start&lt;/a&gt; guide will be useful to you.&lt;/p&gt;
&lt;!-- ## Session Info --&gt;
&lt;!-- ```{r, echo = FALSE} --&gt;
&lt;!-- sessionInfo() --&gt;
&lt;!-- ``` --&gt;
</description>
    </item>
    
    <item>
      <title>Gaussian Bayesian Networks</title>
      <link>https://www.erikkusch.com/courses/bayes-nets/part-2/</link>
      <pubDate>Tue, 01 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/bayes-nets/part-2/</guid>
      <description>&lt;style&gt;

blockquote{
color:#633a00;
}

&lt;/style&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/content/courses/bayes-nets/4-Gaussian-Networks_11-10-22.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gaussian Bayesian Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;exercises&#34;&gt;Exercises&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to the exercises at the end of Part 2 in 
&lt;a href=&#34;https://www.bnlearn.com/book-crc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bayesian Networks with Examples in R&lt;/a&gt; by M. Scutari and J.-B. Denis. Much of my inspiration for these solutions, where necessary, by consulting the solutions provided by the authors themselves as in the appendix.&lt;/p&gt;
&lt;h3 id=&#34;r-environment&#34;&gt;&lt;code&gt;R&lt;/code&gt; Environment&lt;/h3&gt;
&lt;p&gt;For today&amp;rsquo;s exercise, I load the following packages:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(bnlearn)
library(ggplot2)
library(tidyr)
library(tidybayes)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;scutari-21&#34;&gt;Scutari 2.1&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Prove that Equation (2.2) implies Equation (2.3).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Equation 2.2 reads:&lt;/p&gt;
&lt;p&gt;$$f(C | G = g) \neq f(C)$$&lt;/p&gt;
&lt;p&gt;Equation 2.3 reads:&lt;/p&gt;
&lt;p&gt;$$f(G | C = c) \neq f(G)$$&lt;/p&gt;
&lt;p&gt;So how do we go about demonstrating that the first implies the latter? Well, we are using Bayesian theory here so why not use the Bayes&#39; theorem? So let&amp;rsquo;s start by rewriting equation 2.2:&lt;/p&gt;
&lt;p&gt;$$f(C | G) = \frac{f(C, G)}{f(G)} = \frac{f(G | C) f(C)}{f(G)}$$
So how does this relate to the question that equation 2.2 implies equation 2.3? Well, if $f(C|G) = f(C)$ then this equation would reveal that $f(G|C) = f(G)$ (so that the $f(G)$ terms factor out). Our proof stipulates that these statements aren&amp;rsquo;t true, but one still implies the other and we land of quod erat demonstrandum.&lt;/p&gt;
&lt;h3 id=&#34;scutari-22&#34;&gt;Scutari 2.2&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Within the context of the DAG shown in Figure 2.1, prove that Equation (2.5) is true using Equation (2.6).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is the DAG in question:&lt;/p&gt;
&lt;img src=&#34;https://www.erikkusch.com/courses/networksscutari/Screenshot 2021-06-02 135704.png&#34; width=&#34;900&#34;/&gt;
&lt;p&gt;The equation to prove (2.5) is:&lt;/p&gt;
&lt;p&gt;$$f(N, W | V = v) = f(N | V = v) f(W | V = v)$$&lt;/p&gt;
&lt;p&gt;and we use this equation (2.6) for our proof:&lt;/p&gt;
&lt;p&gt;$$f(G, E, V, N, W, C) = f(G) f(E) f(V | G, E) f(N | V) f(W | V) f(C | N, W)$$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start the proof by integrating over all variables that aren&amp;rsquo;t $N$, $W$, and $V$ (the variables contained in the equation we are tasked to prove):&lt;/p&gt;
&lt;p&gt;$$f(V, W, N) = \int_G \int_E \int_Cf(G,E,V,N,W,C)$$&lt;/p&gt;
&lt;p&gt;We do this to remove all but the variables we are after from our equation so let&amp;rsquo;s follow this rationale:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\int_G \int_E \int_Cf(G,E,V,N,W,C) = &amp;amp;f(V) f(N|V) f(W|V)  \newline
&amp;amp;\times \left( \int_G \int_E f(G) f(E) f(V|G,E) \right) \newline
&amp;amp;\left( \int_C f(C|N,W) \right)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Simplifying this mess, we arrive at:&lt;/p&gt;
&lt;p&gt;$$f(V, W, N) = f(V) f(N|V) f(W|V)$$&lt;/p&gt;
&lt;p&gt;Finally, we can obtain our original formula:&lt;/p&gt;
&lt;p&gt;$$f(W,N|V) = \frac{f(V,W,N)}{f(V)} = \frac{f(V) f(N|V) f(W|V)}{f(V)} = f(N|V) f(W|N)$$&lt;/p&gt;
&lt;p&gt;Another case of the quod erat demonstrandums.&lt;/p&gt;
&lt;h3 id=&#34;scutari-23&#34;&gt;Scutari 2.3&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Compute the marginal variance of the two nodes with two parents from the local distributions proposed in Table 2.1. Why is it much more complicated for C than for V?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Table 2.1 is hardly a table at all, but I did locate it. Basically, it is an amalgamation of the probability distributions proposed for the DAG from the previous exercise:&lt;/p&gt;
&lt;img src=&#34;https://www.erikkusch.com/courses/networksscutari/Screenshot 2021-06-02 141404.png&#34; width=&#34;900&#34;/&gt;
&lt;p&gt;Note that the parameter $07v$ in the second-to-last row should read $0.7v$.&lt;/p&gt;
&lt;p&gt;The two nodes we are after are $V$ and $C$. Since the task already tells us that the computation of the marginal variance for $V$ is easier than for $C$, I start with this one.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Computation for $V$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Simply translating the probability distribution into a linear model, we receive:&lt;/p&gt;
&lt;p&gt;$$V = -10.35534 + 0.5G + 0.70711E + \epsilon_V$$&lt;/p&gt;
&lt;p&gt;with the variances of our independent variables $G$, $E$, and $\epsilon_V$ being $10^2$, $10^2$, and $5^2$ respectively. Consequently the variance of $V$ can be calculated as follows:&lt;/p&gt;
&lt;p&gt;$$VAR(V) = 0.5^2VAR(G) + 0.70711^2VAR(E) + VAR(\epsilon_V)$$
$$VAR(V) = 0.5^210^2+0.70711^210^2+5^2 = 10$$&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Computation for $C$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For $C$, we can transform our portability distribution into a linear model again:&lt;/p&gt;
&lt;p&gt;$$C = 0.3N+0.7W+\epsilon_C$$&lt;/p&gt;
&lt;p&gt;this time, however the &lt;strong&gt;predictors variables&lt;/strong&gt; are &lt;strong&gt;not independent&lt;/strong&gt; since they share node $V$ as their parent. Consequently, we have to compute their covariance:&lt;/p&gt;
&lt;p&gt;$$COV(N,W) = COV(0.1V, 0.7V) = 0.1 * 0.7 * Var(V) = 0.1 * 0.7 * 10^2$$&lt;/p&gt;
&lt;p&gt;So we actually needed to calculate the variance for $V$ to even be able to calculate the variance for $C$. Let&amp;rsquo;s round this out now, then:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
Var(C) &amp;amp;= 0.3^2 * VAR(N) + 0.7^2VAR(W) \newline
&amp;amp;+ VAR(\epsilon_C) + 2 * 0.3 * 0.7 * COV(N,W)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Now, I simply plug the values into the formula and arrive at:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
Var(C) &amp;amp;= 0.3^2 * 9.949874^2+0.7^2 * 7.141428 \newline
&amp;amp;+6.25^2+2 * 0.3 * 0.7 * 0.1 * 0.7 * 10^2  \newline
&amp;amp; = 54.4118
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Curiously, the book suggest this as the solution:&lt;/p&gt;
&lt;p&gt;$$Var(C) = (0.3^2+0.7^2+0.3 * 0.7 * 0.14)10^2+6.25^2 = 100.0024$$&lt;/p&gt;
&lt;p&gt;I am not sure where the values for VAR(N) and VAR(W) have gone here. If anyone who is reading this knows the answer to it, please contact me and let me know as well.&lt;/p&gt;
&lt;h3 id=&#34;scutari-24&#34;&gt;Scutari 2.4&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Write an R script using only the &lt;code&gt;rnorm&lt;/code&gt; and &lt;code&gt;cbind&lt;/code&gt; functions to create a 100 Ã 6 matrix of 100 observations simulated from the BN defined in Table 2.1. Compare the result with those produced by a call to &lt;code&gt;cpdist&lt;/code&gt; function.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To simulate a table of observation using the formulae in the probability distribution collection from the previous question (Table 1), we simply select random values for all parent nodes according to their distributions and let the distributions for all offspring nodes do the rest. One important note here, is that the &lt;code&gt;rnorm()&lt;/code&gt; function in &lt;code&gt;R&lt;/code&gt; takes as an argument of variation the standard deviation $\sigma$ rather than the variance $\sigma^2$:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(42) # making things reproducible
n &amp;lt;- 1e2 # number of replicates
G &amp;lt;- rnorm(n, 50, 10)
E &amp;lt;- rnorm(n, 50, 10)
V &amp;lt;- rnorm(n, -10.35534 + 0.5 * G + 0.70711 * E, 5)
N &amp;lt;- rnorm(n, 45 + 0.1 * V, 9.949874)
W &amp;lt;- rnorm(n, 15 + 0.7 * V, 7.141428)
C &amp;lt;- rnorm(n, 0.3 * N + 0.7 * W, 6.25)
sim1 &amp;lt;- data.frame(cbind(G, E, V, N, W, C))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we do this using the &lt;code&gt;cpdist()&lt;/code&gt; function. To do so, we first have to create our Bayesian Network:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dag.bnlearn &amp;lt;- model2network(&amp;quot;[G][E][V|G:E][N|V][W|V][C|N:W]&amp;quot;)
disE &amp;lt;- list(coef = c(&amp;quot;(Intercept)&amp;quot; = 50), sd = 10)
disG &amp;lt;- list(coef = c(&amp;quot;(Intercept)&amp;quot; = 50), sd = 10)
disV &amp;lt;- list(coef = c(&amp;quot;(Intercept)&amp;quot; = -10.35534, E = 0.70711, G = 0.5), sd = 5)
disN &amp;lt;- list(coef = c(&amp;quot;(Intercept)&amp;quot; = 45, V = 0.1), sd = 9.949874)
disW &amp;lt;- list(coef = c(&amp;quot;(Intercept)&amp;quot; = 15, V = 0.7), sd = 7.141428)
disC &amp;lt;- list(coef = c(&amp;quot;(Intercept)&amp;quot; = 0, N = 0.3, W = 0.7), sd = 6.25)
dis.list &amp;lt;- list(E = disE, G = disG, V = disV, N = disN, W = disW, C = disC)
gbn.bnlearn &amp;lt;- custom.fit(dag.bnlearn, dist = dis.list)
sim2 &amp;lt;- data.frame(cpdist(gbn.bnlearn, nodes = nodes(gbn.bnlearn), evidence = TRUE))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;this is pretty much exactly what is done in the chapter.&lt;/p&gt;
&lt;p&gt;So let&amp;rsquo;s compare these simulation outputs:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# preparing all data together in one data frame for plotting
sim1$sim &amp;lt;- 1
sim2$sim &amp;lt;- 2
Plot_df &amp;lt;- rbind(sim1, sim2[, match(colnames(sim1), colnames(sim2))])
Plot_df &amp;lt;- gather(data = Plot_df, key = &amp;quot;node&amp;quot;, value = &amp;quot;value&amp;quot;, G:C)
Plot_df$sim &amp;lt;- as.factor(Plot_df$sim)
## plotting
ggplot(Plot_df, aes(x = value, y = sim)) +
  stat_halfeye() +
  facet_wrap(~node, scales = &amp;quot;free&amp;quot;) +
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-25-crc_part2_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As is apparent from this, all results fall close to the expected values of roughly 50. There are noticeable differences between the simulations. I would suggest that these are due to the fairly low sample size for &lt;code&gt;sim1&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;scutari-25&#34;&gt;Scutari 2.5&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Imagine two ways other than changing the size of the points (as in Section 2.7.2) to introduce a third variable in the plot.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The plot in question is this one:&lt;/p&gt;
&lt;img src=&#34;https://www.erikkusch.com/courses/networksscutari/Screenshot 2021-06-02 163301.png&#34; width=&#34;900&#34;/&gt;
&lt;p&gt;this plot is aimed at showing the distribution of $C$ when both $E$ and $V$ vary. Here, the variation in $V$ is shown along the x-axis, while the variation of $E$ is contained within the sizes of the circles. The y-axis represents the values of $C$ according to its distributions.&lt;/p&gt;
&lt;p&gt;So how else could we add information of $E$ to a plot of $V$ and $C$? I reckon we could:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Make three scatter plots. One for each pairing of our variables.&lt;/li&gt;
&lt;li&gt;Represent the values of $E$ with a colour saturation gradient.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;scutari-26&#34;&gt;Scutari 2.6&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Can GBNs be extended to log-normal distributions? If so how, if not, why?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;GBNs are Gaussian Bayesian Networks - Bayesian Networks where each node follows a Gaussian distribution.&lt;/p&gt;
&lt;p&gt;Yes, absolutely they can! We can simply take the logarithm of all initial variables and apply the GBN right away. Of course, all values that shall be transformed using the logarithm have to be positive.&lt;/p&gt;
&lt;h3 id=&#34;scutari-27&#34;&gt;Scutari 2.7&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;How can we generalise GBNs as defined in Section 2.3 in order to make each nodeâs variance depend on the nodeâs parents?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I see absolutely no problem here. Let&amp;rsquo;s say we have two nodes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$A$; parent node with a constant variance&lt;/li&gt;
&lt;li&gt;$B$; child node with a variance dependant the parent node&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then we can easily define the variance of $B$ given $A$ ($VAR(B|A)$) as follows:&lt;/p&gt;
&lt;p&gt;$$VAR(B|A) = \left(A-E(A)\right)^2 * \sigma^2_B$$&lt;/p&gt;
&lt;h3 id=&#34;scutari-28&#34;&gt;Scutari 2.8&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;From the first three lines of Table 2.1, prove that the joint distribution of E, G and V is trivariate normal.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This one is a doozy and I really needed to consult the solutions in the book for this one. Let&amp;rsquo;s first remind ourselves of the first lines of said table:&lt;/p&gt;
&lt;img src=&#34;https://www.erikkusch.com/courses/networksscutari/Screenshot 2021-06-02 141404_2.png&#34; width=&#34;900&#34;/&gt;
&lt;p&gt;To approach this problem it is useful to point out that the logarithm of the density of a multivariate normal distribution is defined as such:&lt;/p&gt;
&lt;p&gt;$$f(x) \propto -\frac{1}{2}(x-\mu)^T\sum^{-2}(x-\mu)$$&lt;/p&gt;
&lt;p&gt;with $x$ being a random vector and $\mu$ denoting our expectation. $\sum$ identifies the covariance matrix.&lt;/p&gt;
&lt;p&gt;Simplifying this, we can transform our variables $G$, $E$, and $V$ to give them a zero marginal expectation and a unity marginal variance. That is a very long-winded way of saying: we normalise our variables:&lt;/p&gt;
&lt;p&gt;$$\overline G = \frac{G-E(G)}{\sqrt{VAR(G)}} = \frac{G-50}{10} \sim Normal(0, 1)$$
$$\overline E = \frac{E-E(E)}{\sqrt{VAR(E)}} = \frac{E-50}{10} \sim Normal(0, 1)$$
$$\overline V = \frac{V-E(V)}{\sqrt{VAR(V)}} = \frac{V-50}{10}$$
Solving for $\overline V | \overline G, \overline E$, we obtain:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\overline V | \overline G, \overline E = Normal\left(\frac{1}{2} \overline G + \sqrt{\frac{1}{2}} \overline E , (\frac{1}{2})^2 \right)
\end{equation}&lt;/p&gt;
&lt;!-- $$\overline V | \overline G, \overline E = Normal\left(\frac{1}{2} \overline G + \sqrt{\frac{1}{2}} \overline E , (\frac{1}{2})^2 \right)$$ --&gt;
&lt;p&gt;I have to honestly that I don&amp;rsquo;t quite understand how this happened and if anyone reading this has intuition for this solution, please let me know.&lt;/p&gt;
&lt;p&gt;Now, we can compute the joint density distribution of these three normalised variables:&lt;/p&gt;
&lt;p&gt;$$\begin{eqnarray}  f(\overline G, \overline E, \overline V) &amp;amp;\propto&amp;amp; f(\overline G)+f(\overline E)+f(\overline V | \overline G, \overline E) \newline 
&amp;amp;=&amp;amp; -\frac{g^2}{2}-\frac{e^2}{2}-2 \left( v- \frac{1}{2}g - \sqrt{\frac{1}{2}}e \right)^2     \newline
&amp;amp;=&amp;amp; -\begin{bmatrix} g \newline e \newline v\end{bmatrix}^T \begin{bmatrix} 1 &amp;amp; \frac{\sqrt{2}}{2} &amp;amp; -1\newline \frac{\sqrt{2}}{2} &amp;amp; \frac{3}{2} &amp;amp; -\sqrt{2} \newline -1 &amp;amp; -\sqrt{2} &amp;amp; 2 \end{bmatrix} \begin{bmatrix} g \newline e \newline v\end{bmatrix}  \newline
&amp;amp;=&amp;amp; -\frac{1}{2} \begin{bmatrix} g \newline e \newline v\end{bmatrix}^T \begin{bmatrix} 1 &amp;amp; 0 &amp;amp; \frac{1}{2}\newline 0 &amp;amp; 1 &amp;amp; \frac{1}{2} \newline \frac{1}{2} &amp;amp; \sqrt{\frac{1}{2}} &amp;amp; 1 \end{bmatrix} \begin{bmatrix} g \newline e \newline v\end{bmatrix}  \newline
\end{eqnarray}$$&lt;/p&gt;
&lt;p&gt;I have to admit that most of this is, as of right now, beyond me as I came to this book for the &amp;ldquo;&lt;em&gt;applications in R&lt;/em&gt;&amp;rdquo; in the first place. The book concludes that this results in:&lt;/p&gt;
&lt;p&gt;$$VAR \left( \begin{bmatrix} \overline G \newline \overline E \newline \overline V\end{bmatrix} \right) = \begin{bmatrix} 1 &amp;amp; 0 &amp;amp; \frac{1}{2}\newline 0 &amp;amp; 1 &amp;amp; \frac{1}{2} \newline \frac{1}{2} &amp;amp; \sqrt{\frac{1}{2}} &amp;amp; 1 \end{bmatrix} $$&lt;/p&gt;
&lt;p&gt;which results in our proof.&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.2.1 (2022-06-23 ucrt)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19044)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_Germany.utf8  LC_CTYPE=English_Germany.utf8    LC_MONETARY=English_Germany.utf8 LC_NUMERIC=C                     LC_TIME=English_Germany.utf8    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] tidybayes_3.0.2 tidyr_1.2.0     ggplot2_3.3.6   bnlearn_4.8.1  
## 
## loaded via a namespace (and not attached):
##  [1] styler_1.8.0         tidyselect_1.1.2     xfun_0.33            bslib_0.4.0          purrr_0.3.4          lattice_0.20-45      colorspace_2.0-3     vctrs_0.4.1          generics_0.1.3      
## [10] htmltools_0.5.3      yaml_2.3.5           utf8_1.2.2           rlang_1.0.5          R.oo_1.25.0          jquerylib_0.1.4      pillar_1.8.1         glue_1.6.2           withr_2.5.0         
## [19] DBI_1.1.3            R.utils_2.12.0       distributional_0.3.1 R.cache_0.16.0       lifecycle_1.0.2      stringr_1.4.1        posterior_1.3.1      munsell_0.5.0        blogdown_1.13       
## [28] gtable_0.3.1         R.methodsS3_1.8.2    coda_0.19-4          evaluate_0.16        labeling_0.4.2       knitr_1.40           fastmap_1.1.0        parallel_4.2.1       fansi_1.0.3         
## [37] highr_0.9            arrayhelpers_1.1-0   backports_1.4.1      checkmate_2.1.0      scales_1.2.1         cachem_1.0.6         jsonlite_1.8.0       abind_1.4-5          farver_2.1.1        
## [46] tensorA_0.36.2       digest_0.6.29        svUnit_1.0.6         stringi_1.7.8        bookdown_0.29        dplyr_1.0.9          grid_4.2.1           ggdist_3.2.0         cli_3.3.0           
## [55] tools_4.2.1          magrittr_2.0.3       sass_0.4.2           tibble_3.1.8         pkgconfig_2.0.3      ellipsis_0.3.2       assertthat_0.2.1     rmarkdown_2.16       rstudioapi_0.14     
## [64] R6_2.5.1             compiler_4.2.1
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 01 &amp; 02</title>
      <link>https://www.erikkusch.com/courses/rethinking/chapter-02/</link>
      <pubDate>Tue, 05 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/rethinking/chapter-02/</guid>
      <description>&lt;h1 id=&#34;small-worlds-in-large-worlds&#34;&gt;Small Worlds in Large Worlds&lt;/h1&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/rethinking/1_11-12-2020_SUMMARY_The-Golem-Of-Prague.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides Chapter 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/rethinking/2_18-12-2020_SUMMARY_-Basics-of-Bayesian-Inference-and-Counting.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides Chapter 2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to the exercises at the end of chapter 2 in 
&lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Satistical Rethinking 2&lt;/a&gt; by Richard McElreath. I have created these notes as a part of my ongoing involvement in the 
&lt;a href=&#34;https://www.erikkusch.com/project/aubayes/&#34;&gt;AU Bayes Study Group&lt;/a&gt;. Much of my inspiration for these solutions, where necessary, has been obtained from 
&lt;a href=&#34;https://github.com/jffist/statistical-rethinking-solutions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Taras Svirskyi&lt;/a&gt; and 
&lt;a href=&#34;https://jmgirard.com/statistical-rethinking-ch2/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jeffrey Girard&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;easy-exercises&#34;&gt;Easy Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-e1&#34;&gt;Practice E1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Which of the expressions below correspond to the statement: the probability of rain on Monday?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$Pr(rain)$&lt;/li&gt;
&lt;li&gt;$Pr(rain|Monday)$&lt;/li&gt;
&lt;li&gt;$Pr(Monday|rain)$&lt;/li&gt;
&lt;li&gt;$\frac{Pr(rain,Monday)}{Pr(Monday)}$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2, $Pr(rain|Monday)$ - reads as &amp;ldquo;the probability of rain, given that it is Monday&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;4, $\frac{Pr(rain|Monday)}{Pr(Monday)}$ - reads as &amp;ldquo;the probability that is raining and a Monday, divided by the probability of it being a Monday&amp;rdquo; which is the same as &amp;ldquo;the probability of rain, given that it is Monday. This is simply just the Bayes theorem in action.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;practice-e2&#34;&gt;Practice E2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Which of the following statements corresponds to the expression: $Pr(Monday|rain)$?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The probability of rain on Monday.&lt;/li&gt;
&lt;li&gt;The probability of rain, given that it is Monday.&lt;/li&gt;
&lt;li&gt;The probability that it is Monday, given that it is raining.&lt;/li&gt;
&lt;li&gt;The probability that it is Monday and that it is raining.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;3, The probability that it is Monday, given that it is raining.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;practice-e3&#34;&gt;Practice E3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Which of the expressions below correspond to the statement: the probability that it is Monday, given that it is raining?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$Pr(Monday|rain)$&lt;/li&gt;
&lt;li&gt;$Pr(rain|Monday)$&lt;/li&gt;
&lt;li&gt;$Pr(rain|Monday)Pr(Monday)$&lt;/li&gt;
&lt;li&gt;$\frac{Pr(rain|Monday)Pr(Monday)}{Pr(rain)}$&lt;/li&gt;
&lt;li&gt;$\frac{Pr(Monday|rain)Pr(rain)}{Pr(Monday)}$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1, $Pr(Monday|rain)$&lt;/li&gt;
&lt;li&gt;4, $\frac{Pr(rain|Monday)Pr(Monday)}{Pr(rain)}$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;practice-e4&#34;&gt;Practice E4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; The Bayesian statistician Bruno de Finetti (1906-1985) began his book on probability theory with the declaration: âPROBABILITY DOES NOT EXIST.â The capitals appeared in the original, so I imagine de Finetti wanted us to shout the statement. What he meant is that probability is a device for describing uncertainty from the perspective of an observer with limited knowledge; it has no objective reality. Discuss the globe tossing example from the chapter, in light of this statement. What does it mean to say âthe probability of water is 0.7â?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Completely uninformed and looking only at random samples, we would come to the conclusion that 70% of the Earth are covered by water. However, this estimate could be heavily biased once confounding factors are taken into account. Factors such as inaccuracy of the globe model we are tossing when compared to the real globe (i.e. the model is perfectly spherical, but the Earth itself is a flattened ellipsoid).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;medium-exercises&#34;&gt;Medium Exercises&lt;/h2&gt;
&lt;p&gt;This is where we get into &lt;code&gt;R&lt;/code&gt; application of Bayes. Here, I load a few packages to make my outputs a bit nicer to look at:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rm(list = ls())
library(ggplot2)
library(cowplot)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-m1--m2&#34;&gt;Practice M1 &amp;amp; M2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Recall the globe tossing model from the chapter. Compute and plot the grid approximate posterior distribution for each of the following sets of observations. In each case, assume a uniform prior for p.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;W,W,W&lt;/li&gt;
&lt;li&gt;W,W,W,L&lt;/li&gt;
&lt;li&gt;L,W,W,L,W,W,W&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now assume a prior for p that is equal to zero when p&amp;lt;0.5 and is a positive constant when pâ¥0.5. Again compute and plot the grid approximate posterior distribution for each of the sets of observations in the problem just above.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Register the data we observed
data1 &amp;lt;- c(&amp;quot;W&amp;quot;, &amp;quot;W&amp;quot;, &amp;quot;W&amp;quot;)
data2 &amp;lt;- c(&amp;quot;W&amp;quot;, &amp;quot;W&amp;quot;, &amp;quot;W&amp;quot;, &amp;quot;L&amp;quot;)
data3 &amp;lt;- c(&amp;quot;L&amp;quot;, &amp;quot;W&amp;quot;, &amp;quot;W&amp;quot;, &amp;quot;L&amp;quot;, &amp;quot;W&amp;quot;, &amp;quot;W&amp;quot;, &amp;quot;W&amp;quot;)
data_ls &amp;lt;- list(data1, data2, data3)
names(data_ls) &amp;lt;- c(paste(data1, collapse = &amp;quot;&amp;quot;), paste(data2, collapse = &amp;quot;&amp;quot;), paste(data3, collapse = &amp;quot;&amp;quot;))

# Define grid to sample
p_grid &amp;lt;- seq(0, 1, length.out = 25)
ng &amp;lt;- length(p_grid)

# Register the priors
priors_ls &amp;lt;- list(NA, NA)
names(priors_ls) &amp;lt;- c(&amp;quot;uniform prior (M1)&amp;quot;, &amp;quot;disjunct prior (M2)&amp;quot;)
priors_ls[[1]] &amp;lt;- rep(1, ng)
priors_ls[[2]] &amp;lt;- ifelse(p_grid &amp;lt; 0.5, 0, 1)

# Generate a list within which to store plots for later output all together
plot_ls &amp;lt;- as.list(rep(NA, length(data_ls) * length(priors_ls)))
counter &amp;lt;- 1

# Calculate Posteriors
for (i in 1:length(data_ls)) { # loop over data sets
  data &amp;lt;- data_ls[[i]] # extract data from list
  n &amp;lt;- length(data) # number of observations
  w &amp;lt;- sum(data == &amp;quot;W&amp;quot;) # number of observed water
  for (k in 1:length(priors_ls)) { # loop over priors
    prior &amp;lt;- priors_ls[[k]] # extract prior
    likelihood &amp;lt;- dbinom(w, n, p_grid) # calculate likelihood of water in grid
    posterior &amp;lt;- likelihood * prior # compute posterior
    posterior &amp;lt;- posterior / sum(posterior) # standardise posterior
    # save data to data frame for ggplot
    df &amp;lt;- data.frame(
      param = p_grid,
      prob = posterior,
      ptype = rep(&amp;quot;posterior&amp;quot;, ng)
    )
    # ggplotting to plot list
    plot_ls[[counter]] &amp;lt;- ggplot(df, aes(x = param, y = prob)) +
      geom_line() +
      geom_point() +
      theme_bw() +
      labs(title = paste(names(data_ls)[i], names(priors_ls)[k], sep = &amp;quot; &amp;quot;))
    counter &amp;lt;- counter + 1
  } # end of priors loop
} # end of data set loop
plot_grid(plotlist = plot_ls, ncol = 2, labels = &amp;quot;AUTO&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-05-statistical-rethinking-chapter-02_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;practice-m3&#34;&gt;Practice M3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Suppose there are two globes, one for Earth and one for Mars. The Earth globe is 70% covered in water. The Mars globe is 100% land. Further suppose that one of these globesâyou donât know whichâwas tossed in the air and produces a âlandâ observation. Assume that each globe was equally likely to be tossed. Show that the posterior probability that the globe was the Earth, conditional on seeing âlandâ ($Pr(Earth|land)$), is 0.23.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;br&gt;
From the question, we know that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$P(land|Earth) = 1-0.7 = 0.3$&lt;/li&gt;
&lt;li&gt;$P(land|Mars) = 1$&lt;/li&gt;
&lt;li&gt;$P(Earth) = P(Mars) = .5$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;According to Bayes&#39; theorem, we know that:&lt;br&gt;
$P(Earth|Land) = \frac{P(land|Earth)P(Earth)}{P(land)}$&lt;/p&gt;
&lt;p&gt;Conclusively, we are only missing $P(land)$ which we can calculate from:&lt;/p&gt;
&lt;p&gt;$P(land) = P(land|Earth)P(Earth)+P(land|Mars)Pr(Mars)$&lt;br&gt;
$= 0.3&lt;em&gt;0.5+1&lt;/em&gt;0.5=0.65$&lt;/p&gt;
&lt;p&gt;Finally, we plug all these values into the above Bayes&#39; theorem and obtain:
$P(Earth|Land) = \frac{0.3*0.5}{0.65} = 0.2307692$&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;R&lt;/code&gt;, we can do this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(p_e_l &amp;lt;- (1 - 0.7) * 0.5 / ((1 - 0.7) * 0.5 + 1. * 0.5))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2307692
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-m4&#34;&gt;Practice M4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Suppose you have a deck with only three cards. Each card has two sides, and each side is either black or white. One card has two black sides. The second card has one black and one white side. The third card has two white sides. Now suppose all three cards are placed in a bag and shuffled. Someone reaches into the bag and pulls out a card and places it flat on a table. A black side is shown facing up, but you donât know the colour of the side facing down. Show that the probability that the other side is also black is 2/3. Use the counting method (Section 2 of the chapter) to approach this problem. This means counting up the ways that each card could produce the observed data (a black card facing up on the table).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;br&gt;
I made a small visualisation of this here:
&lt;img src=&#34;https://www.erikkusch.com/courses/rethinking/Ch02_Cards.jpg&#34; width=&#34;900&#34;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;By Counting:&lt;/em&gt;&lt;br&gt;
As you can see, out of all draws that start with a black side facing up on the first draw (3 total paths), only two conclude to the other side also being black. Thus, the probability is two thirds.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Bayes&#39; Theorem:&lt;/em&gt;&lt;br&gt;
$P(Second = B|First = B) = \frac{P(Second = B, First = B)}{P(First = B)} = \frac{P(BB)}{P(BB)+P(WB)*P(First = B|WB)}$
$= \frac{1/3}{(1/3 + 1/3 * 1/2)} = 2/3$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Via &lt;em&gt;&lt;code&gt;R&lt;/code&gt;&lt;/em&gt;:&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bb.ways &amp;lt;- 2
wb.ways &amp;lt;- 1
ww.ways &amp;lt;- 0
likelihood &amp;lt;- c(bb.ways, wb.ways, ww.ways)
prior &amp;lt;- c(1, 1, 1)
posterior &amp;lt;- prior * likelihood
posterior &amp;lt;- posterior / sum(posterior)
posterior[1] == 2 / 3
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-m5&#34;&gt;Practice M5&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Now suppose there are four cards: BB, BW, WW, and another BB. Again suppose a card is drawn from the bag and a black side appears face up. Again calculate the probability that the other side is black.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;br&gt;
Here&amp;rsquo;s an update of my previous visualisation for this:
&lt;img src=&#34;https://www.erikkusch.com/courses/rethinking/Ch02_Cards2.jpg&#34; width=&#34;900&#34;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;By Counting:&lt;/em&gt;&lt;br&gt;
As you can see, out of all draws that start with a black side facing up on the first draw (5 total paths), four conclude to the other side also being black. Thus, the probability is 4/5 which is 0.8.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Bayes&#39; Theorem:&lt;/em&gt;&lt;br&gt;
$P(Second = B|First = B) = \frac{P(Second = B, First = B)}{P(First = B)} = \frac{P(BB)}{P(BB)+P(WB)*P(First = B|WB)}$
$=\frac{1/2}{(1/2 + 1/4 * 1/2)} = 4/5$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Via &lt;em&gt;&lt;code&gt;R&lt;/code&gt;&lt;/em&gt;:&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bb.ways &amp;lt;- 2
wb.ways &amp;lt;- 1
ww.ways &amp;lt;- 0
likelihood &amp;lt;- c(bb.ways, wb.ways, ww.ways)
prior &amp;lt;- c(2, 1, 1)
posterior &amp;lt;- prior * likelihood
posterior &amp;lt;- posterior / sum(posterior)
posterior[1] == 4 / 5
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-m6&#34;&gt;Practice M6&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Imagine that black ink is heavy, and so cards with black sides are heavier than cards with white sides. As a result, itâs less likely that a card with black sides is pulled from the bag. So again assume that there are three cards: BB, BW, and WW. After experimenting a number of times, you conclude that for every way to pull the BB card from the bag, there are 2 ways to pull the BW card and 3 ways to pull the WW card. Again suppose that a card is pulled and a black side appears face up. Show that the probability the other side is black is now 0.5. Use the counting method, as before.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;br&gt;
Here&amp;rsquo;s an update of my previous visualisation for this:
&lt;img src=&#34;https://www.erikkusch.com/courses/rethinking/Ch02_Cards3.jpg&#34; width=&#34;900&#34;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;By Counting:&lt;/em&gt;&lt;br&gt;
Out of all draws that start with a black side facing up on the first draw (4 total paths), only 2 now conclude to the other side also being black. Thus, the probability is 1/2 which is 0.5.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Bayes&#39; Theorem:&lt;/em&gt;&lt;br&gt;
$P(Second = B|First = B) = \frac{P(Second = B, First = B)}{P(First = B)} = \frac{P(BB)}{P(BB)+P(WB)*P(First = B|WB)}$
$= \frac{1/6}{(1/6 + 2/6 * 1/2)} = 1/2$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Via &lt;em&gt;&lt;code&gt;R&lt;/code&gt;&lt;/em&gt;:&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bb.ways &amp;lt;- 2
wb.ways &amp;lt;- 1
ww.ways &amp;lt;- 0
likelihood &amp;lt;- c(bb.ways, wb.ways, ww.ways)
prior &amp;lt;- c(1, 2, 3)
posterior &amp;lt;- prior * likelihood
posterior &amp;lt;- posterior / sum(posterior)
posterior[1] == 0.5
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-m7&#34;&gt;Practice M7&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Assume again the original card problem, with a single card showing a black side face up. Before looking at the other side, we draw another card from the bag and lay it face up on the table. The face that is shown on the new card is white. Show that the probability that the first card, the one showing a black side, has black on its other side is now 0.75. Use the counting method, if you can. Hint: Treat this like the sequence of globe tosses, counting all the ways to see each observation, for each possible first card.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;br&gt;
Here&amp;rsquo;s an update of my previous visualisation for this:
&lt;img src=&#34;https://www.erikkusch.com/courses/rethinking/Ch02_Cards4.jpg&#34; width=&#34;900&#34;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;By Counting:&lt;/em&gt;&lt;br&gt;
Out of all draws that start with a black side facing up on the first draw (12 total paths), 8 now conclude to the next card being places white-side facing up. Thus, the probability is 8/12 which is 0.75.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Via &lt;em&gt;&lt;code&gt;R&lt;/code&gt;&lt;/em&gt;:&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;card.bb.likelihood &amp;lt;- 2 * 3 # bb pulled first (either side), next card is ww (either side) or wb (white-side up)
card.wb.likelihood &amp;lt;- 1 * 2 # wb pulled black side up, next card is ww (either side)
card.ww.likelihood &amp;lt;- 0
likelihood &amp;lt;- c(card.bb.likelihood, card.wb.likelihood, card.ww.likelihood)
prior &amp;lt;- c(1, 1, 1)
posterior &amp;lt;- prior * likelihood
posterior &amp;lt;- posterior / sum(posterior)
posterior[1] == 0.75
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;hard-exercises&#34;&gt;Hard Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-h1&#34;&gt;Practice H1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Suppose there are two species of panda bear. Both are equally common in the wild and live in the same place. They look exactly alike and eat the same food, and there is yet no genetic assay capable of telling them apart. They differ however in family sizes. Species A gives birth to twins 10% of the time, otherwise birthing a single infant. Species B births twins 20% of the time, otherwise birthing singleton infants. Assume these numbers are known with certainty, from many years of field research. Now suppose you are managing a captive panda breeding program. You have a new female panda of unknown species, and she has just given birth to twins. What is the probability that her next birth will also be twins?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;By Hand:&lt;/em&gt;&lt;br&gt;
From the question, we know that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$Pr(twins|A)=0.1$&lt;/li&gt;
&lt;li&gt;$Pr(twins|B)=0.2$&lt;/li&gt;
&lt;li&gt;$Pr(A)=0.5$&lt;/li&gt;
&lt;li&gt;$Pr(B)=0.5$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We know want to calculate $Pr(twins)$ which is given as:&lt;/p&gt;
&lt;p&gt;$Pr(twins)=Pr(twins|A)Pr(A)+Pr(twins|B)Pr(B)$&lt;/p&gt;
&lt;p&gt;For this, however, we need to know the probability of our individual belonging to species A or B, respectively. For this, we will use the Bayes&#39; Theorem:&lt;/p&gt;
&lt;p&gt;$Pr(A|twins)=\frac{Pr(twins|A)Pr(A)}{Pr(twins)}=\frac{0.1*(0.5)}{0.15}=1/3$&lt;br&gt;
$Pr(B|twins)=\frac{Pr(twins|B)Pr(B)}{Pr(twins)}=\frac{0.2*(0.5)}{0.15}=2/3$&lt;/p&gt;
&lt;p&gt;These values can be used as $Pr(A)$ and $Pr(B)$ respectively in the formula to compute $Pr(twins)$ above as such:&lt;/p&gt;
&lt;p&gt;$Pr(twins) = 0.1&lt;em&gt;1/3 + 0.2&lt;/em&gt;2/3 = 1/6$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In&lt;/em&gt; &lt;code&gt;R&lt;/code&gt;&lt;em&gt;:&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;p_twins_A &amp;lt;- 0.1
p_twins_B &amp;lt;- 0.2
likelihood &amp;lt;- c(p_twins_A, p_twins_B)
prior &amp;lt;- c(1, 1)
posterior &amp;lt;- prior * likelihood
posterior &amp;lt;- posterior / sum(posterior)
sum(posterior * likelihood) == 1 / 6
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-h2&#34;&gt;Practice H2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Recall all the facts from the problem above. Now compute the probability that the panda we have is from species A, assuming we have observed only the first birth and that it was twins.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; We already now from our answer above that it is 1/3.&lt;br&gt;
&lt;em&gt;By Hand:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;$Pr(A|twins)=\frac{Pr(twins|A)Pr(A)}{Pr(twins)}=\frac{0.1*(0.5)}{0.15}=1/3$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In&lt;/em&gt; &lt;code&gt;R&lt;/code&gt;&lt;em&gt;:&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;p_twins_A &amp;lt;- 0.1
p_twins_B &amp;lt;- 0.2
likelihood &amp;lt;- c(p_twins_A, p_twins_B)
prior &amp;lt;- c(1, 1)
posterior &amp;lt;- prior * likelihood
posterior &amp;lt;- posterior / sum(posterior)
posterior[1] # 0.33
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3333333
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-h3&#34;&gt;Practice H3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Continuing on from the previous problem, suppose the same panda mother has a second birth and that it is not twins, but a singleton infant. Compute the posterior probability that this panda is species A.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;By Hand:&lt;/em&gt;&lt;br&gt;
The probability of birthing singleton infants given a certain species-membership is given by:&lt;/p&gt;
&lt;p&gt;$Pr(single|A)=1âPr(twins|A)=1â0.1=0.9$&lt;br&gt;
$Pr(single|B)=1âPr(twins|B)=1â0.2=0.8$&lt;/p&gt;
&lt;p&gt;We already know species membership probability given the first birth having been a twin-birth:&lt;/p&gt;
&lt;p&gt;$Pr(A)=1/3$&lt;br&gt;
$Pr(B)=2/3$&lt;/p&gt;
&lt;p&gt;Next, we require the probability of a singleton birth overall, which we calculate as follows:&lt;/p&gt;
&lt;p&gt;$Pr(single)=Pr(single|A)Pr(A)+Pr(single|B)Pr(B)$&lt;br&gt;
$=0.9(1/3)+0.8(2/3)=5/6$&lt;/p&gt;
&lt;p&gt;Finally, we are ready to use the Bayes&#39; Theorem:&lt;/p&gt;
&lt;p&gt;$Pr(A|single)=\frac{Pr(single|A)Pr(A)}{Pr(single)}=\frac{0.9(1/3)}{5/6}=0.36$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In&lt;/em&gt; &lt;code&gt;R&lt;/code&gt;&lt;em&gt;:&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## TWO STEPS WITH UPDATING
p_twins_A &amp;lt;- 0.1
p_twins_B &amp;lt;- 0.2
# first Bayesian update
likelihood_twins &amp;lt;- c(p_twins_A, p_twins_B)
prior &amp;lt;- c(1, 1)
posterior &amp;lt;- prior * likelihood_twins
posterior &amp;lt;- posterior / sum(posterior)
# second Bayesian update
likelihood_single &amp;lt;- c(1 - p_twins_A, 1 - p_twins_B)
prior &amp;lt;- posterior
posterior &amp;lt;- prior * likelihood_single
posterior &amp;lt;- posterior / sum(posterior)
posterior[1] == 0.36
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## IN ONE STEP
p_twins_A &amp;lt;- 0.1
p_twins_B &amp;lt;- 0.2
# likelihood of two events (p(twins_step1 &amp;amp; single_step2|species=X))
likelihood_twins_single &amp;lt;- c(
  p_twins_A * (1 - p_twins_A),
  p_twins_B * (1 - p_twins_B)
)
prior &amp;lt;- c(1, 1)
posterior &amp;lt;- prior * likelihood_twins_single
posterior &amp;lt;- posterior / sum(posterior)
posterior[1] == 0.36
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-h4&#34;&gt;Practice H4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; A common boast of Bayesian statisticians is that Bayesian inferences makes it easy to use all of the data, even if the data are of different types. So suppose now that a veterinarian comes along who has a new genetic test that she claims can identify the species of our mother panda. But the test, like all tests, is imperfect. This is the information you have about the test:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The probability it correctly identifies a species A panda is 0.8.&lt;/li&gt;
&lt;li&gt;The probability it correctly identifies a species B panda is 0.65.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The vet administers the test to your panda and tells you that the test is positive for species A. First ignore your previous information from the births and compute the posterior probability that your panda is species A. Then redo your calculation, now using the birth data as well.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;By Hand:&lt;/em&gt;&lt;br&gt;
Again, the question hands us some information:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$Pr(+A|A)=0.8$&lt;/li&gt;
&lt;li&gt;$Pr(+A|B)=1 â 0.65 = 0.35$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Importantly here, the test was positive for species A. I had previously overlooked this. Thanks to 
&lt;a href=&#34;https://fariasaramis.github.io/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Aramis Farias&lt;/a&gt; for pointing this out to me!&lt;/p&gt;
&lt;p&gt;Currently, we have to assume that both species are equally likely here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$Pr(A)=0.5$&lt;/li&gt;
&lt;li&gt;$Pr(B)=0.5$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now, to calculate $Pr(A|+)$, we require $Pr(+)$:&lt;/p&gt;
&lt;p&gt;$Pr(+)=Pr(+|A)Pr(A)+Pr(+|B)Pr(B)$&lt;br&gt;
$=0.8(0.5)+0.35(0.5)=0.575$&lt;/p&gt;
&lt;p&gt;Finally, we calculate $Pr(A|+)$:
$Pr(A|+)=\frac{Pr(+A|A)Pr(A)}{Pr(+)}=\frac{0.8(0.5)}{0.575}\sim 0.6987$&lt;/p&gt;
&lt;p&gt;Taking into account our previous knowledge on births, we have to set:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$Pr(A)=0.36$&lt;/li&gt;
&lt;li&gt;$Pr(B)=1-Pr(A)=0.64$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We plug these values into the formulae above and obtain:&lt;/p&gt;
&lt;p&gt;$Pr(+)=Pr(+|A)Pr(A)+Pr(+|B)Pr(B)$&lt;br&gt;
$=0.8(0.36)+0.35(0.64)=0.512$&lt;/p&gt;
&lt;p&gt;Finally, we calculate $Pr(A|+)$:
$Pr(A|+)=\frac{Pr(+A|A)Pr(A)}{Pr(+)}=\frac{0.8(0.36)}{0.512}=0.5625$&lt;/p&gt;
&lt;p&gt;Given our test alone, we are probably overestimating assignment to species A, here.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In&lt;/em&gt; &lt;code&gt;R&lt;/code&gt;&lt;em&gt;:&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# WITHOUT BIRTH-INFORMATION
likelihood_test &amp;lt;- c(0.8, 0.35)
prior &amp;lt;- c(1, 1)
posterior_vet &amp;lt;- prior * likelihood_test
posterior_vet &amp;lt;- posterior_vet / sum(posterior_vet)
posterior_vet[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6956522
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# WITH BIRT-INFORMATION
p_twins_A &amp;lt;- 0.1
p_twins_B &amp;lt;- 0.2
# likelihood of two events (p(twins_step1 &amp;amp; single_step2|species=X))
likelihood_twins_single &amp;lt;- c(
  p_twins_A * (1 - p_twins_A),
  p_twins_B * (1 - p_twins_B)
)
prior &amp;lt;- posterior_vet
posterior &amp;lt;- prior * likelihood_twins_single
posterior &amp;lt;- posterior / sum(posterior)
posterior[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5625
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.2.3 (2023-03-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Big Sur ... 10.16
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] cowplot_1.1.1 ggplot2_3.4.1
## 
## loaded via a namespace (and not attached):
##  [1] highr_0.10        bslib_0.4.2       compiler_4.2.3    pillar_1.8.1      jquerylib_0.1.4   R.methodsS3_1.8.2 R.utils_2.12.2    tools_4.2.3       digest_0.6.31     jsonlite_1.8.4   
## [11] evaluate_0.20     lifecycle_1.0.3   tibble_3.2.1      gtable_0.3.1      R.cache_0.16.0    pkgconfig_2.0.3   rlang_1.1.0       cli_3.6.0         rstudioapi_0.14   yaml_2.3.7       
## [21] blogdown_1.16     xfun_0.37         fastmap_1.1.1     withr_2.5.0       dplyr_1.1.0       styler_1.9.1      knitr_1.42        generics_0.1.3    vctrs_0.6.1       sass_0.4.5       
## [31] tidyselect_1.2.0  grid_4.2.3        glue_1.6.2        R6_2.5.1          fansi_1.0.4       rmarkdown_2.20    bookdown_0.33     farver_2.1.1      purrr_1.0.1       magrittr_2.0.3   
## [41] scales_1.2.1      htmltools_0.5.4   colorspace_2.1-0  labeling_0.4.2    utf8_1.2.3        munsell_0.5.0     cachem_1.0.7      R.oo_1.25.0
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Statistical Terminology</title>
      <link>https://www.erikkusch.com/courses/excursions-into-biostatistics/2_statistical-terminology-the-basics-misconceptions-and-pedantics/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/excursions-into-biostatistics/2_statistical-terminology-the-basics-misconceptions-and-pedantics/</guid>
      <description>&lt;p&gt;I have prepared some &lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/Excursions-into-Biostatistics/Statistical-Terminology---The-Basics,-Misconceptions,-and-Pedantics.html&#34; target=&#34;_blank&#34;&gt; Lecture Slides &lt;/a&gt; for this session.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to R</title>
      <link>https://www.erikkusch.com/courses/an-introduction-to-biostatistics/introduction-to-r/</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/an-introduction-to-biostatistics/introduction-to-r/</guid>
      <description>&lt;h2 id=&#34;theory&#34;&gt;Theory&lt;/h2&gt;
&lt;p&gt;These are the solutions to the exercises contained within the handout to Introduction to R which walks you through the basics of the &lt;code&gt;R&lt;/code&gt; machinery. &lt;code&gt;R&lt;/code&gt; is a coding language that can be highly individualised and hence there are often multiple solutions to the same problem. Within these solutions, I shall only present you with one solution for every given task. However, do keep in mind that there is probably a myriad of other ways to achieve your goal.&lt;/p&gt;
&lt;p&gt;I have prepared some &lt;a href=&#34;https://www.erikkusch.com/courses/an-introduction-to-biostatistics/02---Introduction-to-R_Handout.html&#34; target=&#34;_blank&#34;&gt; Lecture Slides &lt;/a&gt; for this session.&lt;/p&gt;
&lt;!-- [Lecture Slides](erikkusch.com/courses/An Introduction to Biostatistics/02---Introduction-to-R_Handout.html) for this session. --&gt;
&lt;h2 id=&#34;creating-and-inspecting-objects&#34;&gt;Creating and Inspecting Objects&lt;/h2&gt;
&lt;h3 id=&#34;vector&#34;&gt;Vector&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A vector reading: &amp;ldquo;A&amp;rdquo;, &amp;ldquo;B&amp;rdquo;, &amp;ldquo;C&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Letters_vec &amp;lt;- c(&amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;C&amp;quot;)
Letters_vec
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;C&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;length(Letters_vec)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;A vector reading: 1, 2, 3&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Numbers_vec &amp;lt;- c(1, 2, 3)
Numbers_vec
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 2 3
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;length(Numbers_vec)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;A vector reading: &lt;code&gt;TRUE&lt;/code&gt;, &lt;code&gt;FALSE&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Logic_vec &amp;lt;- c(TRUE, FALSE)
Logic_vec
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  TRUE FALSE
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;length(Logic_vec)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;A vector of the elements of the first three vectors&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Big_vec &amp;lt;- c(Letters_vec, Numbers_vec, Logic_vec)
Big_vec
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;A&amp;quot;     &amp;quot;B&amp;quot;     &amp;quot;C&amp;quot;     &amp;quot;1&amp;quot;     &amp;quot;2&amp;quot;     &amp;quot;3&amp;quot;     &amp;quot;TRUE&amp;quot;  &amp;quot;FALSE&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;length(Big_vec)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;A vector reading as a sequence of full numbers from 1 to 20&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Seq_vec &amp;lt;- c(1:20)
Seq_vec
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;length(Seq_vec)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 20
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;factor&#34;&gt;Factor&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A factor reading: &amp;ldquo;A&amp;rdquo;, &amp;ldquo;B&amp;rdquo;, &amp;ldquo;C&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Letters_fac &amp;lt;- factor(x = c(&amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;C&amp;quot;))
Letters_fac
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] A B C
## Levels: A B C
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;length(Letters_fac)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;A factor reading: 1, 2, 3&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Numbers_fac &amp;lt;- factor(x = c(1, 2, 3))
Numbers_fac
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 2 3
## Levels: 1 2 3
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;length(Numbers_fac)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;A factor reading: 1, 2, 3 but only levels 1 and 2 are allowed&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Constrained_fac &amp;lt;- factor(x = c(1, 2, 3), levels = c(1, 2))
Constrained_fac
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1    2    &amp;lt;NA&amp;gt;
## Levels: 1 2
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;length(Constrained_fac)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;A factor reading: 1, 2, 3 levels 1 - 4 are allowed&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Expanded_fac &amp;lt;- factor(x = c(1, 2, 3), levels = c(1, 2, 3, 4))
Expanded_fac
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 2 3
## Levels: 1 2 3 4
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;length(Expanded_fac)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;matrix&#34;&gt;Matrix&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The first two vectors we established in distinct columns of a matrix&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Combine_mat &amp;lt;- matrix(data = c(Numbers_vec, Letters_vec), ncol = 2)
Combine_mat
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,] &amp;quot;1&amp;quot;  &amp;quot;A&amp;quot; 
## [2,] &amp;quot;2&amp;quot;  &amp;quot;B&amp;quot; 
## [3,] &amp;quot;3&amp;quot;  &amp;quot;C&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dim(Combine_mat)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3 2
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The first two vectors we established in distinct rows of a matrix&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Pivot_mat &amp;lt;- matrix(data = c(Numbers_vec, Letters_vec), nrow = 2, byrow = TRUE)
Pivot_mat
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3]
## [1,] &amp;quot;1&amp;quot;  &amp;quot;2&amp;quot;  &amp;quot;3&amp;quot; 
## [2,] &amp;quot;A&amp;quot;  &amp;quot;B&amp;quot;  &amp;quot;C&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dim(Pivot_mat)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2 3
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The above matrix with meaningful names&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Names_mat &amp;lt;- Pivot_mat
dimnames(Names_mat) &amp;lt;- list(c(&amp;quot;Numbers&amp;quot;, &amp;quot;Letters&amp;quot;))
Names_mat
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         [,1] [,2] [,3]
## Numbers &amp;quot;1&amp;quot;  &amp;quot;2&amp;quot;  &amp;quot;3&amp;quot; 
## Letters &amp;quot;A&amp;quot;  &amp;quot;B&amp;quot;  &amp;quot;C&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dim(Names_mat)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2 3
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;data-frame&#34;&gt;Data Frame&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The first matrix we established as a data frame&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Combine_df &amp;lt;- data.frame(Combine_mat)
Combine_df
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   X1 X2
## 1  1  A
## 2  2  B
## 3  3  C
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dim(Combine_df)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3 2
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The previous data frame with meaningful names&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Names_df &amp;lt;- Combine_df
colnames(Names_df) &amp;lt;- c(&amp;quot;Numbers&amp;quot;, &amp;quot;Letters&amp;quot;)
Names_df
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Numbers Letters
## 1       1       A
## 2       2       B
## 3       3       C
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dim(Names_df)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3 2
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;list&#34;&gt;List&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The first two vectors we created&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Vectors_ls &amp;lt;- list(Numbers_vec, Letters_vec)
Vectors_ls
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] 1 2 3
## 
## [[2]]
## [1] &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;C&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;length(Vectors_ls)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;statements-and-loops&#34;&gt;Statements and Loops&lt;/h2&gt;
&lt;h3 id=&#34;statements&#34;&gt;Statements&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Numbers_vec&lt;/code&gt; contains more elements than &lt;code&gt;Letters_fac&lt;/code&gt;?&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;length(Numbers_vec) &amp;gt; length(Letters_fac)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The first column of &lt;code&gt;Combine_df&lt;/code&gt; is shorter than &lt;code&gt;Vectors_ls&lt;/code&gt;?&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;length(Combine_df[, 1]) &amp;lt; length(Vectors_ls)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The elements of &lt;code&gt;Letters_vec&lt;/code&gt; are the same as the elements of &lt;code&gt;Letters_fac&lt;/code&gt;?&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Letters_vec == Letters_fac
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE TRUE TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;loops&#34;&gt;Loops&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Print each element of &lt;code&gt;Vectors_ls&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;for (i in 1:length(Vectors_ls)) {
    print(Vectors_ls[[i]])
}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 2 3
## [1] &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;C&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Print each element of &lt;code&gt;Numbers_vec&lt;/code&gt; + 1&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Numbers_veca &amp;lt;- Numbers_vec + 1
for (i in 1:length(Numbers_veca)) {
    print(Numbers_veca[i])
}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2
## [1] 3
## [1] 4
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Subtract 1 from each element of the first column of &lt;code&gt;Combine_mat&lt;/code&gt; and print each element separately&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Mat_column &amp;lt;- Combine_mat[, 1]  # extract data
Mat_column &amp;lt;- as.numeric(Mat_column)  # convert to numeric
Mat_column &amp;lt;- Mat_column - 1  # substract 1
for (i in 1:length(Mat_column)) {
    print(Mat_column[i])
}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0
## [1] 1
## [1] 2
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;useful-commands&#34;&gt;Useful Commands&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Read out your current working directory (not showing you the result as it is different on every machine, it should start like this &amp;ldquo;C:/Users/&amp;hellip;.&amp;quot;)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;getwd()
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Inspect the &lt;code&gt;Vectors_ls&lt;/code&gt; object using the &lt;code&gt;View()&lt;/code&gt; function (again, I am not showing you the result as this only works directly in &lt;code&gt;R&lt;/code&gt; or Rstudio)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;View(Vectors_ls)
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Inspect the &lt;code&gt;Combine_df&lt;/code&gt; object using the &lt;code&gt;View()&lt;/code&gt; function (again, I am not showing you the result as this only works directly in &lt;code&gt;R&lt;/code&gt; or Rstudio)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;View(Combine_df)
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Get the help documentation for the &lt;code&gt;as.matrix()&lt;/code&gt; function (again, I am not showing you the result as this only works directly in &lt;code&gt;R&lt;/code&gt; or Rstudio)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;`?`(as.matrix)
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Install and load the &lt;code&gt;dplyr&lt;/code&gt; package&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&amp;quot;dplyr&amp;quot;)
library(dplyr)
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Remove the &lt;code&gt;Logic_vec&lt;/code&gt; object from your working environment&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rm(Logic_vec)
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Clear your entire working environment&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ls()  # this command shows you all the object in the environment
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;Big_vec&amp;quot;         &amp;quot;Combine_df&amp;quot;      &amp;quot;Combine_mat&amp;quot;     &amp;quot;Constrained_fac&amp;quot;
##  [5] &amp;quot;Expanded_fac&amp;quot;    &amp;quot;i&amp;quot;               &amp;quot;Letters_fac&amp;quot;     &amp;quot;Letters_vec&amp;quot;    
##  [9] &amp;quot;Mat_column&amp;quot;      &amp;quot;Names_df&amp;quot;        &amp;quot;Names_mat&amp;quot;       &amp;quot;Numbers_fac&amp;quot;    
## [13] &amp;quot;Numbers_vec&amp;quot;     &amp;quot;Numbers_veca&amp;quot;    &amp;quot;Pivot_mat&amp;quot;       &amp;quot;Seq_vec&amp;quot;        
## [17] &amp;quot;Vectors_ls&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rm(list = ls())
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to R</title>
      <link>https://www.erikkusch.com/courses/biostat101/introduction-to-r/</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/biostat101/introduction-to-r/</guid>
      <description>&lt;h2 id=&#34;theory&#34;&gt;Theory&lt;/h2&gt;
&lt;p&gt;These are the solutions to the exercises contained within the handout to Introduction to R which walks you through the basics of the &lt;code&gt;R&lt;/code&gt; machinery. &lt;code&gt;R&lt;/code&gt; is a coding language that can be highly individualised and hence there are often multiple solutions to the same problem. Within these solutions, I shall only present you with one solution for every given task. However, do keep in mind that there is probably a myriad of other ways to achieve your goal.&lt;/p&gt;
&lt;div class=&#34;alert alert-success&#34;&gt;
  &lt;div&gt;
    &lt;details&gt;
  &lt;summary&gt;Theory slides for this session.&lt;/summary&gt;
  Click the outline of the presentation below to get to the HTML version of the slides for this session.
    &lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/biostat101/02---Introduction-to-R_Handout.html&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://www.erikkusch.com/courses/biostat101/02---BioStat101_featured.png&#34; width=&#34;900&#34; margin-top = &#34;0&#34;/&gt;&lt;/a&gt;  
&lt;/details&gt; 
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;creating-and-inspecting-objects&#34;&gt;Creating and Inspecting Objects&lt;/h2&gt;
&lt;h3 id=&#34;vector&#34;&gt;Vector&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A vector reading: &amp;ldquo;A&amp;rdquo;, &amp;ldquo;B&amp;rdquo;, &amp;ldquo;C&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Letters_vec &amp;lt;- c(&amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;C&amp;quot;)
Letters_vec
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;C&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;length(Letters_vec)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;A vector reading: 1, 2, 3&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Numbers_vec &amp;lt;- c(1, 2, 3)
Numbers_vec
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 2 3
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;length(Numbers_vec)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;A vector reading: &lt;code&gt;TRUE&lt;/code&gt;, &lt;code&gt;FALSE&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Logic_vec &amp;lt;- c(TRUE, FALSE)
Logic_vec
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  TRUE FALSE
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;length(Logic_vec)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;A vector of the elements of the first three vectors&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Big_vec &amp;lt;- c(Letters_vec, Numbers_vec, Logic_vec)
Big_vec
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;A&amp;quot;     &amp;quot;B&amp;quot;     &amp;quot;C&amp;quot;     &amp;quot;1&amp;quot;     &amp;quot;2&amp;quot;     &amp;quot;3&amp;quot;     &amp;quot;TRUE&amp;quot;  &amp;quot;FALSE&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;length(Big_vec)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;A vector reading as a sequence of full numbers from 1 to 20&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Seq_vec &amp;lt;- c(1:20)
Seq_vec
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;length(Seq_vec)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 20
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;factor&#34;&gt;Factor&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A factor reading: &amp;ldquo;A&amp;rdquo;, &amp;ldquo;B&amp;rdquo;, &amp;ldquo;C&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Letters_fac &amp;lt;- factor(x = c(&amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;C&amp;quot;))
Letters_fac
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] A B C
## Levels: A B C
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;length(Letters_fac)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;A factor reading: 1, 2, 3&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Numbers_fac &amp;lt;- factor(x = c(1, 2, 3))
Numbers_fac
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 2 3
## Levels: 1 2 3
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;length(Numbers_fac)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;A factor reading: 1, 2, 3 but only levels 1 and 2 are allowed&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Constrained_fac &amp;lt;- factor(x = c(1, 2, 3), levels = c(1, 2))
Constrained_fac
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1    2    &amp;lt;NA&amp;gt;
## Levels: 1 2
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;length(Constrained_fac)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;A factor reading: 1, 2, 3 levels 1 - 4 are allowed&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Expanded_fac &amp;lt;- factor(x = c(1, 2, 3), levels = c(1, 2, 3, 4))
Expanded_fac
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 2 3
## Levels: 1 2 3 4
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;length(Expanded_fac)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;matrix&#34;&gt;Matrix&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The first two vectors we established in distinct columns of a matrix&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Combine_mat &amp;lt;- matrix(data = c(Numbers_vec, Letters_vec), ncol = 2)
Combine_mat
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,] &amp;quot;1&amp;quot;  &amp;quot;A&amp;quot; 
## [2,] &amp;quot;2&amp;quot;  &amp;quot;B&amp;quot; 
## [3,] &amp;quot;3&amp;quot;  &amp;quot;C&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dim(Combine_mat)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3 2
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The first two vectors we established in distinct rows of a matrix&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Pivot_mat &amp;lt;- matrix(data = c(Numbers_vec, Letters_vec), nrow = 2, byrow = TRUE)
Pivot_mat
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3]
## [1,] &amp;quot;1&amp;quot;  &amp;quot;2&amp;quot;  &amp;quot;3&amp;quot; 
## [2,] &amp;quot;A&amp;quot;  &amp;quot;B&amp;quot;  &amp;quot;C&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dim(Pivot_mat)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2 3
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The above matrix with meaningful names&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Names_mat &amp;lt;- Pivot_mat
dimnames(Names_mat) &amp;lt;- list(c(&amp;quot;Numbers&amp;quot;, &amp;quot;Letters&amp;quot;))
Names_mat
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         [,1] [,2] [,3]
## Numbers &amp;quot;1&amp;quot;  &amp;quot;2&amp;quot;  &amp;quot;3&amp;quot; 
## Letters &amp;quot;A&amp;quot;  &amp;quot;B&amp;quot;  &amp;quot;C&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dim(Names_mat)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2 3
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;data-frame&#34;&gt;Data Frame&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The first matrix we established as a data frame&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Combine_df &amp;lt;- data.frame(Combine_mat)
Combine_df
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   X1 X2
## 1  1  A
## 2  2  B
## 3  3  C
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dim(Combine_df)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3 2
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The previous data frame with meaningful names&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Names_df &amp;lt;- Combine_df
colnames(Names_df) &amp;lt;- c(&amp;quot;Numbers&amp;quot;, &amp;quot;Letters&amp;quot;)
Names_df
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Numbers Letters
## 1       1       A
## 2       2       B
## 3       3       C
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dim(Names_df)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3 2
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;list&#34;&gt;List&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The first two vectors we created&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Vectors_ls &amp;lt;- list(Numbers_vec, Letters_vec)
Vectors_ls
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] 1 2 3
## 
## [[2]]
## [1] &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;C&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;length(Vectors_ls)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;statements-and-loops&#34;&gt;Statements and Loops&lt;/h2&gt;
&lt;h3 id=&#34;statements&#34;&gt;Statements&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Numbers_vec&lt;/code&gt; contains more elements than &lt;code&gt;Letters_fac&lt;/code&gt;?&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;length(Numbers_vec) &amp;gt; length(Letters_fac)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The first column of &lt;code&gt;Combine_df&lt;/code&gt; is shorter than &lt;code&gt;Vectors_ls&lt;/code&gt;?&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;length(Combine_df[, 1]) &amp;lt; length(Vectors_ls)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The elements of &lt;code&gt;Letters_vec&lt;/code&gt; are the same as the elements of &lt;code&gt;Letters_fac&lt;/code&gt;?&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Letters_vec == Letters_fac
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE TRUE TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;loops&#34;&gt;Loops&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Print each element of &lt;code&gt;Vectors_ls&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;for (i in 1:length(Vectors_ls)) {
    print(Vectors_ls[[i]])
}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 2 3
## [1] &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;C&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Print each element of &lt;code&gt;Numbers_vec&lt;/code&gt; + 1&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Numbers_veca &amp;lt;- Numbers_vec + 1
for (i in 1:length(Numbers_veca)) {
    print(Numbers_veca[i])
}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2
## [1] 3
## [1] 4
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Subtract 1 from each element of the first column of &lt;code&gt;Combine_mat&lt;/code&gt; and print each element separately&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Mat_column &amp;lt;- Combine_mat[, 1]  # extract data
Mat_column &amp;lt;- as.numeric(Mat_column)  # convert to numeric
Mat_column &amp;lt;- Mat_column - 1  # substract 1
for (i in 1:length(Mat_column)) {
    print(Mat_column[i])
}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0
## [1] 1
## [1] 2
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;useful-commands&#34;&gt;Useful Commands&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Read out your current working directory (not showing you the result as it is different on every machine, it should start like this &amp;ldquo;C:/Users/&amp;hellip;.&amp;quot;)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;getwd()
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Inspect the &lt;code&gt;Vectors_ls&lt;/code&gt; object using the &lt;code&gt;View()&lt;/code&gt; function (again, I am not showing you the result as this only works directly in &lt;code&gt;R&lt;/code&gt; or Rstudio)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;View(Vectors_ls)
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Inspect the &lt;code&gt;Combine_df&lt;/code&gt; object using the &lt;code&gt;View()&lt;/code&gt; function (again, I am not showing you the result as this only works directly in &lt;code&gt;R&lt;/code&gt; or Rstudio)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;View(Combine_df)
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Get the help documentation for the &lt;code&gt;as.matrix()&lt;/code&gt; function (again, I am not showing you the result as this only works directly in &lt;code&gt;R&lt;/code&gt; or Rstudio)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;`?`(as.matrix)
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Install and load the &lt;code&gt;dplyr&lt;/code&gt; package&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&amp;quot;dplyr&amp;quot;)
library(dplyr)
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Remove the &lt;code&gt;Logic_vec&lt;/code&gt; object from your working environment&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rm(Logic_vec)
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Clear your entire working environment&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ls()  # this command shows you all the object in the environment
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;Big_vec&amp;quot;         &amp;quot;Combine_df&amp;quot;      &amp;quot;Combine_mat&amp;quot;     &amp;quot;Constrained_fac&amp;quot;
##  [5] &amp;quot;Expanded_fac&amp;quot;    &amp;quot;i&amp;quot;               &amp;quot;Letters_fac&amp;quot;     &amp;quot;Letters_vec&amp;quot;    
##  [9] &amp;quot;Mat_column&amp;quot;      &amp;quot;Names_df&amp;quot;        &amp;quot;Names_mat&amp;quot;       &amp;quot;Numbers_fac&amp;quot;    
## [13] &amp;quot;Numbers_vec&amp;quot;     &amp;quot;Numbers_veca&amp;quot;    &amp;quot;Pivot_mat&amp;quot;       &amp;quot;Seq_vec&amp;quot;        
## [17] &amp;quot;Vectors_ls&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rm(list = ls())
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Cluster Analysis</title>
      <link>https://www.erikkusch.com/courses/bftp-biome-detection/cluster-analysis/</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/bftp-biome-detection/cluster-analysis/</guid>
      <description>&lt;h2 id=&#34;preparing-the-work&#34;&gt;Preparing The Work&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s create our basic structure for this document:&lt;/p&gt;
&lt;h3 id=&#34;head&#34;&gt;Head&lt;/h3&gt;
&lt;p&gt;Not much has changed in the &lt;strong&gt;head&lt;/strong&gt; when compared to our last exercise. We merely change the &lt;em&gt;contents&lt;/em&gt; and and the &lt;em&gt;edit&lt;/em&gt; tag, since the rest stays the same for the entire project.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# ####################################################################### #
# PROJECT: [BFTP] Identifying Biomes And Their Shifts Using Remote Sensing
# CONTENTS: Functionality to identify clusters of NDVI mean and seasonality
# AUTHOR: Erik Kusch
# EDIT: 18/03/20
# ####################################################################### #
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;preamble&#34;&gt;Preamble&lt;/h3&gt;
&lt;p&gt;I am keeping the same &lt;strong&gt;preamble&lt;/strong&gt; as last time because we will need to index the data and the plot directory in this exercise. Our preamble then looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rm(list=ls()) # clearing the entire environment
Dir.Base &amp;lt;- getwd() # identifying the current directory
Dir.Data &amp;lt;- paste(Dir.Base, &amp;quot;Data&amp;quot;, sep=&amp;quot;/&amp;quot;) # generating the folder path for data folder
Dir.Plots &amp;lt;- paste(Dir.Base, &amp;quot;Plots&amp;quot;, sep=&amp;quot;/&amp;quot;) # generating the folder path for figures folder
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice, that we do not call the function &lt;code&gt;dir.create()&lt;/code&gt; this time. We don&amp;rsquo;t need to do so, because we already created the two directories established above in our last exercise. Usually, we would create this entire analysis of your BFTP project in one &lt;code&gt;R&lt;/code&gt; code script. In this case, we would only have one preamble which defines and creates directories instead of doing this step for every single sub-part of the analysis. Alas, we want to break this down for you. Therefore, you see this preamble here and will again in the next exercise.&lt;/p&gt;
&lt;p&gt;Again, this is where would load packages, but I am going to install and load the necessary packages when needed to show you what they are good for. Personally, I recommend you always load all necessary packages at the beginning of your code file and leave comments as to what you load them for. This will make it easier to remove packages you don&amp;rsquo;t need anymore when you change things.&lt;/p&gt;
&lt;h3 id=&#34;coding&#34;&gt;Coding&lt;/h3&gt;
&lt;p&gt;Again, all of the important &lt;strong&gt;Coding&lt;/strong&gt; happens after the head and the preamble are written and run in &lt;code&gt;R&lt;/code&gt;. Basically, this is the rest of this document once more.&lt;/p&gt;
&lt;h2 id=&#34;cluster-analysis&#34;&gt;Cluster Analysis&lt;/h2&gt;
&lt;p&gt;Cluster analyses come in many forms. Here, we are interested in a k-means clustering approach. These approaches identify $k$ (a number) clusters. One of the most prominent ways to do this in &lt;code&gt;R&lt;/code&gt; is undoubtedly the &lt;code&gt;mclust&lt;/code&gt; &lt;code&gt;R&lt;/code&gt; package. Clusters can be thought of as groupings of data in multi-dimensional space. The number of dimensions is equal to the number of clustering components. In the &lt;code&gt;mclust&lt;/code&gt; &lt;code&gt;R&lt;/code&gt; package, the characteristics of these clusters (orientation, volume, shape) are, if not specified otherwise, estimated from the data.&lt;/p&gt;
&lt;!-- They can be set to vary between clusters or constrained to be the same for all clusters. Depending on cluster characteristics, `mclust` distinguishes 20 individual models which you can see in table \ref{tab:MClustModels}, but are not expected to understand fully.   --&gt;
&lt;!-- \begin{table}[ht!] --&gt;
&lt;!--   \centering --&gt;
&lt;!--   \caption[Models in mclust]{\textbf{Models in mclust:} The r-package mclust distinguishes 20 different models for data clustering based on distribution and cluster characteristics.} --&gt;
&lt;!--     \begin{tabular}{|lcccc|} --&gt;
&lt;!--     Acronym &amp; Distribution &amp; Volume &amp; Shape &amp; Orientation \\ --&gt;
&lt;!--     \hline --&gt;
&lt;!--     E     &amp; univariate &amp; equal &amp; -     &amp; - \\ --&gt;
&lt;!--     V     &amp; univariate &amp; variable &amp; -     &amp; - \\ \hdashline --&gt;
&lt;!--     EII   &amp; spherical  &amp; equal &amp; equal &amp; NA \\ --&gt;
&lt;!--     VII   &amp; spherical  &amp; variable &amp; equal &amp; NA \\ \hdashline --&gt;
&lt;!--     EEI   &amp; diagonal &amp; equal &amp; equal &amp; coordinate axes \\ --&gt;
&lt;!--     VEI   &amp; diagonal &amp; variable &amp; equal &amp; coordinate axes \\ --&gt;
&lt;!--     EVI   &amp; diagonal &amp; equal &amp; variable &amp; coordinate axes \\ --&gt;
&lt;!--     VVI   &amp; diagonal &amp; variable &amp; variable &amp; coordinate axes \\ \hdashline --&gt;
&lt;!--     EEE   &amp; ellipsoidal &amp; equal &amp; equal &amp; equal \\ --&gt;
&lt;!--     EVE   &amp; ellipsoidal &amp; equal &amp; variable &amp; equal \\ --&gt;
&lt;!-- 	VEE   &amp; ellipsoidal &amp; variable &amp; equal &amp; equal \\     --&gt;
&lt;!--     VVE   &amp; ellipsoidal &amp; variable &amp; variable &amp; equal \\ --&gt;
&lt;!--     EEV   &amp; ellipsoidal &amp; equal &amp; equal &amp; variable \\ --&gt;
&lt;!--     VEV   &amp; ellipsoidal &amp; variable &amp; equal &amp; variable \\ --&gt;
&lt;!--     EEV   &amp; ellipsoidal &amp; equal &amp; variable &amp; variable \\ --&gt;
&lt;!--     VVV   &amp; ellipsoidal &amp; variable &amp; variable &amp; variable \\ \hdashline --&gt;
&lt;!--     X	  &amp; \multicolumn{4}{c|}{univariate normal} \\ --&gt;
&lt;!--     XII	  &amp; \multicolumn{4}{c|}{spherical multivariate normal} \\ --&gt;
&lt;!--     XXI	  &amp; \multicolumn{4}{c|}{diagonal multivariate normal} \\ --&gt;
&lt;!--     XXX	  &amp; \multicolumn{4}{c|}{ellipsoidal multivariate normal} \\ --&gt;
&lt;!--     \hline --&gt;
&lt;!--     \end{tabular}% --&gt;
&lt;!--   \label{tab:MClustModels}% --&gt;
&lt;!-- \end{table} --&gt;
&lt;p&gt;&lt;code&gt;mclust&lt;/code&gt; provides the user with a very autonomous process of model calculation and selection. First, if not specified otherwise, &lt;code&gt;mclust&lt;/code&gt; calculates all available models for a range of cluster component numbers (by default one to nine clusters). Secondly, once the models are established, &lt;code&gt;mclust&lt;/code&gt; selects the most appropriate of the models according to their respective Bayesian Information Criterion (BIC) value. The BIC is an indicator of model quality: the lower the BIC, the better the model fits the data. Conclusively, &lt;code&gt;mclust&lt;/code&gt; chooses the model with the lowest BIC available for clustering the data.&lt;/p&gt;
&lt;!-- As an example: for a clustering of data with four individual variables, `mclust` will, by default, calculate 126 individual models (14 model classes $*$ 9 cluster possibilities). It will calculate models from only 14 classes, since E, V, X, XII, XXI and XXX models are only appropriate for single variable clustering.   --&gt;
&lt;h3 id=&#34;loading-data&#34;&gt;Loading Data&lt;/h3&gt;
&lt;p&gt;Before we can get started with our analysis, we have to load our NDVI mean and seasonality data (see last exercise) back into &lt;code&gt;R&lt;/code&gt;, we do this as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(raster) # the raster package for rasters
Mean1982_ras &amp;lt;- raster(paste(Dir.Data, &amp;quot;1982Mean.nc&amp;quot;, sep=&amp;quot;/&amp;quot;)) # loading means
Season1982_ras &amp;lt;- raster(paste(Dir.Data, &amp;quot;1982Season.nc&amp;quot;, sep=&amp;quot;/&amp;quot;)) # loading seasonalities
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have loaded the data into &lt;code&gt;R&lt;/code&gt;, it is time to introduce you to another useful feature of the &lt;code&gt;raster&lt;/code&gt; package - the &lt;strong&gt;stack&lt;/strong&gt;. With a stack of rasters, you can do exactly what the name suggests, stack rasters of the same resolution, and extent into one &lt;code&gt;R&lt;/code&gt; object. You do this by calling the &lt;code&gt;stack()&lt;/code&gt;function in &lt;code&gt;R&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;All1982_ras &amp;lt;- stack(Mean1982_ras, Season1982_ras) # creating a stack
names(All1982_ras) &amp;lt;- c(&amp;quot;Mean&amp;quot;, &amp;quot;Seasonality&amp;quot;) # assign names to stack layers
All1982_ras
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## class      : RasterStack 
## dimensions : 237, 590, 139830, 2  (nrow, ncol, ncell, nlayers)
## resolution : 0.083, 0.083  (x, y)
## extent     : -179, -130, 51, 71  (xmin, xmax, ymin, ymax)
## crs        : +proj=longlat +datum=WGS84 +no_defs 
## names      : Mean, Seasonality 
## min values :    0,           0 
## max values : 0.84,        1.00
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, this object contains both rasters as &lt;em&gt;layers&lt;/em&gt; which we have already assigned names to.&lt;/p&gt;
&lt;p&gt;Now let&amp;rsquo;s see how plotting works with this. This time, I am adding a couple of arguments to the &lt;code&gt;plot()&lt;/code&gt; function to make the plots nicer than before:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(All1982_ras, # what to plot
     colNA = &amp;quot;black&amp;quot;, # which colour to assign to NA values
     legend.shrink=1, # vertical size of legend
     legend.width=2 # horizontal size of legend
     )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;02---classifications_files/figure-html/Loading3-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Using stacks makes plotting easier in &lt;code&gt;R&lt;/code&gt; if you want to plot more than one raster at a time.&lt;/p&gt;
&lt;h3 id=&#34;data-extraction&#34;&gt;Data Extraction&lt;/h3&gt;
&lt;p&gt;We&amp;rsquo;re now ready to extract data from our data sets. &lt;code&gt;mclust&lt;/code&gt; let&amp;rsquo;s us assess multi-dimensional clusters but wants the data to be handed over in one file - as a matrix, to be precise. Let&amp;rsquo;s see what happens when we just look the first few (&lt;code&gt;head()&lt;/code&gt;) values (&lt;code&gt;values()&lt;/code&gt;) of our raster stack:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;head(values(All1982_ras))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Mean Seasonality
## [1,]   NA          NA
## [2,]   NA          NA
## [3,]   NA          NA
## [4,]   NA          NA
## [5,]   NA          NA
## [6,]   NA          NA
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, the data gets extracted but there are NA values here. This is because the top-left corner of our rasters (which is where values start) contains a lot of NA cells.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s see what kind of object this is:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(values(All1982_ras))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;matrix&amp;quot; &amp;quot;array&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is a matrix! Just what &lt;code&gt;mclust&lt;/code&gt; wants! Let&amp;rsquo;s actually create that as an object:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Vals1982_mat &amp;lt;- values(All1982_ras)
rownames(Vals1982_mat) &amp;lt;- 1:dim(Vals1982_mat)[1] # rownames to index raster cell number
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, let&amp;rsquo;s carry out a sanity check to make sure that we really have ported all values from both source rasters to our matrix. For this to be the case, the rownumber of our matrix (&lt;code&gt;dim()[1]&lt;/code&gt;) needs to be the same as the amount (&lt;code&gt;length()&lt;/code&gt;) of values (&lt;code&gt;values()&lt;/code&gt;) in our rasters:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dim(Vals1982_mat)[1] == length(values(Mean1982_ras)) &amp;amp; 
  dim(Vals1982_mat)[1] == length(values(Season1982_ras))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This checks out!&lt;/p&gt;
&lt;h3 id=&#34;data-prepartion&#34;&gt;Data Prepartion&lt;/h3&gt;
&lt;p&gt;As you remember, there were plenty of NA values in our data set. No cluster algorithm can handle these. Therefore, we need to get rid of them. This is done as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Vals1982_mat &amp;lt;- na.omit(Vals1982_mat) # omit all rows which contain at least one NA record
dim(Vals1982_mat) # new dimensions of our matrix
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 39460     2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This seriously cut our data down and will speed up our clustering approach a lot.&lt;/p&gt;
&lt;h3 id=&#34;cluster-identification&#34;&gt;Cluster Identification&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s install and load the &lt;code&gt;mclust&lt;/code&gt; package.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&amp;quot;mclust&amp;quot;)
library(mclust)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;cluster-model-selection&#34;&gt;Cluster Model Selection&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s start with the &lt;code&gt;mclust&lt;/code&gt; functionality to identify the best fitting clustering with a range of 1 to 9 clusters. To do so, we first need to identify the BIC fit for all of our possible cluster models. &lt;code&gt;mclust&lt;/code&gt; does this automatically:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dataBIC &amp;lt;- mclustBIC(Vals1982_mat) # identify BICs for different models
print(summary(dataBIC)) # show summary of top-ranking models
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Best BIC values:
##           EVV,8    EVV,9  EVE,8
## BIC      136809 136800.2 135504
## BIC diff      0     -8.6  -1304
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output above tells us that the best performing model was of type EVV (ellipsoidal distribution,  equal volume, variable shape, and variable orientation of clusters) identifying 9 clusters.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s see a visual overview of this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(dataBIC)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;02---classifications_files/figure-html/MClust1b-1.png&#34; width=&#34;1152&#34; /&gt;
Here, you can see different models compared to each other given certain numbers of clusters that have been considered.&lt;/p&gt;
&lt;p&gt;Now we can build our model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mod &amp;lt;- Mclust(Vals1982_mat, # data for the cluster model
                   G = 7 # BIC index for model to be built
                   )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have our full model! How many clusters did it identify?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mod$G # number of groups/clusters in model
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 7
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;No surprises here, we&amp;rsquo;ve got 7 clusters.&lt;/p&gt;
&lt;p&gt;Now let&amp;rsquo;s look at the mean values of the clusters:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mod[[&amp;quot;parameters&amp;quot;]][[&amp;quot;mean&amp;quot;]] # mean values of clusters
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             [,1] [,2] [,3]  [,4] [,5] [,6] [,7]
## Mean        0.36 0.53 0.67 0.081 0.44 0.26 0.21
## Seasonality 0.76 0.56 0.35 0.269 0.72 0.64 0.59
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These can be interpreted biologically, but I will leave that to you.&lt;/p&gt;
&lt;p&gt;Now let&amp;rsquo;s see how well these clusters distinguish the mean-seasonality space:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(mod, what = &amp;quot;uncertainty&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;02---classifications_files/figure-html/MClust1f-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;How do we map this? We &lt;em&gt;predict&lt;/em&gt; our clusters for our initial data as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ModPred &amp;lt;- predict.Mclust(mod, Vals1982_mat) # prediction
Pred_ras &amp;lt;- Mean1982_ras # establishing a rediction raster
values(Pred_ras) &amp;lt;- NA # set everything to NA
# set values of prediction raster to corresponding classification according to rowname
values(Pred_ras)[as.numeric(rownames(Vals1982_mat))] &amp;lt;- as.vector(ModPred$classification)
Pred_ras
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## class      : RasterLayer 
## dimensions : 237, 590, 139830  (nrow, ncol, ncell)
## resolution : 0.083, 0.083  (x, y)
## extent     : -179, -130, 51, 71  (xmin, xmax, ymin, ymax)
## crs        : +proj=longlat +datum=WGS84 +no_defs 
## source     : memory
## names      : layer 
## values     : 1, 7  (min, max)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, this has the same extent and resolution as our source rasters but the values range from 1 to 7. These are our cluster assignments.&lt;/p&gt;
&lt;p&gt;Now let&amp;rsquo;s plot this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;colours &amp;lt;- rainbow(mod$G) # define 7 colours
plot(Pred_ras, # what to plot
     col = colours, # colours for groups
     colNA = &amp;quot;black&amp;quot;, # which colour to assign to NA values
     legend.shrink=1, # vertical size of legend
     legend.width=2 # horizontal size of legend
     )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;02---classifications_files/figure-html/MClust1h-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;How often do we observe which assignment?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(values(Pred_ras))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##     1     2     3     4     5     6     7 
## 13101  1902  1118  2939  5608  8047  6745
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;pre-defined-number&#34;&gt;Pre-Defined Number&lt;/h4&gt;
&lt;p&gt;As biologists, we have got decades of work already present concerning biome distributions across the Earth. One such classification are the Terrestrial Ecoregions of the World (\url{https://www.worldwildlife.org/publications/terrestrial-ecoregions-of-the-world}). We want to identify how many biomes this data set identifies across Australia.&lt;/p&gt;
&lt;p&gt;Firstly, we download the data and unpack it:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# downloading Terrestrial Ecoregion Shapefile as zip
download.file(&amp;quot;http://assets.worldwildlife.org/publications/15/files/original/official_teow.zip&amp;quot;,
              destfile = file.path(Dir.Data, &amp;quot;wwf_ecoregions.zip&amp;quot;)
              )
# unpacking the zip
unzip(file.path(Dir.Data, &amp;quot;wwf_ecoregions.zip&amp;quot;), 
      exdir = file.path(Dir.Data, &amp;quot;WWF_ecoregions&amp;quot;)
      )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Secondly, we load the data into &lt;code&gt;R&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# loading shapefile for biomes
wwf &amp;lt;- readOGR(file.path(Dir.Data, &amp;quot;WWF_ecoregions&amp;quot;, &amp;quot;official&amp;quot;, &amp;quot;wwf_terr_ecos.shp&amp;quot;),
               verbose = FALSE)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thirdly, we need to limit the global terrestrial ecoregion shapefile to the state of Alaska and need our Alaska shapefile for this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Shapes &amp;lt;- readOGR(Dir.Data, # where to look for the file
                  &amp;quot;ne_10m_admin_1_states_provinces&amp;quot;, # the file name
                  verbose = FALSE) # we don&#39;t want an overview of the loaded data
Position &amp;lt;- which(Shapes$name_en == &amp;quot;Alaska&amp;quot;) # find the english name that&#39;s &amp;quot;Alaska&amp;quot;
Alaska_Shp &amp;lt;- Shapes[Position,] # extract the Alaska shapefile
Alaska_Shp &amp;lt;- crop(Alaska_Shp, # what to crop
                   extent(-190, -130, 51, 71)) # which extent to crop to
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we need to limit the global biome shapefile to the shape of Alaska:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;wwf_ready &amp;lt;- crop(wwf, extent(Alaska_Shp)) # cropping to Alaska extent
wwf_ready &amp;lt;- intersect(Alaska_Shp, wwf) # masking of two shapefiles
plot(wwf_ready,  # plotting final shape
     col = wwf_ready@data[[&amp;quot;BIOME&amp;quot;]] # use BIOME specification for colours
     )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;02---classifications_files/figure-html/MClust2c-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We first identify the BICs:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# identify BICs for different models
dataBIC2 &amp;lt;- mclustBIC(Vals1982_mat, 
                     G = length(unique(wwf_ready@data[[&amp;quot;G200_BIOME&amp;quot;]]))) 
print(summary(dataBIC2)) # show summary of top-ranking models
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Best BIC values:
##           EVV,4  VVE,4  EVE,4
## BIC      133035 132345 125463
## BIC diff      0   -690  -7572
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, the shapefile gives us 4 clusters across Alaska even though the map only shows 3. The fourth biome is only represented by a single polygon across all of Alaska and we might want to reduce the set to 3.&lt;/p&gt;
&lt;p&gt;For now, we are running with the idea of 4 clusters:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mod2 &amp;lt;- Mclust(Vals1982_mat, # data for the cluster model
                   G = 4 # BIC index for model to be built
                   )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have our full model!&lt;/p&gt;
&lt;p&gt;Now let&amp;rsquo;s look at the mean values of the clusters:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mod2[[&amp;quot;parameters&amp;quot;]][[&amp;quot;mean&amp;quot;]] # mean values of clusters
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             [,1] [,2] [,3] [,4]
## Mean        0.41 0.13 0.60 0.27
## Seasonality 0.73 0.39 0.44 0.67
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, I leave the biological interpretation to you.&lt;/p&gt;
&lt;p&gt;Finally, we will plot our assignments in mean-seasonality space:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(mod2, what = &amp;quot;uncertainty&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;02---classifications_files/figure-html/MClust2h-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Again, let&amp;rsquo;s &lt;em&gt;predict&lt;/em&gt; our clusters for our initial data as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ModPred2 &amp;lt;- predict.Mclust(mod2, Vals1982_mat) # prediction
Pred2_ras &amp;lt;- Mean1982_ras # establishing a rediction raster
values(Pred2_ras) &amp;lt;- NA # set everything to NA
# set values of prediction raster to corresponding classification according to rowname
values(Pred2_ras)[as.numeric(rownames(Vals1982_mat))] &amp;lt;- as.vector(ModPred2$classification)
Pred2_ras
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## class      : RasterLayer 
## dimensions : 237, 590, 139830  (nrow, ncol, ncell)
## resolution : 0.083, 0.083  (x, y)
## extent     : -179, -130, 51, 71  (xmin, xmax, ymin, ymax)
## crs        : +proj=longlat +datum=WGS84 +no_defs 
## source     : memory
## names      : layer 
## values     : 1, 4  (min, max)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, this has the same extent and resolution as our source rasters but the values range from 1 to 4. These are our cluster assignments.&lt;/p&gt;
&lt;p&gt;Now let&amp;rsquo;s plot this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;colours &amp;lt;- rainbow(mod2$G) # define 4 colours
plot(Pred2_ras, # what to plot
     col = colours, # colours for groups
     colNA = &amp;quot;black&amp;quot;, # which colour to assign to NA values
     legend.shrink=1, # vertical size of legend
     legend.width=2 # horizontal size of legend
     )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;02---classifications_files/figure-html/MClust2j-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;How often do we observe which assignment?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(values(Pred2_ras))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##     1     2     3     4 
## 12223  4066  2327 20844
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;saving-workspace&#34;&gt;Saving Workspace&lt;/h2&gt;
&lt;h3 id=&#34;what-is-it-and-why-do-we-do-it&#34;&gt;What Is It And Why Do We Do It?&lt;/h3&gt;
&lt;p&gt;The workspace records all of our elements in &lt;code&gt;R&lt;/code&gt;. Since we want to pick up from this point in our next exercise, we want to save the workspace and restore it at a later point to assess all of our elements again.&lt;/p&gt;
&lt;h3 id=&#34;saving-and-loading-the-workspace&#34;&gt;Saving And Loading The Workspace&lt;/h3&gt;
&lt;p&gt;Saving a workspace goes as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# save workspace
save.image(file = (paste(Dir.Base, &amp;quot;Workspace.RData&amp;quot;, sep=&amp;quot;/&amp;quot;)))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let&amp;rsquo;s load it again:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rm(list=ls()) # clean workspace
load(file = &amp;quot;Workspace.RData&amp;quot;) # load workspace
ls() # list elements in workspace
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;Alaska_Shp&amp;quot;     &amp;quot;All1982_ras&amp;quot;    &amp;quot;colours&amp;quot;       
##  [4] &amp;quot;dataBIC&amp;quot;        &amp;quot;dataBIC2&amp;quot;       &amp;quot;Dir.Base&amp;quot;      
##  [7] &amp;quot;Dir.Data&amp;quot;       &amp;quot;Dir.Plots&amp;quot;      &amp;quot;Mean1982_ras&amp;quot;  
## [10] &amp;quot;mod&amp;quot;            &amp;quot;mod2&amp;quot;           &amp;quot;ModPred&amp;quot;       
## [13] &amp;quot;ModPred2&amp;quot;       &amp;quot;Position&amp;quot;       &amp;quot;Pred_ras&amp;quot;      
## [16] &amp;quot;Pred2_ras&amp;quot;      &amp;quot;Season1982_ras&amp;quot; &amp;quot;Shapes&amp;quot;        
## [19] &amp;quot;Vals1982_mat&amp;quot;   &amp;quot;wwf&amp;quot;            &amp;quot;wwf_ready&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All our files are back!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An Introduction to GBIF &amp; rgbif</title>
      <link>https://www.erikkusch.com/courses/gbif/theory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/gbif/theory/</guid>
      <description>&lt;p&gt;To obtain billions of biodiversity data records at global coverage, one may obtain data from the The Global Biodiversity Information Facility (GBIF) using the &lt;code&gt;rgbif&lt;/code&gt; package. Both are complex. Here I present and link to material for a deeper understanding of them.&lt;/p&gt;
&lt;h2 id=&#34;the-global-biodiversity-information-facility-gbif&#34;&gt;The Global Biodiversity Information Facility (GBIF)&lt;/h2&gt;
&lt;p&gt;Within these slides, I present the organisation of GBIF, the data it makes available, how to discover relevant data, query download thereof, what to be aware of with regards to data quality, richness and availability, as well as how GBIF handles accreditation of the data it mediates. Simply click the screenshot of the presentation below:
&lt;a href=&#34;http://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/content/courses/gbif/GBIFWorkshop_LivingNorway_presentation.html&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://www.erikkusch.com/courses/gbif/featured.png&#34; width=&#34;900&#34;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;the-rbif-package&#34;&gt;The &lt;code&gt;rbif&lt;/code&gt; Package&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;rgbif&lt;/code&gt; package has been designed to offer a programmatic interface to the GBIF API and enable:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Data discovery&lt;/li&gt;
&lt;li&gt;Data query and download&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The following material in this workshop is aimed at giving an overview to the functionality in &lt;code&gt;rgbif&lt;/code&gt; and has been heavily inspired by the 
&lt;a href=&#34;https://docs.ropensci.org/rgbif/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;rgbif&lt;/code&gt; website&lt;/a&gt; and a 
&lt;a href=&#34;https://gbif-europe.github.io/nordic_oikos_2018_r/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;previous GBIF workshop&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Setting up rgbif</title>
      <link>https://www.erikkusch.com/courses/gbif/setup/</link>
      <pubDate>Sun, 21 May 2023 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/gbif/setup/</guid>
      <description>&lt;p&gt;&lt;code&gt;rgbif&lt;/code&gt; enables programmatic access to GBIF mediated data via its API.&lt;/p&gt;
&lt;h2 id=&#34;installing-rgbif&#34;&gt;Installing &lt;code&gt;rgbif&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Before we can use the &lt;code&gt;R&lt;/code&gt; package, we need to install it. In the simplest way this can be done with the &lt;code&gt;install.packages(...)&lt;/code&gt; command:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&amp;quot;rgbif&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Following successful installation of &lt;code&gt;rgbif&lt;/code&gt;, we simply need to load it into our &lt;code&gt;R&lt;/code&gt; session to make its functionality available to use via the &lt;code&gt;library(...)&lt;/code&gt; command:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rgbif)
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-success&#34;&gt;
  &lt;div&gt;
    You can now use for &lt;code&gt;rgbif&lt;/code&gt; functionality.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;I &lt;strong&gt;strongly&lt;/strong&gt; suggest &lt;strong&gt;never&lt;/strong&gt; installing and loading packages this way in a script that is meant to be reproducible. Using the &lt;code&gt;install.packages(...)&lt;/code&gt; command forces installation of an &lt;code&gt;R&lt;/code&gt; package irrespective of whether it is already installed or not which leads to three distinct issues:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Already installed versions of &lt;code&gt;R&lt;/code&gt; packages are overwritten potentially breaking pre-existing scripts.&lt;/li&gt;
&lt;li&gt;Package installation time is wasted time if packages are already installed ion the first place.&lt;/li&gt;
&lt;li&gt;Package installation is dependent on internet connection making offline sourcing of scripts impossible.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;alert alert-info&#34;&gt;
  &lt;div&gt;
    &lt;details&gt;
  &lt;summary&gt;For a user-defined function that avoids these issues, click here:&lt;/summary&gt;
    Below, I create a user defined function called `install.load.package(...)`. This function takes as an input the name of an `R` package just like the `install.packages(...)` function does. Instead of simply forcing package installation, however, `install.load.package(...)` checks whether the package in question is already installed and only carries out installation if it isn&#39;t. Before concluding, `install.load.package(...)` loads the package.
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Custom install &amp;amp; load function
install.load.package &amp;lt;- function(x) {
  if (!require(x, character.only = TRUE)) {
    install.packages(x, repos = &amp;quot;http://cran.us.r-project.org&amp;quot;)
  }
  require(x, character.only = TRUE)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To further streamline package installation and loading, we can use &lt;code&gt;sapply(...)&lt;/code&gt; to execute the &lt;code&gt;install.load.package(...)&lt;/code&gt; function sequentially for all the packages we desire or require:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## names of packages we want installed (if not installed yet) and loaded
package_vec &amp;lt;- c(
  &amp;quot;rgbif&amp;quot;
)
## executing install &amp;amp; load for each package
sapply(package_vec, install.load.package)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## rgbif 
##  TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt; 
  &lt;/div&gt;
&lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;gbif-account&#34;&gt;GBIF Account&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;rgbif&lt;/code&gt; functionality can be used without registering an account at 
&lt;a href=&#34;https://www.gbif.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GBIF&lt;/a&gt;. However, doing so imposes some crucial limitations:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Downloads are capped at 100,000 records per download&lt;/li&gt;
&lt;li&gt;Once downloaded, GBIF mediated data needs to be queried again for re-download&lt;/li&gt;
&lt;li&gt;Accrediting and referencing GBIF mediated data becomes difficult&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Having a GBIF account and using it for download querying resolves all these limitations. So, let&amp;rsquo;s get you registered with GBIF.&lt;/p&gt;
&lt;h3 id=&#34;opening-an-account-at-gbif&#34;&gt;Opening an Account at GBIF&lt;/h3&gt;
&lt;p&gt;First, navigate to 
&lt;a href=&#34;https://www.gbif.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gbif.org&lt;/a&gt; and click the &lt;strong&gt;Login&lt;/strong&gt; button:
&lt;a href=&#34;https://www.gbif.org/&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://www.erikkusch.com/courses/gbif/GBIFHome.jpeg&#34; width=&#34;900&#34;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Chose your preferred method of registering your account:
&lt;img src=&#34;https://www.erikkusch.com/courses/gbif/GBIFRegister.png&#34; width=&#34;900&#34;/&gt;&lt;/p&gt;
&lt;p&gt;Connect your 
&lt;a href=&#34;https://orcid.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ORCID&lt;/a&gt;. This step is not strictly necessary, but will make subsequent logins very straightforward and streamline citation and data use tracking:
&lt;img src=&#34;https://www.erikkusch.com/courses/gbif/GBIF_ORCID.jpeg&#34; width=&#34;900&#34;/&gt;&lt;/p&gt;
&lt;h3 id=&#34;registering-your-gbif-account-in-r&#34;&gt;Registering your GBIF Account in &lt;code&gt;R&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Lastly, we need to tell your &lt;code&gt;R&lt;/code&gt; session about your GBIF credentials. This can either be done for each individual function call executed from &lt;code&gt;rgbif&lt;/code&gt; to the GBIF API, or set once per &lt;code&gt;R&lt;/code&gt; session. I prefer the latter, so let&amp;rsquo;s register your GBIF credentials as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;options(gbif_user = &amp;quot;my gbif username&amp;quot;)
options(gbif_email = &amp;quot;my registred gbif e-mail&amp;quot;)
options(gbif_pwd = &amp;quot;my gbif password&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-success&#34;&gt;
  &lt;div&gt;
    You are now &lt;strong&gt;ready&lt;/strong&gt; for the full suite of functionality of &lt;code&gt;rgbif&lt;/code&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;!-- ## Session Info --&gt;
&lt;!-- ```{r, echo = FALSE} --&gt;
&lt;!-- sessionInfo() --&gt;
&lt;!-- ``` --&gt;
</description>
    </item>
    
    <item>
      <title>Quick Guide</title>
      <link>https://www.erikkusch.com/courses/krigr/quickstart/</link>
      <pubDate>Thu, 26 May 2022 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/krigr/quickstart/</guid>
      <description>&lt;!-- # ```{r, sourcing-previous, echo = FALSE} --&gt;
&lt;!-- # source_rmd &lt;- function(file, local = FALSE, ...){ --&gt;
&lt;!-- #   options(knitr.duplicate.label = &#39;allow&#39;) --&gt;
&lt;!-- #  --&gt;
&lt;!-- #   tempR &lt;- tempfile(tmpdir = &#34;.&#34;, fileext = &#34;.R&#34;) --&gt;
&lt;!-- #   on.exit(unlink(tempR)) --&gt;
&lt;!-- #   knitr::purl(file, output=tempR, quiet = TRUE) --&gt;
&lt;!-- #  --&gt;
&lt;!-- #   envir &lt;- globalenv() --&gt;
&lt;!-- #   source(tempR, local = envir, ...) --&gt;
&lt;!-- # } --&gt;
&lt;!-- # source_rmd(&#34;krigr-locations.Rmd&#34;) --&gt;
&lt;!-- # ``` --&gt;
&lt;div class=&#34;alert alert-danger&#34;&gt;
  &lt;div&gt;
    This part of the workshop is meant to give a &lt;strong&gt;very brief&lt;/strong&gt; introduction to &lt;code&gt;KrigR&lt;/code&gt; and I highly recommend you peruse the rest of the content, too. If you are already committed to going through the more thorough workshop material in the &lt;strong&gt;Workshop&lt;/strong&gt; tab on the left, I would recommend skipping this quickstart guide as you will gain more knowledge at a more approachable pace in that more exhaustive part of the material.
  &lt;/div&gt;
&lt;/div&gt;
&lt;!-- &lt;div class=&#34;alert alert-danger&#34;&gt;
  &lt;div&gt;
    &amp;ndash;&amp;gt;&lt;/p&gt;
&lt;!-- This part of the workshop is dependant on set-up and preparation done previously [here](/courses/krigr/prep/). --&gt;
&lt;!--
  &lt;/div&gt;
&lt;/div&gt;
 --&gt;
&lt;h2 id=&#34;pre-krigr-housekeeping&#34;&gt;Pre-&lt;code&gt;KrigR&lt;/code&gt; Housekeeping&lt;/h2&gt;
&lt;p&gt;Before we can commence the quick start guide, I want to set up a directory structure and prepare some plotting functions to make the rest of the guide run more smoothly.&lt;/p&gt;
&lt;h3 id=&#34;cds-api-credentials&#34;&gt;CDS API Credentials&lt;/h3&gt;
&lt;p&gt;As explained in the 
&lt;a href=&#34;courses/krigr/setup/#cds-api-access&#34;&gt;&lt;code&gt;KrigR&lt;/code&gt; setup&lt;/a&gt;, please register your CDS API credentials into your &lt;code&gt;R&lt;/code&gt; session like so:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;API_User &amp;lt;- &amp;quot;youremail@somethingortheother&amp;quot;
API_Key &amp;lt;- &amp;quot;YourApiKeyGoesHereAsACharacterString&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;setting-up-directories&#34;&gt;Setting up Directories&lt;/h3&gt;
&lt;p&gt;For this guide to run in a structured way, we create a folder/directory structure. We create the following directories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;strong&gt;Data&lt;/strong&gt; directory for all of our data downloads&lt;/li&gt;
&lt;li&gt;A &lt;strong&gt;Covariate&lt;/strong&gt; directory for all of our covariate data&lt;/li&gt;
&lt;li&gt;An &lt;strong&gt;Exports&lt;/strong&gt; directory for all of our Kriging outputs&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Dir.Base &amp;lt;- getwd() # identifying the current directory
Dir.Data &amp;lt;- file.path(Dir.Base, &amp;quot;Data&amp;quot;) # folder path for data
Dir.Covariates &amp;lt;- file.path(Dir.Base, &amp;quot;Covariates&amp;quot;) # folder path for covariates
Dir.Exports &amp;lt;- file.path(Dir.Base, &amp;quot;Exports&amp;quot;) # folder path for exports
## create directories, if they don&#39;t exist yet
Dirs &amp;lt;- sapply(
  c(Dir.Data, Dir.Covariates, Dir.Exports),
  function(x) if (!dir.exists(x)) dir.create(x)
)
rm(Dirs) # we don&#39;t need to keep the response to directory creation
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;using-krigr&#34;&gt;Using &lt;code&gt;KrigR&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Before we start these exercises, we need to load &lt;code&gt;KrigR&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(KrigR)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since &lt;code&gt;KrigR&lt;/code&gt; works with &lt;code&gt;terra&lt;/code&gt; objects to handle raster data, we may also want to load the &lt;code&gt;terra&lt;/code&gt; package at this point:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(terra)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;KrigR&lt;/code&gt; is conceptualised around a three-step progression through its functionality using the three core functions &lt;code&gt;CDownloadS()&lt;/code&gt;, &lt;code&gt;CovariateSetup()&lt;/code&gt;, and &lt;code&gt;Kriging()&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Workflow.png&#34; alt=&#34;&#34;  /&gt;&lt;/p&gt;
&lt;h3 id=&#34;data-retrieval--handling---cdownloads&#34;&gt;Data Retrieval &amp;amp; Handling - &lt;code&gt;CDownloadS()&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Using the &lt;code&gt;CDownloadS()&lt;/code&gt; function, you gain access to a number of CDS-hosted data products. More details on how you can find out which data products are supported and how to query them, refer to the 
&lt;a href=&#34;https://www.erikkusch.com/courses/krigr/meta/&#34;&gt;Finding CDS-Hosted Data Products&lt;/a&gt; section. For the sake of this quickstart overview of &lt;code&gt;KrigR&lt;/code&gt; capabilities, we will execute a very simple call to &lt;code&gt;CDownloadS()&lt;/code&gt;. For a deeper understanding of the capabilities of &lt;code&gt;KrigR&lt;/code&gt; for CDS-data download and processing, please refer to the 
&lt;a href=&#34;https://www.erikkusch.com/courses/krigr/cdownloads&#34;&gt;Data Retrieval &amp;amp; Handling&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The most simple way in which you can run the functions of the &lt;code&gt;KrigR&lt;/code&gt; package is by specifying a rectangular bounding box (i.e., an &lt;code&gt;SpatExtent&lt;/code&gt;) to specify your study region(s). For this quickstart, we focus on an area covering southern and central Norway:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Extent_ext &amp;lt;- ext(c(4.05, 12.95, 58.05, 63.55))
Extent_ext
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## SpatExtent : 4.05, 12.95, 58.05, 63.55 (xmin, xmax, ymin, ymax)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, you specify, which variable, from which data product, for which time-window you want to obtain and at which temporal resolution. For this part of the workshop, we download air temperature for a three-day interval around 2022-08-18 - when I camped on a mountain flank below Hurrungane facing FannarÃ¥ki - the mountain that inspired my relocation to Norway. Loaded with this information, &lt;code&gt;CDownloadS()&lt;/code&gt; then executes preliminary checks of validity of your data request, breaks the request into separate chunks if it is too big to be handled by CDS all at once, hands the request to CDS, waits for request completion on CDS followed by data download, spatial limiting, temporal aggregation, and finally, saving of the resulting file to your hard drive.&lt;/p&gt;
&lt;div class=&#34;alert alert-&#34;&gt;
  &lt;div&gt;
    Notice that the downloading of CDS-hosted data may take a short while to start as the download request gets queued with CDS before it is executed. An overview of your CDS requests can be seen &lt;a href=&#34;https://cds.climate.copernicus.eu/requests?tab=all&#34;&gt;here&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-danger&#34;&gt;
  &lt;div&gt;
    You need to accept the required licences for each data product before download queries are accepted by CDS. You only have to do this once per data product. If licenses haven&amp;rsquo;t been accepted yet, &lt;code&gt;CDownloadS()&lt;/code&gt; terminates with an error message containing the URL to the data product page where you will find a &amp;ldquo;Terms of use&amp;rdquo; section under which you need to accept the required license(s).
  &lt;/div&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Note that I have already downloaded the QuickStart raw data and CDownloadS() is simply loading this from the disk for me here. Your console output while CDownloadS() is being executed will look differently.
QuickStart_Raw &amp;lt;- CDownloadS(
  ## Variable and Data Product
  Variable = &amp;quot;2m_temperature&amp;quot;, # this is air temperature
  DataSet = &amp;quot;reanalysis-era5-land&amp;quot;, # data product from which we want to download
  ## Time-Window
  DateStart = &amp;quot;2022-08-17&amp;quot;, # date at which time window opens
  DateStop = &amp;quot;2022-08-19&amp;quot;, # date at which time window terminates
  TZone = &amp;quot;CET&amp;quot;, # European Central Time to align with our study region
  ## Temporal Aggregation
  TResolution = &amp;quot;day&amp;quot;, # we want daily aggregates
  TStep = 1, # we want aggregates of 1 day each
  ## Spatial Limiting
  Extent = Extent_ext, # our rectangular bounding box
  ## File Storing
  Dir = Dir.Data, # where to store the data
  FileName = &amp;quot;QuickStart_Raw&amp;quot;, # what to call the resulting file
  ## API User Credentials
  API_User = API_User,
  API_Key = API_Key
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ###### CDS Request &amp;amp; Data Download
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Building request&amp;quot;
## [1] &amp;quot;Checking request validity&amp;quot;
## [1] &amp;quot;A file with the name QuickStart_Raw.nc already exists in C:/Users/erikkus/Documents/Homepage/content/courses/krigr/Data.&amp;quot;
## [1] &amp;quot;Loading this file for you from the disk.&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;QuickStart_Raw
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## class       : SpatRaster 
## dimensions  : 55, 88, 3  (nrow, ncol, nlyr)
## resolution  : 0.1, 0.1  (x, y)
## extent      : 4.1, 12.9, 58.1, 63.6  (xmin, xmax, ymin, ymax)
## coord. ref. : lon/lat Coordinate System imported from GRIB file 
## source      : QuickStart_Raw.nc 
## names       : QuickStart_Raw_1, QuickStart_Raw_2, QuickStart_Raw_3 
## time        : 2022-08-16 to 2022-08-18 CEST
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;See how we have obtained a &lt;code&gt;SpatRaster&lt;/code&gt; corresponding the three dates we indicated to &lt;code&gt;CDownloadS()&lt;/code&gt;? Great! The raw, hourly ERA5-Land data we queried has been aggregated to daily intervals as specified by us. You may also notice that the &lt;code&gt;SpatRaster&lt;/code&gt; we obtained has a slightly different extent than what we queried. This is because CDS aligns the data with a data product specific grid. If in doubt on this, simply specify a slightly larger extent than you ultimately need for your study.&lt;/p&gt;
&lt;p&gt;Using the &lt;code&gt;KrigR&lt;/code&gt; function &lt;code&gt;Plot.SpatRast()&lt;/code&gt;, we can easily visualise the data we just obtained.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Plot.SpatRast(QuickStart_Raw)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2-KRIG~1/figure-html/ClimExtVis-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As you can see the &lt;code&gt;CDownloadS()&lt;/code&gt; function updates you on what it is currently working on at each major step. I implemented this to make sure people don&amp;rsquo;t get too anxious staring at an empty console in &lt;code&gt;R&lt;/code&gt;. If this feature is not appealing to you, you can turn this progress tracking off by setting &lt;code&gt;verbose = FALSE&lt;/code&gt; in the function call to &lt;code&gt;CDownloadS()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;CDownloadS()&lt;/code&gt; also saves metadata pertaining to your download &amp;amp; handling query directly to the final output. While we store all settings of your function call (sans your API credentials), the most relevant metadata appended to your files obtained with &lt;code&gt;CDownloadS()&lt;/code&gt; will most likely be the citation command by which to reference this data in your subsequent research outputs. You can retrieve this information as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;metags(QuickStart_Raw)[&amp;quot;Citation&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                                                                                                                 Citation 
## &amp;quot;reanalysis-era5-land data (DOI:10.24381/cds.e2161bac) obtained with KrigR (DOI:10.1088/1748-9326/ac48b3) on 2024-10-04 14:29:18.291047&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-success&#34;&gt;
  &lt;div&gt;
    More detailed instructions on how to make the most effective use of the &lt;code&gt;CDownloadS()&lt;/code&gt; function and ensure you receive the data you require can be found &lt;a href=&#34;https://www.erikkusch.com/courses/krigr/download/&#34;&gt;here&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Keep in mind that every function within the &lt;code&gt;KrigR&lt;/code&gt; package produces NetCDF (.nc) or TIFF (.tif) files (depending on your specification of the &lt;code&gt;FileExtension&lt;/code&gt; argument in &lt;code&gt;CDownloadS()&lt;/code&gt;) in the specified directory (&lt;code&gt;Dir&lt;/code&gt; argument in the function call) to allow for further manipulation outside of &lt;code&gt;R&lt;/code&gt; if necessary (for example, using Panoply).&lt;/p&gt;
&lt;h3 id=&#34;downscaling-covariates---covariatesetup&#34;&gt;Downscaling Covariates - &lt;code&gt;CovariateSetup()&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Next, we use the &lt;code&gt;CovariateSetup()&lt;/code&gt; function which comes with &lt;code&gt;KrigR&lt;/code&gt; to obtain elevation data as our covariate of choice. This produces two &lt;code&gt;SpatRasters&lt;/code&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A raster of &lt;strong&gt;training&lt;/strong&gt; resolution which matches the input data in all attributes except for the data in each cell.&lt;/li&gt;
&lt;li&gt;A raster of &lt;strong&gt;target&lt;/strong&gt; resolution which matches the input data as closely as possible in all attributes except for the resolution (which is specified by the user).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Both of these products are bundled into a &lt;code&gt;list&lt;/code&gt; where the first element corresponds to the &lt;em&gt;training&lt;/em&gt; resolution and the second element contains the &lt;em&gt;target&lt;/em&gt; resolution covariate data. Here, we specify a target resolution of &lt;code&gt;.02&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;alert alert-&#34;&gt;
  &lt;div&gt;
    Alternatively to specifying a target resolution, you can specify a different &lt;code&gt;SpatRaster&lt;/code&gt; which should be matched in all attributes by the raster at target resolution.&lt;/p&gt;
&lt;!-- This is explained more in-depth in [this part of the workshop](/courses/krigr/third-party).  --&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Note that I have already downloaded the global GMTED2010 data with this function prior, your output will show the download itself as well
Covs_ls &amp;lt;- CovariateSetup(
  Training = QuickStart_Raw,
  Target = .02,
  Dir = Dir.Covariates,
  Keep_Global = TRUE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;GMTED2010 covariate data already downloaded.&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Covs_ls
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $Training
## class       : SpatRaster 
## dimensions  : 55, 88, 1  (nrow, ncol, nlyr)
## resolution  : 0.1, 0.1  (x, y)
## extent      : 4.1, 12.9, 58.1, 63.6  (xmin, xmax, ymin, ymax)
## coord. ref. : lon/lat WGS 84 (EPSG:4326) 
## source      : Covariates_Train.nc 
## name        : GMTED2010 
## 
## $Target
## class       : SpatRaster 
## dimensions  : 330, 528, 1  (nrow, ncol, nlyr)
## resolution  : 0.01666667, 0.01666667  (x, y)
## extent      : 4.09986, 12.89986, 58.09986, 63.59986  (xmin, xmax, ymin, ymax)
## coord. ref. : lon/lat WGS 84 (EPSG:4326) 
## source      : Covariates_Target.nc 
## name        : GMTED2010
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, we can use a &lt;code&gt;KrigR&lt;/code&gt; plotting function to easily visualise this data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Plot.Covariates(Covs_ls)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2-KRIG~1/figure-html/CovExtViz-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;div class=&#34;alert alert-success&#34;&gt;
  &lt;div&gt;
    The &lt;code&gt;CovariateSetup()&lt;/code&gt; function can also be used to prepare raster data you already have at hand for use in subsequent Kriging.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;statistical-downscaling---kriging&#34;&gt;Statistical Downscaling - &lt;code&gt;Kriging()&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Now let&amp;rsquo;s statistically downscale the data we just obtained with the covariates we just prepared. We do so using the &lt;code&gt;Kriging()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;QuickStart_Krig &amp;lt;- Kriging(
  Data = QuickStart_Raw, # data we want to krig as a raster object
  Covariates_training = Covs_ls[[1]], # training covariate as a raster object
  Covariates_target = Covs_ls[[2]], # target covariate as a raster object
  Equation = &amp;quot;GMTED2010&amp;quot;, # the covariate(s) we want to use
  nmax = 40, # degree of localisation
  Cores = 3, # we want to krig using three cores to speed this process up
  FileName = &amp;quot;QuickStart_Krig&amp;quot;, # the file name for our full kriging output
  Dir = Dir.Exports # which directory to save our final input in
)
QuickStart_Krig
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $Prediction
## class       : SpatRaster 
## dimensions  : 330, 528, 3  (nrow, ncol, nlyr)
## resolution  : 0.01666667, 0.01666667  (x, y)
## extent      : 4.09986, 12.89986, 58.09986, 63.59986  (xmin, xmax, ymin, ymax)
## coord. ref. : lon/lat Coordinate System imported from GRIB file 
## source      : QuickStart_Krig_Kriged.nc 
## varname     : QuickStart_Raw 
## names       : QuickStart~g_Kriged_1, QuickStart~g_Kriged_2, QuickStart~g_Kriged_3 
## time        : 2022-08-16 to 2022-08-18 CEST 
## 
## $StDev
## class       : SpatRaster 
## dimensions  : 330, 528, 3  (nrow, ncol, nlyr)
## resolution  : 0.01666667, 0.01666667  (x, y)
## extent      : 4.09986, 12.89986, 58.09986, 63.59986  (xmin, xmax, ymin, ymax)
## coord. ref. : lon/lat Coordinate System imported from GRIB file 
## source      : QuickStart_Krig_STDev.nc 
## varname     : QuickStart_Raw 
## names       : QuickStart~ig_STDev_1, QuickStart~ig_STDev_2, QuickStart~ig_STDev_3 
## time        : 2022-08-16 to 2022-08-18 CEST
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This operation took 2 seconds on my machine (this may vary drastically on other devices). There we go. All the data has been downscaled and we do have uncertainties recorded for all of our outputs. Let&amp;rsquo;s visualise this again with a &lt;code&gt;KrigR&lt;/code&gt; function - &lt;code&gt;Plot.Kriged()&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Plot.Kriged(QuickStart_Krig)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2-KRIG~1/figure-html/KrigExtViz-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As you can see, the elevation patterns show up clearly in our kriged air temperature output. Seems like Norway got warmer the day I left camp on August of 2022 - I do remember the night from the 18th to the 19th being the first clear night after a few days of constant cloud cover and it did get cold in my tent that night, but the heat during daytime thereafter seems to have balanced that out and swung the daily average into being a warmer day. Furthermore, you can see that our certainty of Kriging predictions steadily increases towards the 2022-08-18 in comparison to the preceding days. However, do keep in mind that a maximum standard error of 2.84, 1.89, 1.5 (for each layer of our output respectively, and across the sea at that, where there are no topographical variations we can exploit for kriging) on a total range of data of 15.31, 14.8, 14.73 (again, for each layer in the output respectively) is evident of a downscaling result we can be confident in.&lt;/p&gt;
&lt;div class=&#34;alert alert-&#34;&gt;
  &lt;div&gt;
    More detailed instructions on how to make the most effective use of the &lt;code&gt;krigR()&lt;/code&gt; function can be found &lt;a href=&#34;https://www.erikkusch.com/courses/krigr/kriging/&#34;&gt;here&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-success&#34;&gt;
  &lt;div&gt;
    This concludes the quick start tutorial for &lt;code&gt;KrigR&lt;/code&gt;. For more effective use of the &lt;code&gt;KrigR&lt;/code&gt; toolbox, I suggest you peruse the rest of the workshop material or use the search function if you have specific queries.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.4.0 (2024-04-24 ucrt)
## Platform: x86_64-w64-mingw32/x64
## Running under: Windows 11 x64 (build 22631)
## 
## Matrix products: default
## 
## 
## locale:
## [1] C
## 
## time zone: Europe/Oslo
## tzcode source: internal
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] terra_1.7-78 KrigR_0.4.0 
## 
## loaded via a namespace (and not attached):
##  [1] tidyselect_1.2.1   viridisLite_0.4.2  dplyr_1.1.4        farver_2.1.2      
##  [5] viridis_0.6.5      R.utils_2.12.3     fastmap_1.2.0      reshape_0.8.9     
##  [9] blogdown_1.19      digest_0.6.37      timechange_0.3.0   lifecycle_1.0.4   
## [13] sf_1.0-17          magrittr_2.0.3     compiler_4.4.0     rlang_1.1.4       
## [17] sass_0.4.9         progress_1.2.3     doSNOW_1.0.20      tools_4.4.0       
## [21] utf8_1.2.4         yaml_2.3.10        knitr_1.48         FNN_1.1.4.1       
## [25] prettyunits_1.2.0  labeling_0.4.3     sp_2.1-4           classInt_0.4-10   
## [29] plyr_1.8.9         abind_1.4-8        KernSmooth_2.23-22 R.cache_0.16.0    
## [33] withr_3.0.1        purrr_1.0.2        R.oo_1.26.0        grid_4.4.0        
## [37] fansi_1.0.6        xts_0.14.0         e1071_1.7-16       colorspace_2.1-1  
## [41] ggplot2_3.5.1      scales_1.3.0       iterators_1.0.14   cli_3.6.3         
## [45] rmarkdown_2.28     crayon_1.5.3       intervals_0.15.5   generics_0.1.3    
## [49] httr_1.4.7         ncdf4_1.23         DBI_1.2.3          pbapply_1.7-2     
## [53] cachem_1.1.0       proxy_0.4-27       ecmwfr_2.0.2       stringr_1.5.1     
## [57] stars_0.6-6        parallel_4.4.0     vctrs_0.6.5        jsonlite_1.8.8    
## [61] bookdown_0.40      hms_1.1.3          foreach_1.5.2      jquerylib_0.1.4   
## [65] tidyr_1.3.1        units_0.8-5        snow_0.4-4         glue_1.7.0        
## [69] codetools_0.2-20   cowplot_1.1.3      gstat_2.1-2        lubridate_1.9.3   
## [73] stringi_1.8.4      gtable_0.3.5       munsell_0.5.1      tibble_3.2.1      
## [77] styler_1.10.3      pillar_1.9.0       htmltools_0.5.8.1  R6_2.5.1          
## [81] automap_1.1-12     evaluate_0.24.0    lattice_0.22-6     highr_0.11        
## [85] R.methodsS3_1.8.2  memoise_2.0.1      bslib_0.8.0        class_7.3-22      
## [89] Rcpp_1.0.13        gridExtra_2.3      spacetime_1.3-2    xfun_0.47         
## [93] zoo_1.8-12         pkgconfig_2.0.3
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Hybrid Bayesian Networks</title>
      <link>https://www.erikkusch.com/courses/bayes-nets/part-3/</link>
      <pubDate>Tue, 08 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/bayes-nets/part-3/</guid>
      <description>&lt;style&gt;

blockquote{
color:#633a00;
}

&lt;/style&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/content/courses/bayes-nets/5-Hybrid-Bayesian-Networks_18-10-22.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hybrid Bayesian Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/content/courses/bayes-nets/Hybrid-BNs.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hybrid Bayesian Networks&lt;/a&gt; by 
&lt;a href=&#34;https://github.com/arielsaffer&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ariel Saffer&lt;/a&gt; (one of our study group members)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;exercises&#34;&gt;Exercises&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to the exercises at the end of Part 3 in 
&lt;a href=&#34;https://www.bnlearn.com/book-crc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bayesian Networks with Examples in R&lt;/a&gt; by M. Scutari and J.-B. Denis. Much of my inspiration for these solutions, where necessary, by consulting the solutions provided by the authors themselves as in the appendix.&lt;/p&gt;
&lt;h3 id=&#34;r-environment&#34;&gt;&lt;code&gt;R&lt;/code&gt; Environment&lt;/h3&gt;
&lt;p&gt;For today&amp;rsquo;s exercise, I load the following packages:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(bnlearn)
library(rjags)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;scutari-31&#34;&gt;Scutari 3.1&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Explain why it is logical to get a three-step function for the discretised approach in Figure 3.2.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Figure 3.2 shows this discretisation:&lt;/p&gt;
&lt;img src=&#34;https://www.erikkusch.com/courses/networksscutari/Screenshot 2021-06-08 153234.png&#34; width=&#34;900&#34;/&gt;
&lt;p&gt;the reason we obtain a three-step function here is down to the intervals that were chosen to bin the continuous diametre data into discrete categories:&lt;/p&gt;
&lt;p&gt;$$&amp;lt;6.16$$
$$[6.16; 6.19]$$
$$&amp;gt;6.19$$&lt;/p&gt;
&lt;p&gt;Owing to these intervals, we fit all of our continuous data into three categories which mirror the three-step function portrayed in the above figure.&lt;/p&gt;
&lt;h3 id=&#34;scutari-32&#34;&gt;Scutari 3.2&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Starting from the BUGS model in Section 3.1.1, write another BUGSmodel for the discretised model proposed in Section 3.1.2. The functions required for this task are described in the JAGS manual.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The model for the hybrid case (which we are to adapt to the discretised approach) reads as such:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;model{ 
  csup ~ dcat(sp); 
  cdiam ~ dnorm(mu[csup], 1/sigma^2);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To adapt it for discretised data, I simply change the outcome distribution for &lt;code&gt;cdiam&lt;/code&gt; to also be categorical:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;model{ 
  csup ~ dcat(sp); 
  cdiam ~ dcat(Diams[, csup]);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where &lt;code&gt;Diams&lt;/code&gt; is a matrix that contains probabilities for the different diameters (rows) for suppliers (columns).&lt;/p&gt;
&lt;h3 id=&#34;scutari-33&#34;&gt;Scutari 3.3&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Let d = 6.0, 6.1, 6.2, 6.4.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;Using the BUGS model proposed in Section 3.1.1, write the &lt;code&gt;R&lt;/code&gt; script to estimate $P(S = s1 | D = d)$ for the continuous approach demonstrated in the same section.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;I start by simply repeating the code in section 3.1.1:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sp &amp;lt;- c(0.5, 0.5)
mu &amp;lt;- c(6.1, 6.25)
sigma &amp;lt;- 0.05
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From here, I take inspiration from the book in the later section on the crop model and write a sampling loop for which to retain samples. First, I create some objects to store data and outputs as well as do some housekeeping:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## house-keeping
set.seed(42) # there are random processes here

## object creation
diameters_vec &amp;lt;- c(6.0, 6.1, 6.2, 6.4)
prob_vec &amp;lt;- rep(NA, length(diameters_vec))
names(prob_vec) &amp;lt;- diameters_vec
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, I greatly dislike the reliance on model files that the book insists on and so I register my JAGS model code as an active text connection in &lt;code&gt;R&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;jags_mod &amp;lt;- textConnection(&amp;quot;model{
                           csup ~ dcat(sp);
                           cdiam ~ dnorm(mu[csup], 1/sigma^2);
                           }&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, I am ready to estimate $P(S = s1 | D = d)$:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;for (samp_iter in seq(length(diameters_vec))) {
  # create data list
  jags.data &amp;lt;- list(sp = sp, mu = mu, sigma = sigma, cdiam = diameters_vec[samp_iter])
  # compile model
  model &amp;lt;- jags.model(file = jags_mod, data = jags.data, quiet = TRUE)
  update(model, n.iter = 1e4)
  # sample model and retrieve vector of supplier identity (containing values 1 and 2)
  simu &amp;lt;- coda.samples(model = model, variable.names = &amp;quot;csup&amp;quot;, n.iter = 2e4, thin = 20)[[1]]
  # compute probability of supplier 1
  prob_vec[samp_iter] &amp;lt;- sum(simu == 1) / length(simu)
}
prob_vec
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     6   6.1   6.2   6.4 
## 1.000 0.982 0.197 0.000
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Using the BUGS model obtained in Exercise 3.2, write the &lt;code&gt;R&lt;/code&gt; script to estimate $P(S = s1 | D = d)$ for the discretised approach suggested in Section 3.1.2.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;Again, I start by typing out important parameters from the book:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sp &amp;lt;- c(0.5, 0.5)
Diams &amp;lt;- matrix(c(0.88493, 0.07914, 0.03593, 0.03593, 0.07914, 0.88493), 3)
Diams
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         [,1]    [,2]
## [1,] 0.88493 0.03593
## [2,] 0.07914 0.07914
## [3,] 0.03593 0.88493
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once more, I perform housekeeping and object creation prior to sampling. This time, however, I create a probability matrix to store the probability of each rod diametre in each diametre class belonging to supplier 1:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## house-keeping
set.seed(42) # there are random processes here

## object creation
diameters_vec &amp;lt;- c(6.0, 6.1, 6.2, 6.4)
cdiam_vec &amp;lt;- 1:3
dimnames &amp;lt;- list(
  paste(&amp;quot;cdiam&amp;quot;, cdiam_vec, sep = &amp;quot;_&amp;quot;),
  diameters_vec
)
prob_mat &amp;lt;- matrix(rep(NA, 12), ncol = 4)
dimnames(prob_mat) &amp;lt;- dimnames
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And there&amp;rsquo;s our model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;jags_mod &amp;lt;- textConnection(&amp;quot;model{
                            csup ~ dcat(sp);
                            cdiam ~ dcat(Diams[, csup]);
                            }&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, I am ready to estimate $P(S = s1 | D = d)$:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;for (samp_iter in seq(length(cdiam_vec))) {
  # create data list
  jags.data &amp;lt;- list(sp = sp, Diams = Diams, cdiam = cdiam_vec[samp_iter])
  # compile model
  model &amp;lt;- jags.model(file = jags_mod, data = jags.data, quiet = TRUE)
  update(model, n.iter = 1e4)
  # sample model and retrieve vector of supplier identity (containing values 1 and 2)
  simu &amp;lt;- coda.samples(model = model, variable.names = &amp;quot;csup&amp;quot;, n.iter = 2e4, thin = 20)[[1]]
  # compute probability of supplier 1
  prob_mat[samp_iter, ] &amp;lt;- sum(simu == 1) / length(simu)
}
prob_mat
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             6   6.1   6.2   6.4
## cdiam_1 0.966 0.966 0.966 0.966
## cdiam_2 0.490 0.490 0.490 0.490
## cdiam_3 0.035 0.035 0.035 0.035
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;And check the results with Figure 3.2.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Looking at figure 3.2, I&amp;rsquo;d argue the above probabilities align with the figure:&lt;/p&gt;
&lt;img src=&#34;https://www.erikkusch.com/courses/bayes-nets/Scutari3.2.png&#34; width=&#34;900&#34;/&gt;
&lt;h3 id=&#34;scutari-34&#34;&gt;Scutari 3.4&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;In Section 3.1.1, the probability that the supplier is &lt;code&gt;s1&lt;/code&gt; knowing that the diameter is 6.2 was estimated to be 0.1824 which is not identical to the value obtained with JAGS.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;Explain why the calculation with the &lt;code&gt;R&lt;/code&gt; function &lt;code&gt;dnorm&lt;/code&gt; is right and why the value 0.1824 is correct. Can you explain why the JAGS result is not exact? Propose a way to improve it.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;Since either function relies on random processes, differences in seeds may explain the difference in inference. To improve the accuracy of the JAGS result, I would suggest increasing the sample size that led to its creation.&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Would this value be different if we modify the marginal distribution for the two suppliers?&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;Yes. Marginal distributions are essential to the Bayes&#39; Theorem and a change thereof would necessitate a change in inference.&lt;/p&gt;
&lt;h3 id=&#34;scutari-35&#34;&gt;Scutari 3.5&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Revisiting the discretisation in Section 3.1.2, compute the conditional probability tables for  $ D | S $ and  $ S | D $  when the interval boundaries are set to  $ (6.10, 6.18) $ instead of  $ (6.16, 6.19)$ . Compared to the results presented in Section 3.1.2, what is your conclusion?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let&amp;rsquo;s start by repeating book code and updating the intervals to obtain $D | S$:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mu &amp;lt;- c(6.1, 6.25)
sigma &amp;lt;- 0.05
limits &amp;lt;- c(6.10, 6.18)
dsd &amp;lt;- matrix(
  c(
    diff(c(0, pnorm(limits, mu[1], sigma), 1)),
    diff(c(0, pnorm(limits, mu[2], sigma), 1))
  ),
  3, 2
)
dimnames(dsd) &amp;lt;- list(D = c(&amp;quot;thin&amp;quot;, &amp;quot;average&amp;quot;, &amp;quot;thick&amp;quot;), S = c(&amp;quot;s1&amp;quot;, &amp;quot;s2&amp;quot;))
dsd
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          S
## D                 s1          s2
##   thin    0.50000000 0.001349898
##   average 0.44520071 0.079406761
##   thick   0.05479929 0.919243341
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To obtain $ S | D $, we apply Bayes&#39; Theorem:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;jointd &amp;lt;- dsd / 2 # dive by 2 to get joint distribution over suppliers of which we have 2
mardd &amp;lt;- rowSums(jointd) # marginal distribution of diametre class irrespective of supplier
dds &amp;lt;- t(jointd / mardd) # find conditional probabilites
dds
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     D
## S           thin   average      thick
##   s1 0.997307473 0.8486359 0.05625964
##   s2 0.002692527 0.1513641 0.94374036
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One of our new limits in in fact the mean diametre supplied by supplier 1. That is clearly not a helpful limit as it simply shifts probability to supplier 1.&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.2.1 (2022-06-23 ucrt)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19044)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_Germany.utf8  LC_CTYPE=English_Germany.utf8    LC_MONETARY=English_Germany.utf8 LC_NUMERIC=C                     LC_TIME=English_Germany.utf8    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] rjags_4-13    coda_0.19-4   bnlearn_4.8.1
## 
## loaded via a namespace (and not attached):
##  [1] bslib_0.4.0       compiler_4.2.1    pillar_1.8.1      jquerylib_0.1.4   R.methodsS3_1.8.2 R.utils_2.12.0    tools_4.2.1       digest_0.6.29     jsonlite_1.8.0    evaluate_0.16    
## [11] lifecycle_1.0.2   R.cache_0.16.0    lattice_0.20-45   rlang_1.0.5       cli_3.3.0         rstudioapi_0.14   yaml_2.3.5        parallel_4.2.1    blogdown_1.13     xfun_0.33        
## [21] fastmap_1.1.0     styler_1.8.0      stringr_1.4.1     knitr_1.40        vctrs_0.4.1       sass_0.4.2        grid_4.2.1        glue_1.6.2        R6_2.5.1          fansi_1.0.3      
## [31] rmarkdown_2.16    bookdown_0.29     purrr_0.3.4       magrittr_2.0.3    htmltools_0.5.3   utf8_1.2.2        stringi_1.7.8     cachem_1.0.6      R.oo_1.25.0
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 03</title>
      <link>https://www.erikkusch.com/courses/rethinking/chapter-03/</link>
      <pubDate>Thu, 07 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/rethinking/chapter-03/</guid>
      <description>&lt;h1 id=&#34;sampling-the-imaginary&#34;&gt;Sampling the Imaginary&lt;/h1&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/rethinking/2_18-12-2020_SUMMARY_-Basics-of-Bayesian-Inference-and-Counting.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides Chapter 3&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to the exercises at the end of chapter 3 in 
&lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Satistical Rethinking 2&lt;/a&gt; by Richard McElreath. I have created these notes as a part of my ongoing involvement in the 
&lt;a href=&#34;https://www.erikkusch.com/project/aubayes/&#34;&gt;AU Bayes Study Group&lt;/a&gt;. Much of my inspiration for these solutions, where necessary, has been obtained from 
&lt;a href=&#34;https://jmgirard.com/statistical-rethinking-ch3/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jeffrey Girard&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Loading &lt;code&gt;rethinking&lt;/code&gt; package for visualisations:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rm(list = ls())
library(rethinking)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;easy-exercises&#34;&gt;Easy Exercises&lt;/h2&gt;
&lt;p&gt;These problems use the samples from the posterior distribution for the globe tossing example. This code will give you a specific set of samples, so that you can check your answers exactly. Use the values in &lt;code&gt;samples&lt;/code&gt; to answer the questions that follow.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;p_grid &amp;lt;- seq(from = 0, to = 1, length.out = 1000)
prior &amp;lt;- rep(1, 1000)
likelihood &amp;lt;- dbinom(6, size = 9, prob = p_grid)
posterior &amp;lt;- likelihood * prior
posterior &amp;lt;- posterior / sum(posterior)
set.seed(100)
samples &amp;lt;- sample(p_grid, prob = posterior, size = 1e4, replace = TRUE)
hist(samples)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-03_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;practice-e1&#34;&gt;Practice E1&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; How much posterior probability lies below $p=0.2$?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# how the book did it
sum(samples &amp;lt; .2) / length(samples)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4e-04
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# easier way
mean(samples &amp;lt; 0.2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4e-04
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-e2&#34;&gt;Practice E2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; How much posterior probability lies above $p=0.8$?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mean(samples &amp;gt; 0.8)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1116
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-e3&#34;&gt;Practice E3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; How much posterior probability lies between $p=0.2$ and $p=0.8$?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mean(samples &amp;gt; 0.2 &amp;amp; samples &amp;lt; 0.8)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.888
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-e4&#34;&gt;Practice E4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; 20% of the posterior probability lies below which value of $p$?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;quantile(samples, 0.2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       20% 
## 0.5185185
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-e5&#34;&gt;Practice E5&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; 20% of the posterior probability lies above which value of $p$?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;quantile(samples, 0.8)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       80% 
## 0.7557558
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-e6&#34;&gt;Practice E6&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Which values of $p$ contain the narrowest interval equal to 66% of the posterior probability?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;HPDI(samples, prob = 0.66)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     |0.66     0.66| 
## 0.5085085 0.7737738
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-e7&#34;&gt;Practice E7&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Which values of p contain 66% of the posterior probability, assuming equal posterior probability both below and above the interval?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;PI(samples, prob = 0.66)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       17%       83% 
## 0.5025025 0.7697698
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;medium-exercises&#34;&gt;Medium Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-m1&#34;&gt;Practice M1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Suppose the globe tossing data had turned out to be 8 water in 15 tosses. Construct the posterior distribution, using grid approximation. Use the same flat prior as before.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;p_grid &amp;lt;- seq(from = 0, to = 1, length.out = 1000)
prior &amp;lt;- rep(1, 1000)
likelihood &amp;lt;- dbinom(8, size = 15, prob = p_grid)
posterior &amp;lt;- likelihood * prior
posterior &amp;lt;- posterior / sum(posterior)
plot(p_grid, posterior, type = &amp;quot;l&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-03_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;p_grid[which.max(posterior)]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5335335
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-m2&#34;&gt;Practice M2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Draw 10,000 samples from the grid approximation from above. Then use the samples to calculate the 90% HPDI for $p$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(42)
samples &amp;lt;- sample(p_grid, prob = posterior, replace = TRUE, size = 1e+4)
hist(samples)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-03_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;HPDI(samples, prob = .9)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      |0.9      0.9| 
## 0.3393393 0.7267267
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mean(samples)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5295147
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;median(samples)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5305305
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-m3&#34;&gt;Practice M3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Construct a posterior predictive check for this model and data. This means simulate the distribution of samples, averaging over the posterior uncertainty in $p$. What is the probability of observing 8 water in 15 tosses?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;n &amp;lt;- 15
set.seed(42)
dumdata &amp;lt;- rbinom(10000, size = n, prob = samples)
simplehist(dumdata)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-03_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mean(dumdata == 8)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1419
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-m4&#34;&gt;Practice M4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Using the posterior distribution constructed from the new (8/15) data, now calculate the probability of observing 6 water in 9 tosses.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;likelihood_6of9 &amp;lt;- dbinom(6, size = 9, prob = p_grid)
prior_6of9 &amp;lt;- posterior
(p_6of9 &amp;lt;- sum(likelihood_6of9 * prior_6of9))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1763898
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Alternatively, we can generate the data using the same seed as above:
set.seed(100)
dumdata_6of9 &amp;lt;- rbinom(10000, size = 9, prob = samples)
simplehist(dumdata_6of9)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-03_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mean(dumdata_6of9 == 6)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1765
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-m5&#34;&gt;Practice M5&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Start over at 3M1, but now use a prior that is zero below $p=0.5$ and a constant above $p=0.5$. This corresponds to prior information that a majority of the Earthâs surface is water. Repeat each problem above and compare the inferences. What difference does the better prior make? If it helps, compare inferences (using both priors) to the true value $p=0.7$.&lt;/p&gt;
&lt;h4 id=&#34;part-a&#34;&gt;Part A&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Construct the posterior distribution, using grid approximation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;p_grid &amp;lt;- seq(from = 0, to = 1, length.out = 1000)
prior &amp;lt;- ifelse(p_grid &amp;lt; 0.5, 0, 0.5)
likelihood &amp;lt;- dbinom(8, size = 15, prob = p_grid)
posterior &amp;lt;- likelihood * prior
posterior &amp;lt;- posterior / sum(posterior)
plot(p_grid, posterior, type = &amp;quot;l&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-03_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;p_grid[which.max(posterior)]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5335335
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-b&#34;&gt;Part B&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Draw 10,000 samples from the grid approximation from above. Then use the samples to calculate the 90% HPDI for $p$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(42)
samples &amp;lt;- sample(p_grid, prob = posterior, replace = TRUE, size = 1e+4)
hist(samples)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-03_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;HPDI(samples, prob = .9)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      |0.9      0.9| 
## 0.5005005 0.7117117
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mean(samples)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6067921
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;median(samples)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5945946
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-c&#34;&gt;Part C&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; What is the probability of observing 8 water in 15 tosses?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;n &amp;lt;- 15
set.seed(42)
dumdata &amp;lt;- rbinom(10000, size = n, prob = samples)
simplehist(dumdata)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-03_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mean(dumdata == 8)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1516
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(dumdata) / 1e+4
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## dumdata
##      1      2      3      4      5      6      7      8      9     10     11     12     13     14     15 
## 0.0002 0.0004 0.0038 0.0125 0.0329 0.0671 0.1188 0.1516 0.1734 0.1712 0.1276 0.0823 0.0398 0.0157 0.0027
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-d&#34;&gt;Part D&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Using the posterior distribution constructed from the new (8/15) data, now calculate the probability of observing 6 water in 9 tosses.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;likelihood_6of9 &amp;lt;- dbinom(6, size = 9, prob = p_grid)
prior_6of9 &amp;lt;- posterior
(p_6of9 &amp;lt;- sum(likelihood_6of9 * prior_6of9))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2323071
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Alternatively, we can generate the data using the same seed as above:
set.seed(100)
dumdata_6of9 &amp;lt;- rbinom(10000, size = 9, prob = samples)
simplehist(dumdata_6of9)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-03_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mean(dumdata_6of9 == 6)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2321
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-m6&#34;&gt;Practice M6&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Suppose you want to estimate the Earthâs proportion of water very precisely. Specifically, you want the 99% percentile interval of the posterior distribution of $p$ to be only 0.05 wide. This means the distance between the upper and lower bound of the interval should be 0.05. How many times will you have to toss the globe to do this?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Solution taken from 
&lt;a href=&#34;https://github.com/rmcelreath/statrethinking_winter2019/blob/master/homework/week01_solutions.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Richard McElreath&lt;/a&gt; and altered by myself.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;f &amp;lt;- function(N) {
  p_true &amp;lt;- 0.7
  W &amp;lt;- rbinom(1, size = N, prob = p_true)
  prob_grid &amp;lt;- seq(0, 1, length.out = 1000)
  prior &amp;lt;- rep(1, 1000)
  prob_data &amp;lt;- dbinom(W, size = N, prob = prob_grid)
  posterior &amp;lt;- prob_data * prior
  posterior &amp;lt;- posterior / sum(posterior)
  samples &amp;lt;- sample(p_grid, prob = posterior, size = 1e4, replace = TRUE)
  PI99 &amp;lt;- PI(samples, .99)
  as.numeric(PI99[2] - PI99[1])
}
Nlist &amp;lt;- c(20, 50, 100, 200, 500, 1000, 2000)
Nlist &amp;lt;- rep(Nlist, each = 100)
width &amp;lt;- sapply(Nlist, f)
plot(Nlist, width)
abline(h = 0.05, col = &amp;quot;red&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-03_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;hard-exercises&#34;&gt;Hard Exercises&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(homeworkch3)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-h1&#34;&gt;Practice H1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Using grid approximation, compute the posterior distribution for the probability of a birth being a boy. Assume a uniform prior probability. Which parameter value maximizes the posterior probability?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;n_boys &amp;lt;- sum(c(birth1, birth2))
n_ttl &amp;lt;- length(birth1) + length(birth2)
n_pgrid &amp;lt;- 1000
p_grid &amp;lt;- seq(0, 1, length.out = n_pgrid)
prior &amp;lt;- rep(1, n_pgrid)
likelihood &amp;lt;- dbinom(n_boys, size = n_ttl, prob = p_grid)
posterior &amp;lt;- likelihood * prior
posterior &amp;lt;- posterior / sum(posterior)
plot(p_grid, posterior, type = &amp;quot;l&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-03_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;p_grid[which.max(posterior)]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5545546
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-h2&#34;&gt;Practice H2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Using the $sample()$ function, draw 10000 random parameter values from the posterior distribution you calculated above. Use these samples to estimate the 50%, 89% and 97% highest posterior density intervals.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;n_ptrials &amp;lt;- 1e4
p_samples &amp;lt;- sample(p_grid, size = n_ptrials, prob = posterior, replace = TRUE)
(hpi_50 &amp;lt;- HPDI(p_samples, .5))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      |0.5      0.5| 
## 0.5265265 0.5735736
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(hpi_89 &amp;lt;- HPDI(p_samples, .89))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     |0.89     0.89| 
## 0.4964965 0.6076076
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(hpi_97 &amp;lt;- HPDI(p_samples, .97))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     |0.97     0.97| 
## 0.4784785 0.6276276
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;for (w in c(.5, .89, .97)) {
  hpi &amp;lt;- HPDI(p_samples, w)
  print(sprintf(&amp;quot;HPDI %d%% [%f, %f]&amp;quot;, w * 100, hpi[1], hpi[2]))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;HPDI 50% [0.526527, 0.573574]&amp;quot;
## [1] &amp;quot;HPDI 89% [0.496496, 0.607608]&amp;quot;
## [1] &amp;quot;HPDI 97% [0.478478, 0.627628]&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mean(p_samples)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5545528
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;median(p_samples)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5545546
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-h3&#34;&gt;Practice H3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Use &lt;code&gt;rbinom()&lt;/code&gt; to simulate 10000 replicates of 200 births. You should end up with 10000 numbers, each one a count of boys out of 200 births. Compare the distribution of predicted numbers of boys to the actual count in the data (111 boys out of 200 births). There are many good ways to visualize the simulations, but the &lt;code&gt;dens()&lt;/code&gt; command (part of the &lt;code&gt;rethinking&lt;/code&gt; package) is probably the easiest way in this case. Does it look like the model fits the data well? That is, does the distribution of predictions include the actual observation as a central, likely outcome?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;n_btrials &amp;lt;- 1e4 # birth observations
set.seed(42)
b_sample &amp;lt;- rbinom(n_btrials, size = n_ttl, prob = p_samples)
simplehist(b_sample)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-03_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mean(b_sample)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 110.9792
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;median(b_sample)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 111
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dens(b_sample)
abline(v = n_boys, col = &amp;quot;red&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-03_files/figure-html/unnamed-chunk-22-2.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;practice-h4&#34;&gt;Practice H4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Now compare 10000 counts of boys from 100 simulated first-borns only to the number of boys in the first births, &lt;code&gt;birth1&lt;/code&gt;. How does the model look in this light?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;n_boys_b1 &amp;lt;- sum(birth1)
n_ttl_b1 &amp;lt;- length(birth1)
n_btrials &amp;lt;- 1e4 # birth observations
likelihood &amp;lt;- dbinom(sum(birth1), size = length(birth1), prob = p_grid)
posterior &amp;lt;- likelihood * prior
posterior &amp;lt;- posterior / sum(posterior)
samples &amp;lt;- sample(p_grid, prob = posterior, size = 1e4, replace = TRUE)
set.seed(42)
b_sample &amp;lt;- rbinom(n_btrials, size = n_ttl_b1, prob = samples)
simplehist(b_sample)
abline(v = n_boys_b1, col = &amp;quot;red&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-03_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mean(b_sample)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 51.0102
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;median(b_sample)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 51
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dens(b_sample)
abline(v = n_boys_b1, col = &amp;quot;red&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-03_files/figure-html/unnamed-chunk-23-2.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;practice-h5&#34;&gt;Practice H5&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; The model assumes that sex of first and second births are independent. To check this assumption, focus now on second births that followed female first-borns. Compare 10,000 simulated counts of boys to only those second births that followed girls. To do this correctly, you need to count the number of first-borns who were girls and simulate that many births, 10,000 times. Compare the counts of boys in your simulations to the actual observed count of boys following girls. How does the model look in this light? Any guesses what is going on in these data?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;n_ttl_g1 &amp;lt;- sum(birth1 == 0)
n_ttl_g1b2 &amp;lt;- sum(birth2[birth1 == 0])
print(sprintf(&amp;quot;there were %d boys born after first girl. There were ttl %d cases&amp;quot;, n_ttl_g1b2, n_ttl_g1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;there were 39 boys born after first girl. There were ttl 49 cases&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;n_btrials &amp;lt;- 1e4 # birth observations
b_sample &amp;lt;- rbinom(n_btrials, size = n_ttl_g1, prob = p_samples)
mean(b_sample)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 27.1431
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;median(b_sample)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 27
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;simplehist(b_sample)
abline(v = n_ttl_g1b2, col = &amp;quot;red&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-03_files/figure-html/unnamed-chunk-24-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The model underestimates number of boys for the second child after the first girl. Gender of the second child is not independent from the first one.&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.5 (2021-03-31)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19043)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.1252  LC_CTYPE=English_United Kingdom.1252    LC_MONETARY=English_United Kingdom.1252 LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.1252    
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] rethinking_2.13      rstan_2.21.2         ggplot2_3.3.3        StanHeaders_2.21.0-7
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.7         mvtnorm_1.1-1      lattice_0.20-41    prettyunits_1.1.1  ps_1.6.0           assertthat_0.2.1   digest_0.6.27      utf8_1.2.1         V8_3.4.1           R6_2.5.0          
## [11] backports_1.2.1    stats4_4.0.5       evaluate_0.14      coda_0.19-4        highr_0.9          blogdown_1.3       pillar_1.6.0       rlang_0.4.11       curl_4.3.2         callr_3.7.0       
## [21] jquerylib_0.1.4    R.utils_2.10.1     R.oo_1.24.0        rmarkdown_2.7      styler_1.4.1       stringr_1.4.0      loo_2.4.1          munsell_0.5.0      compiler_4.0.5     xfun_0.22         
## [31] pkgconfig_2.0.3    pkgbuild_1.2.0     shape_1.4.5        htmltools_0.5.1.1  tidyselect_1.1.0   tibble_3.1.1       gridExtra_2.3      bookdown_0.22      codetools_0.2-18   matrixStats_0.61.0
## [41] fansi_0.4.2        crayon_1.4.1       dplyr_1.0.5        withr_2.4.2        MASS_7.3-53.1      R.methodsS3_1.8.1  grid_4.0.5         jsonlite_1.7.2     gtable_0.3.0       lifecycle_1.0.0   
## [51] DBI_1.1.1          magrittr_2.0.1     scales_1.1.1       RcppParallel_5.1.2 cli_3.0.0          stringi_1.5.3      bslib_0.2.4        ellipsis_0.3.2     generics_0.1.0     vctrs_0.3.7       
## [61] rematch2_2.1.2     tools_4.0.5        R.cache_0.14.0     glue_1.4.2         purrr_0.3.4        processx_3.5.1     yaml_2.2.1         inline_0.3.17      colorspace_2.0-0   knitr_1.33        
## [71] sass_0.3.1
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Coding Practices - Life with R</title>
      <link>https://www.erikkusch.com/courses/excursions-into-biostatistics/3_coding-practices-life-with-r/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/excursions-into-biostatistics/3_coding-practices-life-with-r/</guid>
      <description>&lt;p&gt;I have prepared some &lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/Excursions-into-Biostatistics/2021-07-08-Coding-Practices_Web.html&#34; target=&#34;_blank&#34;&gt; Lecture Slides &lt;/a&gt; for this session. I also provide the &lt;a href=&#34;https://github.com/ErikKusch/Homepage/raw/master/static/courses/Excursions-into-Biostatistics/CodinBackBone.R&#34; target=&#34;_blank&#34;&gt; code structure &lt;/a&gt; I propose during the talk with useful helper functions.&lt;/p&gt;
&lt;p&gt;An older version of the lecture slides from previous talks on the same subject can be found &lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/Excursions-into-Biostatistics/Coding-Practices---Life-With-R.html&#34; target=&#34;_blank&#34;&gt; here &lt;/a&gt; and are much text-heavier.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Primer For Statistical Tests</title>
      <link>https://www.erikkusch.com/courses/an-introduction-to-biostatistics/a-primer-for-statistical-tests/</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/an-introduction-to-biostatistics/a-primer-for-statistical-tests/</guid>
      <description>&lt;h2 id=&#34;theory&#34;&gt;Theory&lt;/h2&gt;
&lt;p&gt;These are the solutions to the exercises contained within the handout to A Primer For Statistical Tests which walks you through the basics of variables, their scales and distributions. Keep in mind that there is probably a myriad of other ways to reach the same conclusions as presented in these solutions.&lt;/p&gt;
&lt;p&gt;I have prepared some I have prepared some &lt;a href=&#34;https://www.erikkusch.com/courses/an-introduction-to-biostatistics/03---A-Primer-For-Statistical-Tests_Handout.html&#34; target=&#34;_blank&#34;&gt; Lecture Slides &lt;/a&gt; for this session.&lt;/p&gt;
&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;p&gt;Find the data for this exercise &lt;a href=&#34;https://www.erikkusch.com/courses/an-introduction-to-biostatistics/Data/Primer.RData&#34; target=&#34;_blank&#34;&gt; here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;loading-the-r-environment-object&#34;&gt;Loading the &lt;code&gt;R&lt;/code&gt; Environment Object&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;load(&amp;quot;Data/Primer.RData&amp;quot;)  # load data file from Data folder
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;variables&#34;&gt;Variables&lt;/h2&gt;
&lt;h3 id=&#34;finding-variables&#34;&gt;Finding Variables&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ls()  # list all elements in working environment
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Colour&amp;quot;               &amp;quot;Depth&amp;quot;                &amp;quot;IndividualsPassingBy&amp;quot;
## [4] &amp;quot;Length&amp;quot;               &amp;quot;Reproducing&amp;quot;          &amp;quot;Sex&amp;quot;                 
## [7] &amp;quot;Size&amp;quot;                 &amp;quot;Temperature&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;colour&#34;&gt;&lt;code&gt;Colour&lt;/code&gt;&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Colour)  # mode
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;barplot(table(Colour))  # fitting?
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;03---A-Primer-For-Statistical-Tests_files/figure-html/VColour-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Question&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Answer&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Mode?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;character&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Which scale?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Nominal&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;What&amp;rsquo;s implied?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Categorical data that can&amp;rsquo;t be ordered&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Does data fit scale?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;depth&#34;&gt;&lt;code&gt;Depth&lt;/code&gt;&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Depth)  # mode
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;barplot(Depth)  # fitting?
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;03---A-Primer-For-Statistical-Tests_files/figure-html/VDepth-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Question&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Answer&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Mode?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;numeric&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Which scale?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Interval/Discrete&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;What&amp;rsquo;s implied?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Continuous data with a non-absence point of origin&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Does data fit scale?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Debatable (is 0 depth absence of depth?)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;individualspassingby&#34;&gt;&lt;code&gt;IndividualsPassingBy&lt;/code&gt;&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(IndividualsPassingBy)  # mode
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;integer&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;barplot(IndividualsPassingBy)  # fitting?
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;03---A-Primer-For-Statistical-Tests_files/figure-html/VIndPass-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Question&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Answer&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Mode?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Which scale?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Integer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;What&amp;rsquo;s implied?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Only integer numbers with an absence point of origin&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Does data fit scale?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;length&#34;&gt;&lt;code&gt;Length&lt;/code&gt;&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Length)  # mode
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;barplot(Length)  # fitting?
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;03---A-Primer-For-Statistical-Tests_files/figure-html/VLength-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Question&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Answer&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Mode?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;numeric&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Which scale?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Relation/Ratio&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;What&amp;rsquo;s implied?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Continuous data with an absence point of origin&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Does data fit scale?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;reproducing&#34;&gt;&lt;code&gt;Reproducing&lt;/code&gt;&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Reproducing)  # mode
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;integer&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;barplot(Reproducing)  # fitting?
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;03---A-Primer-For-Statistical-Tests_files/figure-html/VRepro-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Question&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Answer&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Mode?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Which scale?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Integer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;What&amp;rsquo;s implied?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Only integer numbers with an absence point of origin&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Does data fit scale?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;sex&#34;&gt;&lt;code&gt;Sex&lt;/code&gt;&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Sex)  # mode
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;factor&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;barplot(table(Sex))  # fitting?
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;03---A-Primer-For-Statistical-Tests_files/figure-html/VSex-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Question&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Answer&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Mode?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;factor&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Which scale?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Binary&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;What&amp;rsquo;s implied?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Only two possible outcomes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Does data fit scale?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;size&#34;&gt;&lt;code&gt;Size&lt;/code&gt;&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Size)  # mode
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;barplot(table(Size))  # fitting?
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;03---A-Primer-For-Statistical-Tests_files/figure-html/VSize-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Question&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Answer&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Mode?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;character&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Which scale?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Ordinal&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;What&amp;rsquo;s implied?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Categorical data that can be ordered&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Does data fit scale?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;temperature&#34;&gt;&lt;code&gt;Temperature&lt;/code&gt;&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Temperature)  # mode
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;barplot(Temperature)  # fitting?
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;03---A-Primer-For-Statistical-Tests_files/figure-html/VTemp-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Question&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Answer&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Mode?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;numeric&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Which scale?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Interval/Discrete&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;What&amp;rsquo;s implied?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Continuous data with a non-absence point of origin&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Does data fit scale?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Yes (the data is clearly recorded in degree Celsius)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;distributions&#34;&gt;Distributions&lt;/h2&gt;
&lt;h3 id=&#34;length-1&#34;&gt;&lt;code&gt;Length&lt;/code&gt;&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(density(Length))  # distribution plot
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;03---A-Primer-For-Statistical-Tests_files/figure-html/DLength-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;shapiro.test(Length)  # normality check
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Shapiro-Wilk normality test
## 
## data:  Length
## W = 0.99496, p-value = 0.4331
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data is &lt;strong&gt;normal distributed&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;reproducing-1&#34;&gt;&lt;code&gt;Reproducing&lt;/code&gt;&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(density(Reproducing))  # distribution
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;03---A-Primer-For-Statistical-Tests_files/figure-html/DRepro-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;shapiro.test(Reproducing)  # normality check
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Shapiro-Wilk normality test
## 
## data:  Reproducing
## W = 0.98444, p-value = 0.2889
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data is &lt;strong&gt;binomial distributed&lt;/strong&gt; (i.e. &amp;ldquo;How many individuals manage to reproduce&amp;rdquo;) but looks &lt;strong&gt;normal distributed&lt;/strong&gt;. The normal distribution doesn&amp;rsquo;t make sense here because it implies continuity whilst the data only comes in integers.&lt;/p&gt;
&lt;h3 id=&#34;individualspassingby-1&#34;&gt;&lt;code&gt;IndividualsPassingBy&lt;/code&gt;&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(density(IndividualsPassingBy))  # distribution
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;03---A-Primer-For-Statistical-Tests_files/figure-html/DIndiv-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;shapiro.test(IndividualsPassingBy)  # normality check
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Shapiro-Wilk normality test
## 
## data:  IndividualsPassingBy
## W = 0.96905, p-value = 0.0187
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data is &lt;strong&gt;poisson distributed&lt;/strong&gt; (i.e. &amp;ldquo;How many individuals pass by an observer in a given time frame?&amp;quot;).&lt;/p&gt;
&lt;h3 id=&#34;depth-1&#34;&gt;&lt;code&gt;Depth&lt;/code&gt;&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(density(Depth))  # distribution
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;03---A-Primer-For-Statistical-Tests_files/figure-html/DDepth-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The data is &lt;strong&gt;uniform distributed&lt;/strong&gt;. You don&amp;rsquo;t know this distribution class from the lectures and I only wanted to confuse you with this to show you that there&amp;rsquo;s much more out there than I can show in our lectures.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Primer For Statistical Tests</title>
      <link>https://www.erikkusch.com/courses/biostat101/a-primer-for-statistical-tests/</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/biostat101/a-primer-for-statistical-tests/</guid>
      <description>&lt;h2 id=&#34;theory&#34;&gt;Theory&lt;/h2&gt;
&lt;p&gt;These are the solutions to the exercises contained within the handout to A Primer For Statistical Tests which walks you through the basics of variables, their scales and distributions. Keep in mind that there is probably a myriad of other ways to reach the same conclusions as presented in these solutions.&lt;/p&gt;
&lt;div class=&#34;alert alert-success&#34;&gt;
  &lt;div&gt;
    &lt;details&gt;
  &lt;summary&gt;Theory slides for this session.&lt;/summary&gt;
  Click the outline of the presentation below to get to the HTML version of the slides for this session.
    &lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/biostat101/03---A-Primer-For-Statistical-Tests_Handout.html&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://www.erikkusch.com/courses/biostat101/03---BioStat101_featured.png&#34; width=&#34;900&#34; margin-top = &#34;0&#34;/&gt;&lt;/a&gt;
&lt;/details&gt; 
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;p&gt;Find the data for this exercise &lt;a href=&#34;https://github.com/ErikKusch/Homepage/raw/master/content/courses/biostat101/Data/Primer.RData&#34; target=&#34;_blank&#34;&gt; here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;loading-the-r-environment-object&#34;&gt;Loading the &lt;code&gt;R&lt;/code&gt; Environment Object&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;load(&amp;quot;Data/Primer.RData&amp;quot;)  # load data file from Data folder
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;variables&#34;&gt;Variables&lt;/h2&gt;
&lt;h3 id=&#34;finding-variables&#34;&gt;Finding Variables&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ls()  # list all elements in working environment
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Colour&amp;quot;               &amp;quot;Depth&amp;quot;                &amp;quot;IndividualsPassingBy&amp;quot;
## [4] &amp;quot;Length&amp;quot;               &amp;quot;Reproducing&amp;quot;          &amp;quot;Sex&amp;quot;                 
## [7] &amp;quot;Size&amp;quot;                 &amp;quot;Temperature&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;colour&#34;&gt;&lt;code&gt;Colour&lt;/code&gt;&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Colour)  # mode
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;barplot(table(Colour))  # fitting?
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;03---A-Primer-For-Statistical-Tests_files/figure-html/VColour-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Question&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Answer&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Mode?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;character&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Which scale?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Nominal&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;What&amp;rsquo;s implied?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Categorical data that can&amp;rsquo;t be ordered&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Does data fit scale?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;depth&#34;&gt;&lt;code&gt;Depth&lt;/code&gt;&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Depth)  # mode
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;barplot(Depth)  # fitting?
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;03---A-Primer-For-Statistical-Tests_files/figure-html/VDepth-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Question&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Answer&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Mode?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;numeric&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Which scale?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Interval/Discrete&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;What&amp;rsquo;s implied?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Continuous data with a non-absence point of origin&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Does data fit scale?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Debatable (is 0 depth absence of depth?)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;individualspassingby&#34;&gt;&lt;code&gt;IndividualsPassingBy&lt;/code&gt;&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(IndividualsPassingBy)  # mode
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;integer&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;barplot(IndividualsPassingBy)  # fitting?
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;03---A-Primer-For-Statistical-Tests_files/figure-html/VIndPass-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Question&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Answer&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Mode?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Which scale?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Integer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;What&amp;rsquo;s implied?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Only integer numbers with an absence point of origin&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Does data fit scale?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;length&#34;&gt;&lt;code&gt;Length&lt;/code&gt;&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Length)  # mode
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;barplot(Length)  # fitting?
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;03---A-Primer-For-Statistical-Tests_files/figure-html/VLength-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Question&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Answer&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Mode?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;numeric&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Which scale?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Relation/Ratio&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;What&amp;rsquo;s implied?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Continuous data with an absence point of origin&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Does data fit scale?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;reproducing&#34;&gt;&lt;code&gt;Reproducing&lt;/code&gt;&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Reproducing)  # mode
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;integer&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;barplot(Reproducing)  # fitting?
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;03---A-Primer-For-Statistical-Tests_files/figure-html/VRepro-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Question&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Answer&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Mode?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Which scale?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Integer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;What&amp;rsquo;s implied?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Only integer numbers with an absence point of origin&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Does data fit scale?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;sex&#34;&gt;&lt;code&gt;Sex&lt;/code&gt;&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Sex)  # mode
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;factor&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;barplot(table(Sex))  # fitting?
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;03---A-Primer-For-Statistical-Tests_files/figure-html/VSex-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Question&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Answer&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Mode?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;factor&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Which scale?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Binary&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;What&amp;rsquo;s implied?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Only two possible outcomes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Does data fit scale?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;size&#34;&gt;&lt;code&gt;Size&lt;/code&gt;&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Size)  # mode
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;barplot(table(Size))  # fitting?
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;03---A-Primer-For-Statistical-Tests_files/figure-html/VSize-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Question&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Answer&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Mode?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;character&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Which scale?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Ordinal&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;What&amp;rsquo;s implied?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Categorical data that can be ordered&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Does data fit scale?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;temperature&#34;&gt;&lt;code&gt;Temperature&lt;/code&gt;&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Temperature)  # mode
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;barplot(Temperature)  # fitting?
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;03---A-Primer-For-Statistical-Tests_files/figure-html/VTemp-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Question&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Answer&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Mode?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;numeric&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Which scale?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Interval/Discrete&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;What&amp;rsquo;s implied?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Continuous data with a non-absence point of origin&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Does data fit scale?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Yes (the data is clearly recorded in degree Celsius)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;distributions&#34;&gt;Distributions&lt;/h2&gt;
&lt;h3 id=&#34;length-1&#34;&gt;&lt;code&gt;Length&lt;/code&gt;&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(density(Length))  # distribution plot
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;03---A-Primer-For-Statistical-Tests_files/figure-html/DLength-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;shapiro.test(Length)  # normality check
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Shapiro-Wilk normality test
## 
## data:  Length
## W = 0.99496, p-value = 0.4331
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data is &lt;strong&gt;normal distributed&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;reproducing-1&#34;&gt;&lt;code&gt;Reproducing&lt;/code&gt;&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(density(Reproducing))  # distribution
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;03---A-Primer-For-Statistical-Tests_files/figure-html/DRepro-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;shapiro.test(Reproducing)  # normality check
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Shapiro-Wilk normality test
## 
## data:  Reproducing
## W = 0.98444, p-value = 0.2889
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data is &lt;strong&gt;binomial distributed&lt;/strong&gt; (i.e. &amp;ldquo;How many individuals manage to reproduce&amp;rdquo;) but looks &lt;strong&gt;normal distributed&lt;/strong&gt;. The normal distribution doesn&amp;rsquo;t make sense here because it implies continuity whilst the data only comes in integers.&lt;/p&gt;
&lt;h3 id=&#34;individualspassingby-1&#34;&gt;&lt;code&gt;IndividualsPassingBy&lt;/code&gt;&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(density(IndividualsPassingBy))  # distribution
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;03---A-Primer-For-Statistical-Tests_files/figure-html/DIndiv-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;shapiro.test(IndividualsPassingBy)  # normality check
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Shapiro-Wilk normality test
## 
## data:  IndividualsPassingBy
## W = 0.96905, p-value = 0.0187
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data is &lt;strong&gt;poisson distributed&lt;/strong&gt; (i.e. &amp;ldquo;How many individuals pass by an observer in a given time frame?&amp;quot;).&lt;/p&gt;
&lt;h3 id=&#34;depth-1&#34;&gt;&lt;code&gt;Depth&lt;/code&gt;&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(density(Depth))  # distribution
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;03---A-Primer-For-Statistical-Tests_files/figure-html/DDepth-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The data is &lt;strong&gt;uniform distributed&lt;/strong&gt;. You don&amp;rsquo;t know this distribution class from the lectures and I only wanted to confuse you with this to show you that there&amp;rsquo;s much more out there than I can show in our lectures.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Change Analysis</title>
      <link>https://www.erikkusch.com/courses/bftp-biome-detection/change-analysis/</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/bftp-biome-detection/change-analysis/</guid>
      <description>&lt;h2 id=&#34;preparing-the-work&#34;&gt;Preparing The Work&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s create our basic structure for this document:&lt;/p&gt;
&lt;h3 id=&#34;head&#34;&gt;Head&lt;/h3&gt;
&lt;p&gt;Not much has changed in the &lt;strong&gt;head&lt;/strong&gt; when compared to our last exercise. We merely change the &lt;em&gt;contents&lt;/em&gt; and and the &lt;em&gt;edit&lt;/em&gt; tag, since the rest stays the same for the entire project.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# ####################################################################### #
# PROJECT: [BFTP] Identifying Biomes And Their Shifts Using Remote Sensing
# CONTENTS: Functionality to identify and analyse changes in spatial cluster distributions
# AUTHOR: Erik Kusch
# EDIT: 19/03/20
# ####################################################################### #
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;preamble&#34;&gt;Preamble&lt;/h3&gt;
&lt;p&gt;I am keeping the same &lt;strong&gt;preamble&lt;/strong&gt; as last time because we will need to index the data and the plot directory in this exercise. Our preamble then looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rm(list=ls()) # clearing the entire environment
Dir.Base &amp;lt;- getwd() # identifying the current directory
Dir.Data &amp;lt;- paste(Dir.Base, &amp;quot;Data&amp;quot;, sep=&amp;quot;/&amp;quot;) # generating the folder path for data folder
Dir.Plots &amp;lt;- paste(Dir.Base, &amp;quot;Plots&amp;quot;, sep=&amp;quot;/&amp;quot;) # generating the folder path for figures folder
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice, that we do not call the function &lt;code&gt;dir.create()&lt;/code&gt; this time. We don&amp;rsquo;t need to do so, because we already created the two directories established above in our last exercise. Usually, we would create this entire analysis of your BFTP project in one &lt;code&gt;R&lt;/code&gt; code script. In this case, we would only have one preamble which defines and creates directories instead of doing this step for every single sub-part of the analysis. Alas, we want to break this down for you. Therefore, you see this preamble here and will again in the next exercise.&lt;/p&gt;
&lt;p&gt;This time, we actually do load packages here as we really only need the &lt;code&gt;raster&lt;/code&gt; package. By now, I am assuming you know what we use it for:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(raster) # the raster package for rasters
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Additionally, we reload our &lt;code&gt;.RData&lt;/code&gt; workspace from the last exercise to gain back our &lt;code&gt;mclust&lt;/code&gt; model objects in particular.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;load(file = &amp;quot;Workspace.RData&amp;quot;) # load workspace
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;coding&#34;&gt;Coding&lt;/h3&gt;
&lt;p&gt;Again, all of the important &lt;strong&gt;Coding&lt;/strong&gt; happens after the head and the preamble are written and run in &lt;code&gt;R&lt;/code&gt;. Basically, this is the rest of this document once more.&lt;/p&gt;
&lt;h2 id=&#34;change-analysis&#34;&gt;Change Analysis&lt;/h2&gt;
&lt;h3 id=&#34;loading-data&#34;&gt;Loading Data&lt;/h3&gt;
&lt;p&gt;Firstly, we need the raw NDVI mean and seasonality data for the time frames we want to compare. Let&amp;rsquo;s deal with that right quick.&lt;/p&gt;
&lt;h4 id=&#34;1982&#34;&gt;1982&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s load our 1982 NDVI mean and seasonality data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Mean1982_ras &amp;lt;- raster(paste(Dir.Data, &amp;quot;1982Mean.nc&amp;quot;, sep=&amp;quot;/&amp;quot;)) # loading means
Season1982_ras &amp;lt;- raster(paste(Dir.Data, &amp;quot;1982Season.nc&amp;quot;, sep=&amp;quot;/&amp;quot;)) # loading seasonalities
All1982_ras &amp;lt;- stack(Mean1982_ras, Season1982_ras) # creating a stack
names(All1982_ras) &amp;lt;- c(&amp;quot;Mean&amp;quot;, &amp;quot;Seasonality&amp;quot;) # assign names to stack layers
Vals1982_mat &amp;lt;- values(All1982_ras) # extract data
rownames(Vals1982_mat) &amp;lt;- 1:dim(Vals1982_mat)[1] # rownames to index raster cell number
Vals1982_mat &amp;lt;- na.omit(Vals1982_mat) # omit all rows which contain at least one NA record
summary(Vals1982_mat) # a summary of the data
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Mean       Seasonality  
##  Min.   :0.00   Min.   :0.00  
##  1st Qu.:0.22   1st Qu.:0.55  
##  Median :0.33   Median :0.67  
##  Mean   :0.32   Mean   :0.64  
##  3rd Qu.:0.41   3rd Qu.:0.76  
##  Max.   :0.84   Max.   :1.00
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2015&#34;&gt;2015&lt;/h4&gt;
&lt;p&gt;In order to assess how biome distributions have changed, we need another time frame to compare our 1982 data to. For this, I have re-run the code from our first BFTP exercise for the year 2015. We now load that data into &lt;code&gt;R&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Mean2015_ras &amp;lt;- raster(paste(Dir.Data, &amp;quot;2015Mean.nc&amp;quot;, sep=&amp;quot;/&amp;quot;)) # loading means
Season2015_ras &amp;lt;- raster(paste(Dir.Data, &amp;quot;2015Season.nc&amp;quot;, sep=&amp;quot;/&amp;quot;)) # loading seasonalities
All2015_ras &amp;lt;- stack(Mean2015_ras, Season2015_ras) # creating a stack
names(All2015_ras) &amp;lt;- c(&amp;quot;Mean&amp;quot;, &amp;quot;Seasonality&amp;quot;) # assign names to stack layers
Vals2015_mat &amp;lt;- values(All2015_ras) # extract data
rownames(Vals2015_mat) &amp;lt;- 1:dim(Vals2015_mat)[1] # rownames to index raster cell number
Vals2015_mat &amp;lt;- na.omit(Vals2015_mat) # omit all rows which contain at least one NA record
summary(Vals2015_mat) # a summary of the data
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Mean       Seasonality  
##  Min.   :0.00   Min.   :0.00  
##  1st Qu.:0.25   1st Qu.:0.56  
##  Median :0.33   Median :0.67  
##  Mean   :0.34   Mean   :0.64  
##  3rd Qu.:0.43   3rd Qu.:0.77  
##  Max.   :0.83   Max.   :1.00
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice, that the output of the &lt;code&gt;summary()&lt;/code&gt; function is different for both matrices built from raster data values. This is important to ensure that our analysis actually references different time frames.&lt;/p&gt;
&lt;p&gt;\newpage&lt;/p&gt;
&lt;h3 id=&#34;predictions&#34;&gt;Predictions&lt;/h3&gt;
&lt;p&gt;Secondly, we want to compare cluster assignments. To do so, we need to use our &lt;code&gt;mclust&lt;/code&gt; models to predict cluster assignments for each cell in our target region raster using the NDVI mean and seasonality data that we loaded previously.&lt;/p&gt;
&lt;h4 id=&#34;1982-1&#34;&gt;1982&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s deal with the 1982 data first. &lt;code&gt;mod2&lt;/code&gt; is the &lt;code&gt;mclust&lt;/code&gt; model object for 4 clusters from our last exercise. Here, we predict clusters and place them on a raster:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ModPred1982 &amp;lt;- predict.Mclust(mod2, Vals1982_mat) # prediction
Pred1982_ras &amp;lt;- Mean1982_ras # establishing a rediction raster
values(Pred1982_ras) &amp;lt;- NA # set everything to NA
# set values of prediction raster to corresponding classification according to rowname
values(Pred1982_ras)[as.numeric(rownames(Vals1982_mat))] &amp;lt;- as.vector(ModPred1982$classification)
colours &amp;lt;- rainbow(mod2$G) # define 4 colours
plot(Pred1982_ras, # what to plot
     col = colours, # colours for groups
     colNA = &amp;quot;black&amp;quot;, # which colour to assign to NA values
     legend.shrink=1, # vertical size of legend
     legend.width=2 # horizontal size of legend
     )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;03---change_files/figure-html/Pred1-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;\newpage&lt;/p&gt;
&lt;h4 id=&#34;2015-1&#34;&gt;2015&lt;/h4&gt;
&lt;p&gt;Now, we deal with the 2015 time frame. Notice, that we are using the &lt;code&gt;mod2&lt;/code&gt; &lt;code&gt;mclust&lt;/code&gt; model which was established for 1982 in our last exercise. It is important that we use the same model when predicting our classes between time frames to ensure comparability. After all, we want to make sure that cluster 1 is the same in 1982 as 2015. It is debatable whether we should use a cluster model built from just one year of data or even from the same time frame as one of the the time frames which are to be compared. In fact, I would argue that we should establish a &lt;code&gt;mclust&lt;/code&gt; model for the mean annual NDVI and mean annual seasonality of NDVI across the entire time for which data is available. For now, we simply use the 1982-reliant model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ModPred2015 &amp;lt;- predict.Mclust(mod2, Vals2015_mat) # prediction
Pred2015_ras &amp;lt;- Mean2015_ras # establishing a rediction raster
values(Pred2015_ras) &amp;lt;- NA # set everything to NA
# set values of prediction raster to corresponding classification according to rowname
values(Pred2015_ras)[as.numeric(rownames(Vals2015_mat))] &amp;lt;- as.vector(ModPred2015$classification)
colours &amp;lt;- rainbow(mod2$G) # define 4 colours
plot(Pred2015_ras, # what to plot
     col = colours, # colours for groups
     colNA = &amp;quot;black&amp;quot;, # which colour to assign to NA values
     legend.shrink=1, # vertical size of legend
     legend.width=2 # horizontal size of legend
     )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;03---change_files/figure-html/Pred2-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You can already see some cluster assignment changes on Nunivak island.&lt;/p&gt;
&lt;p&gt;\newpage&lt;/p&gt;
&lt;h3 id=&#34;initial-assesment&#34;&gt;Initial Assesment&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s first assess how many raster cells have changed cluster assignment between our two time frames:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# identify how many cell assignments don&#39;t match between rasters
Change &amp;lt;- sum(ModPred1982$classification != ModPred2015$classification)
# divide number of mismatches by number of all cells
Change/length(ModPred2015$classification)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.22
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, there is a proportion of 0.22 raster cells which have changed cluster assignment between the two time frames. Now, let&amp;rsquo;s put this on a map:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;PredChange_ras &amp;lt;- Mean2015_ras # establishing a rediction raster
values(PredChange_ras) &amp;lt;- NA # set everything to NA
# set values of prediction raster to corresponding classification according to rowname
values(PredChange_ras)[as.numeric(rownames(Vals2015_mat))] &amp;lt;- 
  ModPred1982$classification != ModPred2015$classification
colours &amp;lt;- c(&amp;quot;green&amp;quot;, &amp;quot;red&amp;quot;) # define 2 colours
plot(PredChange_ras, col = colours, colNA = &amp;quot;black&amp;quot;,
     legend.shrink=1, legend.width=2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;03---change_files/figure-html/qualb-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I leave it to you to interpret these patterns (there actually is an interpretation to be had here).&lt;/p&gt;
&lt;p&gt;\newpage&lt;/p&gt;
&lt;h3 id=&#34;in-depth-assesment&#34;&gt;In-Depth Assesment&lt;/h3&gt;
&lt;p&gt;I&amp;rsquo;d argue that a simple understanding whether things have changed won&amp;rsquo;t be what we want to report. What we want, is to know which cluster took over the cells of which raster. I.e., I&amp;rsquo;d like to answer the question: &amp;ldquo;Which clusters take over the regions of other clusters and which ones?&amp;rdquo;. I hope you&amp;rsquo;re interested in this, too. Here&amp;rsquo;s how we can analyse this: For each cluster assignment we:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Identify the cells corresponding to it in 1982/the past&lt;/li&gt;
&lt;li&gt;Count how many of these cells are classified as the same cluster in 2015/the present&lt;/li&gt;
&lt;li&gt;Repeat the above for all combinations of cluster assignments imaginable&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;NClusters &amp;lt;- mod2$G # identify the number of clusters
present &amp;lt;- as.vector(Pred2015_ras) # assignments in 2015
past &amp;lt;- as.vector(Pred1982_ras) # assignments in 1982
# this matrix will hold the data, rows will show past state, columns will show present state
changematrix &amp;lt;- matrix(rep(NA, NClusters^2), nrow=NClusters, ncol=NClusters)
changevec &amp;lt;- rep(NA, NClusters) # this vector will fill rows in our matrix
for(k in 1:NClusters){ # loop over clusters in past
  changerun &amp;lt;- changevec 
  changeperc &amp;lt;- changevec
  for(m in 1:NClusters){ # loop over clusters in present
    presentcells &amp;lt;- which(present==m) # figure out which cells hold value m
    pastcells &amp;lt;- which(past==k) # figure out which cells hold value k
    # figure out how many of the cell denominators are shared by the two vectors
    rate &amp;lt;- length(Reduce(intersect, list(pastcells,presentcells))) 
    changerun[m] &amp;lt;- rate # save rate to changerun in place m
  } # end of present-loop
  changematrix[k,] &amp;lt;- changerun # save changerun to k row in matrix
  for(n in 1:NClusters){ # turn rates into portions
    # divide number of in a cell by total number of cells in its row
    changeperc[n] &amp;lt;- changematrix[k,n] / sum(changematrix[k,])
  } # end of percentages
  changematrix[k,] &amp;lt;- changeperc # save changeperc to row k
} # end of past-loop
changematrix &amp;lt;- changematrix*100 # turn everything into percentages
rownames(changematrix) &amp;lt;- paste0(&amp;quot;Past&amp;quot;, 1:NClusters)
colnames(changematrix) &amp;lt;- paste0(&amp;quot;Present&amp;quot;, 1:NClusters)
changematrix # show the matrix
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Present1 Present2 Present3 Present4
## Past1     82.0     0.84    4.459       13
## Past2      3.5    60.35    0.074       36
## Past3     23.8     0.00   76.235        0
## Past4     16.9     3.01    0.029       80
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, there&amp;rsquo;s quite a bit going on here. Let me explain how to read this. From past (1982) to present (2015), 82.03% of raster cells assigned to cluster 1 in 1982 are assigned to cluster 1 in 2015 as well. 0.84% of raster cells previously assigned to cluster 1 are classified as cluster 2 in 2015. Notice, how all rows sum up to 100% each. Representing the total of assigned raster cells in the 1982 record.&lt;/p&gt;
&lt;p&gt;Given the biological counterparts of the clusters, how would you interpret these shifts?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Additional Static Exercises</title>
      <link>https://www.erikkusch.com/courses/bayes-nets/static/</link>
      <pubDate>Tue, 25 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/bayes-nets/static/</guid>
      <description>&lt;style&gt;

blockquote{
color:#633a00;
}

&lt;/style&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/content/courses/bayes-nets/Static-Bayesian-Networks.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Static Bayesian Networks&lt;/a&gt; by 
&lt;a href=&#34;https://www.linkedin.com/in/felipe-sanchez-22b335135/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Felipe Sanchez&lt;/a&gt; (one of our study group members)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For detailed summary slides, please consult the separate sections on 
&lt;a href=&#34;https://www.erikkusch.com/courses/bayes-nets/part-1/&#34;&gt;Multinomial&lt;/a&gt;, 
&lt;a href=&#34;https://www.erikkusch.com/courses/bayes-nets/part-2/&#34;&gt;Gaussian&lt;/a&gt;, and 
&lt;a href=&#34;https://www.erikkusch.com/courses/bayes-nets/part-3/&#34;&gt;Hybrid&lt;/a&gt; Bayesian Networks.&lt;/p&gt;
&lt;h2 id=&#34;exercises&#34;&gt;Exercises&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to the exercises at the end of chapter 2 in 
&lt;a href=&#34;https://link.springer.com/book/10.1007/978-1-4614-6446-4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bayesian Networks in R with Applications in Systems Biology&lt;/a&gt; by by Radhakrishnan Nagarajan, Marco Scutari &amp;amp; Sophie LÃ¨bre. Much of my inspiration for these solutions, where necessary, by consulting the solutions provided by the authors themselves as in the appendix.&lt;/p&gt;
&lt;h3 id=&#34;r-environment&#34;&gt;&lt;code&gt;R&lt;/code&gt; Environment&lt;/h3&gt;
&lt;p&gt;For today&amp;rsquo;s exercise, I load the following packages:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(bnlearn)
library(igraph)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;nagarajan-21&#34;&gt;Nagarajan 2.1&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Consider the &lt;code&gt;asia&lt;/code&gt; synthetic data set from Lauritzen and Spiegelhalter (1988), which describes the diagnosis of a patient at a chest clinic who has just come back from a trip to Asia and is showing dyspnea.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;part-a&#34;&gt;Part A&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Load the data set from the &lt;code&gt;bnlearn&lt;/code&gt; package and investigate its characteristics using the exploratory analysis techniques covered in Chap. 1.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(asia)
str(asia)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &#39;data.frame&#39;:	5000 obs. of  8 variables:
##  $ A: Factor w/ 2 levels &amp;quot;no&amp;quot;,&amp;quot;yes&amp;quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ S: Factor w/ 2 levels &amp;quot;no&amp;quot;,&amp;quot;yes&amp;quot;: 2 2 1 1 1 2 1 2 2 2 ...
##  $ T: Factor w/ 2 levels &amp;quot;no&amp;quot;,&amp;quot;yes&amp;quot;: 1 1 2 1 1 1 1 1 1 1 ...
##  $ L: Factor w/ 2 levels &amp;quot;no&amp;quot;,&amp;quot;yes&amp;quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ B: Factor w/ 2 levels &amp;quot;no&amp;quot;,&amp;quot;yes&amp;quot;: 2 1 1 2 1 1 1 2 2 2 ...
##  $ E: Factor w/ 2 levels &amp;quot;no&amp;quot;,&amp;quot;yes&amp;quot;: 1 1 2 1 1 1 1 1 1 1 ...
##  $ X: Factor w/ 2 levels &amp;quot;no&amp;quot;,&amp;quot;yes&amp;quot;: 1 1 2 1 1 1 1 1 1 1 ...
##  $ D: Factor w/ 2 levels &amp;quot;no&amp;quot;,&amp;quot;yes&amp;quot;: 2 1 2 2 2 2 1 2 2 2 ...
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(asia)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    A          S          T          L          B          E          X          D       
##  no :4958   no :2485   no :4956   no :4670   no :2451   no :4630   no :4431   no :2650  
##  yes:  42   yes:2515   yes:  44   yes: 330   yes:2549   yes: 370   yes: 569   yes:2350
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-b&#34;&gt;Part B&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Create a &lt;code&gt;bn&lt;/code&gt; object with the network structure described in the manual page of &lt;code&gt;asia&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dag_2.1 &amp;lt;- model2network(&amp;quot;[A][S][T|A][L|S][B|S][D|B:E][E|T:L][X|E]&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-c&#34;&gt;Part C&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Derive the skeleton, the moral graph, and the CPDAG representing the equivalence class of the network. Plot them using &lt;code&gt;graphviz.plot&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# object creation
skel_2.1 &amp;lt;- skeleton(dag_2.1)
moral_2.1 &amp;lt;- moral(dag_2.1)
equclass_2.1 &amp;lt;- cpdag(dag_2.1)
# plotting
par(mfrow = c(1, 3))
graphviz.plot(skel_2.1, main = &amp;quot;Skeleton&amp;quot;)
graphviz.plot(moral_2.1, main = &amp;quot;Moral&amp;quot;)
graphviz.plot(equclass_2.1, main = &amp;quot;Equivalence Class&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-10-25-nagara-static_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h4 id=&#34;part-d&#34;&gt;Part D&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Identify the parents, the children, the neighbors, and the Markov blanket of each node.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# parents
sapply(nodes(dag_2.1), bnlearn::parents, x = dag_2.1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $A
## character(0)
## 
## $B
## [1] &amp;quot;S&amp;quot;
## 
## $D
## [1] &amp;quot;B&amp;quot; &amp;quot;E&amp;quot;
## 
## $E
## [1] &amp;quot;L&amp;quot; &amp;quot;T&amp;quot;
## 
## $L
## [1] &amp;quot;S&amp;quot;
## 
## $S
## character(0)
## 
## $T
## [1] &amp;quot;A&amp;quot;
## 
## $X
## [1] &amp;quot;E&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# children
sapply(nodes(dag_2.1), bnlearn::children, x = dag_2.1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $A
## [1] &amp;quot;T&amp;quot;
## 
## $B
## [1] &amp;quot;D&amp;quot;
## 
## $D
## character(0)
## 
## $E
## [1] &amp;quot;D&amp;quot; &amp;quot;X&amp;quot;
## 
## $L
## [1] &amp;quot;E&amp;quot;
## 
## $S
## [1] &amp;quot;B&amp;quot; &amp;quot;L&amp;quot;
## 
## $T
## [1] &amp;quot;E&amp;quot;
## 
## $X
## character(0)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# neighbors
sapply(nodes(dag_2.1), bnlearn::nbr, x = dag_2.1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $A
## [1] &amp;quot;T&amp;quot;
## 
## $B
## [1] &amp;quot;D&amp;quot; &amp;quot;S&amp;quot;
## 
## $D
## [1] &amp;quot;B&amp;quot; &amp;quot;E&amp;quot;
## 
## $E
## [1] &amp;quot;D&amp;quot; &amp;quot;L&amp;quot; &amp;quot;T&amp;quot; &amp;quot;X&amp;quot;
## 
## $L
## [1] &amp;quot;E&amp;quot; &amp;quot;S&amp;quot;
## 
## $S
## [1] &amp;quot;B&amp;quot; &amp;quot;L&amp;quot;
## 
## $T
## [1] &amp;quot;A&amp;quot; &amp;quot;E&amp;quot;
## 
## $X
## [1] &amp;quot;E&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# markov blanket
sapply(nodes(dag_2.1), bnlearn::mb, x = dag_2.1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $A
## [1] &amp;quot;T&amp;quot;
## 
## $B
## [1] &amp;quot;D&amp;quot; &amp;quot;E&amp;quot; &amp;quot;S&amp;quot;
## 
## $D
## [1] &amp;quot;B&amp;quot; &amp;quot;E&amp;quot;
## 
## $E
## [1] &amp;quot;B&amp;quot; &amp;quot;D&amp;quot; &amp;quot;L&amp;quot; &amp;quot;T&amp;quot; &amp;quot;X&amp;quot;
## 
## $L
## [1] &amp;quot;E&amp;quot; &amp;quot;S&amp;quot; &amp;quot;T&amp;quot;
## 
## $S
## [1] &amp;quot;B&amp;quot; &amp;quot;L&amp;quot;
## 
## $T
## [1] &amp;quot;A&amp;quot; &amp;quot;E&amp;quot; &amp;quot;L&amp;quot;
## 
## $X
## [1] &amp;quot;E&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;nagarajan-22&#34;&gt;Nagarajan 2.2&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Using the network structures created in Exercise 2.1 for the asia data set, produce the following plots with &lt;code&gt;graphviz.plot&lt;/code&gt;:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;part-a-1&#34;&gt;Part A&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;A plot of the CPDAG of the equivalence class in which the arcs belonging to a v-structure are highlighted (either with a different color or using a thicker line width).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;graphviz.plot(equclass_2.1,
  highlight = list(arcs = vstructs(equclass_2.1, arcs = TRUE), lwd = 2, col = &amp;quot;red&amp;quot;)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-10-25-nagara-static_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h4 id=&#34;part-b-1&#34;&gt;Part B&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Fill the nodes with different colors according to their role in the diagnostic process: causes (âvisit to Asiaâ and âsmokingâ), effects (âtuberculosis,â âlung cancer,â and âbronchitisâ), and the diagnosis proper (âchest X-ray,â âdyspnea,â and âeither tuberculosis or lung cancer/bronchitisâ).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;No clue on how to do this with &lt;code&gt;graphviz.plot&lt;/code&gt; and the solution provided in the book results in an error message. Instead, I use &lt;code&gt;igraph&lt;/code&gt; for plotting:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# create igraph object
equclass_igraph &amp;lt;- graph_from_edgelist(arcs(equclass_2.1))
# assign colours, effects = red; causes = green; diagnosis = blue
V(equclass_igraph)$color &amp;lt;- c(&amp;quot;blue&amp;quot;, &amp;quot;red&amp;quot;, &amp;quot;green&amp;quot;, &amp;quot;green&amp;quot;, &amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;, &amp;quot;red&amp;quot;, &amp;quot;green&amp;quot;)
V(equclass_igraph)$name &amp;lt;- c(&amp;quot;Visit to Asia&amp;quot;, &amp;quot;Tubercolosis&amp;quot;, &amp;quot;Bronchitis&amp;quot;, &amp;quot;Dyspnoea&amp;quot;, &amp;quot;Smoking&amp;quot;, &amp;quot;Tuberculosis vs Cancer&amp;quot;, &amp;quot;X-Ray&amp;quot;, &amp;quot;Lung Cancer&amp;quot;)
# plotting
plot(equclass_igraph,
  layout = layout.circle,
  vertex.size = 30,
  vertex.label.color = &amp;quot;black&amp;quot;
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-10-25-nagara-static_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h4 id=&#34;part-c-1&#34;&gt;Part C&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Explore different layouts by changing the &lt;code&gt;layout&lt;/code&gt; and &lt;code&gt;shape&lt;/code&gt; arguments.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;par(mfrow = c(2, 5))
layout &amp;lt;- c(&amp;quot;dot&amp;quot;, &amp;quot;neato&amp;quot;, &amp;quot;twopi&amp;quot;, &amp;quot;circo&amp;quot;, &amp;quot;fdp&amp;quot;)
shape &amp;lt;- c(&amp;quot;ellipse&amp;quot;, &amp;quot;circle&amp;quot;)
for (l in layout) {
  for (s in shape) {
    graphviz.plot(equclass_2.1, shape = s, layout = l, main = paste(l, s))
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-10-25-nagara-static_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;nagarajan-23&#34;&gt;Nagarajan 2.3&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Consider the &lt;code&gt;marks&lt;/code&gt; data set analyzed in Sect. 2.3.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(marks)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-a-2&#34;&gt;Part A&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Discretize the data using a quantile transform and different numbers of intervals (say, from 2 to 5). How does the network structure learned from the resulting data sets change as the number of intervals increases?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;intervals &amp;lt;- 2:5
par(mfrow = c(1, length(intervals)))
for (int in intervals) {
  disc_data &amp;lt;- discretize(marks, breaks = int, method = &amp;quot;quantile&amp;quot;)
  graphviz.plot(hc(disc_data), main = int)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-10-25-nagara-static_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The network structure becomes flatter. I reckon this is caused by the loss of information as the number of intervals is increased and variables are discretised with no regard for joint distributions.&lt;/p&gt;
&lt;h4 id=&#34;part-b-2&#34;&gt;Part B&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Repeat the discretization using interval discretization using up to 5 intervals, and compare the resulting networks with the ones obtained previously with quantile discretization.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;intervals &amp;lt;- 2:5
par(mfrow = c(1, length(intervals)))
for (int in intervals) {
  disc_data &amp;lt;- discretize(marks, breaks = int, method = &amp;quot;interval&amp;quot;)
  graphviz.plot(hc(disc_data), main = int)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-10-25-nagara-static_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Although the specific placement of the nodes changes between the two discretisation approaches, the general pattern of loss of arcs as number of intervals increases stays constant.&lt;/p&gt;
&lt;h4 id=&#34;part-c-2&#34;&gt;Part C&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Does Harteminkâs discretization algorithm perform better than either quantile or interval discretization? How does its behavior depend on the number of initial breaks?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;intervals &amp;lt;- 2:5
par(mfrow = c(1, length(intervals)))
for (int in intervals) {
  disc_data &amp;lt;- discretize(marks, breaks = int, method = &amp;quot;hartemink&amp;quot;, ibreaks = 50, idisc = &amp;quot;interval&amp;quot;)
  graphviz.plot(hc(disc_data), main = int)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-10-25-nagara-static_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This form of discretisation seems more robust when assessing how accurately the DAG structure is learned when number of intervals is increased.&lt;/p&gt;
&lt;h3 id=&#34;nagarajan-24&#34;&gt;Nagarajan 2.4&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;The ALARM network (Beinlich et al. 1989) is a Bayesian network designed to provide an alarm message system for patients hospitalized in intensive care units (ICU). Since ALARM is commonly used as a benchmark in literature, a synthetic data set of 5000 observations generated from this network is available from bnlearn as &lt;code&gt;alarm&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(alarm)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-a-3&#34;&gt;Part A&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Create a &lt;code&gt;bn&lt;/code&gt; object for the âtrueâ structure of the network using the model string provided in its manual page.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;true_bn &amp;lt;- model2network(paste(&amp;quot;[HIST|LVF][CVP|LVV]&amp;quot;, &amp;quot;[PCWP|LVV][HYP][LVV|HYP:LVF][LVF]&amp;quot;,
  &amp;quot;[STKV|HYP:LVF][ERLO][HRBP|ERLO:HR]&amp;quot;, &amp;quot;[HREK|ERCA:HR][ERCA][HRSA|ERCA:HR][ANES]&amp;quot;,
  &amp;quot;[APL][TPR|APL][ECO2|ACO2:VLNG][KINK]&amp;quot;, &amp;quot;[MINV|INT:VLNG][FIO2][PVS|FIO2:VALV]&amp;quot;,
  &amp;quot;[SAO2|PVS:SHNT][PAP|PMB][PMB][SHNT|INT:PMB]&amp;quot;, &amp;quot;[INT][PRSS|INT:KINK:VTUB][DISC][MVS]&amp;quot;,
  &amp;quot;[VMCH|MVS][VTUB|DISC:VMCH]&amp;quot;, &amp;quot;[VLNG|INT:KINK:VTUB][VALV|INT:VLNG]&amp;quot;,
  &amp;quot;[ACO2|VALV][CCHL|ACO2:ANES:SAO2:TPR]&amp;quot;, &amp;quot;[HR|CCHL][CO|HR:STKV][BP|CO:TPR]&amp;quot;,
  sep = &amp;quot;&amp;quot;
))
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-b-3&#34;&gt;Part B&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Compare the networks learned with different constraint-based algorithms with the true one, both in terms of structural differences and using either BIC or BDe.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# learning
bn.gs &amp;lt;- gs(alarm)
bn.iamb &amp;lt;- iamb(alarm)
bn.inter &amp;lt;- inter.iamb(alarm)
# plotting
par(mfrow = c(2, 2))
graphviz.plot(true_bn, main = &amp;quot;True Structure&amp;quot;)
graphviz.plot(bn.gs, main = &amp;quot;Grow-Shrink&amp;quot;)
graphviz.plot(bn.iamb, main = &amp;quot;IAMB&amp;quot;)
graphviz.plot(bn.inter, main = &amp;quot;Inter-IAMB&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-10-25-nagara-static_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# comparisons
unlist(bnlearn::compare(true_bn, bn.gs))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## tp fp fn 
##  5 14 41
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;unlist(bnlearn::compare(true_bn, bn.iamb))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## tp fp fn 
## 16 18 30
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;unlist(bnlearn::compare(true_bn, bn.inter))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## tp fp fn 
## 27 11 19
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Scores
score(cextend(true_bn), alarm, type = &amp;quot;bde&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -218063
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;score(cextend(bn.gs), alarm, type = &amp;quot;bde&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -337116.1
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;score(cextend(bn.iamb), alarm, type = &amp;quot;bde&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -263670.8
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;score(cextend(bn.inter), alarm, type = &amp;quot;bde&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -259922.1
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-c-3&#34;&gt;Part C&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;The overall performance of constraint-based algorithms suggests that the asymptotic $\chi^2$ conditional independence tests may not be appropriate for analyzing &lt;code&gt;alarm&lt;/code&gt;. Are permutation or shrinkage tests better choices?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This should improve the performance drastically. However, computational time is so high that I refuse to run this code. Even the first call to &lt;code&gt;gs()&lt;/code&gt; below takes more than 12 hours to run. I don&amp;rsquo;t know what the authors of the exercise material had envisioned the learning outcome of this to be.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bn.gs2 &amp;lt;- gs(alarm, test = &amp;quot;smc-x2&amp;quot;)
bn.iamb2 &amp;lt;- iamb(alarm, test = &amp;quot;smc-x2&amp;quot;)
bn.inter2 &amp;lt;- inter.iamb(alarm, test = &amp;quot;smc-x2&amp;quot;)
unlist(compare(true_bn, bn.gs2))
unlist(compare(true_bn, bn.iamb2))
unlist(compare(true_bn, bn.inter2))
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-d-1&#34;&gt;Part D&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;How are the above learning strategies affected by changes to &lt;code&gt;alpha&lt;/code&gt;?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Shrinkage should also improves structure learning performance, but computational time should be much lower than it is with permutation tests. Much like with the previous exercise, however, the code below just takes too long for my liking to finish running.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bn.gs3 &amp;lt;- gs(alarm, test = &amp;quot;smc-x2&amp;quot;, alpha = 0.01)
bn.iamb3 &amp;lt;- iamb(alarm, test = &amp;quot;smc-x2&amp;quot;, alpha = 0.01)
bn.inter3 &amp;lt;- inter.iamb(alarm, test = &amp;quot;smc-x2&amp;quot;, alpha = 0.01)
unlist(compare(true, bn.gs3))
unlist(compare(true, bn.iamb3))
unlist(compare(true, bn.inter3))
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;nagarajan-25&#34;&gt;Nagarajan 2.5&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Consider again the &lt;code&gt;alarm&lt;/code&gt; network used in Exercise 2.4.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;part-a-4&#34;&gt;Part A&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Learn its structure with hill-climbing and tabu search, using the posterior density BDe as a score function. How does the network structure change with the imaginary sample size &lt;code&gt;iss&lt;/code&gt;?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;par(mfrow = c(2, 5))
for (iss in c(1, 5, 10, 20, 50)) {
  bn &amp;lt;- hc(alarm, score = &amp;quot;bde&amp;quot;, iss = iss)
  main &amp;lt;- paste(&amp;quot;hc(..., iss = &amp;quot;, iss, &amp;quot;)&amp;quot;, sep = &amp;quot;&amp;quot;)
  sub &amp;lt;- paste(narcs(bn), &amp;quot;arcs&amp;quot;)
  graphviz.plot(bn, main = main, sub = sub)
}
for (iss in c(1, 5, 10, 20, 50)) {
  bn &amp;lt;- tabu(alarm, score = &amp;quot;bde&amp;quot;, iss = iss)
  main &amp;lt;- paste(&amp;quot;tabu(..., iss = &amp;quot;, iss, &amp;quot;)&amp;quot;, sep = &amp;quot;&amp;quot;)
  sub &amp;lt;- paste(narcs(bn), &amp;quot;arcs&amp;quot;)
  graphviz.plot(bn, main = main, sub = sub)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-10-25-nagara-static_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The number of arcs increases with &lt;code&gt;iss&lt;/code&gt;. Large values of &lt;code&gt;iss&lt;/code&gt; over-smooth the data and thus result in networks with similar scores and therefore allow for many arcs to be included in the final networks.&lt;/p&gt;
&lt;h4 id=&#34;part-b-4&#34;&gt;Part B&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Does the length of the tabu list have a significant impact on the network structures learned with &lt;code&gt;tabu&lt;/code&gt;?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;par(mfrow = c(1, 5))
for (n in c(10, 15, 20, 50, 100)) {
  bn &amp;lt;- tabu(alarm, score = &amp;quot;bde&amp;quot;, tabu = n)
  bde &amp;lt;- score(bn, alarm, type = &amp;quot;bde&amp;quot;)
  main &amp;lt;- paste(&amp;quot;tabu(..., tabu = &amp;quot;, n, &amp;quot;)&amp;quot;, sep = &amp;quot;&amp;quot;)
  sub &amp;lt;- paste(ntests(bn), &amp;quot;steps, score&amp;quot;, bde)
  graphviz.plot(bn, main = main, sub = sub)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-10-25-nagara-static_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Increasing the tabu length severely affects the learned structure of the final network. Firstly, it does so by increasing the raw number of network structures explored by tabu. Secondly, getting stuck in local maxima becomes increasingly unlikely.&lt;/p&gt;
&lt;h4 id=&#34;part-c-4&#34;&gt;Part C&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;How does the BIC score compare with BDe at different sample sizes in terms of structure and score of the learned network?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;par(mfrow = c(2, 6))
for (n in c(100, 200, 500, 1000, 2000, 5000)) {
  bn.bde &amp;lt;- hc(alarm[1:n, ], score = &amp;quot;bde&amp;quot;)
  bn.bic &amp;lt;- hc(alarm[1:n, ], score = &amp;quot;bic&amp;quot;)
  bde &amp;lt;- score(bn.bde, alarm, type = &amp;quot;bde&amp;quot;)
  bic &amp;lt;- score(bn.bic, alarm, type = &amp;quot;bic&amp;quot;)
  main &amp;lt;- paste(&amp;quot;BDe, sample size&amp;quot;, n)
  sub &amp;lt;- paste(ntests(bn.bde), &amp;quot;steps, score&amp;quot;, bde)
  graphviz.plot(bn.bde, main = main, sub = sub)
  main &amp;lt;- paste(&amp;quot;BIC, sample size&amp;quot;, n)
  sub &amp;lt;- paste(ntests(bn.bic), &amp;quot;steps, score&amp;quot;, bic)
  graphviz.plot(bn.bic, main = main, sub = sub)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-10-25-nagara-static_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The networks become more similar as sample size increases. At small sample sizes, BIC results in sparser networks than BDe.&lt;/p&gt;
&lt;h3 id=&#34;nagarajan-26&#34;&gt;Nagarajan 2.6&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Consider the observational data set from Sachs et al. (2005) used in Sect. 2.5.1 (the original data set, not the discretized one).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;part-a-5&#34;&gt;Part A&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Evaluate the networks learned by hill-climbing with BIC and BGe using cross-validation and the log-likelihood loss function.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The sachs data file is available
&lt;a href=&#34;https://www.bnlearn.com/book-useR/code/sachs.data.txt.gz&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sachs &amp;lt;- read.table(&amp;quot;sachs.data.txt&amp;quot;, header = TRUE)
bn.bic &amp;lt;- hc(sachs, score = &amp;quot;bic-g&amp;quot;)
bn.cv(bn.bic, data = sachs)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   k-fold cross-validation for Bayesian networks
## 
##   target network structure:
##    [praf][PIP2][p44.42][PKC][pmek|praf][PIP3|PIP2][pakts473|p44.42][P38|PKC][plcg|PIP3][PKA|p44.42:pakts473][pjnk|PKC:P38] 
##   number of folds:                       10 
##   loss function:                         Log-Likelihood Loss (Gauss.) 
##   expected loss:                         65.48251
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bn.bge &amp;lt;- hc(sachs, score = &amp;quot;bge&amp;quot;)
bn.cv(bn.bge, data = sachs)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   k-fold cross-validation for Bayesian networks
## 
##   target network structure:
##    [praf][plcg][PIP2][p44.42][PKC][pmek|praf][PIP3|PIP2][pakts473|p44.42][P38|PKC][PKA|p44.42:pakts473][pjnk|PKC:P38] 
##   number of folds:                       10 
##   loss function:                         Log-Likelihood Loss (Gauss.) 
##   expected loss:                         65.3477
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The BGe network fits the data slightly better than the BIC-network.&lt;/p&gt;
&lt;h4 id=&#34;part-b-5&#34;&gt;Part B&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Use bootstrap resampling to evaluate the distribution of the number of arcs present in each of the networks learned in the previous point. Do they differ significantly?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(42)
narcs.bic &amp;lt;- bn.boot(sachs, algorithm = &amp;quot;hc&amp;quot;, algorithm.args = list(score = &amp;quot;bic-g&amp;quot;), statistic = narcs)
narcs.bge &amp;lt;- bn.boot(sachs, algorithm = &amp;quot;hc&amp;quot;, algorithm.args = list(score = &amp;quot;bge&amp;quot;), statistic = narcs)
narcs.bic &amp;lt;- unlist(narcs.bic)
narcs.bge &amp;lt;- unlist(narcs.bge)
par(mfrow = c(1, 2))
hist(narcs.bic, main = &amp;quot;BIC&amp;quot;, freq = FALSE)
curve(dnorm(x, mean = mean(narcs.bic), sd = sd(narcs.bic)), add = TRUE, col = 2)
hist(narcs.bge, main = &amp;quot;BGe&amp;quot;, freq = FALSE)
curve(dnorm(x, mean = mean(narcs.bge), sd = sd(narcs.bge)), add = TRUE, col = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2022-10-25-nagara-static_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Number-of-arc-distributions are markedly different between BIC and BGe networks.&lt;/p&gt;
&lt;h4 id=&#34;part-c-5&#34;&gt;Part C&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Compute the averaged network structure for &lt;code&gt;sachs&lt;/code&gt; using hill-climbing with BGe and different imaginary sample sizes. How does the value of the significance threshold change as &lt;code&gt;iss&lt;/code&gt; increases?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(42)
t &amp;lt;- c()
iss &amp;lt;- c(5, 10, 20, 50, 100)
for (i in iss) {
  s &amp;lt;- boot.strength(sachs,
    algorithm = &amp;quot;hc&amp;quot;,
    algorithm.args = list(score = &amp;quot;bge&amp;quot;, iss = i)
  )
  t &amp;lt;- c(t, attr(s, &amp;quot;threshold&amp;quot;))
}
t
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.780 0.415 0.430 0.380 0.440
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.2.1 (2022-06-23 ucrt)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19044)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_Germany.utf8  LC_CTYPE=English_Germany.utf8    LC_MONETARY=English_Germany.utf8 LC_NUMERIC=C                     LC_TIME=English_Germany.utf8    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] igraph_1.3.4  bnlearn_4.8.1
## 
## loaded via a namespace (and not attached):
##  [1] highr_0.9           bslib_0.4.0         compiler_4.2.1      jquerylib_0.1.4     R.methodsS3_1.8.2   R.utils_2.12.0      tools_4.2.1         digest_0.6.29       jsonlite_1.8.0     
## [10] evaluate_0.16       R.cache_0.16.0      pkgconfig_2.0.3     rlang_1.0.5         graph_1.74.0        cli_3.3.0           rstudioapi_0.14     Rgraphviz_2.40.0    yaml_2.3.5         
## [19] parallel_4.2.1      blogdown_1.13       xfun_0.33           fastmap_1.1.0       styler_1.8.0        stringr_1.4.1       knitr_1.40          sass_0.4.2          vctrs_0.4.1        
## [28] grid_4.2.1          stats4_4.2.1        R6_2.5.1            rmarkdown_2.16      bookdown_0.29       purrr_0.3.4         magrittr_2.0.3      htmltools_0.5.3     BiocGenerics_0.42.0
## [37] stringi_1.7.8       cachem_1.0.6        R.oo_1.25.0
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Research Project</title>
      <link>https://www.erikkusch.com/courses/excursions-into-biostatistics/research-project/</link>
      <pubDate>Sat, 27 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/excursions-into-biostatistics/research-project/</guid>
      <description>&lt;h2 id=&#34;our-resarch-project&#34;&gt;Our Resarch Project&lt;/h2&gt;
&lt;p&gt;Here (and over the next few exercises in this &amp;ldquo;course&amp;rdquo;), we are looking at a big (and entirely fictional) data base of the common house sparrow (&lt;em&gt;Passer domesticus&lt;/em&gt;). In particular, we are interested in the &lt;strong&gt;Evolution of &lt;em&gt;Passer domesticus&lt;/em&gt; in Response to Climate Change&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;the-data&#34;&gt;The Data&lt;/h3&gt;
&lt;p&gt;I have created a large data set for this exercise which is available in a cleaned and properly handled version &lt;a href=&#34;https://github.com/ErikKusch/Homepage/raw/master/content/courses/excursions-into-biostatistics/Data.rar&#34; target=&#34;_blank&#34;&gt; here&lt;/a&gt;.&lt;/p&gt;
&lt;h4 id=&#34;reading-the-data-into-r&#34;&gt;Reading the Data into &lt;code&gt;R&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s start by reading the data into &lt;code&gt;R&lt;/code&gt; and taking an initial look at it:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Sparrows_df &amp;lt;- readRDS(file.path(&amp;quot;Data&amp;quot;, &amp;quot;SparrowData.rds&amp;quot;))
Sparrows_df &amp;lt;- Sparrows_df[!is.na(Sparrows_df$Weight), ]
head(Sparrows_df)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Index Latitude Longitude     Climate Population.Status Weight Height Wing.Chord Colour    Sex Nesting.Site Nesting.Height Number.of.Eggs Egg.Weight Flock Home.Range Predator.Presence Predator.Type
## 1    SI       60       100 Continental            Native  34.05  12.87       6.67  Brown   Male         &amp;lt;NA&amp;gt;             NA             NA         NA     B      Large               Yes         Avian
## 2    SI       60       100 Continental            Native  34.86  13.68       6.79   Grey   Male         &amp;lt;NA&amp;gt;             NA             NA         NA     B      Large               Yes         Avian
## 3    SI       60       100 Continental            Native  32.34  12.66       6.64  Black Female        Shrub          35.60              1       3.21     C      Large               Yes         Avian
## 4    SI       60       100 Continental            Native  34.78  15.09       7.00  Brown Female        Shrub          47.75              0         NA     E      Large               Yes         Avian
## 5    SI       60       100 Continental            Native  35.01  13.82       6.81   Grey   Male         &amp;lt;NA&amp;gt;             NA             NA         NA     B      Large               Yes         Avian
## 6    SI       60       100 Continental            Native  32.36  12.67       6.64  Brown Female        Shrub          32.47              1       3.17     E      Large               Yes         Avian
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;variables&#34;&gt;Variables&lt;/h4&gt;
&lt;p&gt;When building models or trying to explain anything about our data set, we need to consider all the different variables and the information contained therein. In this data set, we have access to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;Index&lt;/code&gt; [&lt;em&gt;Factor&lt;/em&gt;] - an abbreviation of &lt;code&gt;Site&lt;/code&gt; records&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Latitude&lt;/code&gt; [&lt;em&gt;Numeric&lt;/em&gt;] - an identifier of where specific sparrow measurements where taken&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Longitude&lt;/code&gt; [&lt;em&gt;Numeric&lt;/em&gt;] - an identifier of where specific sparrow measurements where taken&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Climate&lt;/code&gt; [&lt;em&gt;Factor&lt;/em&gt;] - local climate types that sparrows are subjected to (e.g. coastal, continental, and semi-coastal)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Population.Status&lt;/code&gt; [&lt;em&gt;Factor&lt;/em&gt;] - population status (e.g. native or introduced)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Weight&lt;/code&gt; [&lt;em&gt;Numeric&lt;/em&gt;] - sparrow weight [g]; Range: 13-40g&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Height&lt;/code&gt; [&lt;em&gt;Numeric&lt;/em&gt;] - sparrow height/length [cm]; Range: 10-22cm&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Wing.Chord&lt;/code&gt; [&lt;em&gt;Numeric&lt;/em&gt;] - wing length [cm]; Range: 6-10cm&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Colour&lt;/code&gt; [&lt;em&gt;Factor&lt;/em&gt;] - main plumage colour (e.g. brown, grey, and black)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Sex&lt;/code&gt; [&lt;em&gt;Factor&lt;/em&gt;] - sparrow sex&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Nesting.Site&lt;/code&gt; [&lt;em&gt;Factor&lt;/em&gt;] - nesting conditions, only recorded for females (e.g. tree or shrub)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Nesting.Height&lt;/code&gt; [&lt;em&gt;Numeric&lt;/em&gt;] - nest elevation above ground level, only recorded for females&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Number.of.Eggs&lt;/code&gt; [&lt;em&gt;Numeric&lt;/em&gt;] - number of eggs per nest, only recorded for females&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Egg.Weight&lt;/code&gt; [&lt;em&gt;Numeric&lt;/em&gt;] - mean weight of eggs per nest, only recorded for females&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Flock&lt;/code&gt; [&lt;em&gt;Factor&lt;/em&gt;] - which flock at each location each sparrow belongs to&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Home.Range&lt;/code&gt; [&lt;em&gt;Factor&lt;/em&gt;] - size of home range of each flock (e.g. Small, Medium, and Large)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Predator.Presence&lt;/code&gt; [&lt;em&gt;Factor&lt;/em&gt;] - if a predator is present at a station (e.g. No or Yes)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Predator.Type&lt;/code&gt; [&lt;em&gt;Factor&lt;/em&gt;] - what kind of predator is present (e.g. Avian, Non-Avian, or None)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Note that the variables &lt;code&gt;Longitude&lt;/code&gt; and &lt;code&gt;Latitude&lt;/code&gt; may be used to retrieve climate data variables from a host of data sources.&lt;/p&gt;
&lt;h4 id=&#34;locations&#34;&gt;Locations&lt;/h4&gt;
&lt;p&gt;Looking at our data, we notice that it comes at distinct stations. Let&amp;rsquo;s visualise where they are:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(&amp;quot;leaflet&amp;quot;)
Plot_df &amp;lt;- Sparrows_df[, c(&amp;quot;Longitude&amp;quot;, &amp;quot;Latitude&amp;quot;, &amp;quot;Index&amp;quot;, &amp;quot;Climate&amp;quot;, &amp;quot;Population.Status&amp;quot;)]
Plot_df &amp;lt;- unique(Plot_df)
m &amp;lt;- leaflet()
m &amp;lt;- addTiles(m)
m &amp;lt;- addMarkers(m,
  lng = Plot_df$Longitude,
  lat = Plot_df$Latitude,
  label = Plot_df$Index,
  popup = paste(Plot_df$Population.Status, Plot_df$Climate, sep = &amp;quot;;&amp;quot;)
)
m
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-{=html}&#34;&gt;&amp;lt;div class=&amp;quot;leaflet html-widget html-fill-item-overflow-hidden html-fill-item&amp;quot; id=&amp;quot;htmlwidget-9aac1c390aadeb1fd4f9&amp;quot; style=&amp;quot;width:1440px;height:768px;&amp;quot;&amp;gt;&amp;lt;/div&amp;gt;
&amp;lt;script type=&amp;quot;application/json&amp;quot; data-for=&amp;quot;htmlwidget-9aac1c390aadeb1fd4f9&amp;quot;&amp;gt;{&amp;quot;x&amp;quot;:{&amp;quot;options&amp;quot;:{&amp;quot;crs&amp;quot;:{&amp;quot;crsClass&amp;quot;:&amp;quot;L.CRS.EPSG3857&amp;quot;,&amp;quot;code&amp;quot;:null,&amp;quot;proj4def&amp;quot;:null,&amp;quot;projectedBounds&amp;quot;:null,&amp;quot;options&amp;quot;:{}}},&amp;quot;calls&amp;quot;:[{&amp;quot;method&amp;quot;:&amp;quot;addTiles&amp;quot;,&amp;quot;args&amp;quot;:[&amp;quot;https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png&amp;quot;,null,null,{&amp;quot;minZoom&amp;quot;:0,&amp;quot;maxZoom&amp;quot;:18,&amp;quot;tileSize&amp;quot;:256,&amp;quot;subdomains&amp;quot;:&amp;quot;abc&amp;quot;,&amp;quot;errorTileUrl&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;tms&amp;quot;:false,&amp;quot;noWrap&amp;quot;:false,&amp;quot;zoomOffset&amp;quot;:0,&amp;quot;zoomReverse&amp;quot;:false,&amp;quot;opacity&amp;quot;:1,&amp;quot;zIndex&amp;quot;:1,&amp;quot;detectRetina&amp;quot;:false,&amp;quot;attribution&amp;quot;:&amp;quot;&amp;amp;copy; &amp;lt;a href=\&amp;quot;https://openstreetmap.org\&amp;quot;&amp;gt;OpenStreetMap&amp;lt;\/a&amp;gt; contributors, &amp;lt;a href=\&amp;quot;https://creativecommons.org/licenses/by-sa/2.0/\&amp;quot;&amp;gt;CC-BY-SA&amp;lt;\/a&amp;gt;&amp;quot;}]},{&amp;quot;method&amp;quot;:&amp;quot;addMarkers&amp;quot;,&amp;quot;args&amp;quot;:[[60,54,-25,-21.1,70,55,31,17.25,4,10.5,-51.75],[100,-2,135,55.6,-90,-97,-92,-88.75,-53,-67,-59.17],null,null,null,{&amp;quot;interactive&amp;quot;:true,&amp;quot;draggable&amp;quot;:false,&amp;quot;keyboard&amp;quot;:true,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;zIndexOffset&amp;quot;:0,&amp;quot;opacity&amp;quot;:1,&amp;quot;riseOnHover&amp;quot;:false,&amp;quot;riseOffset&amp;quot;:250},[&amp;quot;Native;Continental&amp;quot;,&amp;quot;Native;Coastal&amp;quot;,&amp;quot;Introduced;Continental&amp;quot;,&amp;quot;Introduced;Coastal&amp;quot;,&amp;quot;Introduced;Coastal&amp;quot;,&amp;quot;Introduced;Semi-Coastal&amp;quot;,&amp;quot;Introduced;Coastal&amp;quot;,&amp;quot;Introduced;Coastal&amp;quot;,&amp;quot;Introduced;Coastal&amp;quot;,&amp;quot;Introduced;Coastal&amp;quot;,&amp;quot;Introduced;Coastal&amp;quot;],null,null,null,[&amp;quot;SI&amp;quot;,&amp;quot;UK&amp;quot;,&amp;quot;AU&amp;quot;,&amp;quot;RE&amp;quot;,&amp;quot;NU&amp;quot;,&amp;quot;MA&amp;quot;,&amp;quot;LO&amp;quot;,&amp;quot;BE&amp;quot;,&amp;quot;FG&amp;quot;,&amp;quot;SA&amp;quot;,&amp;quot;FI&amp;quot;],{&amp;quot;interactive&amp;quot;:false,&amp;quot;permanent&amp;quot;:false,&amp;quot;direction&amp;quot;:&amp;quot;auto&amp;quot;,&amp;quot;opacity&amp;quot;:1,&amp;quot;offset&amp;quot;:[0,0],&amp;quot;textsize&amp;quot;:&amp;quot;10px&amp;quot;,&amp;quot;textOnly&amp;quot;:false,&amp;quot;className&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;sticky&amp;quot;:true},null]}],&amp;quot;limits&amp;quot;:{&amp;quot;lat&amp;quot;:[-51.75,70],&amp;quot;lng&amp;quot;:[-97,135]}},&amp;quot;evals&amp;quot;:[],&amp;quot;jsHooks&amp;quot;:[]}&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that you can zoom and drag the above map as well as click the station markers for some additional information.&lt;/p&gt;
&lt;h3 id=&#34;adding-information&#34;&gt;Adding Information&lt;/h3&gt;
&lt;p&gt;How do we get the data for this? Well, I wrote an &lt;code&gt;R&lt;/code&gt;-Package that does exactly that.&lt;/p&gt;
&lt;p&gt;First, said package needs to be installed from my GitHub repository for it. Subsequently, we need to set API Key and User number obtained at the 
&lt;a href=&#34;https://cds.climate.copernicus.eu/api-how-to&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Climate Data Store&lt;/a&gt;. I have already baked these into my material, so I don&amp;rsquo;t set them here, but include lines of code that ask you for your credentials when copy &amp;amp; pasted over:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;if (&amp;quot;KrigR&amp;quot; %in% rownames(installed.packages()) == FALSE) { # KrigR check
  Sys.setenv(R_REMOTES_NO_ERRORS_FROM_WARNINGS = &amp;quot;true&amp;quot;)
  devtools::install_github(&amp;quot;https://github.com/ErikKusch/KrigR&amp;quot;)
}
library(KrigR)
#### CDS API (needed for ERA5-Land downloads)
if (!exists(&amp;quot;API_Key&amp;quot;) | !exists(&amp;quot;API_User&amp;quot;)) { # CS API check: if CDS API credentials have not been specified elsewhere
  API_User &amp;lt;- readline(prompt = &amp;quot;Please enter your Climate Data Store API user number and hit ENTER.&amp;quot;)
  API_Key &amp;lt;- readline(prompt = &amp;quot;Please enter your Climate Data Store API key number and hit ENTER.&amp;quot;)
} # end of CDS API check

#### NUMBER OF CORES
if (!exists(&amp;quot;numberOfCores&amp;quot;)) { # Core check: if number of cores for parallel processing has not been set yet
  numberOfCores &amp;lt;- readline(prompt = paste(&amp;quot;How many cores do you want to allocate to these processes? Your machine has&amp;quot;, parallel::detectCores()))
} # end of Core check
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have the package, we can download some state-of-the-art climate data. I have already prepared all of this in the data directory you downloaded earlier so this step will automatically be skipped:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;if (!file.exists(file.path(&amp;quot;Data&amp;quot;, &amp;quot;SparrowDataClimate.rds&amp;quot;))) {
  colnames(Plot_df)[1:3] &amp;lt;- c(&amp;quot;Lon&amp;quot;, &amp;quot;Lat&amp;quot;, &amp;quot;ID&amp;quot;) # set column names to be in line with what KrigR wants
  Points_Raw &amp;lt;- download_ERA(
    Variable = &amp;quot;2m_temperature&amp;quot;,
    DataSet = &amp;quot;era5&amp;quot;,
    DateStart = &amp;quot;1982-01-01&amp;quot;,
    DateStop = &amp;quot;2012-12-31&amp;quot;,
    TResolution = &amp;quot;month&amp;quot;,
    TStep = 1,
    Extent = Plot_df, # the point data with Lon and Lat columns
    Buffer = 0.5, # a 0.5 degree buffer should be drawn around each point
    ID = &amp;quot;ID&amp;quot;, # this is the column which holds point IDs
    API_User = API_User,
    API_Key = API_Key,
    Dir = file.path(getwd(), &amp;quot;Data&amp;quot;),
    FileName = &amp;quot;AT_Climatology.nc&amp;quot;
  )
  Points_mean &amp;lt;- calc(Points_Raw, fun = mean)
  Points_sd &amp;lt;- calc(Points_Raw, fun = sd)
  Sparrows_df$TAvg &amp;lt;- as.numeric(extract(x = Points_mean, y = Sparrows_df[, c(&amp;quot;Longitude&amp;quot;, &amp;quot;Latitude&amp;quot;)], buffer = 0.3))
  Sparrows_df$TSD &amp;lt;- as.numeric(extract(x = Points_sd, y = Sparrows_df[, c(&amp;quot;Longitude&amp;quot;, &amp;quot;Latitude&amp;quot;)], buffer = 0.3))
  saveRDS(Sparrows_df, file.path(&amp;quot;Data&amp;quot;, &amp;quot;SparrowDataClimate.rds&amp;quot;))
} else {
  Sparrows_df &amp;lt;- readRDS(file.path(&amp;quot;Data&amp;quot;, &amp;quot;SparrowDataClimate.rds&amp;quot;))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have now effectively added two more variables to the data set:&lt;/p&gt;
&lt;ol start=&#34;19&#34;&gt;
&lt;li&gt;&lt;code&gt;TAvg&lt;/code&gt; [&lt;em&gt;Numeric&lt;/em&gt;] - Average air temperature for a 30-year time-period&lt;/li&gt;
&lt;li&gt;&lt;code&gt;TSD&lt;/code&gt; [&lt;em&gt;Numeric&lt;/em&gt;] - Standard deviation of mean monthly air temperature for a 30-year time-period&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now we have the data set we will look at for the rest of the exercises in this seminar series. But how did we get here? Find the answer 
&lt;a href=&#34;https://www.erikkusch.com/courses/excursions-into-biostatistics/data-handling-and-data-assumptions/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;hypotheses&#34;&gt;Hypotheses&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s consider the following two hypotheses for our exercises for this simulated research project:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Sparrow Morphology&lt;/strong&gt; is determined by:&lt;br&gt;
A. &lt;em&gt;Climate Conditions&lt;/em&gt; with sparrows in stable, warm environments fairing better than those in colder, less stable ones.&lt;br&gt;
B. &lt;em&gt;Competition&lt;/em&gt; with sparrows in small flocks doing better than those in big flocks.&lt;br&gt;
C. &lt;em&gt;Predation&lt;/em&gt; with sparrows under pressure of predation doing worse than those without.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sites&lt;/strong&gt;  accurately represent &lt;strong&gt;sparrow morphology&lt;/strong&gt;. This may mean:&lt;br&gt;
A. &lt;em&gt;Population status&lt;/em&gt; as inferred through morphology.&lt;br&gt;
B. &lt;em&gt;Site index&lt;/em&gt; as inferred through morphology.&lt;br&gt;
C. &lt;em&gt;Climate&lt;/em&gt; as inferred through morphology.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We try to answer these over the next few sessions.&lt;/p&gt;
&lt;h2 id=&#34;sessioninfo&#34;&gt;SessionInfo&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.2.3 (2023-03-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Big Sur ... 10.16
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] KrigR_0.1.2       terra_1.7-21      httr_1.4.5        stars_0.6-0       abind_1.4-5       fasterize_1.0.4   sf_1.0-12         lubridate_1.9.2   automap_1.1-9     doSNOW_1.0.20    
## [11] snow_0.4-4        doParallel_1.0.17 iterators_1.0.14  foreach_1.5.2     rgdal_1.6-5       raster_3.6-20     sp_1.6-0          stringr_1.5.0     keyring_1.3.1     ecmwfr_1.5.0     
## [21] ncdf4_1.21        leaflet_2.1.2    
## 
## loaded via a namespace (and not attached):
##  [1] xts_0.13.0         R.cache_0.16.0     tools_4.2.3        bslib_0.4.2        utf8_1.2.3         R6_2.5.1           KernSmooth_2.23-20 DBI_1.1.3          colorspace_2.1-0   tidyselect_1.2.0  
## [11] compiler_4.2.3     cli_3.6.0          gstat_2.1-0        bookdown_0.33      sass_0.4.5         scales_1.2.1       classInt_0.4-9     proxy_0.4-27       digest_0.6.31      rmarkdown_2.20    
## [21] R.utils_2.12.2     pkgconfig_2.0.3    htmltools_0.5.4    styler_1.9.1       fastmap_1.1.1      htmlwidgets_1.6.1  rlang_1.0.6        rstudioapi_0.14    FNN_1.1.3.2        jquerylib_0.1.4   
## [31] generics_0.1.3     zoo_1.8-11         jsonlite_1.8.4     crosstalk_1.2.0    dplyr_1.1.0        R.oo_1.25.0        magrittr_2.0.3     Rcpp_1.0.10        munsell_0.5.0      fansi_1.0.4       
## [41] lifecycle_1.0.3    R.methodsS3_1.8.2  stringi_1.7.12     yaml_2.3.7         plyr_1.8.8         grid_4.2.3         lattice_0.20-45    knitr_1.42         pillar_1.8.1       spacetime_1.2-8   
## [51] codetools_0.2-19   glue_1.6.2         evaluate_0.20      blogdown_1.16      vctrs_0.5.2        gtable_0.3.1       purrr_1.0.1        reshape_0.8.9      assertthat_0.2.1   cachem_1.0.7      
## [61] ggplot2_3.4.1      xfun_0.37          lwgeom_0.2-11      e1071_1.7-13       class_7.3-21       tibble_3.2.0       intervals_0.15.3   memoise_2.0.1      units_0.8-1        timechange_0.2.0  
## [71] ellipsis_0.3.2
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Research Project</title>
      <link>https://www.erikkusch.com/courses/excursions-into-biostatistics/research-project/</link>
      <pubDate>Sat, 27 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/excursions-into-biostatistics/research-project/</guid>
      <description>&lt;h2 id=&#34;our-resarch-project&#34;&gt;Our Resarch Project&lt;/h2&gt;
&lt;p&gt;Here (and over the next few exercises in this &amp;ldquo;course&amp;rdquo;), we are looking at a big (and entirely fictional) data base of the common house sparrow (&lt;em&gt;Passer domesticus&lt;/em&gt;). In particular, we are interested in the &lt;strong&gt;Evolution of &lt;em&gt;Passer domesticus&lt;/em&gt; in Response to Climate Change&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;the-data&#34;&gt;The Data&lt;/h3&gt;
&lt;p&gt;I have created a large data set for this exercise which is available in a cleaned and properly handled version &lt;a href=&#34;https://github.com/ErikKusch/Homepage/raw/master/content/courses/excursions-into-biostatistics/Data.rar&#34; target=&#34;_blank&#34;&gt; here&lt;/a&gt;.&lt;/p&gt;
&lt;h4 id=&#34;reading-the-data-into-r&#34;&gt;Reading the Data into &lt;code&gt;R&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s start by reading the data into &lt;code&gt;R&lt;/code&gt; and taking an initial look at it:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Sparrows_df &amp;lt;- readRDS(file.path(&amp;quot;Data&amp;quot;, &amp;quot;SparrowData.rds&amp;quot;))
Sparrows_df &amp;lt;- Sparrows_df[!is.na(Sparrows_df$Weight), ]
head(Sparrows_df)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Index Latitude Longitude     Climate Population.Status Weight Height Wing.Chord Colour    Sex Nesting.Site Nesting.Height Number.of.Eggs Egg.Weight Flock Home.Range Predator.Presence Predator.Type
## 1    SI       60       100 Continental            Native  34.05  12.87       6.67  Brown   Male         &amp;lt;NA&amp;gt;             NA             NA         NA     B      Large               Yes         Avian
## 2    SI       60       100 Continental            Native  34.86  13.68       6.79   Grey   Male         &amp;lt;NA&amp;gt;             NA             NA         NA     B      Large               Yes         Avian
## 3    SI       60       100 Continental            Native  32.34  12.66       6.64  Black Female        Shrub          35.60              1       3.21     C      Large               Yes         Avian
## 4    SI       60       100 Continental            Native  34.78  15.09       7.00  Brown Female        Shrub          47.75              0         NA     E      Large               Yes         Avian
## 5    SI       60       100 Continental            Native  35.01  13.82       6.81   Grey   Male         &amp;lt;NA&amp;gt;             NA             NA         NA     B      Large               Yes         Avian
## 6    SI       60       100 Continental            Native  32.36  12.67       6.64  Brown Female        Shrub          32.47              1       3.17     E      Large               Yes         Avian
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;variables&#34;&gt;Variables&lt;/h4&gt;
&lt;p&gt;When building models or trying to explain anything about our data set, we need to consider all the different variables and the information contained therein. In this data set, we have access to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;Index&lt;/code&gt; [&lt;em&gt;Factor&lt;/em&gt;] - an abbreviation of &lt;code&gt;Site&lt;/code&gt; records&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Latitude&lt;/code&gt; [&lt;em&gt;Numeric&lt;/em&gt;] - an identifier of where specific sparrow measurements where taken&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Longitude&lt;/code&gt; [&lt;em&gt;Numeric&lt;/em&gt;] - an identifier of where specific sparrow measurements where taken&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Climate&lt;/code&gt; [&lt;em&gt;Factor&lt;/em&gt;] - local climate types that sparrows are subjected to (e.g. coastal, continental, and semi-coastal)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Population.Status&lt;/code&gt; [&lt;em&gt;Factor&lt;/em&gt;] - population status (e.g. native or introduced)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Weight&lt;/code&gt; [&lt;em&gt;Numeric&lt;/em&gt;] - sparrow weight [g]; Range: 13-40g&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Height&lt;/code&gt; [&lt;em&gt;Numeric&lt;/em&gt;] - sparrow height/length [cm]; Range: 10-22cm&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Wing.Chord&lt;/code&gt; [&lt;em&gt;Numeric&lt;/em&gt;] - wing length [cm]; Range: 6-10cm&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Colour&lt;/code&gt; [&lt;em&gt;Factor&lt;/em&gt;] - main plumage colour (e.g. brown, grey, and black)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Sex&lt;/code&gt; [&lt;em&gt;Factor&lt;/em&gt;] - sparrow sex&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Nesting.Site&lt;/code&gt; [&lt;em&gt;Factor&lt;/em&gt;] - nesting conditions, only recorded for females (e.g. tree or shrub)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Nesting.Height&lt;/code&gt; [&lt;em&gt;Numeric&lt;/em&gt;] - nest elevation above ground level, only recorded for females&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Number.of.Eggs&lt;/code&gt; [&lt;em&gt;Numeric&lt;/em&gt;] - number of eggs per nest, only recorded for females&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Egg.Weight&lt;/code&gt; [&lt;em&gt;Numeric&lt;/em&gt;] - mean weight of eggs per nest, only recorded for females&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Flock&lt;/code&gt; [&lt;em&gt;Factor&lt;/em&gt;] - which flock at each location each sparrow belongs to&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Home.Range&lt;/code&gt; [&lt;em&gt;Factor&lt;/em&gt;] - size of home range of each flock (e.g. Small, Medium, and Large)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Predator.Presence&lt;/code&gt; [&lt;em&gt;Factor&lt;/em&gt;] - if a predator is present at a station (e.g. No or Yes)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Predator.Type&lt;/code&gt; [&lt;em&gt;Factor&lt;/em&gt;] - what kind of predator is present (e.g. Avian, Non-Avian, or None)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Note that the variables &lt;code&gt;Longitude&lt;/code&gt; and &lt;code&gt;Latitude&lt;/code&gt; may be used to retrieve climate data variables from a host of data sources.&lt;/p&gt;
&lt;h4 id=&#34;locations&#34;&gt;Locations&lt;/h4&gt;
&lt;p&gt;Looking at our data, we notice that it comes at distinct stations. Let&amp;rsquo;s visualise where they are:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(&amp;quot;leaflet&amp;quot;)
Plot_df &amp;lt;- Sparrows_df[, c(&amp;quot;Longitude&amp;quot;, &amp;quot;Latitude&amp;quot;, &amp;quot;Index&amp;quot;, &amp;quot;Climate&amp;quot;, &amp;quot;Population.Status&amp;quot;)]
Plot_df &amp;lt;- unique(Plot_df)
m &amp;lt;- leaflet()
m &amp;lt;- addTiles(m)
m &amp;lt;- addMarkers(m,
  lng = Plot_df$Longitude,
  lat = Plot_df$Latitude,
  label = Plot_df$Index,
  popup = paste(Plot_df$Population.Status, Plot_df$Climate, sep = &amp;quot;;&amp;quot;)
)
m
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-{=html}&#34;&gt;&amp;lt;div class=&amp;quot;leaflet html-widget html-fill-item-overflow-hidden html-fill-item&amp;quot; id=&amp;quot;htmlwidget-198293ac646900c5a74e&amp;quot; style=&amp;quot;width:1440px;height:768px;&amp;quot;&amp;gt;&amp;lt;/div&amp;gt;
&amp;lt;script type=&amp;quot;application/json&amp;quot; data-for=&amp;quot;htmlwidget-198293ac646900c5a74e&amp;quot;&amp;gt;{&amp;quot;x&amp;quot;:{&amp;quot;options&amp;quot;:{&amp;quot;crs&amp;quot;:{&amp;quot;crsClass&amp;quot;:&amp;quot;L.CRS.EPSG3857&amp;quot;,&amp;quot;code&amp;quot;:null,&amp;quot;proj4def&amp;quot;:null,&amp;quot;projectedBounds&amp;quot;:null,&amp;quot;options&amp;quot;:{}}},&amp;quot;calls&amp;quot;:[{&amp;quot;method&amp;quot;:&amp;quot;addTiles&amp;quot;,&amp;quot;args&amp;quot;:[&amp;quot;https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png&amp;quot;,null,null,{&amp;quot;minZoom&amp;quot;:0,&amp;quot;maxZoom&amp;quot;:18,&amp;quot;tileSize&amp;quot;:256,&amp;quot;subdomains&amp;quot;:&amp;quot;abc&amp;quot;,&amp;quot;errorTileUrl&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;tms&amp;quot;:false,&amp;quot;noWrap&amp;quot;:false,&amp;quot;zoomOffset&amp;quot;:0,&amp;quot;zoomReverse&amp;quot;:false,&amp;quot;opacity&amp;quot;:1,&amp;quot;zIndex&amp;quot;:1,&amp;quot;detectRetina&amp;quot;:false,&amp;quot;attribution&amp;quot;:&amp;quot;&amp;amp;copy; &amp;lt;a href=\&amp;quot;https://openstreetmap.org\&amp;quot;&amp;gt;OpenStreetMap&amp;lt;\/a&amp;gt; contributors, &amp;lt;a href=\&amp;quot;https://creativecommons.org/licenses/by-sa/2.0/\&amp;quot;&amp;gt;CC-BY-SA&amp;lt;\/a&amp;gt;&amp;quot;}]},{&amp;quot;method&amp;quot;:&amp;quot;addMarkers&amp;quot;,&amp;quot;args&amp;quot;:[[60,54,-25,-21.1,70,55,31,17.25,4,10.5,-51.75],[100,-2,135,55.6,-90,-97,-92,-88.75,-53,-67,-59.17],null,null,null,{&amp;quot;interactive&amp;quot;:true,&amp;quot;draggable&amp;quot;:false,&amp;quot;keyboard&amp;quot;:true,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;zIndexOffset&amp;quot;:0,&amp;quot;opacity&amp;quot;:1,&amp;quot;riseOnHover&amp;quot;:false,&amp;quot;riseOffset&amp;quot;:250},[&amp;quot;Native;Continental&amp;quot;,&amp;quot;Native;Coastal&amp;quot;,&amp;quot;Introduced;Continental&amp;quot;,&amp;quot;Introduced;Coastal&amp;quot;,&amp;quot;Introduced;Coastal&amp;quot;,&amp;quot;Introduced;Semi-Coastal&amp;quot;,&amp;quot;Introduced;Coastal&amp;quot;,&amp;quot;Introduced;Coastal&amp;quot;,&amp;quot;Introduced;Coastal&amp;quot;,&amp;quot;Introduced;Coastal&amp;quot;,&amp;quot;Introduced;Coastal&amp;quot;],null,null,null,[&amp;quot;SI&amp;quot;,&amp;quot;UK&amp;quot;,&amp;quot;AU&amp;quot;,&amp;quot;RE&amp;quot;,&amp;quot;NU&amp;quot;,&amp;quot;MA&amp;quot;,&amp;quot;LO&amp;quot;,&amp;quot;BE&amp;quot;,&amp;quot;FG&amp;quot;,&amp;quot;SA&amp;quot;,&amp;quot;FI&amp;quot;],{&amp;quot;interactive&amp;quot;:false,&amp;quot;permanent&amp;quot;:false,&amp;quot;direction&amp;quot;:&amp;quot;auto&amp;quot;,&amp;quot;opacity&amp;quot;:1,&amp;quot;offset&amp;quot;:[0,0],&amp;quot;textsize&amp;quot;:&amp;quot;10px&amp;quot;,&amp;quot;textOnly&amp;quot;:false,&amp;quot;className&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;sticky&amp;quot;:true},null]}],&amp;quot;limits&amp;quot;:{&amp;quot;lat&amp;quot;:[-51.75,70],&amp;quot;lng&amp;quot;:[-97,135]}},&amp;quot;evals&amp;quot;:[],&amp;quot;jsHooks&amp;quot;:[]}&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that you can zoom and drag the above map as well as click the station markers for some additional information.&lt;/p&gt;
&lt;h3 id=&#34;adding-information&#34;&gt;Adding Information&lt;/h3&gt;
&lt;p&gt;How do we get the data for this? Well, I wrote an &lt;code&gt;R&lt;/code&gt;-Package that does exactly that.&lt;/p&gt;
&lt;p&gt;First, said package needs to be installed from my GitHub repository for it. Subsequently, we need to set API Key and User number obtained at the 
&lt;a href=&#34;https://cds.climate.copernicus.eu/api-how-to&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Climate Data Store&lt;/a&gt;. I have already baked these into my material, so I don&amp;rsquo;t set them here, but include lines of code that ask you for your credentials when copy &amp;amp; pasted over:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;if (&amp;quot;KrigR&amp;quot; %in% rownames(installed.packages()) == FALSE) { # KrigR check
  Sys.setenv(R_REMOTES_NO_ERRORS_FROM_WARNINGS = &amp;quot;true&amp;quot;)
  devtools::install_github(&amp;quot;https://github.com/ErikKusch/KrigR&amp;quot;)
}
library(KrigR)
#### CDS API (needed for ERA5-Land downloads)
if (!exists(&amp;quot;API_Key&amp;quot;) | !exists(&amp;quot;API_User&amp;quot;)) { # CS API check: if CDS API credentials have not been specified elsewhere
  API_User &amp;lt;- readline(prompt = &amp;quot;Please enter your Climate Data Store API user number and hit ENTER.&amp;quot;)
  API_Key &amp;lt;- readline(prompt = &amp;quot;Please enter your Climate Data Store API key number and hit ENTER.&amp;quot;)
} # end of CDS API check

#### NUMBER OF CORES
if (!exists(&amp;quot;numberOfCores&amp;quot;)) { # Core check: if number of cores for parallel processing has not been set yet
  numberOfCores &amp;lt;- readline(prompt = paste(&amp;quot;How many cores do you want to allocate to these processes? Your machine has&amp;quot;, parallel::detectCores()))
} # end of Core check
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have the package, we can download some state-of-the-art climate data. I have already prepared all of this in the data directory you downloaded earlier so this step will automatically be skipped:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;if (!file.exists(file.path(&amp;quot;Data&amp;quot;, &amp;quot;SparrowDataClimate.rds&amp;quot;))) {
  colnames(Plot_df)[1:3] &amp;lt;- c(&amp;quot;Lon&amp;quot;, &amp;quot;Lat&amp;quot;, &amp;quot;ID&amp;quot;) # set column names to be in line with what KrigR wants
  Points_Raw &amp;lt;- download_ERA(
    Variable = &amp;quot;2m_temperature&amp;quot;,
    DataSet = &amp;quot;era5&amp;quot;,
    DateStart = &amp;quot;1982-01-01&amp;quot;,
    DateStop = &amp;quot;2012-12-31&amp;quot;,
    TResolution = &amp;quot;month&amp;quot;,
    TStep = 1,
    Extent = Plot_df, # the point data with Lon and Lat columns
    Buffer = 0.5, # a 0.5 degree buffer should be drawn around each point
    ID = &amp;quot;ID&amp;quot;, # this is the column which holds point IDs
    API_User = API_User,
    API_Key = API_Key,
    Dir = file.path(getwd(), &amp;quot;Data&amp;quot;),
    FileName = &amp;quot;AT_Climatology.nc&amp;quot;
  )
  Points_mean &amp;lt;- calc(Points_Raw, fun = mean)
  Points_sd &amp;lt;- calc(Points_Raw, fun = sd)
  Sparrows_df$TAvg &amp;lt;- as.numeric(extract(x = Points_mean, y = Sparrows_df[, c(&amp;quot;Longitude&amp;quot;, &amp;quot;Latitude&amp;quot;)], buffer = 0.3))
  Sparrows_df$TSD &amp;lt;- as.numeric(extract(x = Points_sd, y = Sparrows_df[, c(&amp;quot;Longitude&amp;quot;, &amp;quot;Latitude&amp;quot;)], buffer = 0.3))
  saveRDS(Sparrows_df, file.path(&amp;quot;Data&amp;quot;, &amp;quot;SparrowDataClimate.rds&amp;quot;))
} else {
  Sparrows_df &amp;lt;- readRDS(file.path(&amp;quot;Data&amp;quot;, &amp;quot;SparrowDataClimate.rds&amp;quot;))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have now effectively added two more variables to the data set:&lt;/p&gt;
&lt;ol start=&#34;19&#34;&gt;
&lt;li&gt;&lt;code&gt;TAvg&lt;/code&gt; [&lt;em&gt;Numeric&lt;/em&gt;] - Average air temperature for a 30-year time-period&lt;/li&gt;
&lt;li&gt;&lt;code&gt;TSD&lt;/code&gt; [&lt;em&gt;Numeric&lt;/em&gt;] - Standard deviation of mean monthly air temperature for a 30-year time-period&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now we have the data set we will look at for the rest of the exercises in this seminar series. But how did we get here? Find the answer &lt;a href=&#34;https://www.erikkusch.com/courses/excursions-into-biostatistics/data-handling-and-data-assumptions/&#34; target=&#34;_blank&#34;&gt; here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;hypotheses&#34;&gt;Hypotheses&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s consider the following two hypotheses for our exercises for this simulated research project:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Sparrow Morphology&lt;/strong&gt; is determined by:&lt;br&gt;
A. &lt;em&gt;Climate Conditions&lt;/em&gt; with sparrows in stable, warm environments fairing better than those in colder, less stable ones.&lt;br&gt;
B. &lt;em&gt;Competition&lt;/em&gt; with sparrows in small flocks doing better than those in big flocks.&lt;br&gt;
C. &lt;em&gt;Predation&lt;/em&gt; with sparrows under pressure of predation doing worse than those without.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sites&lt;/strong&gt;  accurately represent &lt;strong&gt;sparrow morphology&lt;/strong&gt;. This may mean:&lt;br&gt;
A. &lt;em&gt;Population status&lt;/em&gt; as inferred through morphology.&lt;br&gt;
B. &lt;em&gt;Site index&lt;/em&gt; as inferred through morphology.&lt;br&gt;
C. &lt;em&gt;Climate&lt;/em&gt; as inferred through morphology.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We try to answer these over the next few sessions.&lt;/p&gt;
&lt;h2 id=&#34;sessioninfo&#34;&gt;SessionInfo&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.2.3 (2023-03-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Big Sur ... 10.16
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] KrigR_0.1.2       terra_1.7-21      httr_1.4.5        stars_0.6-0       abind_1.4-5       fasterize_1.0.4   sf_1.0-12         lubridate_1.9.2   automap_1.1-9     doSNOW_1.0.20    
## [11] snow_0.4-4        doParallel_1.0.17 iterators_1.0.14  foreach_1.5.2     rgdal_1.6-5       raster_3.6-20     sp_1.6-0          stringr_1.5.0     keyring_1.3.1     ecmwfr_1.5.0     
## [21] ncdf4_1.21        leaflet_2.1.2    
## 
## loaded via a namespace (and not attached):
##  [1] xts_0.13.0         R.cache_0.16.0     tools_4.2.3        bslib_0.4.2        utf8_1.2.3         R6_2.5.1           KernSmooth_2.23-20 DBI_1.1.3          colorspace_2.1-0   tidyselect_1.2.0  
## [11] compiler_4.2.3     cli_3.6.0          gstat_2.1-0        bookdown_0.33      sass_0.4.5         scales_1.2.1       classInt_0.4-9     proxy_0.4-27       digest_0.6.31      rmarkdown_2.20    
## [21] R.utils_2.12.2     pkgconfig_2.0.3    htmltools_0.5.4    styler_1.9.1       fastmap_1.1.1      htmlwidgets_1.6.1  rlang_1.0.6        rstudioapi_0.14    FNN_1.1.3.2        jquerylib_0.1.4   
## [31] generics_0.1.3     zoo_1.8-11         jsonlite_1.8.4     crosstalk_1.2.0    dplyr_1.1.0        R.oo_1.25.0        magrittr_2.0.3     Rcpp_1.0.10        munsell_0.5.0      fansi_1.0.4       
## [41] lifecycle_1.0.3    R.methodsS3_1.8.2  stringi_1.7.12     yaml_2.3.7         plyr_1.8.8         grid_4.2.3         lattice_0.20-45    knitr_1.42         pillar_1.8.1       spacetime_1.2-8   
## [51] codetools_0.2-19   glue_1.6.2         evaluate_0.20      blogdown_1.16      vctrs_0.5.2        gtable_0.3.1       purrr_1.0.1        reshape_0.8.9      assertthat_0.2.1   cachem_1.0.7      
## [61] ggplot2_3.4.1      xfun_0.37          lwgeom_0.2-11      e1071_1.7-13       class_7.3-21       tibble_3.2.0       intervals_0.15.3   memoise_2.0.1      units_0.8-1        timechange_0.2.0  
## [71] ellipsis_0.3.2
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 04</title>
      <link>https://www.erikkusch.com/courses/rethinking/chapter-04/</link>
      <pubDate>Fri, 08 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/rethinking/chapter-04/</guid>
      <description>&lt;h1 id=&#34;geocentric-models&#34;&gt;Geocentric Models&lt;/h1&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/rethinking/3__08-01-2021_SUMMARY_-Linear-Regressions.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides Chapter 4&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to the exercises at the end of chapter 4 in 
&lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Satistical Rethinking 2&lt;/a&gt; by Richard McElreath. I have created these notes as a part of my ongoing involvement in the 
&lt;a href=&#34;https://www.erikkusch.com/project/aubayes/&#34;&gt;AU Bayes Study Group&lt;/a&gt;. Much of my inspiration for these solutions, where necessary, has been obtained from 
&lt;a href=&#34;https://jmgirard.com/statistical-rethinking-ch4/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jeffrey Girard&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;easy-exercises&#34;&gt;Easy Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-e1&#34;&gt;Practice E1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; In the model definition below, which line is the likelihood?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$y_i \sim Normal(\mu, \sigma)$&lt;/li&gt;
&lt;li&gt;$\mu \sim Normal(0, 10)$&lt;/li&gt;
&lt;li&gt;$\sigma \sim Uniform(0, 10)$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; 1, $y_i \sim Normal(\mu, \sigma)$ - This is the likelihood specification (see also page 84 in the book). Everything else is a specification of priors.&lt;/p&gt;
&lt;h3 id=&#34;practice-e2&#34;&gt;Practice E2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; In the model definition just above, how many parameters are in the posterior distribution?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; $y_i$ is not to be estimated, but represents the data we have at hand and want to understand through parameters. Both $\mu$ and $\sigma$ are parameters which we attempt to estimate.&lt;/p&gt;
&lt;h3 id=&#34;practice-e3&#34;&gt;Practice E3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Using the model definition above, write down the appropriate form of Bayesâ theorem that includes the proper likelihood and priors.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Following the specification in the &amp;ldquo;Overthinking&amp;rdquo; box on page 87, we can write the Bayes&#39; Theorem for the model above as:&lt;/p&gt;
&lt;p&gt;$Pr(\mu, \sigma | y) = \frac{\prod_i Normal(y_i| \mu, \sigma) Normal(\mu| 0, 10) Uniform(\sigma | 0, 10)}{\int\int\prod_i Normal(y_i| \mu, \sigma) Normal(\mu| 0, 10) Uniform(\sigma | 0, 10) d\mu d\sigma}$&lt;/p&gt;
&lt;h3 id=&#34;practice-e4&#34;&gt;Practice E4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; In the model definition below, which line is the linear model?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$y_i \sim Normal(\mu, \sigma)$&lt;/li&gt;
&lt;li&gt;$\mu_i=\alpha+\beta x_i$&lt;/li&gt;
&lt;li&gt;$\alpha \sim Normal(0,10)$&lt;/li&gt;
&lt;li&gt;$\beta \sim Normal(0,1)$&lt;/li&gt;
&lt;li&gt;$\sigma \sim Uniform(0,10)$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; The linear model is &lt;em&gt;deterministic&lt;/em&gt; in nature (i.e. the parameters determine the value of the response variable). This is identified with the mathematical notation of $=$. Therefore, line 2. $\mu_i=\alpha+\beta x_i$ is the linear model. All other specifications are &lt;em&gt;stochastic&lt;/em&gt; links (identified with $\sim$).&lt;/p&gt;
&lt;h3 id=&#34;practice-e5&#34;&gt;Practice E5&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; In the model definition just above, how many parameters are in the posterior distribution?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Three. $\alpha$, $\beta$, and $\sigma$ are the parameters we estimate. $y_i \sim Normal(\mu, \sigma)$ is the likelihood.&lt;/p&gt;
&lt;h2 id=&#34;medium-exercises&#34;&gt;Medium Exercises&lt;/h2&gt;
&lt;p&gt;This is where we get into &lt;code&gt;R&lt;/code&gt; applications.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rm(list = ls())
library(rethinking)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-m1&#34;&gt;Practice M1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; For the model definition below, simulate observed heights from the prior (not the posterior).&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$y_i \sim Normal(\mu, \sigma)$&lt;/li&gt;
&lt;li&gt;$\mu \sim Normal(0, 10)$&lt;/li&gt;
&lt;li&gt;$\sigma \sim Uniform(0, 10)$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(42)
sample_mu &amp;lt;- rnorm(1e4, 0, 10)
sample_sigma &amp;lt;- runif(1e4, 0, 10)
prior_y &amp;lt;- rnorm(1e4, sample_mu, sample_sigma)
dens(prior_y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-04_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;practice-m2&#34;&gt;Practice M2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Translate the model just above into a &lt;code&gt;quap()&lt;/code&gt; formula.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;formula &amp;lt;- alist(
  y ~ dnorm(mu, sigma),
  mu ~ dnorm(0, 10),
  sigma ~ dunif(0, 10)
)
formula
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## y ~ dnorm(mu, sigma)
## 
## [[2]]
## mu ~ dnorm(0, 10)
## 
## [[3]]
## sigma ~ dunif(0, 10)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-m3&#34;&gt;Practice M3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Translate the &lt;code&gt;quap()&lt;/code&gt; model formula below into a mathematical model definition.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;flist &amp;lt;- alist(
  y ~ dnorm(mu, sigma),
  mu &amp;lt;- a + b * x,
  a ~ dnorm(0, 50),
  b ~ dunif(0, 10),
  sigma ~ dunif(0, 50)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$y_i \sim Normal(\mu, \sigma)$&lt;/li&gt;
&lt;li&gt;$\mu_i=\alpha+\beta x_i$&lt;/li&gt;
&lt;li&gt;$\alpha \sim Normal(0,50)$&lt;/li&gt;
&lt;li&gt;$\beta \sim Uniform(0,10)$&lt;/li&gt;
&lt;li&gt;$\sigma \sim Uniform(0,50)$&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;practice-m4&#34;&gt;Practice M4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; A sample of students is measured for height each year for 3 years. After the third year, you want to fit a linear regression predicting height using year as a predictor. Write down the mathematical model definition for this regression, using any variable names and priors you choose. Be prepared to defend you choice of priors.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Firstly, I start by identifying the likelihood of heights $h$. I assume these to be normally distributed around some mean $\mu$, with a standard deviation of $\sigma$:&lt;/p&gt;
&lt;p&gt;$h_i = Normal(\mu, \sigma)$&lt;/p&gt;
&lt;p&gt;$\mu$ is the mean of heights and can be obtained as follows:&lt;/p&gt;
&lt;p&gt;$\mu_i = \alpha + \beta x_i$&lt;/p&gt;
&lt;p&gt;Setting aside the issue of independence here - each student shows up in the data multiple times, thus making the time-series of heights dependent and potentially autocorrelated - no age range has been specified for our students in question. This leaves us with little information regarding potential priors (elementary school students are much smaller than university students). Therefore, I chose a weak prior with a large range. I call this prior for the height intercept $\alpha$ and assume a normal distribution with a wide range:&lt;/p&gt;
&lt;p&gt;$\alpha \sim Normal(150,25)$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(42)
dens(rnorm(1e4, 150, 25))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-04_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;$\beta$ represents the average increase of in height for each year. Again, the potentially large age range means we have to use a somewhat uninformative prior because people grow very differently at different ages. The distribution here could be argued to be uniform or normal. I am going with normal because it emphasises a much more peaked distribution of growth rates with an emphasis for the mean:&lt;/p&gt;
&lt;p&gt;$\beta \sim Normal(4,1)$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(42)
dens(rnorm(1e4, 4, 2))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-04_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As you can see, this choice of prior for $\alpha$ would allow for negative growth rates which might be an issue.&lt;/p&gt;
&lt;p&gt;Finally, we need to identify our $\sigma$ where I specify a uniform range that covers the full range of heights when given a large range of students (in age, that is):&lt;/p&gt;
&lt;p&gt;$\sigma \sim Uniform(0,30)$&lt;/p&gt;
&lt;p&gt;Plugging this one into our height simulation, we get a wide but overall sensible range:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(42)
dens(rnorm(1e4, 150, 30))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-04_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Personally, I think a second-order polynomial regression could be sensible here to account for the change in growth rate over time, but I assume a latter question will deal with that.&lt;/p&gt;
&lt;h3 id=&#34;practice-m5&#34;&gt;Practice M5&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Now suppose I remind you that every student got taller each year. Does this information lead you to change your choice of priors? How?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Yes, I can change my prior for $\beta$ to always be positive by using a log-normal distribution:&lt;/p&gt;
&lt;p&gt;$\beta \sim LogNormal(2,0.5)$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(42)
dens(rlnorm(1e4, 2, .5))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-04_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;1440&#34; /&gt;
Now this makes much more sense!&lt;/p&gt;
&lt;!-- ## Practice M6 --&gt;
&lt;!-- **Question:** Now suppose I tell you that the average height in the first year was 120 cm and that every student got taller each year. Does this information lead you to change your choice of priors? How?   --&gt;
&lt;!-- **Answer:** This really changes things. We now know that we are talking about school-age students (around 6-8 years of age). This means we can alter our priors quite significantly as follows:   --&gt;
&lt;!-- $\alpha$ - We know the average height to be 120cm, and can set a relatively small standard deviation on that distribution:   --&gt;
&lt;!-- $\alpha \sim Normal(150,25)$   --&gt;
&lt;!-- $\beta$ - We know that children this age grow much faster than older people and so can increase our prior for this with more certainty:   --&gt;
&lt;!-- $\beta \sim Normal(7,1)$ --&gt;
&lt;!-- $\sigma$ - this is the standard deviation of the overall height likelihood function. We can make this more conservative, knowing that we have a tighter age range: --&gt;
&lt;!-- $\sigma \sim Uniform(0,20)$ --&gt;
&lt;h3 id=&#34;practice-m6&#34;&gt;Practice M6&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Now suppose I tell you that the variance among heights for students of the same age is never more than 64 cm. How does this lead you to revise your priors?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; The variance is the square product of $\sigma$, so we know that $\sigma$ never exceeds $\sqrt(64) = 8$ and can update our prior accordingly:&lt;/p&gt;
&lt;p&gt;$\sigma \sim Uniform(0,8)$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(42)
dens(runif(1e4, 0, 8))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-04_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;hard-exercises&#34;&gt;Hard Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-h1&#34;&gt;Practice H1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; The weights listed below were recorded in the !Kung census, but heights were not recorded for these individuals. Provide predicted heights and 89% intervals (either HPDI or PI) for each of these individuals. That is, fill in the table below, using model-based predictions.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Individual&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;weight&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;expected height&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;89% interval&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;46.95&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;43.72&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;64.78&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;32.59&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;54.63&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(&amp;quot;Howell1&amp;quot;)
d &amp;lt;- Howell1
formula &amp;lt;- alist(
  height ~ dnorm(mu, sigma),
  mu &amp;lt;- a + b * weight,
  a ~ dnorm(150, 30), # a relatively short height with a large SD to account for large age spread in data set and the overall smaller stature of the peoples in the !Kung census
  b ~ dlnorm(0, 1), # I want strictly positive weight-height relationships
  sigma ~ dunif(0, 50) # rather large SD
)
(m &amp;lt;- quap(formula, data = d))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Quadratic approximate posterior distribution
## 
## Formula:
## height ~ dnorm(mu, sigma)
## mu &amp;lt;- a + b * weight
## a ~ dnorm(150, 30)
## b ~ dlnorm(0, 1)
## sigma ~ dunif(0, 50)
## 
## Posterior means:
##         a         b     sigma 
## 75.550586  1.761449  9.345982 
## 
## Log-likelihood: -1987.71
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we can obtain the posterior distributions for our weight values in the table:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;new_weight &amp;lt;- c(46.95, 43.72, 64.78, 32.59, 54.63)
pred_height &amp;lt;- link(m, data = data.frame(weight = new_weight))
expected &amp;lt;- apply(pred_height, 2, mean)
interval &amp;lt;- apply(pred_height, 2, HPDI, prob = 0.89)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we merge this into a data frame:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data.frame(
  individual = 1:5,
  weight = new_weight,
  expected = expected,
  lower = interval[1, ],
  upper = interval[2, ]
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   individual weight expected    lower    upper
## 1          1  46.95 158.2948 157.4868 159.1242
## 2          2  43.72 152.6012 151.9167 153.3826
## 3          3  64.78 189.7242 188.3729 191.2549
## 4          4  32.59 132.9821 132.3565 133.6889
## 5          5  54.63 171.8326 170.9136 173.0242
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-h2&#34;&gt;Practice H2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Select out all the rows in the Howell1 data with ages below 18 years of age. If you do it right, you should end up with a new data frame with 192 rows in it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;d2 &amp;lt;- Howell1[Howell1$age &amp;lt; 18, ]
weight_bar &amp;lt;- mean(d2$weight)
nrow(d2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 192
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-a&#34;&gt;Part A&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Fit a linear regression to these data, using &lt;code&gt;quap()&lt;/code&gt;. Present and interpret the estimates. For every 10 units of increase in weight, how much taller does the model predict a child gets?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;formula &amp;lt;- alist(
  height ~ dnorm(mu, sigma),
  mu &amp;lt;- a + b * (weight - weight_bar),
  a ~ dnorm(110, 30),
  b ~ dlnorm(0, 1),
  sigma ~ dunif(0, 60)
)
m &amp;lt;- quap(formula, data = d2)
precis(m)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean        sd       5.5%      94.5%
## a     108.319563 0.6087746 107.346624 109.292503
## b       2.716656 0.0683154   2.607475   2.825838
## sigma   8.437165 0.4305635   7.749042   9.125289
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For a 10-unit increase in weight, we see a 27.17cm increase in height.&lt;/p&gt;
&lt;h4 id=&#34;part-b&#34;&gt;Part B&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Plot the raw data, with height on the vertical axis and weight on the horizontal axis. Superimpose the MAP regression line and 89% HPDI for the mean. Also superimpose the 89% HPDI for predicted heights.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Data for plot
weight.seq &amp;lt;- seq(from = min(d2$weight), to = max(d2$weight), by = 1) # sequence to do predictions for
mu &amp;lt;- link(m, data = data.frame(weight = weight.seq)) # do predictions
mu.mean &amp;lt;- apply(mu, 2, mean) # calculate mean
mu.HPDI &amp;lt;- apply(mu, 2, HPDI, prob = 0.89) # identify interval
sim.height &amp;lt;- sim(m, data = list(weight = weight.seq)) # simulate full predictions
height.HPDI &amp;lt;- apply(sim.height, 2, HPDI, prob = 0.89) # identify interval

# Plotting
plot(height ~ weight, data = d2, col = col.alpha(rangi2, 0.7)) # base plot
lines(weight.seq, mu.mean) # add mean line
shade(mu.HPDI, weight.seq) # add hdpi interval
shade(height.HPDI, weight.seq) # add full-hdpi interval
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-04_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h4 id=&#34;part-c&#34;&gt;Part C&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; What aspects of the model fit concern you? Describe the kinds of assumptions you would change, if any, to improve the model. You donât have to write any new code. Just explain what the model appears to be doing a bad job of, and what you hypothesize would be a better model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; The model woefully overpredicts height at the low-weight end of the spectrum as well as the upper end of the weight spectrum. At the mid-range of the weight spectrum, our model underpredicts height. It looks as though the data fall onto a curve and so we could potentially do better with a polynomial model.&lt;/p&gt;
&lt;h3 id=&#34;practice-h3&#34;&gt;Practice H3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Suppose a colleague of yours, who works on allometry, glances at the practice problems just above. Your colleague exclaims, âThatâs silly. Everyone knows that itâs only the logarithm of body weight that scales with height!â Letâs take your colleagueâs advice and see what happens.&lt;/p&gt;
&lt;h4 id=&#34;part-a-1&#34;&gt;Part A&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Model the relationship between height (cm) and the natural logarithm of weight (log-kg). Use the entire Howell1 data frame, all 544 rows, adults and non-adults. Fit this model, using the quadratic approximation:&lt;/p&gt;
&lt;p&gt;$h_i â¼ Normal(\mu_i, \sigma)$&lt;br&gt;
$\mu_i = \alpha + \beta log(w_i)$&lt;br&gt;
$\alpha â¼ Normal(178, 20)$&lt;br&gt;
$\beta â¼ LogNormal(0, 1)$&lt;br&gt;
$\sigma â¼ Uniform(0, 50)$&lt;/p&gt;
&lt;p&gt;where $h_i$ is the height of individual $i$ and $w_i$ is the weight (in kg) of individual $i$. The function for computing a natural log in &lt;code&gt;R&lt;/code&gt; is just &lt;code&gt;log()&lt;/code&gt;. Can you interpret the resulting estimates?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;d &amp;lt;- Howell1
formula &amp;lt;- alist(
  height ~ dnorm(mu, sigma),
  mu &amp;lt;- a + b * log(weight),
  a ~ dnorm(178, 20),
  b ~ dlnorm(0, 1),
  sigma ~ dunif(0, 50)
)
m &amp;lt;- quap(formula, data = d)
precis(m)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean        sd       5.5%      94.5%
## a     -22.874329 1.3343120 -25.006817 -20.741840
## b      46.817801 0.3823300  46.206764  47.428838
## sigma   5.137168 0.1558908   4.888025   5.386312
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our $\alpha$ estimate seems to be out-of-line at -22.87. This is simply the predicted height when the weight is 0 log-kg and thus somewhat uninformative. $\beta$ tells us that our individual grow, on average, by 46.82cm per increase in log-kg by 1. The standard deviation around our height predictions is 5.14.&lt;/p&gt;
&lt;p&gt;Transforming a variable makes interpreting diffuclt.&lt;/p&gt;
&lt;h4 id=&#34;part-b-1&#34;&gt;Part B&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Begin with this plot: &lt;code&gt;plot(height ~ weight, data = Howell1), col = col.alpha(rangi2, 0.4))&lt;/code&gt;. Then use samples from the quadratic approximate posterior of the model in (a) to superimpose on the plot: (1) the predicted mean height as a function of weight, (2) the 97% HPDI for the mean, and (3) the 97% HPDI for predicted heights.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(height ~ weight, data = d, col = col.alpha(rangi2, 0.4))
# Estimate and plot the quap regression line and 97% HPDI for the mean
weight.seq &amp;lt;- seq(from = min(d$weight), to = max(d$weight), by = 1)
mu &amp;lt;- link(m, data = data.frame(weight = weight.seq))
mu.mean &amp;lt;- apply(mu, 2, mean)
mu.HPDI &amp;lt;- apply(mu, 2, HPDI, prob = 0.97)
lines(weight.seq, mu.mean)
shade(mu.HPDI, weight.seq)
# Estimate and plot the 97% HPDI for the predicted heights
sim.height &amp;lt;- sim(m, data = list(weight = weight.seq))
height.HPDI &amp;lt;- apply(sim.height, 2, HPDI, prob = 0.97)
shade(height.HPDI, weight.seq)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-04_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;1440&#34; /&gt;
Yup, this does fit much more neatly.&lt;/p&gt;
&lt;h3 id=&#34;practice-h4&#34;&gt;Practice H4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Plot the prior predictive distribution for the polynomial regression model in the chapter. You can modify the code that plots the linear regression prior predictive distribution. Can you modify the prior distributions of $\alpha$, $\beta_1$, and $\beta_2$ so that the prior predictions stay within the biologically reasonable outcome space? That is to say: Do not try to fit the data by hand. But do try to keep the curves consistent with what you know about height and weight, before seeing these exact data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(&amp;quot;Howell1&amp;quot;)
d &amp;lt;- Howell1
# standardising weight
d$weight_s &amp;lt;- with(d, (weight - mean(weight)) / sd(weight))
# quadratic weight
d$weight_s2 &amp;lt;- d$weight_s^2

# MODEL
M_Poly &amp;lt;- quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu &amp;lt;- a + b1 * weight_s + b2 * weight_s2,
    a ~ dnorm(178, 20),
    b1 ~ dlnorm(0, 1),
    b2 ~ dnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  data = d
)
precis(M_Poly)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean        sd       5.5%      94.5%
## a     146.054799 0.3689900 145.465082 146.644517
## b1     21.734548 0.2888949  21.272839  22.196258
## b2     -7.800570 0.2742037  -8.238800  -7.362339
## sigma   5.774487 0.1764685   5.492456   6.056517
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The prior prediction can be obtained using &lt;code&gt;extract.prior()&lt;/code&gt;. The obtained sample can then be passed on to the &lt;code&gt;link()&lt;/code&gt; function for the weight space in question. Since we want to try multiple priors, I use a function that takes the &lt;code&gt;alist&lt;/code&gt; object as well as the number of predicted curves as arguments. This function-idea has been adapted from 
&lt;a href=&#34;https://gregor-mathes.netlify.app/2021/01/01/rethinking-chapter-4/#question-4m5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gregor Mathes&lt;/a&gt; solution which is based on the use of &lt;code&gt;tidyverse&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidyr) # we don&#39;t get around using the function pivot_longer()
library(ggplot2)
modify_prior_poly &amp;lt;- function(my_alist, N) {
  # set seed for reproducibility
  set.seed(42)
  # fit model
  m_poly &amp;lt;- quap(my_alist, data = d)
  # make weight sequence with both standardised weight and the square of it
  weight_seq &amp;lt;- seq(from = min(d$weight), to = max(d$weight), by = 1)
  weight_seq &amp;lt;- data.frame(
    weight = weight_seq,
    weight_s = (weight_seq - mean(weight_seq)) / sd(weight_seq),
    weight_s2 = ((weight_seq - mean(weight_seq)) / sd(weight_seq))^2
  )
  # extract samples from the prior
  m_poly_prior &amp;lt;- extract.prior(m_poly, n = N)
  # now apply the polynomial equation to the priors to get predicted heights
  m_poly_mu &amp;lt;- link(
    m_poly,
    post = m_poly_prior,
    data = list(
      weight_s = weight_seq$weight_s,
      weight_s2 = weight_seq$weight_s2
    )
  )
  m_poly_mu &amp;lt;- as.data.frame(m_poly_mu)
  m_poly_mu &amp;lt;- as.data.frame(pivot_longer(m_poly_mu, cols = everything(), values_to = &amp;quot;height&amp;quot;))
  m_poly_mu$weight &amp;lt;- rep(weight_seq$weight, N)
  m_poly_mu$type &amp;lt;- rep(as.character(1:N), each = length(weight_seq$weight))
  # plot it
  ggplot(m_poly_mu) +
    geom_line(aes(x = weight, y = height, group = type), alpha = 0.5) +
    geom_hline(yintercept = c(0, 272), colour = &amp;quot;steelblue4&amp;quot;) +
    annotate(
      geom = &amp;quot;text&amp;quot;,
      x = c(6, 12),
      y = c(11, 285),
      label = c(&amp;quot;Embryo&amp;quot;, &amp;quot;World&#39;s tallest person&amp;quot;),
      colour = c(rep(&amp;quot;steelblue4&amp;quot;, 2))
    ) +
    labs(x = &amp;quot;Weight in kg&amp;quot;, y = &amp;quot;Height in cm&amp;quot;) +
    theme_minimal()
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s run this for our initial model specification:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;modify_prior_poly(
  my_alist = alist(
    height ~ dnorm(mu, sigma),
    mu &amp;lt;- a + b1 * weight_s + b2 * weight_s2,
    a ~ dnorm(178, 20),
    b1 ~ dlnorm(0, 1),
    b2 ~ dnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  N = 40
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-04_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The priors should cover the whole biologically sensible space (unless we have some really strong indication for this not being the case). Let&amp;rsquo;s start by decreasing the mean for $\alpha$ and increasing its standard deviation:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;modify_prior_poly(
  my_alist = alist(
    height ~ dnorm(mu, sigma),
    mu &amp;lt;- a + b1 * weight_s + b2 * weight_s2,
    a ~ dnorm(130, 35), # decrease mean and increase sd
    b1 ~ dlnorm(0, 1),
    b2 ~ dnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  N = 40
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-04_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Better, but not quite there yet. The lines themselves could do with stronger positive relationship here between weight and height. We know this relationship to be stronger:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;modify_prior_poly(
  my_alist = alist(
    height ~ dnorm(mu, sigma),
    mu &amp;lt;- a + b1 * weight_s + b2 * weight_s2,
    a ~ dnorm(130, 35),
    b1 ~ dlnorm(1, 1), # increase mean, but not sd (we don&#39;t want negative relationships)
    b2 ~ dnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  N = 40
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-04_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I am already happy with this. However, we can see some downward curving weight-height relationships here. That&amp;rsquo;s probably not what we find in the real-world and so we might want to force these relationships to always curve upwards, by having a positive $\beta_2$ with a narrow log-normal distribution:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;modify_prior_poly(
  my_alist = alist(
    height ~ dnorm(mu, sigma),
    mu &amp;lt;- a + b1 * weight_s + b2 * weight_s2,
    a ~ dnorm(130, 35),
    b1 ~ dlnorm(1, 1),
    b2 ~ dlnorm(0, .05), # force positive parameter
    sigma ~ dunif(0, 50)
  ),
  N = 40
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-08-statistical-rethinking-chapter-04_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Phew. I can&amp;rsquo;t think of more, to be honest. This looks good to me.&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.5 (2021-03-31)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19043)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.1252  LC_CTYPE=English_United Kingdom.1252    LC_MONETARY=English_United Kingdom.1252 LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.1252    
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] tidyr_1.1.3          rethinking_2.13      rstan_2.21.2         ggplot2_3.3.3        StanHeaders_2.21.0-7
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.7         mvtnorm_1.1-1      lattice_0.20-41    prettyunits_1.1.1  ps_1.6.0           assertthat_0.2.1   digest_0.6.27      utf8_1.2.1         V8_3.4.1           R6_2.5.0          
## [11] backports_1.2.1    stats4_4.0.5       evaluate_0.14      coda_0.19-4        highr_0.9          blogdown_1.3       pillar_1.6.0       rlang_0.4.11       curl_4.3.2         callr_3.7.0       
## [21] jquerylib_0.1.4    R.utils_2.10.1     R.oo_1.24.0        rmarkdown_2.7      styler_1.4.1       labeling_0.4.2     stringr_1.4.0      loo_2.4.1          munsell_0.5.0      compiler_4.0.5    
## [31] xfun_0.22          pkgconfig_2.0.3    pkgbuild_1.2.0     shape_1.4.5        htmltools_0.5.1.1  tidyselect_1.1.0   tibble_3.1.1       gridExtra_2.3      bookdown_0.22      codetools_0.2-18  
## [41] matrixStats_0.61.0 fansi_0.4.2        crayon_1.4.1       dplyr_1.0.5        withr_2.4.2        MASS_7.3-53.1      R.methodsS3_1.8.1  grid_4.0.5         jsonlite_1.7.2     gtable_0.3.0      
## [51] lifecycle_1.0.0    DBI_1.1.1          magrittr_2.0.1     scales_1.1.1       RcppParallel_5.1.2 cli_3.0.0          stringi_1.5.3      farver_2.1.0       bslib_0.2.4        ellipsis_0.3.2    
## [61] generics_0.1.0     vctrs_0.3.7        rematch2_2.1.2     tools_4.0.5        R.cache_0.14.0     glue_1.4.2         purrr_0.3.4        processx_3.5.1     yaml_2.2.1         inline_0.3.17     
## [71] colorspace_2.0-0   knitr_1.33         sass_0.3.1
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Descriptive Statistics</title>
      <link>https://www.erikkusch.com/courses/an-introduction-to-biostatistics/descriptive-statistics/</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/an-introduction-to-biostatistics/descriptive-statistics/</guid>
      <description>&lt;h1 id=&#34;theory&#34;&gt;Theory&lt;/h1&gt;
&lt;p&gt;These are the solutions to the exercises contained within the handout to Descriptive Statistics which walks you through the basics of descriptive statistics and its parameters. The analyses presented here are using data from the &lt;code&gt;StarWars&lt;/code&gt; data set supplied through the &lt;code&gt;dplyr&lt;/code&gt; package that have been saved as a .csv file. Keep in mind that there is probably a myriad of other ways to reach the same conclusions as presented in these solutions.&lt;/p&gt;
&lt;p&gt;I have prepared some &lt;a href=&#34;https://www.erikkusch.com/courses/an-introduction-to-biostatistics/04---Descriptive-Statistics_Handout.html&#34; target=&#34;_blank&#34;&gt; Lecture Slides &lt;/a&gt; for this session.&lt;/p&gt;
&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;p&gt;Find the data for this exercise &lt;a href=&#34;https://www.erikkusch.com/courses/an-introduction-to-biostatistics/Data/DescriptiveData.csv&#34; target=&#34;_blank&#34;&gt; here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;packages&#34;&gt;Packages&lt;/h2&gt;
&lt;p&gt;As you will remember from our lecture slides, the calculation of the mode in &lt;code&gt;R&lt;/code&gt; can either be achieved through some intense coding or simply by using the &lt;code&gt;mlv(..., method=&amp;quot;mfv&amp;quot;)&lt;/code&gt; function contained within the &lt;code&gt;modeest&lt;/code&gt; package (unfortunately, this package is out of date and can sometimes be challenging to install).&lt;/p&gt;
&lt;p&gt;Conclusively, it is now time for you to get familiar with how packages work in &lt;code&gt;R&lt;/code&gt;. Packages are the way by which &lt;code&gt;R&lt;/code&gt; is supplied with user-created and moderator-mediated functionality that exceeds the base applicability of &lt;code&gt;R&lt;/code&gt;. Many things you will want to accomplish in more advanced statistics is impossible without such packages and even basic tasks such as data visualisation (dealt with in our next seminar) are reliant on &lt;code&gt;R&lt;/code&gt; packages.&lt;/p&gt;
&lt;p&gt;If you want to get a package and its functions into &lt;code&gt;R&lt;/code&gt; there are two ways we will discuss in the following. In general, it pays to load all packages at the beginning of a coding document before any actual analyses happen (in the preamble) so you get a good overview of what the program is calling upon.&lt;/p&gt;
&lt;h3 id=&#34;basic-preamble&#34;&gt;Basic Preamble&lt;/h3&gt;
&lt;p&gt;This is the most basic version of getting packages into &lt;code&gt;R&lt;/code&gt; and is widely practised and taught. Unsurprisingly, I am not a big fan of it.&lt;/p&gt;
&lt;p&gt;First, you use function &lt;code&gt;install.packages()&lt;/code&gt; to download the desired package off dedicated servers (usually CRAN-mirrors) to your machine where it is then unpacked into a library (a folder that&amp;rsquo;s located in your documents section by default). Secondly, you need to invoke the &lt;code&gt;library()&lt;/code&gt; function to load the &lt;code&gt;R&lt;/code&gt; package you need into your active &lt;code&gt;R&lt;/code&gt; session. In our case of the package &lt;code&gt;modeest&lt;/code&gt; it would look something like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&amp;quot;modeest&amp;quot;)
library(modeest)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The reason I am not overly fond of this procedure is that it is clunky, can break easily through spelling mistakes and starts cluttering your preamble super fast if the analyses you are wanting to perform require excessive amounts of packages. Additionally, when you are some place with a bad internet connection you might not want to re-download packages that are already contained on your hard drive.&lt;/p&gt;
&lt;h3 id=&#34;advanced-preamble&#34;&gt;Advanced Preamble&lt;/h3&gt;
&lt;p&gt;There is a myriad of different preamble styles (just as there are tons of different, personalised coding styles). I am left with presenting my preamble of choice here but I do not claim that this is the most sophisticated one out there.&lt;/p&gt;
&lt;p&gt;The way this preamble works is that it is structured around a user-defined function (something we will touch on later in our seminar series) which first checks whether a package is already downloaded and then installs (if necessary) and/or loads it into &lt;code&gt;R&lt;/code&gt;. This function is called &lt;code&gt;install.load.package()&lt;/code&gt; and you can see its specification down below (don&amp;rsquo;t worry if it doesn&amp;rsquo;t make sense to you yet - it is not supposed to at this point). Unfortunately, it can only ever be applied to one package at a time and so we need a workaround to make it work on multiple packages at once. This can be achieved by establishing a vector of all desired package names (&lt;code&gt;package_vec&lt;/code&gt;) and then applying (&lt;code&gt;sapply()&lt;/code&gt;) the &lt;code&gt;install.load.package()&lt;/code&gt; function to every item of the package name vector iteratively as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# function to load packages and install them if they haven&#39;t been installed yet
install.load.package &amp;lt;- function(x) {
    if (!require(x, character.only = TRUE)) 
        install.packages(x)
    require(x, character.only = TRUE)
}
# packages to load/install if necessary
package_vec &amp;lt;- c(&amp;quot;modeest&amp;quot;)
# applying function install.load.package to all packages specified in package_vec
sapply(package_vec, install.load.package)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: modeest
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## modeest 
##    TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why do I prefer this? Firstly, it is way shorter than the basic method when dealing with many packages (which you will get into fast, I promise), reduces the chance for typos by 50% and does not override already installed packages hence speeding up your processing time.&lt;/p&gt;
&lt;h2 id=&#34;loading-the-excel-data-into-r&#34;&gt;Loading the Excel data into &lt;code&gt;R&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Our data is located in the &lt;code&gt;Data&lt;/code&gt; folder and is called &lt;code&gt;DescriptiveData.csv&lt;/code&gt;. Since it is a .csv file, we can simply use the &lt;code&gt;R&lt;/code&gt; in-built function &lt;code&gt;read.csv()&lt;/code&gt; to load the data by combining the former two identifiers into one long string with a backslash separating the two (the backslash indicates a step down in the folder hierarchy). Given this argument, &lt;code&gt;read.csv()&lt;/code&gt; will produce an object of type &lt;code&gt;data.frame&lt;/code&gt; in &lt;code&gt;R&lt;/code&gt; which we want to keep in our environment and hence need to assign a name to. In our case, let that name be &lt;code&gt;Data_df&lt;/code&gt; (I recommend using endings to your data object names that indicate their type for easier coding without constant type checking):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df &amp;lt;- read.csv(&amp;quot;Data/DescriptiveData.csv&amp;quot;)  # load data file from Data folder
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;whats-contained-within-our-data&#34;&gt;What&amp;rsquo;s contained within our data?&lt;/h2&gt;
&lt;p&gt;Now that our data set is finally loaded into &lt;code&gt;R&lt;/code&gt;, we can finally get to trying to make sense of it. Usually, this shouldn&amp;rsquo;t ever be something one has to do in &lt;code&gt;R&lt;/code&gt; but should be manageable through a project-/data-specific README file (we will cover this in our seminar on hypotheses testing and project planning) but for now we are stuck with pure exploration of our data set. Get your goggles on and let&amp;rsquo;s dive in!&lt;/p&gt;
&lt;p&gt;Firstly, it always pays to asses the basic attributes of any data object (remember the Introduction to &lt;code&gt;R&lt;/code&gt; seminar):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Name&lt;/em&gt; - we know the name (it is &lt;code&gt;Data_df&lt;/code&gt;) since we named it that&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Type&lt;/em&gt; - we already know that it is a &lt;code&gt;data.frame&lt;/code&gt; because we created it using the &lt;code&gt;read.csv&lt;/code&gt; function&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Mode&lt;/em&gt; - this is an interesting one as it means having to subset our data frame&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Dimensions&lt;/em&gt; - a crucial information about how many observations and variables are contained within our data set&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;dimensions&#34;&gt;Dimensions&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s start with the &lt;em&gt;dimensions&lt;/em&gt; because these will tell us how many &lt;em&gt;modes&lt;/em&gt; (these are object attribute modes and not descriptive parameter modes) to asses:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dim(Data_df)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 87  8
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the &lt;code&gt;dim()&lt;/code&gt; function, we arrive at the conclusion that our &lt;code&gt;Data_df&lt;/code&gt; contains 87 rows and 8 columns. Since data frames are usually ordered as observations $\times$ variables, we can conclude that we have 87 observations and 8 variables at our hands.&lt;br&gt;
You can arrive at the same point by using the function &lt;code&gt;View()&lt;/code&gt; in &lt;code&gt;R&lt;/code&gt;. I&amp;rsquo;m not showing this here because it does not translate well to paper and would make whoever chooses to print this waste paper.&lt;/p&gt;
&lt;h3 id=&#34;modes&#34;&gt;Modes&lt;/h3&gt;
&lt;p&gt;Now it&amp;rsquo;s time to get a hang of the &lt;em&gt;modes&lt;/em&gt; of the variable records within our data set. To do so, we have two choices, we can either subset the data frame by columns and apply the &lt;code&gt;class()&lt;/code&gt; function to each column subset or simply apply the &lt;code&gt;str()&lt;/code&gt; function to the data frame object. The reason &lt;code&gt;str()&lt;/code&gt; may be favourable in this case is due to the fact that &lt;code&gt;str()&lt;/code&gt; automatically breaks down the structure of &lt;code&gt;R&lt;/code&gt;-internal objects and hence saves us the sub-setting:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(Data_df)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &#39;data.frame&#39;:	87 obs. of  8 variables:
##  $ name      : chr  &amp;quot;Luke Skywalker&amp;quot; &amp;quot;C-3PO&amp;quot; &amp;quot;R2-D2&amp;quot; &amp;quot;Darth Vader&amp;quot; ...
##  $ height    : int  172 167 96 202 150 178 165 97 183 182 ...
##  $ mass      : num  77 75 32 136 49 120 75 32 84 77 ...
##  $ hair_color: chr  &amp;quot;blond&amp;quot; &amp;quot;&amp;quot; &amp;quot;&amp;quot; &amp;quot;none&amp;quot; ...
##  $ skin_color: chr  &amp;quot;fair&amp;quot; &amp;quot;gold&amp;quot; &amp;quot;white, blue&amp;quot; &amp;quot;white&amp;quot; ...
##  $ eye_color : chr  &amp;quot;blue&amp;quot; &amp;quot;yellow&amp;quot; &amp;quot;red&amp;quot; &amp;quot;yellow&amp;quot; ...
##  $ birth_year: num  19 112 33 41.9 19 52 47 NA 24 57 ...
##  $ gender    : chr  &amp;quot;male&amp;quot; &amp;quot;&amp;quot; &amp;quot;&amp;quot; &amp;quot;male&amp;quot; ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As it turns out, our data frame knows the 8 variables of name, height, mass, hair_color, skin_color, eye_color, birth_year, gender which range from &lt;code&gt;integer&lt;/code&gt; to &lt;code&gt;numeric&lt;/code&gt; and &lt;code&gt;factor&lt;/code&gt; modes.&lt;/p&gt;
&lt;h3 id=&#34;data-content&#34;&gt;Data Content&lt;/h3&gt;
&lt;p&gt;So what does our data actually tell us? Answering this question usually comes down to some analyses but for now we are only really interested in what kind of information our data frame is storing.&lt;/p&gt;
&lt;p&gt;Again, this would be easiest to asses using a README file or the &lt;code&gt;View()&lt;/code&gt; function in &lt;code&gt;R&lt;/code&gt;. However, for the sake of brevity we can make due with the following to commands which present the user with the first and last five rows of any respective data frame:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;head(Data_df)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             name height mass  hair_color  skin_color eye_color birth_year
## 1 Luke Skywalker    172   77       blond        fair      blue       19.0
## 2          C-3PO    167   75                    gold    yellow      112.0
## 3          R2-D2     96   32             white, blue       red       33.0
## 4    Darth Vader    202  136        none       white    yellow       41.9
## 5    Leia Organa    150   49       brown       light     brown       19.0
## 6      Owen Lars    178  120 brown, grey       light      blue       52.0
##   gender
## 1   male
## 2       
## 3       
## 4   male
## 5 female
## 6   male
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;tail(Data_df)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              name height mass hair_color skin_color eye_color birth_year gender
## 82           Finn     NA   NA      black       dark      dark         NA   male
## 83            Rey     NA   NA      brown      light     hazel         NA female
## 84    Poe Dameron     NA   NA      brown      light     brown         NA   male
## 85            BB8     NA   NA       none       none     black         NA   none
## 86 Captain Phasma     NA   NA    unknown    unknown   unknown         NA female
## 87  PadmÃ© Amidala    165   45      brown      light     brown         46 female
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The avid reader will surely have picked up on the fact that all the records in the &lt;code&gt;name&lt;/code&gt; column of &lt;code&gt;Data_df&lt;/code&gt; belong to characters from the Star Wars universe. In fact, this data set is a modified version of the &lt;code&gt;StarWars&lt;/code&gt; data set supplied by the &lt;code&gt;dplyr&lt;/code&gt; package and contains information of many of the important cast members of the Star Wars movie universe.&lt;/p&gt;
&lt;h2 id=&#34;parameters-of-descriptive-statistics&#34;&gt;Parameters of descriptive statistics&lt;/h2&gt;
&lt;h3 id=&#34;names&#34;&gt;Names&lt;/h3&gt;
&lt;p&gt;As it turns out (and should&amp;rsquo;ve been obvious from the onset if we&amp;rsquo;re honest), every major character in the cinematic Star Wars Universe has a unique name to themselves. Conclusively, the calculation of any parameters of descriptive statistics makes no sense with the names of our characters for the two following reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The name variable is of mode character/factor and only allows for the calculation of the mode&lt;/li&gt;
&lt;li&gt;Since every name only appears once, there is no mode&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As long as the calculation of descriptive parameters of the &lt;code&gt;name&lt;/code&gt; variable of our data set is concerned, Admiral Ackbar said it best: &amp;ldquo;It&amp;rsquo;s a trap&amp;rdquo;.&lt;/p&gt;
&lt;h3 id=&#34;height&#34;&gt;Height&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s get started on figuring out some parameters of descriptive statistics for the &lt;code&gt;height&lt;/code&gt; variable of our Star Wars characters.&lt;/p&gt;
&lt;h4 id=&#34;subsetting&#34;&gt;Subsetting&lt;/h4&gt;
&lt;p&gt;First, we need to extract the data in question from our big data frame object. This can be achieved by indexing through the column name as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Height &amp;lt;- Data_df$height
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;location-parameters&#34;&gt;Location Parameters&lt;/h4&gt;
&lt;p&gt;Now, with our &lt;code&gt;Height&lt;/code&gt; vector being the numeric height records of the Star Wars characters in our data set, we are primed to calculate location parameters as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mean &amp;lt;- mean(Height, na.rm = TRUE)
median &amp;lt;- median(Height, na.rm = TRUE)
mode &amp;lt;- mlv(na.omit(Height), method = &amp;quot;mfv&amp;quot;)
min &amp;lt;- min(Height, na.rm = TRUE)
max &amp;lt;- max(Height, na.rm = TRUE)
range &amp;lt;- max - min

# Combining all location parameters into one vector for easier viewing
LocPars_vec &amp;lt;- c(mean, median, mode, min, max, range)
names(LocPars_vec) &amp;lt;- c(&amp;quot;mean&amp;quot;, &amp;quot;median&amp;quot;, &amp;quot;mode&amp;quot;, &amp;quot;minimum&amp;quot;, &amp;quot;maximum&amp;quot;, &amp;quot;range&amp;quot;)
LocPars_vec
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    mean  median    mode minimum maximum   range 
## 174.358 180.000 183.000  66.000 264.000 198.000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can clearly see, there is a big range in the heights of our respective Star Wars characters with mean and median being fairly disjunct due to the outliers in height on especially either end.&lt;/p&gt;
&lt;h4 id=&#34;distribution-parameters&#34;&gt;Distribution Parameters&lt;/h4&gt;
&lt;p&gt;Now that we are aware of the location parameters of the Star Wars height records, we can move on to the distribution parameters/parameters of spread. Those can be calculated in &lt;code&gt;R&lt;/code&gt; as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;var &amp;lt;- var(Height, na.rm = TRUE)
sd &amp;lt;- sd(Height, na.rm = TRUE)
quantiles &amp;lt;- quantile(Height, na.rm = TRUE)

# Combining all location parameters into one vector for easier viewing
DisPars_vec &amp;lt;- c(var, sd, quantiles)
names(DisPars_vec) &amp;lt;- c(&amp;quot;var&amp;quot;, &amp;quot;sd&amp;quot;, &amp;quot;0%&amp;quot;, &amp;quot;25%&amp;quot;, &amp;quot;50%&amp;quot;, &amp;quot;75%&amp;quot;, &amp;quot;100%&amp;quot;)
DisPars_vec
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        var         sd         0%        25%        50%        75%       100% 
## 1208.98272   34.77043   66.00000  167.00000  180.00000  191.00000  264.00000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice how some of the quantiles (actually quartiles in this case) link up with some of the parameters of central tendency.&lt;/p&gt;
&lt;h4 id=&#34;summary&#34;&gt;Summary&lt;/h4&gt;
&lt;p&gt;Just to round this off, have a look at what the &lt;code&gt;summary()&lt;/code&gt; function in &lt;code&gt;R&lt;/code&gt; supplies you with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary &amp;lt;- summary(na.omit(Height))
summary
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    66.0   167.0   180.0   174.4   191.0   264.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a nice assortment of location and dispersion parameters.&lt;/p&gt;
&lt;h3 id=&#34;mass&#34;&gt;Mass&lt;/h3&gt;
&lt;p&gt;Now let&amp;rsquo;s focus on the weight of our Star Wars characters.&lt;/p&gt;
&lt;h4 id=&#34;subsetting-1&#34;&gt;Subsetting&lt;/h4&gt;
&lt;p&gt;Again, we need to extract our data from the data frame. For the sake of brevity, I will refrain from showing you the rest of the analysis and only present its results to save some space.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Mass &amp;lt;- Data_df$mass
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;location-parameters-1&#34;&gt;Location Parameters&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;##       mean     median       mode    minimum    maximum      range 
##   97.31186   79.00000   80.00000   15.00000 1358.00000 1343.00000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, there is a huge range in weight records of Star Wars characters and especially the outlier on the upper end (1358kg) push the mean towards the upper end of the weight range and away from the median. We&amp;rsquo;ve got Jabba Desilijic Tiure to thank for that.&lt;/p&gt;
&lt;h4 id=&#34;distribution-parameters-1&#34;&gt;Distribution Parameters&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;##        var         sd         0%        25%        50%        75%       100% 
## 28715.7300   169.4572    15.0000    55.6000    79.0000    84.5000  1358.0000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Quite obviously, the wide range of weight records also prompts a large variance and standard deviation.&lt;/p&gt;
&lt;h3 id=&#34;hair-color&#34;&gt;Hair Color&lt;/h3&gt;
&lt;p&gt;Hair colour in our data set is saved in column 4 of our data set and so when sub-setting the data frame to obtain information about a characters hair colour, instead of calling on &lt;code&gt;Data_df$hair_color&lt;/code&gt; we can also do so as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;HCs &amp;lt;- Data_df[, 4]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Of course, hair colour is not a &lt;code&gt;numeric&lt;/code&gt; variable and much better represent by being of mode &lt;code&gt;factor&lt;/code&gt;. Therefore, we are unable to obtain most parameters of descriptive statistics but we can show a frequency count as follows which allows for the calculation of the mode:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(HCs)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## HCs
##                      auburn  auburn, grey auburn, white         black 
##             5             1             1             1            13 
##         blond        blonde         brown   brown, grey          grey 
##             3             1            18             1             1 
##          none       unknown         white 
##            37             1             4
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;eye-colour&#34;&gt;Eye Colour&lt;/h3&gt;
&lt;p&gt;Eye colour is another &lt;code&gt;factor&lt;/code&gt; mode variable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ECs &amp;lt;- Data_df$eye_color
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can only calculate the mode by looking for the maximum in our &lt;code&gt;table()&lt;/code&gt; output:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(ECs)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ECs
##         black          blue     blue-gray         brown          dark 
##            10            19             1            21             1 
##          gold green, yellow         hazel        orange          pink 
##             1             1             3             8             1 
##           red     red, blue       unknown         white        yellow 
##             5             1             3             1            11
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;birth-year&#34;&gt;Birth Year&lt;/h3&gt;
&lt;h4 id=&#34;subsetting-2&#34;&gt;Subsetting&lt;/h4&gt;
&lt;p&gt;As another &lt;code&gt;numeric&lt;/code&gt; variable, birth year allows for the calculation of the full range of parameters of descriptive statistics:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;BY &amp;lt;- Data_df$birth_year
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Keep in mind that StarWars operates on a different time reference scale than we do.&lt;/p&gt;
&lt;h4 id=&#34;location-parameters-2&#34;&gt;Location Parameters&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;##      mean    median      mode   minimum   maximum     range 
##  87.56512  52.00000  19.00000   8.00000 896.00000 888.00000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, there is a big disparity here between mean and median which stems from extreme outliers on both ends of the age spectrum (Yoda and Wicket Systri Warrick, respectively).&lt;/p&gt;
&lt;h4 id=&#34;distribution-parameters-2&#34;&gt;Distribution Parameters&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;##        var         sd         0%        25%        50%        75%       100% 
## 23929.4414   154.6914     8.0000    35.0000    52.0000    72.0000   896.0000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unsurprisingly, there is a big variance and standard deviation for the observed birth year/age records.&lt;/p&gt;
&lt;h3 id=&#34;gender&#34;&gt;Gender&lt;/h3&gt;
&lt;p&gt;Gender is another &lt;code&gt;factor&lt;/code&gt; mode variable (obviously):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Gender &amp;lt;- Data_df$gender
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can, again, only judge the mode of our data from the output of the &lt;code&gt;table()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(Gender)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Gender
##                      female hermaphrodite          male          none 
##             3            19             1            62             2
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Descriptive Statistics</title>
      <link>https://www.erikkusch.com/courses/biostat101/descriptive-statistics/</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/biostat101/descriptive-statistics/</guid>
      <description>&lt;h2 id=&#34;theory&#34;&gt;Theory&lt;/h2&gt;
&lt;p&gt;These are the solutions to the exercises contained within the handout to Descriptive Statistics which walks you through the basics of descriptive statistics and its parameters. The analyses presented here are using data from the &lt;code&gt;StarWars&lt;/code&gt; data set supplied through the &lt;code&gt;dplyr&lt;/code&gt; package that have been saved as a .csv file. Keep in mind that there is probably a myriad of other ways to reach the same conclusions as presented in these solutions. I have prepared some slides for this session:
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/biostat101/04---Descriptive-Statistics_Handout.html&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://www.erikkusch.com/courses/biostat101/04---BioStat101_featured.png&#34; width=&#34;900&#34; margin-top = &#34;0&#34;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;p&gt;Find the data for this exercise &lt;a href=&#34;https://github.com/ErikKusch/Homepage/raw/master/content/courses/biostat101/Data/DescriptiveData.csv&#34; target=&#34;_blank&#34;&gt; here&lt;/a&gt;. Do not worry about downloading it for now.&lt;/p&gt;
&lt;h2 id=&#34;packages&#34;&gt;Packages&lt;/h2&gt;
&lt;p&gt;As you will remember from our lecture slides, the calculation of the mode in &lt;code&gt;R&lt;/code&gt; can either be achieved through some intense coding or simply by using the &lt;code&gt;mlv(..., method=&amp;quot;mfv&amp;quot;)&lt;/code&gt; function contained within the &lt;code&gt;modeest&lt;/code&gt; package (unfortunately, this package is out of date and can sometimes be challenging to install).&lt;/p&gt;
&lt;p&gt;Conclusively, it is now time for you to get familiar with how packages work in &lt;code&gt;R&lt;/code&gt;. Packages are the way by which &lt;code&gt;R&lt;/code&gt; is supplied with user-created and moderator-mediated functionality that exceeds the base applicability of &lt;code&gt;R&lt;/code&gt;. Many things you will want to accomplish in more advanced statistics is impossible without such packages and even basic tasks such as data visualisation (dealt with in our next seminar) are reliant on &lt;code&gt;R&lt;/code&gt; packages.&lt;/p&gt;
&lt;p&gt;If you want to get a package and its functions into &lt;code&gt;R&lt;/code&gt; there are two ways we will discuss in the following. In general, it pays to load all packages at the beginning of a coding document before any actual analyses happen (in the preamble) so you get a good overview of what the program is calling upon.&lt;/p&gt;
&lt;h3 id=&#34;basic-preamble&#34;&gt;Basic Preamble&lt;/h3&gt;
&lt;p&gt;This is the most basic version of getting packages into &lt;code&gt;R&lt;/code&gt; and is widely practised and taught. Unsurprisingly, I am not a big fan of it.&lt;/p&gt;
&lt;p&gt;First, you use function &lt;code&gt;install.packages()&lt;/code&gt; to download the desired package off dedicated servers (usually CRAN-mirrors) to your machine where it is then unpacked into a library (a folder that&amp;rsquo;s located in your documents section by default). Secondly, you need to invoke the &lt;code&gt;library()&lt;/code&gt; function to load the &lt;code&gt;R&lt;/code&gt; package you need into your active &lt;code&gt;R&lt;/code&gt; session. In our case of the package &lt;code&gt;modeest&lt;/code&gt; it would look something like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&amp;quot;modeest&amp;quot;)
library(modeest)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The reason I am not overly fond of this procedure is that it is clunky, can break easily through spelling mistakes and starts cluttering your preamble super fast if the analyses you are wanting to perform require excessive amounts of packages. Additionally, when you are some place with a bad internet connection you might not want to re-download packages that are already contained on your hard drive.&lt;/p&gt;
&lt;h3 id=&#34;advanced-preamble&#34;&gt;Advanced Preamble&lt;/h3&gt;
&lt;p&gt;There is a myriad of different preamble styles (just as there are tons of different, personalised coding styles). I am left with presenting my preamble of choice here but I do not claim that this is the most sophisticated one out there.&lt;/p&gt;
&lt;p&gt;The way this preamble works is that it is structured around a user-defined function (something we will touch on later in our seminar series) which first checks whether a package is already downloaded and then installs (if necessary) and/or loads it into &lt;code&gt;R&lt;/code&gt;. This function is called &lt;code&gt;install.load.package()&lt;/code&gt; and you can see its specification down below (don&amp;rsquo;t worry if it doesn&amp;rsquo;t make sense to you yet - it is not supposed to at this point). Unfortunately, it can only ever be applied to one package at a time and so we need a workaround to make it work on multiple packages at once. This can be achieved by establishing a vector of all desired package names (&lt;code&gt;package_vec&lt;/code&gt;) and then applying (&lt;code&gt;sapply()&lt;/code&gt;) the &lt;code&gt;install.load.package()&lt;/code&gt; function to every item of the package name vector iteratively as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# function to load packages and install them if they haven&#39;t been installed yet
install.load.package &amp;lt;- function(x) {
    if (!require(x, character.only = TRUE))
        install.packages(x)
    require(x, character.only = TRUE)
}
# packages to load/install if necessary
package_vec &amp;lt;- c(&amp;quot;modeest&amp;quot;)
# applying function install.load.package to all packages specified in
# package_vec
sapply(package_vec, install.load.package)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: modeest
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## modeest 
##    TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why do I prefer this? Firstly, it is way shorter than the basic method when dealing with many packages (which you will get into fast, I promise), reduces the chance for typos by 50% and does not override already installed packages hence speeding up your processing time.&lt;/p&gt;
&lt;h2 id=&#34;loading-the-excel-data-into-r&#34;&gt;Loading the Excel data into &lt;code&gt;R&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Our data is located in the &lt;code&gt;Data&lt;/code&gt; folder and is called &lt;code&gt;DescriptiveData.csv&lt;/code&gt;. Since it is a .csv file, we can simply use the &lt;code&gt;R&lt;/code&gt; in-built function &lt;code&gt;read.csv()&lt;/code&gt; to load the data by combining the former two identifiers into one long string with a backslash separating the two (the backslash indicates a step down in the folder hierarchy). Given this argument, &lt;code&gt;read.csv()&lt;/code&gt; will produce an object of type &lt;code&gt;data.frame&lt;/code&gt; in &lt;code&gt;R&lt;/code&gt; which we want to keep in our environment and hence need to assign a name to. In our case, let that name be &lt;code&gt;Data_df&lt;/code&gt; (I recommend using endings to your data object names that indicate their type for easier coding without constant type checking):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Data_df &amp;lt;- read.csv(&#39;Data/DescriptiveData.csv&#39;) # load data file from Data
# folder if you downloaded the data as a .csv alternatively, read the csv
# directly from the url
Data_df &amp;lt;- read.csv(&amp;quot;https://github.com/ErikKusch/Homepage/raw/master/content/courses/biostat101/Data/DescriptiveData.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;whats-contained-within-our-data&#34;&gt;What&amp;rsquo;s contained within our data?&lt;/h2&gt;
&lt;p&gt;Now that our data set is finally loaded into &lt;code&gt;R&lt;/code&gt;, we can finally get to trying to make sense of it. Usually, this shouldn&amp;rsquo;t ever be something one has to do in &lt;code&gt;R&lt;/code&gt; but should be manageable through a project-/data-specific README file (we will cover this in our seminar on hypotheses testing and project planning) but for now we are stuck with pure exploration of our data set. Get your goggles on and let&amp;rsquo;s dive in!&lt;/p&gt;
&lt;p&gt;Firstly, it always pays to asses the basic attributes of any data object (remember the Introduction to &lt;code&gt;R&lt;/code&gt; seminar):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Name&lt;/em&gt; - we know the name (it is &lt;code&gt;Data_df&lt;/code&gt;) since we named it that&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Type&lt;/em&gt; - we already know that it is a &lt;code&gt;data.frame&lt;/code&gt; because we created it using the &lt;code&gt;read.csv&lt;/code&gt; function&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Mode&lt;/em&gt; - this is an interesting one as it means having to subset our data frame&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Dimensions&lt;/em&gt; - a crucial information about how many observations and variables are contained within our data set&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;dimensions&#34;&gt;Dimensions&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s start with the &lt;em&gt;dimensions&lt;/em&gt; because these will tell us how many &lt;em&gt;modes&lt;/em&gt; (these are object attribute modes and not descriptive parameter modes) to asses:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dim(Data_df)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 87  8
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the &lt;code&gt;dim()&lt;/code&gt; function, we arrive at the conclusion that our &lt;code&gt;Data_df&lt;/code&gt; contains 87 rows and 8 columns. Since data frames are usually ordered as observations $\times$ variables, we can conclude that we have 87 observations and 8 variables at our hands.&lt;br&gt;
You can arrive at the same point by using the function &lt;code&gt;View()&lt;/code&gt; in &lt;code&gt;R&lt;/code&gt;. I&amp;rsquo;m not showing this here because it does not translate well to paper and would make whoever chooses to print this waste paper.&lt;/p&gt;
&lt;h3 id=&#34;modes&#34;&gt;Modes&lt;/h3&gt;
&lt;p&gt;Now it&amp;rsquo;s time to get a hang of the &lt;em&gt;modes&lt;/em&gt; of the variable records within our data set. To do so, we have two choices, we can either subset the data frame by columns and apply the &lt;code&gt;class()&lt;/code&gt; function to each column subset or simply apply the &lt;code&gt;str()&lt;/code&gt; function to the data frame object. The reason &lt;code&gt;str()&lt;/code&gt; may be favourable in this case is due to the fact that &lt;code&gt;str()&lt;/code&gt; automatically breaks down the structure of &lt;code&gt;R&lt;/code&gt;-internal objects and hence saves us the sub-setting:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(Data_df)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &#39;data.frame&#39;:	87 obs. of  8 variables:
##  $ name      : chr  &amp;quot;Luke Skywalker&amp;quot; &amp;quot;C-3PO&amp;quot; &amp;quot;R2-D2&amp;quot; &amp;quot;Darth Vader&amp;quot; ...
##  $ height    : int  172 167 96 202 150 178 165 97 183 182 ...
##  $ mass      : num  77 75 32 136 49 120 75 32 84 77 ...
##  $ hair_color: chr  &amp;quot;blond&amp;quot; &amp;quot;&amp;quot; &amp;quot;&amp;quot; &amp;quot;none&amp;quot; ...
##  $ skin_color: chr  &amp;quot;fair&amp;quot; &amp;quot;gold&amp;quot; &amp;quot;white, blue&amp;quot; &amp;quot;white&amp;quot; ...
##  $ eye_color : chr  &amp;quot;blue&amp;quot; &amp;quot;yellow&amp;quot; &amp;quot;red&amp;quot; &amp;quot;yellow&amp;quot; ...
##  $ birth_year: num  19 112 33 41.9 19 52 47 NA 24 57 ...
##  $ gender    : chr  &amp;quot;male&amp;quot; &amp;quot;&amp;quot; &amp;quot;&amp;quot; &amp;quot;male&amp;quot; ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As it turns out, our data frame knows the 8 variables of name, height, mass, hair_color, skin_color, eye_color, birth_year, gender which range from &lt;code&gt;integer&lt;/code&gt; to &lt;code&gt;numeric&lt;/code&gt; and &lt;code&gt;factor&lt;/code&gt; modes.&lt;/p&gt;
&lt;h3 id=&#34;data-content&#34;&gt;Data Content&lt;/h3&gt;
&lt;p&gt;So what does our data actually tell us? Answering this question usually comes down to some analyses but for now we are only really interested in what kind of information our data frame is storing.&lt;/p&gt;
&lt;p&gt;Again, this would be easiest to asses using a README file or the &lt;code&gt;View()&lt;/code&gt; function in &lt;code&gt;R&lt;/code&gt;. However, for the sake of brevity we can make due with the following to commands which present the user with the first and last five rows of any respective data frame:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;head(Data_df)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             name height mass  hair_color  skin_color eye_color birth_year
## 1 Luke Skywalker    172   77       blond        fair      blue       19.0
## 2          C-3PO    167   75                    gold    yellow      112.0
## 3          R2-D2     96   32             white, blue       red       33.0
## 4    Darth Vader    202  136        none       white    yellow       41.9
## 5    Leia Organa    150   49       brown       light     brown       19.0
## 6      Owen Lars    178  120 brown, grey       light      blue       52.0
##   gender
## 1   male
## 2       
## 3       
## 4   male
## 5 female
## 6   male
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;tail(Data_df)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                name height mass hair_color skin_color eye_color birth_year
## 82             Finn     NA   NA      black       dark      dark         NA
## 83              Rey     NA   NA      brown      light     hazel         NA
## 84      Poe Dameron     NA   NA      brown      light     brown         NA
## 85              BB8     NA   NA       none       none     black         NA
## 86   Captain Phasma     NA   NA    unknown    unknown   unknown         NA
## 87 Padm\xe9 Amidala    165   45      brown      light     brown         46
##    gender
## 82   male
## 83 female
## 84   male
## 85   none
## 86 female
## 87 female
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The avid reader will surely have picked up on the fact that all the records in the &lt;code&gt;name&lt;/code&gt; column of &lt;code&gt;Data_df&lt;/code&gt; belong to characters from the Star Wars universe. In fact, this data set is a modified version of the &lt;code&gt;StarWars&lt;/code&gt; data set supplied by the &lt;code&gt;dplyr&lt;/code&gt; package and contains information of many of the important cast members of the Star Wars movie universe.&lt;/p&gt;
&lt;h2 id=&#34;parameters-of-descriptive-statistics&#34;&gt;Parameters of descriptive statistics&lt;/h2&gt;
&lt;h3 id=&#34;names&#34;&gt;Names&lt;/h3&gt;
&lt;p&gt;As it turns out (and should&amp;rsquo;ve been obvious from the onset if we&amp;rsquo;re honest), every major character in the cinematic Star Wars Universe has a unique name to themselves. Conclusively, the calculation of any parameters of descriptive statistics makes no sense with the names of our characters for the two following reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The name variable is of mode character/factor and only allows for the calculation of the mode&lt;/li&gt;
&lt;li&gt;Since every name only appears once, there is no mode&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As long as the calculation of descriptive parameters of the &lt;code&gt;name&lt;/code&gt; variable of our data set is concerned, Admiral Ackbar said it best: &amp;ldquo;It&amp;rsquo;s a trap&amp;rdquo;.&lt;/p&gt;
&lt;h3 id=&#34;height&#34;&gt;Height&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s get started on figuring out some parameters of descriptive statistics for the &lt;code&gt;height&lt;/code&gt; variable of our Star Wars characters.&lt;/p&gt;
&lt;h4 id=&#34;subsetting&#34;&gt;Subsetting&lt;/h4&gt;
&lt;p&gt;First, we need to extract the data in question from our big data frame object. This can be achieved by indexing through the column name as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Height &amp;lt;- Data_df$height
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;location-parameters&#34;&gt;Location Parameters&lt;/h4&gt;
&lt;p&gt;Now, with our &lt;code&gt;Height&lt;/code&gt; vector being the numeric height records of the Star Wars characters in our data set, we are primed to calculate location parameters as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mean &amp;lt;- mean(Height, na.rm = TRUE)
median &amp;lt;- median(Height, na.rm = TRUE)
mode &amp;lt;- mlv(na.omit(Height), method = &amp;quot;mfv&amp;quot;)
min &amp;lt;- min(Height, na.rm = TRUE)
max &amp;lt;- max(Height, na.rm = TRUE)
range &amp;lt;- max - min

# Combining all location parameters into one vector for easier viewing
LocPars_vec &amp;lt;- c(mean, median, mode, min, max, range)
names(LocPars_vec) &amp;lt;- c(&amp;quot;mean&amp;quot;, &amp;quot;median&amp;quot;, &amp;quot;mode&amp;quot;, &amp;quot;minimum&amp;quot;, &amp;quot;maximum&amp;quot;, &amp;quot;range&amp;quot;)
LocPars_vec
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    mean  median    mode minimum maximum   range 
## 174.358 180.000 183.000  66.000 264.000 198.000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can clearly see, there is a big range in the heights of our respective Star Wars characters with mean and median being fairly disjunct due to the outliers in height on especially either end.&lt;/p&gt;
&lt;h4 id=&#34;distribution-parameters&#34;&gt;Distribution Parameters&lt;/h4&gt;
&lt;p&gt;Now that we are aware of the location parameters of the Star Wars height records, we can move on to the distribution parameters/parameters of spread. Those can be calculated in &lt;code&gt;R&lt;/code&gt; as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;var &amp;lt;- var(Height, na.rm = TRUE)
sd &amp;lt;- sd(Height, na.rm = TRUE)
quantiles &amp;lt;- quantile(Height, na.rm = TRUE)

# Combining all location parameters into one vector for easier viewing
DisPars_vec &amp;lt;- c(var, sd, quantiles)
names(DisPars_vec) &amp;lt;- c(&amp;quot;var&amp;quot;, &amp;quot;sd&amp;quot;, &amp;quot;0%&amp;quot;, &amp;quot;25%&amp;quot;, &amp;quot;50%&amp;quot;, &amp;quot;75%&amp;quot;, &amp;quot;100%&amp;quot;)
DisPars_vec
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        var         sd         0%        25%        50%        75%       100% 
## 1208.98272   34.77043   66.00000  167.00000  180.00000  191.00000  264.00000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice how some of the quantiles (actually quartiles in this case) link up with some of the parameters of central tendency.&lt;/p&gt;
&lt;h4 id=&#34;summary&#34;&gt;Summary&lt;/h4&gt;
&lt;p&gt;Just to round this off, have a look at what the &lt;code&gt;summary()&lt;/code&gt; function in &lt;code&gt;R&lt;/code&gt; supplies you with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary &amp;lt;- summary(na.omit(Height))
summary
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    66.0   167.0   180.0   174.4   191.0   264.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a nice assortment of location and dispersion parameters.&lt;/p&gt;
&lt;h3 id=&#34;mass&#34;&gt;Mass&lt;/h3&gt;
&lt;p&gt;Now let&amp;rsquo;s focus on the weight of our Star Wars characters.&lt;/p&gt;
&lt;h4 id=&#34;subsetting-1&#34;&gt;Subsetting&lt;/h4&gt;
&lt;p&gt;Again, we need to extract our data from the data frame. For the sake of brevity, I will refrain from showing you the rest of the analysis and only present its results to save some space.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Mass &amp;lt;- Data_df$mass
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;location-parameters-1&#34;&gt;Location Parameters&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;##       mean     median       mode    minimum    maximum      range 
##   97.31186   79.00000   80.00000   15.00000 1358.00000 1343.00000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, there is a huge range in weight records of Star Wars characters and especially the outlier on the upper end (1358kg) push the mean towards the upper end of the weight range and away from the median. We&amp;rsquo;ve got Jabba Desilijic Tiure to thank for that.&lt;/p&gt;
&lt;h4 id=&#34;distribution-parameters-1&#34;&gt;Distribution Parameters&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;##        var         sd         0%        25%        50%        75%       100% 
## 28715.7300   169.4572    15.0000    55.6000    79.0000    84.5000  1358.0000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Quite obviously, the wide range of weight records also prompts a large variance and standard deviation.&lt;/p&gt;
&lt;h3 id=&#34;hair-color&#34;&gt;Hair Color&lt;/h3&gt;
&lt;p&gt;Hair colour in our data set is saved in column 4 of our data set and so when sub-setting the data frame to obtain information about a characters hair colour, instead of calling on &lt;code&gt;Data_df$hair_color&lt;/code&gt; we can also do so as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;HCs &amp;lt;- Data_df[, 4]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Of course, hair colour is not a &lt;code&gt;numeric&lt;/code&gt; variable and much better represent by being of mode &lt;code&gt;factor&lt;/code&gt;. Therefore, we are unable to obtain most parameters of descriptive statistics but we can show a frequency count as follows which allows for the calculation of the mode:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(HCs)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## HCs
##                      auburn  auburn, grey auburn, white         black 
##             5             1             1             1            13 
##         blond        blonde         brown   brown, grey          grey 
##             3             1            18             1             1 
##          none       unknown         white 
##            37             1             4
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;eye-colour&#34;&gt;Eye Colour&lt;/h3&gt;
&lt;p&gt;Eye colour is another &lt;code&gt;factor&lt;/code&gt; mode variable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ECs &amp;lt;- Data_df$eye_color
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can only calculate the mode by looking for the maximum in our &lt;code&gt;table()&lt;/code&gt; output:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(ECs)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ECs
##         black          blue     blue-gray         brown          dark 
##            10            19             1            21             1 
##          gold green, yellow         hazel        orange          pink 
##             1             1             3             8             1 
##           red     red, blue       unknown         white        yellow 
##             5             1             3             1            11
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;birth-year&#34;&gt;Birth Year&lt;/h3&gt;
&lt;h4 id=&#34;subsetting-2&#34;&gt;Subsetting&lt;/h4&gt;
&lt;p&gt;As another &lt;code&gt;numeric&lt;/code&gt; variable, birth year allows for the calculation of the full range of parameters of descriptive statistics:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;BY &amp;lt;- Data_df$birth_year
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Keep in mind that StarWars operates on a different time reference scale than we do.&lt;/p&gt;
&lt;h4 id=&#34;location-parameters-2&#34;&gt;Location Parameters&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;##      mean    median      mode   minimum   maximum     range 
##  87.56512  52.00000  19.00000   8.00000 896.00000 888.00000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, there is a big disparity here between mean and median which stems from extreme outliers on both ends of the age spectrum (Yoda and Wicket Systri Warrick, respectively).&lt;/p&gt;
&lt;h4 id=&#34;distribution-parameters-2&#34;&gt;Distribution Parameters&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;##        var         sd         0%        25%        50%        75%       100% 
## 23929.4414   154.6914     8.0000    35.0000    52.0000    72.0000   896.0000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unsurprisingly, there is a big variance and standard deviation for the observed birth year/age records.&lt;/p&gt;
&lt;h3 id=&#34;gender&#34;&gt;Gender&lt;/h3&gt;
&lt;p&gt;Gender is another &lt;code&gt;factor&lt;/code&gt; mode variable (obviously):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Gender &amp;lt;- Data_df$gender
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can, again, only judge the mode of our data from the output of the &lt;code&gt;table()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(Gender)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Gender
##                      female hermaphrodite          male          none 
##             3            19             1            62             2
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Discovering Data with rgbif</title>
      <link>https://www.erikkusch.com/courses/gbif/datadiscovery/</link>
      <pubDate>Sun, 21 May 2023 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/gbif/datadiscovery/</guid>
      <description>&lt;div class=&#34;alert alert-normal&#34;&gt;
  &lt;div&gt;
    &lt;details&gt;
  &lt;summary&gt;Preamble, Package-Loading, and GBIF API Credential Registering (click here):&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Custom install &amp;amp; load function
install.load.package &amp;lt;- function(x) {
  if (!require(x, character.only = TRUE)) {
    install.packages(x, repos = &amp;quot;http://cran.us.r-project.org&amp;quot;)
  }
  require(x, character.only = TRUE)
}
## names of packages we want installed (if not installed yet) and loaded
package_vec &amp;lt;- c(
  &amp;quot;rgbif&amp;quot;,
  &amp;quot;knitr&amp;quot;, # for rmarkdown table visualisations
  &amp;quot;rnaturalearth&amp;quot;, # for shapefile access
  &amp;quot;sf&amp;quot;, # for transforming polygons into wkt format
  &amp;quot;ggplot2&amp;quot;, # for visualisations
  &amp;quot;networkD3&amp;quot;, # for sankey plots
  &amp;quot;htmlwidgets&amp;quot;, # for sankey inclusion in website
  &amp;quot;leaflet&amp;quot;, # for map visualisation
  &amp;quot;pbapply&amp;quot; # for apply functions with progress bars which I use in hidden code chunks
)
## executing install &amp;amp; load for each package
sapply(package_vec, install.load.package)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         rgbif         knitr rnaturalearth            sf       ggplot2     networkD3 
##          TRUE          TRUE          TRUE          TRUE          TRUE          TRUE 
##   htmlwidgets       leaflet       pbapply 
##          TRUE          TRUE          TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;options(gbif_user = &amp;quot;my gbif username&amp;quot;)
options(gbif_email = &amp;quot;my registred gbif e-mail&amp;quot;)
options(gbif_pwd = &amp;quot;my gbif password&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt; 
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;First, we need to register the correct &lt;strong&gt;usageKey&lt;/strong&gt; for &lt;em&gt;Lagopus muta&lt;/em&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sp_name &amp;lt;- &amp;quot;Lagopus muta&amp;quot;
sp_backbone &amp;lt;- name_backbone(name = sp_name)
sp_key &amp;lt;- sp_backbone$usageKey
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With usageKey in hand, we are now ready to discover relevant data for our study needs. Doing so is a complex task mediated through rigurous metadata standards on GBIF.&lt;/p&gt;
&lt;div class=&#34;alert alert-danger&#34;&gt;
  &lt;div&gt;
    GBIF hosts tons of data. Finding the right subset thereof can be difficult.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Data on GBIF is indexed according to the 
&lt;a href=&#34;https://dwc.tdwg.org/list/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Darwin Core Standard&lt;/a&gt;. Any discovery of data can be augmented by matching terms of the Darwin Core with desired subset characteristics. Here, I will show how we can build an increasingly complex query for &lt;em&gt;Lagopus muta&lt;/em&gt; records.&lt;/p&gt;
&lt;p&gt;At base, we will start with the functions &lt;code&gt;occc_count(...)&lt;/code&gt; and &lt;code&gt;occ_search(...)&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;occ_count(taxonKey = sp_key)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 107535
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;occ_search(taxonKey = sp_key, limit = 0)$meta$count
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 107535
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using both of these, we obtain the same output of $1.07535\times 10^{5}$ &lt;em&gt;Lagopus muta&lt;/em&gt; records mediated by GBIF. Note that this number will likely be higher for you as data is continually added to the GBIF indexing and this document here is frozen in time (it was last updated on 2024-10-30).&lt;/p&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    When wanting to discover data matching Darwin Core Terms with multiple characteristics, these can be fed to the &lt;code&gt;occ_search(...)&lt;/code&gt; function as strings with semicolons separating desired characteristics (e.g., &lt;code&gt;year = &amp;quot;2000;2001&amp;quot;&lt;/code&gt;).
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;spatial-limitation&#34;&gt;Spatial Limitation&lt;/h2&gt;
&lt;div class=&#34;alert alert-info&#34;&gt;
  &lt;div&gt;
    By default, discovery of data through GBIF considers the entire Earth. However, that is rarely needed for specific study purposes and so it may be useful to constrain data discovery to areas of interest.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;by-country-code&#34;&gt;By Country Code&lt;/h3&gt;
&lt;p&gt;First, let&amp;rsquo;s limit our search by a specific region. In this case, we are interested in occurrences of &lt;em&gt;Lagopus muta&lt;/em&gt; across Norway. Countries are indexed according to ISO2 names (two-letter codes, see &lt;code&gt;enumeration_country()&lt;/code&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;occ_NO &amp;lt;- occ_search(taxonKey = sp_key, country = &amp;quot;NO&amp;quot;)
occ_NO$meta$count
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 17024
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is how &lt;em&gt;Lagopus muta&lt;/em&gt; records are distributed across countries according to GBIF on 2024-10-30:&lt;/p&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for code necessary to create the figure below.&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;occ_countries &amp;lt;- occ_count(
  taxonKey = sp_key,
  facet = c(&amp;quot;country&amp;quot;),
  facetLimit = nrow(enumeration_country())
)
occ_countries &amp;lt;- cbind(
  occ_countries,
  data.frame(
    GBIF = &amp;quot;GBIF&amp;quot;,
    Countries = enumeration_country()$title[
      match(occ_countries$country, enumeration_country()$iso2)
    ]
  )
)
occ_countries$Records &amp;lt;- as.numeric(occ_countries$count)
occ_countries &amp;lt;- occ_countries[occ_countries$Records != 0, ]
occ_countries &amp;lt;- occ_countries[, -1]

plot_df &amp;lt;- rbind(
  occ_countries[order(occ_countries$Records, decreasing = TRUE)[1:10], -1],
  data.frame(
    GBIF = &amp;quot;GBIF&amp;quot;,
    Countries = &amp;quot;Others&amp;quot;,
    Records = sum(occ_countries$Records[order(occ_countries$Records, decreasing = TRUE)[-1:-10]])
  )
)
plot_df$Countries[plot_df$Countries == &amp;quot;United Kingdom of Great Britain and Northern Ireland&amp;quot;] &amp;lt;- &amp;quot;United Kingdom&amp;quot;
alluvial_df &amp;lt;- plot_df


my_color &amp;lt;- &#39;d3.scaleOrdinal() .range([&amp;quot;#FDE725FF&amp;quot;,&amp;quot;#1F9E89FF&amp;quot;])&#39;

links &amp;lt;- alluvial_df
colnames(links) &amp;lt;- c(&amp;quot;source&amp;quot;, &amp;quot;target&amp;quot;, &amp;quot;value&amp;quot;)
nodes &amp;lt;- data.frame(
  name = c(&amp;quot;GBIF&amp;quot;, alluvial_df$Countries),
  group = c(&amp;quot;a&amp;quot;, rep(&amp;quot;b&amp;quot;, nrow(alluvial_df)))
)
links$IDsource &amp;lt;- match(links$source, nodes$name) - 1
links$IDtarget &amp;lt;- match(links$target, nodes$name) - 1

SN &amp;lt;- sankeyNetwork(
  Links = links, Nodes = nodes,
  Source = &amp;quot;IDsource&amp;quot;, Target = &amp;quot;IDtarget&amp;quot;,
  Value = &amp;quot;value&amp;quot;, NodeID = &amp;quot;name&amp;quot;,
  colourScale = my_color, NodeGroup = &amp;quot;group&amp;quot;,
  sinksRight = FALSE, nodeWidth = 40, fontSize = 13, nodePadding = 5
)
networkD3::saveNetwork(network = SN, file = &amp;quot;SankeyCountry.html&amp;quot;, selfcontained = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt; 
&lt;iframe seamless width=&#34;100%&#34; height = &#34;600px&#34; src=&#34;https://www.erikkusch.com/courses/gbif/SankeyCountry.html&#34; title=&#34;Sankey diagram of Lagopus muta records by country&#34;&gt;&lt;/iframe&gt; 
&lt;p&gt;As it turns out, roughly 15.83% of &lt;em&gt;Lagopus muta&lt;/em&gt; records mediated by GBIF fall within Norway.&lt;/p&gt;
&lt;h3 id=&#34;by-shapefile--polygon&#34;&gt;By Shapefile / Polygon&lt;/h3&gt;
&lt;p&gt;Oftentimes, you won&amp;rsquo;t be interested in occurrences according to a specific country, but rather a specific area on Earth as identified through a shapefile. To demonstrate data discovery by shapefile, let&amp;rsquo;s obtain the shapefile for Norway from 
&lt;a href=&#34;https://www.naturalearthdata.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;naturalearth&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;NO_shp &amp;lt;- rnaturalearth::ne_countries(country = &amp;quot;Norway&amp;quot;, scale = &amp;quot;small&amp;quot;, returnclass = &amp;quot;sf&amp;quot;)[, 1] # I am extracting only the first column for ease of use, I don&#39;t need additional information
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This shapefile subsequently looks like this:&lt;/p&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for code necessary to create the figure below.&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;shape_leaflet &amp;lt;- leaflet(NO_shp) %&amp;gt;%
  addTiles() %&amp;gt;%
  addPolygons(col = &amp;quot;red&amp;quot;)
saveWidget(shape_leaflet, &amp;quot;leaflet_shape.html&amp;quot;, selfcontained = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;iframe seamless width=&#34;100%&#34; height = &#34;600px&#34; src=&#34;https://www.erikkusch.com/courses/gbif/leaflet_shape.html&#34; title=&#34;Map of initial shapefile&#34;&gt;&lt;/iframe&gt; 
&lt;p&gt;Let&amp;rsquo;s focus on just continental Norway:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;NO_shp &amp;lt;- st_crop(NO_shp, xmin = 4.98, xmax = 31.3, ymin = 58, ymax = 71.14)
&lt;/code&gt;&lt;/pre&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for code necessary to create the figure below.&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;shape_leaflet &amp;lt;- leaflet(NO_shp) %&amp;gt;%
  addTiles() %&amp;gt;%
  addPolygons(col = &amp;quot;red&amp;quot;)
saveWidget(shape_leaflet, &amp;quot;leaflet_shapeCrop.html&amp;quot;, selfcontained = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;iframe seamless width=&#34;100%&#34; height = &#34;600px&#34; src=&#34;https://www.erikkusch.com/courses/gbif/leaflet_shapeCrop.html&#34; title=&#34;Map of cropped shapefile&#34;&gt;&lt;/iframe&gt; 
&lt;p&gt;Unfortunately, &lt;code&gt;rgbif&lt;/code&gt; prefers to be told about shapefiles in Well Known Text (WKT) format so we need to reformat our polygon data frame obtained above. We do so like such:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;NO_wkt &amp;lt;- st_as_text(st_geometry(NO_shp))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now pass this information into the &lt;code&gt;occ_search(...)&lt;/code&gt; function using the &lt;code&gt;geometry&lt;/code&gt; argument:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;occ_wkt &amp;lt;- occ_search(taxonKey = sp_key, geometry = NO_wkt)
occ_wkt$meta$count
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 14748
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we find that there are fewer records available when using the shapefile for data discovery. Why would that be? In this case, you will find that we are using a pretty coarse shapefile for Norway which probably cuts off some obersvations of &lt;em&gt;Lagopus muta&lt;/em&gt; that do belong rightfully into Norway.&lt;/p&gt;
&lt;p&gt;When searching for data by country code (or continent code for that matter), the returned data need not necessarily contain coordinates so long as a record is assigned to the relevant country code. While all records whose coordinates fall within a certain country are assigned the corresponding country code, not all records with a country code have coordinates attached. Additionally, the polygon defintion used by GBIF may be different to the one used by naturalearth.&lt;/p&gt;
&lt;h2 id=&#34;by-time-frame&#34;&gt;By Time-Frame&lt;/h2&gt;
&lt;div class=&#34;alert alert-info&#34;&gt;
  &lt;div&gt;
    By default, discovery of data through GBIF considers records obtained at any time. However, one may want to constrain data discovery to relevant time periods particularly when relating GBIF-mediated records to particular environmental conditions in time and space.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;single-year&#34;&gt;Single-Year&lt;/h3&gt;
&lt;p&gt;When interested in a single year or month of records being obtained, we can use the &lt;code&gt;year&lt;/code&gt; and &lt;code&gt;month&lt;/code&gt; arguments, respectively. Both take numeric inputs. Here, we are just looking at occurrences of &lt;em&gt;Lagopus muta&lt;/em&gt; throughout the year 2020:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;occ_year &amp;lt;- occ_search(
  taxonKey = sp_key,
  country = &amp;quot;NO&amp;quot;,
  year = 2020
)
occ_year$meta$count
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1031
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;time-window&#34;&gt;Time-Window&lt;/h3&gt;
&lt;p&gt;When investigating long-term trends and patterns of biodiversity, we are rarely concerned with a single year or month, but rather time-windows of data. These are also discovered using the &lt;code&gt;year&lt;/code&gt; and &lt;code&gt;month&lt;/code&gt; arguments. In addition to specifying strings with semicolons separating years, we can alternatively also specify a sequence of integers:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;occ_window &amp;lt;- occ_search(
  taxonKey = sp_key,
  country = &amp;quot;NO&amp;quot;,
  year = 2000:2020
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This returns a list of discovered data with each list element corresponding to one year worth of data. To sum up how many records are available in the time-window we thus need to run the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sum(unlist(lapply(occ_window, FUN = function(x) {
  x$meta$count
})))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10866
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the &lt;code&gt;occ_count(...)&lt;/code&gt; is easier in this example:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;occ_count(
  taxonKey = sp_key,
  country = &amp;quot;NO&amp;quot;,
  year = &amp;quot;2000,2020&amp;quot;
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10866
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Through time, the number of records develops like this:&lt;/p&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for code necessary to create the figure below.&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot_df &amp;lt;- data.frame(
  Year = 2000:2020,
  Records = unlist(lapply(occ_window, FUN = function(x) {
    x$meta$count
  })),
  Cumulative = cumsum(unlist(lapply(occ_window, FUN = function(x) {
    x$meta$count
  })))
)
ggplot(data = plot_df, aes(x = as.factor(Year), y = Cumulative)) +
  geom_bar(stat = &amp;quot;identity&amp;quot;, aes(fill = &amp;quot;black&amp;quot;)) +
  geom_bar(stat = &amp;quot;identity&amp;quot;, aes(y = Records, fill = &amp;quot;forestgreen&amp;quot;)) +
  theme_bw() +
  scale_fill_manual(
    name = &amp;quot;Records&amp;quot;,
    values = c(&amp;quot;black&amp;quot; = &amp;quot;black&amp;quot;, &amp;quot;forestgreen&amp;quot; = &amp;quot;forestgreen&amp;quot;), labels = c(&amp;quot;Cumulative Total&amp;quot;, &amp;quot;New per Year&amp;quot;)
  ) +
  theme(legend.position = c(0.15, 0.8), legend.key.size = unit(2, &amp;quot;cm&amp;quot;), legend.text = element_text(size = 20), legend.title = element_text(size = 25)) +
  labs(x = &amp;quot;Year&amp;quot;, y = &amp;quot;Records of Lagopus muta throughout Norway starting 2000&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;&lt;img src=&#34;rgbif-datadiscovery_files/figure-html/TimePlot-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;by-basis-of-record&#34;&gt;By Basis of Record&lt;/h2&gt;
&lt;div class=&#34;alert alert-info&#34;&gt;
  &lt;div&gt;
    Records indexed by GBIF can come from multiple sources of recording. Some may be much more relevant to specific study set-ups than others.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;human-observation&#34;&gt;Human Observation&lt;/h3&gt;
&lt;p&gt;First, let&amp;rsquo;s look at &lt;em&gt;Lagopus muta&lt;/em&gt; occurrences identified by human observation:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;occ_human &amp;lt;- occ_search(
  taxonKey = sp_key,
  country = &amp;quot;NO&amp;quot;,
  year = 2020,
  basisOfRecord = &amp;quot;HUMAN_OBSERVATION&amp;quot;
)
occ_human$meta$count
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1021
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These account for 99.03% of all &lt;em&gt;Lagopus muta&lt;/em&gt; observations in Norway throughout the year 2020. So, what might be the basis of record for the remaining 10 records?&lt;/p&gt;
&lt;h3 id=&#34;preserved-specimen&#34;&gt;Preserved Specimen&lt;/h3&gt;
&lt;p&gt;As it turns out, the remaining 10 records are based on preserved specimen:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;occ_preserved &amp;lt;- occ_search(
  taxonKey = sp_key,
  country = &amp;quot;NO&amp;quot;,
  year = 2020,
  basisOfRecord = &amp;quot;PRESERVED_SPECIMEN&amp;quot;
)
occ_preserved$meta$count
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These are probably not very useful for many ecological applications which would rather focus on observations of specimen directly.&lt;/p&gt;
&lt;h3 id=&#34;other-basis-of-record&#34;&gt;Other Basis&#39; of Record&lt;/h3&gt;
&lt;p&gt;There are additional characteristics for basis of record indexing. These are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;FOSSIL_SPECIMEN&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;&amp;ldquo;HUMAN_OBSERVATION&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;&amp;ldquo;MATERIAL_CITATION&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;&amp;ldquo;MATERIAL_SAMPLE&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;&amp;ldquo;LIVING_SPECIMEN&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;&amp;ldquo;MACHINE_OBSERVATION&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;&amp;ldquo;OBSERVATION&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;&amp;ldquo;PRESERVED_SPECIMEN&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;&amp;ldquo;OCCURRENCE&amp;rdquo;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Per country containing records of &lt;em&gt;Lagopus muta&lt;/em&gt;, the data split ends up looking like this:&lt;/p&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for code necessary to create the figure below.&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## query basis of record per country
basis_occ &amp;lt;- data.frame(basisofRecord = c(
  &amp;quot;FOSSIL_SPECIMEN&amp;quot;, &amp;quot;HUMAN_OBSERVATION&amp;quot;, &amp;quot;MATERIAL_CITATION&amp;quot;, &amp;quot;MATERIAL_SAMPLE&amp;quot;,
  &amp;quot;LIVING_SPECIMEN&amp;quot;, &amp;quot;MACHINE_OBSERVATION&amp;quot;, &amp;quot;OBSERVATION&amp;quot;, &amp;quot;PRESERVED_SPECIMEN&amp;quot;, &amp;quot;OCCURRENCE&amp;quot;
))
basis_occ &amp;lt;- expand.grid(basis_occ$basisofRecord, enumeration_country()$&amp;quot;iso2&amp;quot;)

basis_occ$Records &amp;lt;- pbapply(basis_occ, MARGIN = 1, FUN = function(x) {
  occ_search(
    taxonKey = sp_key,
    country = x[2],
    basisOfRecord = x[1]
  )$meta$count
})
basis_occ$Country &amp;lt;- enumeration_country()$&amp;quot;title&amp;quot;[match(basis_occ$Var2, enumeration_country()$&amp;quot;iso2&amp;quot;)]
basis_occ$Country[basis_occ$Country == &amp;quot;United Kingdom of Great Britain and Northern Ireland&amp;quot;] &amp;lt;- &amp;quot;United Kingdom&amp;quot;

## build summary data frame
links2 &amp;lt;- expand.grid(links$target, unique(basis_occ$Var1))
colnames(links2) &amp;lt;- c(&amp;quot;source&amp;quot;, &amp;quot;target&amp;quot;)
links2$value &amp;lt;- 0

links2$value &amp;lt;- unlist(apply(links2, MARGIN = 1, FUN = function(x) {
  # print(x)
  if (x[1] == &amp;quot;Others&amp;quot;) {
    aggregate(
      data = basis_occ[!(basis_occ$Country %in% nodes$name) &amp;amp; basis_occ$Var1 %in% as.character(x[2]), ],
      x = Records ~ 1, FUN = sum
    )
  } else {
    aggr_df &amp;lt;- basis_occ[basis_occ$Country %in% x[1] &amp;amp; basis_occ$Var1 %in% as.character(x[2]), ]
    if (nrow(aggr_df) == 0) {
      0
    } else {
      aggregate(
        data = aggr_df,
        x = Records ~ 1, FUN = sum
      )
    }
  }
}))

## build links and nodes
links_plot &amp;lt;- rbind(links[, 1:3], links2)
links_plot &amp;lt;- links_plot[links_plot$value != 0, ]

nodes_plot &amp;lt;- rbind(
  nodes,
  data.frame(
    name = unique(basis_occ$Var1),
    group = &amp;quot;c&amp;quot;
  )
)
nodes_plot &amp;lt;- nodes_plot[nodes_plot$name %in% c(links_plot$source, links_plot$target), ]

links_plot$IDsource &amp;lt;- match(links_plot$source, nodes_plot$name) - 1
links_plot$IDtarget &amp;lt;- match(links_plot$target, nodes_plot$name) - 1

## make sankey
my_color &amp;lt;- &#39;d3.scaleOrdinal() .range([&amp;quot;#FDE725FF&amp;quot;,&amp;quot;#1F9E89FF&amp;quot;,&amp;quot;#440154FF&amp;quot;])&#39;

SN &amp;lt;- sankeyNetwork(
  Links = links_plot, Nodes = nodes_plot,
  Source = &amp;quot;IDsource&amp;quot;, Target = &amp;quot;IDtarget&amp;quot;,
  Value = &amp;quot;value&amp;quot;, NodeID = &amp;quot;name&amp;quot;,
  colourScale = my_color, NodeGroup = &amp;quot;group&amp;quot;,
  sinksRight = FALSE, nodeWidth = 40, fontSize = 13, nodePadding = 5, margin = 2
)
networkD3::saveNetwork(network = SN, file = &amp;quot;SankeyBasis.html&amp;quot;, selfcontained = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;iframe seamless width=&#34;100%&#34; height = &#34;600px&#34; src=&#34;https://www.erikkusch.com/courses/gbif/SankeyBasis.html&#34; title=&#34;Sankey diagram of Lagopus muta records by country and basis of record&#34;&gt;&lt;/iframe&gt; 
&lt;p&gt;As should be plain to see from the list above, there exists some ambiguity in regards to which basis of record may apply to any single occurrence record. It is thus always best to carefully examine on which basis of record research projects should be based.&lt;/p&gt;
&lt;h2 id=&#34;occurrence-status&#34;&gt;Occurrence Status&lt;/h2&gt;
&lt;div class=&#34;alert alert-info&#34;&gt;
  &lt;div&gt;
    &amp;ldquo;Occurrence&amp;rdquo; records imply presence of an entity at a location in time and space. However, the GBIF-indexing of data does not constrain itself to this interpretation alone. Any data record may indicate presence or absence respectively.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;To avoid erroneous use of GBIF-mediated data, it is thus always necessary to be precise about what is understood by &amp;ldquo;occurrence&amp;rdquo;. This can be controlled with the &lt;code&gt;ocurrenceStatus&lt;/code&gt; argument.&lt;/p&gt;
&lt;h3 id=&#34;present&#34;&gt;Present&lt;/h3&gt;
&lt;p&gt;First, we ought to look at which occurrences actually report presence of our target organism:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;occ_present &amp;lt;- occ_search(
  taxonKey = sp_key,
  country = &amp;quot;NO&amp;quot;,
  year = 2020,
  basisOfRecord = &amp;quot;HUMAN_OBSERVATION&amp;quot;,
  occurrenceStatus = &amp;quot;PRESENT&amp;quot;
)
occ_present$meta$count
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1021
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Well, that is all of them.&lt;/p&gt;
&lt;h3 id=&#34;absent&#34;&gt;Absent&lt;/h3&gt;
&lt;p&gt;Therefore, we should find 0 records reporting absences given our additional limiting characteristics for data discovery:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;occ_absent &amp;lt;- occ_search(
  taxonKey = sp_key,
  country = &amp;quot;NO&amp;quot;,
  year = 2020,
  basisOfRecord = &amp;quot;HUMAN_OBSERVATION&amp;quot;,
  occurrenceStatus = &amp;quot;ABSENT&amp;quot;
)
occ_absent$meta$count
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That is indeed true.&lt;/p&gt;
&lt;p&gt;Are there even any records of absences of &lt;em&gt;Lagopus muta&lt;/em&gt;? Let&amp;rsquo;s check:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;occ_absent &amp;lt;- occ_search(
  taxonKey = sp_key,
  occurrenceStatus = &amp;quot;ABSENT&amp;quot;
)
occ_absent$meta$count
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2316
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Yes, there are, but there are far fewer reported absences than presences.&lt;/p&gt;
&lt;p&gt;Let me visualise this one final time on a country-by-country basis:&lt;/p&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for code necessary to create the figure below.&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## query basis of record per country
basis_occ &amp;lt;- expand.grid(
  c(&amp;quot;PRESENT&amp;quot;, &amp;quot;ABSENT&amp;quot;),
  c(
    &amp;quot;FOSSIL_SPECIMEN&amp;quot;, &amp;quot;HUMAN_OBSERVATION&amp;quot;, &amp;quot;MATERIAL_CITATION&amp;quot;, &amp;quot;MATERIAL_SAMPLE&amp;quot;,
    &amp;quot;LIVING_SPECIMEN&amp;quot;, &amp;quot;MACHINE_OBSERVATION&amp;quot;, &amp;quot;OBSERVATION&amp;quot;, &amp;quot;PRESERVED_SPECIMEN&amp;quot;, &amp;quot;OCCURRENCE&amp;quot;
  )
)

basis_occ$Records &amp;lt;- pbapply(basis_occ, MARGIN = 1, FUN = function(x) {
  occ_search(
    taxonKey = sp_key,
    basisOfRecord = x[2],
    occurrenceStatus = x[1]
  )$meta$count
})

## build summary data frame
colnames(basis_occ) &amp;lt;- c(&amp;quot;target&amp;quot;, &amp;quot;source&amp;quot;, &amp;quot;value&amp;quot;)

## build links and nodes
links_final &amp;lt;- rbind(links_plot[, 1:3], basis_occ)
links_final &amp;lt;- links_final[links_final$value != 0, ]

nodes_final &amp;lt;- rbind(
  nodes_plot,
  data.frame(
    name = unique(basis_occ$target),
    group = &amp;quot;d&amp;quot;
  )
)
nodes_final &amp;lt;- nodes_final[nodes_final$name %in% c(links_final$source, links_final$target), ]

links_final$IDsource &amp;lt;- match(links_final$source, nodes_final$name) - 1
links_final$IDtarget &amp;lt;- match(links_final$target, nodes_final$name) - 1

## make sankey
my_color &amp;lt;- &#39;d3.scaleOrdinal() .range([&amp;quot;#FDE725FF&amp;quot;,&amp;quot;#1F9E89FF&amp;quot;,&amp;quot;#440154FF&amp;quot;, &amp;quot;#6b0311&amp;quot;])&#39;

SN &amp;lt;- sankeyNetwork(
  Links = links_final, Nodes = nodes_final,
  Source = &amp;quot;IDsource&amp;quot;, Target = &amp;quot;IDtarget&amp;quot;,
  Value = &amp;quot;value&amp;quot;, NodeID = &amp;quot;name&amp;quot;,
  colourScale = my_color, NodeGroup = &amp;quot;group&amp;quot;,
  sinksRight = FALSE, nodeWidth = 40, fontSize = 13, nodePadding = 5, margin = 0,
  width = 1100, height = 600
)
networkD3::saveNetwork(network = SN, file = &amp;quot;SankeyFinal.html&amp;quot;, selfcontained = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.erikkusch.com/courses/gbif/SankeyFinal.png&#34; width=&#34;900&#34;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;iframe seamless width=&#34;100%&#34; height = &#34;600px&#34; src=&#34;https://www.erikkusch.com/courses/gbif/SankeyFinal.html&#34; title=&#34;Sankey diagram of Lagopus muta records by country and basis of record and whether presence or absence is recorded&#34;&gt;&lt;/iframe&gt; 
&lt;p&gt;&lt;strong&gt;Note for Firefox users:&lt;/strong&gt; Sankey diagrams are contained in a viewbox which scales poorly on Firefox. You can open this webpage in a different browser to avoid this issue. Alternatively, I have included a .png of this particular diagram in the code-fold above it.&lt;/p&gt;
&lt;h2 id=&#34;data-discovery-beyond-counts&#34;&gt;Data Discovery Beyond Counts&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;occ_search(...)&lt;/code&gt; function is useful for much more than &amp;ldquo;just&amp;rdquo; finding out how many GBIF mediated records fit your requirements.&lt;/p&gt;
&lt;p&gt;Let me demonstrate the richness of the output returned by &lt;code&gt;occ_search(...)&lt;/code&gt; with a simple example of &lt;em&gt;Lagopus muta&lt;/em&gt; focussed on Norway:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;occ_meta &amp;lt;- occ_search(taxonKey = sp_key, country = &amp;quot;NO&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let&amp;rsquo;s look at the structure of this object:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(occ_meta, max.level = 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## List of 5
##  $ meta     :List of 4
##  $ hierarchy:List of 1
##  $ data     : tibble [500 Ã 111] (S3: tbl_df/tbl/data.frame)
##  $ media    :List of 500
##  $ facets   : Named list()
##  - attr(*, &amp;quot;class&amp;quot;)= chr &amp;quot;gbif&amp;quot;
##  - attr(*, &amp;quot;args&amp;quot;)=List of 6
##  - attr(*, &amp;quot;type&amp;quot;)= chr &amp;quot;single&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;occ_search(...)&lt;/code&gt; returns this information:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;meta&lt;/code&gt; - this is the metadata which we have already used part of&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hierarchy&lt;/code&gt; - this is the taxonomic hierarchy for the &lt;code&gt;taxonKey&lt;/code&gt;(s) being queried&lt;/li&gt;
&lt;li&gt;&lt;code&gt;data&lt;/code&gt; - individual data records for our query (a maximum of 100,000 can be obtained per &lt;code&gt;occ_search(...)&lt;/code&gt; query)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;media&lt;/code&gt; - media metadata&lt;/li&gt;
&lt;li&gt;&lt;code&gt;facets&lt;/code&gt; - results can be faceted into individual lists using the &lt;code&gt;facet&lt;/code&gt; argument&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We will look at the downloaded data more explicitly in the next section of this workshop.&lt;/p&gt;
&lt;h2 id=&#34;additional-data-discovery-considerations&#34;&gt;Additional Data Discovery Considerations&lt;/h2&gt;
&lt;p&gt;The Darwin Core is large and there are many ways by which to discover different subsets of data left to explore. I leave it up to you, the reader to do so. A good place to start is the documentation for &lt;code&gt;occ_search(...)&lt;/code&gt; or the documentation of the 
&lt;a href=&#34;https://dwc.tdwg.org/list/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Darwin Core&lt;/a&gt; itself:&lt;/p&gt;
&lt;iframe src=&#34;https://dwc.tdwg.org/list/&#34; width=&#34;100%&#34; height=&#34;700&#34; data-external=&#34;1&#34;&gt;&lt;/iframe&gt;
&lt;div class=&#34;alert alert-success&#34;&gt;
  &lt;div&gt;
    You are now &lt;strong&gt;ready&lt;/strong&gt; to discover any data you require through &lt;code&gt;rgbif&lt;/code&gt;. Next, you will need to learn how to actually obtain that data.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;building-a-final-data-discovery-query&#34;&gt;Building a Final Data Discovery Query&lt;/h2&gt;
&lt;p&gt;To facilitate the rest of this workshop, let&amp;rsquo;s assume we are interested in all records of &lt;em&gt;Lagopus muta&lt;/em&gt; across Norway which have been made by humans and found our study organism to be present between and including 2000 and 2020.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;occ_final &amp;lt;- occ_search(
  taxonKey = sp_key,
  country = &amp;quot;NO&amp;quot;,
  year = &amp;quot;2000,2020&amp;quot;,
  facet = c(&amp;quot;year&amp;quot;), # facetting by year will break up the occurrence record counts
  year.facetLimit = 23, # this must either be the same number as facets needed or larger
  basisOfRecord = &amp;quot;HUMAN_OBSERVATION&amp;quot;,
  occurrenceStatus = &amp;quot;PRESENT&amp;quot;
)
knitr::kable(t(occ_final$facet$year))
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;name&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2018&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2019&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2020&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2017&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2016&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2015&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2014&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2012&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2013&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2011&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2010&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2009&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2008&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2007&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2005&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2006&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2004&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2001&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2003&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2000&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2002&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;count&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1156&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1100&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1021&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;865&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;842&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;841&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;774&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;757&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;691&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;600&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;503&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;354&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;269&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;148&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;114&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;110&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;102&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;95&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;92&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;89&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;83&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;How many records does this return to us? Let&amp;rsquo;s see:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;occ_count(
  taxonKey = sp_key,
  country = &amp;quot;NO&amp;quot;,
  year = &amp;quot;2000,2020&amp;quot;, # the comma here is an alternative way of specifying a range
  basisOfRecord = &amp;quot;HUMAN_OBSERVATION&amp;quot;,
  occurrenceStatus = &amp;quot;PRESENT&amp;quot;
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10606
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We could have also just summed up the facet counts, but it is good to remember this more direct function exists.&lt;/p&gt;
&lt;p&gt;Note that we have to change the way we sum the number of records as data discovery for any argument being matched by multiple characteristics generates an output of type list:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(occ_final, max.level = 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## List of 5
##  $ meta     :List of 4
##  $ hierarchy:List of 1
##  $ data     : tibble [500 Ã 117] (S3: tbl_df/tbl/data.frame)
##  $ media    :List of 500
##  $ facets   :List of 2
##  - attr(*, &amp;quot;class&amp;quot;)= chr &amp;quot;gbif&amp;quot;
##  - attr(*, &amp;quot;args&amp;quot;)=List of 10
##  - attr(*, &amp;quot;type&amp;quot;)= chr &amp;quot;single&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.4.0 (2024-04-24 ucrt)
## Platform: x86_64-w64-mingw32/x64
## Running under: Windows 11 x64 (build 22631)
## 
## Matrix products: default
## 
## 
## locale:
## [1] C
## 
## time zone: Europe/Oslo
## tzcode source: internal
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] pbapply_1.7-2       leaflet_2.2.2       htmlwidgets_1.6.4   networkD3_0.4      
## [5] ggplot2_3.5.1       sf_1.0-17           rnaturalearth_1.0.1 knitr_1.48         
## [9] rgbif_3.8.1        
## 
## loaded via a namespace (and not attached):
##  [1] gtable_0.3.6       xfun_0.47          bslib_0.8.0        vctrs_0.6.5       
##  [5] tools_4.4.0        crosstalk_1.2.1    generics_0.1.3     curl_5.2.2        
##  [9] parallel_4.4.0     tibble_3.2.1       proxy_0.4-27       fansi_1.0.6       
## [13] highr_0.11         pkgconfig_2.0.3    R.oo_1.26.0        KernSmooth_2.23-22
## [17] data.table_1.16.0  lifecycle_1.0.4    R.cache_0.16.0     farver_2.1.2      
## [21] compiler_4.4.0     stringr_1.5.1      munsell_0.5.1      terra_1.7-78      
## [25] codetools_0.2-20   htmltools_0.5.8.1  class_7.3-22       sass_0.4.9        
## [29] yaml_2.3.10        lazyeval_0.2.2     pillar_1.9.0       jquerylib_0.1.4   
## [33] whisker_0.4.1      R.utils_2.12.3     classInt_0.4-10    cachem_1.1.0      
## [37] wk_0.9.4           styler_1.10.3      tidyselect_1.2.1   digest_0.6.37     
## [41] stringi_1.8.4      dplyr_1.1.4        purrr_1.0.2        bookdown_0.40     
## [45] labeling_0.4.3     fastmap_1.2.0      grid_4.4.0         colorspace_2.1-1  
## [49] cli_3.6.3          magrittr_2.0.3     triebeard_0.4.1    crul_1.5.0        
## [53] utf8_1.2.4         e1071_1.7-16       withr_3.0.1        scales_1.3.0      
## [57] oai_0.4.0          rmarkdown_2.28     httr_1.4.7         igraph_2.1.1      
## [61] blogdown_1.19      R.methodsS3_1.8.2  evaluate_0.24.0    s2_1.1.7          
## [65] urltools_1.7.3     rlang_1.1.4        Rcpp_1.0.13        httpcode_0.3.0    
## [69] glue_1.7.0         DBI_1.2.3          xml2_1.3.6         jsonlite_1.8.8    
## [73] R6_2.5.1           plyr_1.8.9         units_0.8-5
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Preparing the Workshop</title>
      <link>https://www.erikkusch.com/courses/krigr/prep/</link>
      <pubDate>Thu, 26 May 2022 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/krigr/prep/</guid>
      <description>&lt;p&gt;&lt;div class=&#34;alert alert-danger&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;&lt;code&gt;KrigR&lt;/code&gt; is currently undergoing development. As a result, this part of the workshop has become deprecated. Please refer to the &lt;a href=&#34;https://www.erikkusch.com/courses/krigr/setup/&#34;&gt;setup&lt;/a&gt; &lt;a href=&#34;https://www.erikkusch.com/courses/krigr/quickstart/&#34;&gt;quick guide&lt;/a&gt; portions of this material as these are up-to-date. &lt;/strong&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;!-- # Preparing the Workshop --&gt;
&lt;h2 id=&#34;r-packages-for-the-workshop&#34;&gt;&lt;code&gt;R&lt;/code&gt; Packages for the Workshop&lt;/h2&gt;
&lt;p&gt;For the sake of this series of tutorials, we need some extra packages for visualisations. To load them, we use a custom function (&lt;code&gt;install.load.package()&lt;/code&gt;, see below). This function checks whether a package is already installed, subsequently install (if necessary) and loads the package. To carry this operation out for several packages, we simply apply it to a vector of package names using &lt;code&gt;sapply()&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.load.package &amp;lt;- function(x){
  if (!require(x, character.only = TRUE))
    install.packages(x, repos=&#39;http://cran.us.r-project.org&#39;)
  require(x, character.only = TRUE)
}
package_vec &amp;lt;- c(&amp;quot;tidyr&amp;quot;, # for turning rasters into ggplot-dataframes
                 &amp;quot;ggplot2&amp;quot;, # for plotting
                 &amp;quot;viridis&amp;quot;, # colour palettes
                 &amp;quot;cowplot&amp;quot;, # gridding multiple plots
                 &amp;quot;ggmap&amp;quot;, # obtaining satellite maps
                 &amp;quot;gimms&amp;quot;, # to get some pre-existing data to match in our downscaling
                 &amp;quot;rnaturalearth&amp;quot;, # for shapefiles
                 &amp;quot;rnaturalearthdata&amp;quot;, # for high-resolution shapefiles
                 &amp;quot;mapview&amp;quot; # for generating mapview outputs
                 )
sapply(package_vec, install.load.package)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             tidyr           ggplot2           viridis           cowplot             ggmap 
##              TRUE              TRUE              TRUE              TRUE              TRUE 
##             gimms     rnaturalearth rnaturalearthdata           mapview 
##              TRUE              TRUE              TRUE              TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;setting-up-directories&#34;&gt;Setting up Directories&lt;/h2&gt;
&lt;p&gt;The workshop is designed to run completely from scratch. For this to work in a structured way, we create a folder/directory structure so that we got some nice compartments on our hard drives. We create the following directories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;strong&gt;Data&lt;/strong&gt; directory for all of our data downloads&lt;/li&gt;
&lt;li&gt;A &lt;strong&gt;Covariate&lt;/strong&gt; directory for all of our covariate data&lt;/li&gt;
&lt;li&gt;An &lt;strong&gt;Exports&lt;/strong&gt; directory for all of our Kriging outputs&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Dir.Base &amp;lt;- getwd() # identifying the current directory
Dir.Data &amp;lt;- file.path(Dir.Base, &amp;quot;Data&amp;quot;) # folder path for data
Dir.Covariates &amp;lt;- file.path(Dir.Base, &amp;quot;Covariates&amp;quot;) # folder path for covariates
Dir.Exports &amp;lt;- file.path(Dir.Base, &amp;quot;Exports&amp;quot;) # folder path for exports
## create directories, if they don&#39;t exist yet
Dirs &amp;lt;- sapply(c(Dir.Data, Dir.Covariates, Dir.Exports), 
               function(x) if(!dir.exists(x)) dir.create(x))
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;visualiation-functions&#34;&gt;Visualiation Functions&lt;/h2&gt;
&lt;p&gt;In order to easily visualise our Kriging procedure including (1) inputs, (2) covariates, and (3) outputs without repeating too much of the same code, we have prepared some plotting functions which you can download as 
&lt;a href=&#34;https://raw.githubusercontent.com/ErikKusch/Homepage/master/content/courses/krigr/FUN_Plotting.R&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FUN_Plotting.R&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;With the &lt;code&gt;FUN_Plotting.R&lt;/code&gt; file placed in the project directory of your workshop material (i.e., the directory returned by &lt;code&gt;Dir.Base&lt;/code&gt;), running the following will register the three plotting functions in your &lt;code&gt;R&lt;/code&gt; environment.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;source(&amp;quot;FUN_Plotting.R&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The plotting functions you have just loaded are called:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Plot_Raw()&lt;/code&gt; - we will use this function to visualise data downloaded with &lt;code&gt;KrigR&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Plot_Covs()&lt;/code&gt; - this function will help us visualise the covariates we use for statistical interpolation&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Plot_Krigs()&lt;/code&gt; - kriged products and their associated uncertainty will be visualised using this function&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;alert alert-normal&#34;&gt;
  &lt;div&gt;
    Donât worry about understanding how these functions work off the bat here. Kriging and the package &lt;code&gt;KrigR&lt;/code&gt; are what we want to demonstrate here - not visualisation strategies.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;locations-of-interest&#34;&gt;Locations of Interest&lt;/h2&gt;
&lt;h3 id=&#34;our-workshop-target-region&#34;&gt;Our Workshop Target Region&lt;/h3&gt;
&lt;p&gt;To keep this workshop material concise and make it so you don&amp;rsquo;t need access to a server of cluster throughout the following demonstrations of &lt;code&gt;KrigR&lt;/code&gt;, we will specify a set of locations in which we are interested.&lt;/p&gt;
&lt;p&gt;The locations we focus on for this workshop are situated throughout eastern Germany and the north-western parts of the Czech Republic. Why do we focus on this particular part of the Earth? There are three reasons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Topographical Heterogeneity&lt;/em&gt; - the area we select here contains large swaths of flat lowlands as well as some mountain ridges. This will make for visually pleasing plots and highlight the capability of kriging.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Geographic Scale&lt;/em&gt; - the area we are selecting here hits a certain sweet-spot for our purposes as its size makes it so that all &lt;code&gt;KrigR&lt;/code&gt; functions run to completion in a relatively short time.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Familiarity&lt;/em&gt; - I was born and grew up in this region and have fond memories of the place. Please excuse my indulging in a bit of nostalgia.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;alert alert-danger&#34;&gt;
  &lt;div&gt;
    Change the locations of interest at your own risk.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Using a different set of locations than the ones we specify here will change computational load and time as well as disk space required when working through the workshop material.&lt;/p&gt;
&lt;div class=&#34;alert alert-info&#34;&gt;
  &lt;div&gt;
    &lt;code&gt;KrigR&lt;/code&gt; will be able to get you the data you want for the locations you desire, but computational requirements will vary.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;spatial-preferences-in-krigr&#34;&gt;Spatial Preferences in &lt;code&gt;KrigR&lt;/code&gt;&lt;/h3&gt;
&lt;div class=&#34;alert alert-info&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;code&gt;KrigR&lt;/code&gt; is capable of learning about your spatial preferences in three ways:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;As an &lt;code&gt;extent&lt;/code&gt; input (a rectangular box).&lt;/li&gt;
&lt;li&gt;As a &lt;code&gt;SpatialPolygons&lt;/code&gt; input (a polygon or set of polygons).&lt;/li&gt;
&lt;li&gt;As a set of locations stored in a &lt;code&gt;data.frame&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;To demonstrate the range of specifications permitted in &lt;code&gt;KrigR&lt;/code&gt;, we make use of all three specifications. As we will see in this tutorial, masking out unnecessary areas from our analyses speeds up Kriging tremendously hence why we strongly suggest you make use of &lt;code&gt;SpatialPolygons&lt;/code&gt; or &lt;code&gt;data.frames&lt;/code&gt; whenever possible.&lt;/p&gt;
&lt;h4 id=&#34;area-of-interest-extent&#34;&gt;Area of Interest (&lt;code&gt;extent&lt;/code&gt;)&lt;/h4&gt;
&lt;p&gt;The simplest way in which you can run the functions of the &lt;code&gt;KrigR&lt;/code&gt; package is by specifying a rectangular bounding box (i.e., an &lt;code&gt;extent&lt;/code&gt;) to specify your study region(s). We simply specify the longitude and latitude ranges and store the object as an &lt;code&gt;extent&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Extent_ext &amp;lt;- extent(c(9.87, 15.03, 49.89, 53.06))
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;shape-of-interest-spatialpolygons&#34;&gt;Shape of Interest (&lt;code&gt;SpatialPolygons&lt;/code&gt;)&lt;/h4&gt;
&lt;p&gt;To define &lt;code&gt;SpatialPolygons&lt;/code&gt; for our purposes, I make use of the 
&lt;a href=&#34;https://www.naturalearthdata.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NaturalEarthData&lt;/a&gt;. Here, I select a set of polygons corresponding to some states in Germany and the Czech Republic:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Shape_shp &amp;lt;- ne_states(country = c(&amp;quot;Germany&amp;quot;, &amp;quot;Czech Republic&amp;quot;))
Shape_shp &amp;lt;- Shape_shp[Shape_shp$name_en %in% c(&amp;quot;Saxony&amp;quot;, &amp;quot;Saxony-Anhalt&amp;quot;, &amp;quot;Thuringia&amp;quot;, 
                                                &amp;quot;ÃstÃ­ nad Labem Region&amp;quot;, &amp;quot;Karlovy Vary Region&amp;quot;), ]
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    The above requires the &lt;code&gt;naturalhighres&lt;/code&gt; package which can give some users troubles.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Here&amp;rsquo;s a workaround if &lt;code&gt;naturalhighres&lt;/code&gt; does not work for you:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;download.file(&amp;quot;https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_admin_1_states_provinces.zip&amp;quot;,
              destfile = &amp;quot;highres.zip&amp;quot;)
unzip(&amp;quot;highres.zip&amp;quot;)
Shape_shp &amp;lt;- readOGR(&amp;quot;ne_10m_admin_1_states_provinces.shp&amp;quot;)
Shape_shp &amp;lt;- Shape_shp[Shape_shp$name_en %in% c(&amp;quot;Saxony&amp;quot;, &amp;quot;Saxony-Anhalt&amp;quot;, &amp;quot;Thuringia&amp;quot;,
                                                &amp;quot;ÃÂÃÂ¡stÃÂÃÂ­ nad Labem&amp;quot;, &amp;quot;Karlovy Vary&amp;quot;), ]
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;points-of-interest-dataframe&#34;&gt;Points of Interest (&lt;code&gt;data.frame&lt;/code&gt;)&lt;/h4&gt;
&lt;p&gt;Finally, to represent specific points of interest, I have prepared a small data set of mountains for each state in the shapefile above. You can download this file here: 
&lt;a href=&#34;https://github.com/ErikKusch/Homepage/raw/master/content/courses/krigr/Data/Mountains_df.RData&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mountains_df.RData&lt;/a&gt;. Simply place this file into your data directory and continue the workshop.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s load this data set and quickly visualise it:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;load(file.path(Dir.Data, &amp;quot;Mountains_df.RData&amp;quot;)) # load an sp object called Mountains_sp
Mountains_df
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          Mountain      Lon      Lat
## 1     Fichtelberg 12.95472 50.42861
## 2         Brocken 10.61722 51.80056
## 3 GroÃer Beerberg 10.74611 50.65944
## 4        MeluzÃ­na 13.00778 50.39028
## 5       MileÅ¡ovka 13.93153 50.55523
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-normal&#34;&gt;
  &lt;div&gt;
    We now have all of our objects for spatial preferences ready for the workshop.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;visualising-our-study-setting&#34;&gt;Visualising our Study Setting&lt;/h3&gt;
&lt;p&gt;To finish our preparations for this workshop, let&amp;rsquo;s visualise the different locations of interest:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Establish rectangular bounding box from extent
bbox &amp;lt;- as.numeric(as(Extent_ext, &#39;SpatialPolygons&#39;)@bbox)
names(bbox) &amp;lt;- c(&amp;quot;left&amp;quot;, &amp;quot;bottom&amp;quot;, &amp;quot;right&amp;quot;, &amp;quot;top&amp;quot;)

## Make locations of mountains into SpatialPoints
Mountains_sp &amp;lt;- Mountains_df
coordinates(Mountains_sp) &amp;lt;- ~Lon+Lat

## download a map of the area specified by the extent
back_gg &amp;lt;- get_map(bbox, maptype = &#39;terrain&#39;)

## combine locations of interest into one plot
ggmap(back_gg, extent = &amp;quot;device&amp;quot;) + # plot the extent area
  ## display the SpatialPolygons area
  geom_polygon(aes(x = long, y = lat, group = id), data = fortify(Shape_shp),
               colour = &#39;black&#39;, size = 1, fill = &#39;black&#39;, alpha = .5) + 
  ## add the data.frame data
  geom_point(aes(x = Lon, y = Lat), data = data.frame(Mountains_sp), 
             colour = &amp;quot;red&amp;quot;, size = 4, pch = 13) + 
  ## some style additions
  theme_bw() + labs(x= &amp;quot;Longitude [Â°]&amp;quot;, y = &amp;quot;Latitude  [Â°]&amp;quot;) + 
  theme(plot.margin=unit(c(0, 1, 0, 1),&amp;quot;lines&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-locations_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the above figure, the map area designates the &lt;code&gt;extent&lt;/code&gt; specifications while the grey overlay display the &lt;code&gt;SpatialPolygons&lt;/code&gt; preference and points of interest (form our &lt;code&gt;data.frame&lt;/code&gt; input) are highlighted with red plotting symbols.&lt;/p&gt;
&lt;div class=&#34;alert alert-normal&#34;&gt;
  &lt;div&gt;
    We are now ready to start the &lt;code&gt;KrigR&lt;/code&gt; portion of the workshop!
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.5 (2021-03-31)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19043)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.1252  LC_CTYPE=English_United Kingdom.1252   
## [3] LC_MONETARY=English_United Kingdom.1252 LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.1252    
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] mapview_2.10.2          rnaturalearthdata_0.1.0 rnaturalearth_0.1.0    
##  [4] gimms_1.2.0             ggmap_3.0.0             cowplot_1.1.1          
##  [7] viridis_0.6.0           viridisLite_0.4.0       ggplot2_3.3.6          
## [10] tidyr_1.1.3             KrigR_0.1.2             httr_1.4.2             
## [13] stars_0.5-3             abind_1.4-5             fasterize_1.0.3        
## [16] sf_1.0-0                lubridate_1.7.10        automap_1.0-14         
## [19] doSNOW_1.0.19           snow_0.4-3              doParallel_1.0.16      
## [22] iterators_1.0.13        foreach_1.5.1           rgdal_1.5-23           
## [25] raster_3.4-13           sp_1.4-5                stringr_1.4.0          
## [28] keyring_1.2.0           ecmwfr_1.3.0            ncdf4_1.17             
## 
## loaded via a namespace (and not attached):
##  [1] bitops_1.0-7             satellite_1.0.2          xts_0.12.1              
##  [4] webshot_0.5.2            tools_4.0.5              bslib_0.3.1             
##  [7] utf8_1.2.1               R6_2.5.0                 zyp_0.10-1.1            
## [10] KernSmooth_2.23-18       DBI_1.1.1                colorspace_2.0-0        
## [13] withr_2.4.2              tidyselect_1.1.0         gridExtra_2.3           
## [16] leaflet_2.0.4.1          curl_4.3.2               compiler_4.0.5          
## [19] leafem_0.1.3             gstat_2.0-7              labeling_0.4.2          
## [22] bookdown_0.22            sass_0.4.1               scales_1.1.1            
## [25] classInt_0.4-3           proxy_0.4-25             digest_0.6.27           
## [28] rmarkdown_2.14           base64enc_0.1-3          jpeg_0.1-8.1            
## [31] pkgconfig_2.0.3          htmltools_0.5.2          highr_0.9               
## [34] fastmap_1.1.0            htmlwidgets_1.5.3        rlang_0.4.11            
## [37] FNN_1.1.3                farver_2.1.0             jquerylib_0.1.4         
## [40] generics_0.1.0           zoo_1.8-9                jsonlite_1.7.2          
## [43] crosstalk_1.1.1          dplyr_1.0.5              magrittr_2.0.1          
## [46] Rcpp_1.0.7               munsell_0.5.0            fansi_0.4.2             
## [49] lifecycle_1.0.0          stringi_1.5.3            yaml_2.2.1              
## [52] plyr_1.8.6               grid_4.0.5               crayon_1.4.1            
## [55] lattice_0.20-41          knitr_1.33               pillar_1.6.0            
## [58] boot_1.3-27              rjson_0.2.20             spacetime_1.2-4         
## [61] stats4_4.0.5             codetools_0.2-18         glue_1.4.2              
## [64] evaluate_0.14            blogdown_1.3             vctrs_0.3.7             
## [67] png_0.1-7                RgoogleMaps_1.4.5.3      gtable_0.3.0            
## [70] purrr_0.3.4              reshape_0.8.8            assertthat_0.2.1        
## [73] cachem_1.0.4             xfun_0.31                lwgeom_0.2-6            
## [76] e1071_1.7-6              rnaturalearthhires_0.2.0 class_7.3-18            
## [79] Kendall_2.2              tibble_3.1.1             intervals_0.15.2        
## [82] memoise_2.0.0            units_0.7-2              ellipsis_0.3.2
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Data Handling and Data Assumptions</title>
      <link>https://www.erikkusch.com/courses/excursions-into-biostatistics/data-handling-and-data-assumptions/</link>
      <pubDate>Sat, 27 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/excursions-into-biostatistics/data-handling-and-data-assumptions/</guid>
      <description>&lt;h2 id=&#34;theory&#34;&gt;Theory&lt;/h2&gt;
&lt;p&gt;These are exercises and solutions meant as a compendium to my talk on Model Selection and Model Building.&lt;/p&gt;
&lt;p&gt;I have prepared some &lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/Excursions-into-Biostatistics/Data-Handling-and-Assumptions---Making-the-Most-of-Your-Data.html&#34; target=&#34;_blank&#34;&gt; Lecture Slides &lt;/a&gt; for this session.&lt;/p&gt;
&lt;h2 id=&#34;exercise&#34;&gt;Exercise&lt;/h2&gt;
&lt;p&gt;First, imagine we have been out and about collecting samples for our sparrow populations. You can find the data came home with after our field work season &lt;a href=&#34;https://github.com/ErikKusch/Homepage/raw/master/content/courses/excursions-into-biostatistics/Data.rar&#34; target=&#34;_blank&#34;&gt; here&lt;/a&gt;. This data set contains errors/mis-specified data entry and other slip-ups that can happen as a part of data collection exercises. We need to fix that.&lt;/p&gt;
&lt;h3 id=&#34;preparing-our-procedure&#34;&gt;Preparing Our Procedure&lt;/h3&gt;
&lt;p&gt;The following three sections are what I consider to be &lt;em&gt;essential&lt;/em&gt; parts of the preamble to any &lt;code&gt;R&lt;/code&gt;-based analysis. I highly recommend clearly indicating these bits in your code.&lt;/p&gt;
&lt;p&gt;More often than not, you will use variations of these code chunks whether you are working on data handling, data exploration or full-fledged statistical analyses.&lt;/p&gt;
&lt;h3 id=&#34;necessary-steps-for-reproducibility&#34;&gt;Necessary Steps For Reproducibility&lt;/h3&gt;
&lt;p&gt;Reproducibility is the be-all and end-all of any statistical analysis, particularly in light of the peer-review process in life sciences.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rm(list = ls()) # clearing environment
Dir.Base &amp;lt;- getwd() # soft-coding our working directory
Dir.Data &amp;lt;- paste(Dir.Base, &amp;quot;Data&amp;quot;, sep = &amp;quot;/&amp;quot;) # soft-coding our data directory
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once you get into highly complex statistical analyses, you may wish to break up chunks of your analysis into separate documents. To ensure that remnants of an earlier analysis or analysis chunk do not influence the results of your current analysis, you may wish to &lt;em&gt;empty&lt;/em&gt; &lt;code&gt;R&lt;/code&gt;&amp;rsquo;s cache (&lt;em&gt;Environment&lt;/em&gt;) before attempting a new analysis. This is achieved via the command &lt;code&gt;rm(list=ls())&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Next, you &lt;em&gt;need&lt;/em&gt; to remember the importance of &lt;em&gt;soft-coding&lt;/em&gt; for the sake of reproducibility. One of the worst offences to the peer-review process in &lt;code&gt;R&lt;/code&gt;-based statistics is the erroneous hard-coding of the working directory. The &lt;code&gt;getwd()&lt;/code&gt; function shown above solves this exact problem. However, for this workaround to function properly you need to open the code document of interest by double-clicking it within its containing folder.&lt;/p&gt;
&lt;p&gt;When using the &lt;code&gt;xlsx&lt;/code&gt; package or any &lt;em&gt;Excel&lt;/em&gt;-reliant process via &lt;code&gt;R&lt;/code&gt;, your code will automatically run a Java process in the background. By default the Java engine is limited as far as RAM allocation goes and tends to fail when faced with enormous data sets. The workaround &lt;code&gt;options(java.parameters = &amp;quot;-Xmx8g&amp;quot;)&lt;/code&gt; gets rid of this issue by allocation 8 GBs of RAM to Java.&lt;/p&gt;
&lt;h3 id=&#34;packages&#34;&gt;Packages&lt;/h3&gt;
&lt;p&gt;Packages are &lt;code&gt;R&lt;/code&gt;&amp;rsquo;s way of giving you access to a seemingly infinite repository of functions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# function to load packages and install them if they haven&#39;t been installed yet
install.load.package &amp;lt;- function(x) {
  if (!require(x, character.only = TRUE)) {
    install.packages(x)
  }
  require(x, character.only = TRUE)
}
package_vec &amp;lt;- c(
  &amp;quot;dplyr&amp;quot; # we need this package to fix the most common data errors
)
sapply(package_vec, install.load.package)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## dplyr 
##  TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the above function is way more sophisticated than the usual &lt;code&gt;install.packages()&lt;/code&gt; + &lt;code&gt;library()&lt;/code&gt; approach since it automatically detects which packages require installing and only install these thus not overwriting already installed packages.&lt;/p&gt;
&lt;h3 id=&#34;loading-the-data&#34;&gt;Loading The Data&lt;/h3&gt;
&lt;p&gt;Loading data is crucial to any analysis in &lt;code&gt;R&lt;/code&gt;. Period.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;R&lt;/code&gt; offers a plethora of approaches to data loading and you will usually be taught the &lt;code&gt;read.table()&lt;/code&gt; command in basic biostatistics courses. However, I have found to prefer the functionality provided by the &lt;code&gt;xlsx&lt;/code&gt; package since most data recording is taking place in Excel. As this package is dependant on the installation of Java and &lt;code&gt;RJava&lt;/code&gt;, we will settle on the base &lt;code&gt;R&lt;/code&gt; function &lt;code&gt;read.csv()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df_base &amp;lt;- read.csv(file = paste(Dir.Data, &amp;quot;/SparrowData.csv&amp;quot;, sep = &amp;quot;&amp;quot;), header = TRUE)
Data_df &amp;lt;- Data_df_base # duplicate and save initial data on a new object
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another trick to have up your sleeve (if your RAM enables you to act on it) is to duplicate your initial data onto a new object once loaded into &lt;code&gt;R&lt;/code&gt;. This will enable you to easily remedy mistakes in data treatment without having to reload your initial data set from the data file.&lt;/p&gt;
&lt;h2 id=&#34;inspecting-the-data&#34;&gt;Inspecting The Data&lt;/h2&gt;
&lt;p&gt;Once the data is loaded into &lt;code&gt;R&lt;/code&gt;, you &lt;em&gt;need to inspect&lt;/em&gt; it to make sure it is ready for use.&lt;/p&gt;
&lt;h3 id=&#34;assessing-a-data-frame-in-r&#34;&gt;Assessing A Data Frame in &lt;code&gt;R&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Most, if not all, data you will ever load into &lt;code&gt;R&lt;/code&gt; will be stored as a &lt;code&gt;data.frame&lt;/code&gt; within &lt;code&gt;R&lt;/code&gt;. Some of the most important functions for inspecting data frames (&amp;ldquo;df&amp;rdquo; in the following) in base &lt;code&gt;R&lt;/code&gt; are the following four:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;dim(df)&lt;/code&gt; returns the dimensions (Rows x Columns)of the data frame&lt;/li&gt;
&lt;li&gt;&lt;code&gt;head(df)&lt;/code&gt; returns the first 6 rows of the data frame by default (here changed to 4)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tail(df)&lt;/code&gt; returns the last 6 rows of the data frame by default (here changed to 4)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;View(df)&lt;/code&gt; opens nearly any &lt;code&gt;R&lt;/code&gt; object in a separate tab for further inspection. Since we are dealing with an enormous data set here, I will exclude this function for now to save you from printing unnecessary pages.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dim(Data_df)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1068   21
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;head(Data_df, n = 4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   X    Site Index Latitude Longitude     Climate Population.Status Weight Height Wing.Chord Colour    Sex Nesting.Site Nesting.Height Number.of.Eggs Egg.Weight Flock Home.Range Flock.Size
## 1 1 Siberia    SI       60       100 Continental            Native  34,05  12.87       6.67  Brown   Male         &amp;lt;NA&amp;gt;             NA             NA         NA     B      Large         16
## 2 2 Siberia    SI       60       100 Continental            Native  34,86  13.68       6.79   Grey   Male         &amp;lt;NA&amp;gt;             NA             NA         NA     B      Large         16
## 3 3 Siberia    SI       60       100 Continental            Native  32,34  12.66       6.64  Black Female        Shrub           35.6              1       3.21     C      Large         14
## 4 4 Siberia    SI       60       100 Continental            Native  34,78  15.09       7.00  Brown Female        Shrub          47.75              0         NA     E      Large         10
##   Predator.Presence Predator.Type
## 1               Yes         Avian
## 2               Yes         Avian
## 3               Yes         Avian
## 4               Yes         Avian
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;tail(Data_df, n = 4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         X           Site Index Latitude Longitude Climate Population.Status Weight Height Wing.Chord Colour  Sex Nesting.Site Nesting.Height Number.of.Eggs Egg.Weight Flock Home.Range Flock.Size
## 1065 1065 Falkland Isles    FI   -51.75    -59.17 Coastal        Introduced  34.25  15.26       7.04   Grey Male                                                           A      Large         19
## 1066 1066 Falkland Isles    FI   -51.75    -59.17 Coastal        Introduced  31.76  12.78       6.67   Grey Male                                                           A      Large         19
## 1067 1067 Falkland Isles    FI   -51.75    -59.17 Coastal        Introduced  31.48  12.49       6.63  Black Male                                                           C      Large         18
## 1068 1068 Falkland Isles    FI   -51.75    -59.17 Coastal        Introduced  31.94  12.96       6.70   Grey Male                                                           A      Large         19
##      Predator.Presence Predator.Type
## 1065               Yes         Avian
## 1066               Yes         Avian
## 1067               Yes         Avian
## 1068               Yes         Avian
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When having an initial look at the results of &lt;code&gt;head(Data_df)&lt;/code&gt; and &lt;code&gt;tail(Data_df)&lt;/code&gt; we can spot two important things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;NA&lt;/code&gt;s in head and tail of our data set are stored differently. This is a common problem with biological data sets and we will deal with this issue extensively in the next few sections of this document.&lt;/li&gt;
&lt;li&gt;Due to our data loading procedure we ended up with a redundant first column that is simply showing the respective row numbers. However, this is unnecessary in &lt;code&gt;R&lt;/code&gt; and so we can delete this column as seen below.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df &amp;lt;- Data_df[, -1] # eliminating the erroneous first column as it is redundant
dim(Data_df) # checking if the elimination went right
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1068   20
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;the-summary-function&#34;&gt;The &lt;code&gt;Summary()&lt;/code&gt; Function&lt;/h3&gt;
&lt;p&gt;As already stated in our seminar series, the &lt;code&gt;summary()&lt;/code&gt; function is &lt;em&gt;invaluable&lt;/em&gt; to data exploration and data inspection. However, it is only partially applicable as it will not work flawlessly on every class of data. Examples of this are shown below.&lt;/p&gt;
&lt;p&gt;The weight data contained within our data frame should be numeric and thus pose no issue to the &lt;code&gt;summary()&lt;/code&gt; function. However, as shown in the next section, it is currently of type character which leads the &lt;code&gt;summary()&lt;/code&gt; function to work improperly.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Weight)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1068 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The height data within our data set, on the other hand, is stored correctly as class numeric. Thus the &lt;code&gt;summary()&lt;/code&gt; function performs flawlessly.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Height)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    1.35   13.52   14.52   15.39   16.22  135.40
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Making data inspection more easy, one may which to automate the use of the &lt;code&gt;summary()&lt;/code&gt; function. However, this only makes sense, when every data column is presenting data in the correct class type. Therefore, we will first fix the column classes and then use the &lt;code&gt;summary()&lt;/code&gt; command.&lt;/p&gt;
&lt;h2 id=&#34;data-cleaning-workflow&#34;&gt;Data Cleaning Workflow&lt;/h2&gt;
&lt;h3 id=&#34;identifying-problems&#34;&gt;Identifying Problems&lt;/h3&gt;
&lt;p&gt;Indentifying most problems in any data set you may ever encounter comes down to mostly two manifestations of inadequate data entry or handling:&lt;/p&gt;
&lt;p&gt;**1. Types/Classes  **&lt;br&gt;
Before even opening a data set, we should know what kind of data classes we expect for every variable (for example, height records as a &lt;code&gt;factor&lt;/code&gt; don&amp;rsquo;t make much sense). Problems with data/variable classes can have lasting influence on your analyses and so we need to test the class for each variable (column) individually. Before we alter any column classes, we will first need to identify columns whose classes need fixing. Doing so is as easy applying the &lt;code&gt;class()&lt;/code&gt; function to the data contained within every column of our data frame separately.&lt;br&gt;
&lt;code&gt;R&lt;/code&gt; offers multiple functions for this but I find the &lt;code&gt;lapply()&lt;/code&gt; function to perform flawlessly as shown below. Since &lt;code&gt;lapply()&lt;/code&gt; returns a &lt;code&gt;list&lt;/code&gt; of class identifiers and these don&amp;rsquo;t translate well to paper, I have opted to transform the list into a named character vector using the &lt;code&gt;unlist()&lt;/code&gt; command. One could also use the &lt;code&gt;str()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;unlist(lapply(Data_df, class))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              Site             Index          Latitude         Longitude           Climate Population.Status            Weight            Height        Wing.Chord            Colour               Sex 
##       &amp;quot;character&amp;quot;       &amp;quot;character&amp;quot;         &amp;quot;numeric&amp;quot;         &amp;quot;numeric&amp;quot;       &amp;quot;character&amp;quot;       &amp;quot;character&amp;quot;       &amp;quot;character&amp;quot;         &amp;quot;numeric&amp;quot;         &amp;quot;numeric&amp;quot;       &amp;quot;character&amp;quot;       &amp;quot;character&amp;quot; 
##      Nesting.Site    Nesting.Height    Number.of.Eggs        Egg.Weight             Flock        Home.Range        Flock.Size Predator.Presence     Predator.Type 
##       &amp;quot;character&amp;quot;       &amp;quot;character&amp;quot;       &amp;quot;character&amp;quot;       &amp;quot;character&amp;quot;       &amp;quot;character&amp;quot;       &amp;quot;character&amp;quot;         &amp;quot;integer&amp;quot;       &amp;quot;character&amp;quot;       &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For further inspection, one may want to combine the information obtained by using the &lt;code&gt;class()&lt;/code&gt; function with either the &lt;code&gt;summary()&lt;/code&gt; function (for all non-numeric records) or the &lt;code&gt;hist&lt;/code&gt; function (particularly useful for numeric records).&lt;/p&gt;
&lt;p&gt;**2. Contents/Values  **&lt;br&gt;
Typos and the like will always lead to some data that simply doesn&amp;rsquo;t make sense given the context of your project. Sometimes, errors like these are salvageable but doing so can be a very difficult process. Before we alter any column contents, we will first need to identify columns whose contents need fixing, however. Doing so is as easy applying an automated version of &lt;code&gt;summary()&lt;/code&gt; to the data contained within every column of our data frame separately after having fixed possibly erroneous data classes.&lt;/p&gt;
&lt;h3 id=&#34;fixing-the-problems&#34;&gt;Fixing The Problems&lt;/h3&gt;
&lt;p&gt;Fixing the problems in our data sets always comes down to altering data classes, altering faulty values or removing them entirely.&lt;br&gt;
To make sure we fix all problems, we may often wish to enlist the &lt;code&gt;summary()&lt;/code&gt; function as well as the &lt;code&gt;hist()&lt;/code&gt; function for data inspection and visualisation.&lt;/p&gt;
&lt;p&gt;Before we alter any column contents, we will first need to identify columns whose contents need fixing.&lt;/p&gt;
&lt;!-- Doing so is as easy applying an automated version of `summary()` to the data contained within every column of our data frame separately which is now possible since we have fixed the column types.   --&gt;
&lt;!-- The code below does exactly that: --&gt;
&lt;!-- ```{r ColContProblems} --&gt;
&lt;!-- for(i in 1:dim(Data_df)[2]){ --&gt;
&lt;!--   print(colnames(Data_df)[i]) --&gt;
&lt;!--   print(summary(Data_df[,i])) --&gt;
&lt;!--   print(&#34;------------------------------------------------------&#34;) --&gt;
&lt;!-- } --&gt;
&lt;!-- ``` --&gt;
&lt;!-- There are some glaring issues her which we will address in the following sections. --&gt;
&lt;h2 id=&#34;our-data&#34;&gt;Our Data&lt;/h2&gt;
&lt;h3 id=&#34;site&#34;&gt;Site&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;factor&lt;/code&gt; (only 11 possible values)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-1&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Site records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Site)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Site)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1068 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, they do behave just like we&amp;rsquo;d expect them to.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;We don&amp;rsquo;t need to fix anything here.&lt;/p&gt;
&lt;h3 id=&#34;index&#34;&gt;Index&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;factor&lt;/code&gt; (only 11 possible values)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-2&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Index records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Index)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Index)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1068 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, they do behave just like we&amp;rsquo;d expect them to. Pay attention that thes shortened index numbers lign up with the numbers of site records!&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-1&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;We don&amp;rsquo;t need to fix anything here.&lt;/p&gt;
&lt;p&gt;\newpage&lt;/p&gt;
&lt;h3 id=&#34;latitude&#34;&gt;Latitude&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;numeric&lt;/code&gt; (Latitude is inherently continuous)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-3&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Latitude records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Latitude)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(Data_df$Latitude) # use this instead of summary due to station-dependency here
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## -51.75    -25  -21.1      4   10.5  17.25     31     54     55     60     70 
##     69     88     95    250    114    105     81     68     68     66     64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, they do behave just like we&amp;rsquo;d expect them to.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-2&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;We don&amp;rsquo;t need to fix anything here.&lt;/p&gt;
&lt;h3 id=&#34;longitude&#34;&gt;Longitude&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;numeric&lt;/code&gt; (Longitude is inherently continuous)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-4&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Longitude records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Longitude)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(Data_df$Longitude) # use this instead of summary due to station-dependency here
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##    -97    -92    -90 -88.75    -67 -59.17    -53     -2   55.6    100    135 
##     68     81     64    105    114     69    250     68     95     66     88
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, they do behave just like we&amp;rsquo;d expect them to.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-3&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;We don&amp;rsquo;t need to fix anything here.&lt;/p&gt;
&lt;h3 id=&#34;climate&#34;&gt;Climate&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;factor&lt;/code&gt; (three levels: coastal, semi-coastal, continental)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-5&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Climate records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Climate)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Climate)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1068 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, they do behave just like we&amp;rsquo;d expect them to.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-4&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;We don&amp;rsquo;t need to fix anything here.&lt;/p&gt;
&lt;h3 id=&#34;population-status&#34;&gt;Population Status&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;factor&lt;/code&gt; (two levels: native, introduced)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-6&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Population Status records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Population.Status)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Population.Status)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1068 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, they do behave just like we&amp;rsquo;d expect them to.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-5&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;We don&amp;rsquo;t need to fix anything here.&lt;/p&gt;
&lt;h3 id=&#34;weight&#34;&gt;Weight&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;numeric&lt;/code&gt; (weight is a continuous metric)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-7&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Weight records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Weight)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Weight)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1068 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Obviously, something is wrong.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-6&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;As seen above, weight records are currently stored as character which they shouldn&amp;rsquo;t. So how do we fix this?&lt;/p&gt;
&lt;p&gt;Firstly, let&amp;rsquo;s try an intuitive &lt;code&gt;as.numeric()&lt;/code&gt; approach which attempts to convert all values contained within a vector into numeric records.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df$Weight &amp;lt;- as.numeric(Data_df_base$Weight)
summary(Data_df$Weight)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##   19.38   26.34   30.38   29.40   31.87  420.00      66
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Apparently, this didn&amp;rsquo;t do the trick since weight data values (recorded in g) below 13 and above 40 are highly unlikely for &lt;em&gt;Passer domesticus&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Sometimes, the &lt;code&gt;as.numeric()&lt;/code&gt; can be made more powerful by handing it data of class &lt;code&gt;character&lt;/code&gt;. To do so, simply combine &lt;code&gt;as.numeric()&lt;/code&gt; with &lt;code&gt;as.character()&lt;/code&gt; as shown below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df$Weight &amp;lt;- as.numeric(as.character(Data_df_base$Weight))
summary(Data_df$Weight)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##   19.38   26.34   30.38   29.40   31.87  420.00      66
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That still didn&amp;rsquo;t resolve our problem. Weight measurements were taken for all study organisms and so there shouldn&amp;rsquo;t be any &lt;code&gt;NA&lt;/code&gt;s and yet we find 66.&lt;/p&gt;
&lt;p&gt;Interestingly enough this is the exact same number as observations available for Siberia. A closer look at the data frame shows us that weight data for Siberia has been recorded with commas as decimal delimiters whilst the rest of the data set utilises dots.&lt;/p&gt;
&lt;p&gt;Fixing this is not necessarily difficult but it is an erroneous issue for data handling which comes up often and is easy to avoid. Getting rid of the flaws is as simple as using the &lt;code&gt;gsub()&lt;/code&gt; function contained within the &lt;code&gt;dplyr&lt;/code&gt; package.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df$Weight &amp;lt;- as.numeric(gsub(pattern = &amp;quot;,&amp;quot;, replacement = &amp;quot;.&amp;quot;, x = Data_df_base$Weight))
summary(Data_df$Weight)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   19.38   27.90   30.63   29.69   32.24  420.00
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is one data record left hat exceeds the biologically viable span for body weight records of &lt;em&gt;Passer domesticus&lt;/em&gt;. This data record holds the value 420. Since this is unlikely to be a simple mistake of placing the decimal delimiter in the wrong place (both 4.2 and 42 grams are also not feasible weight records for house sparrows), we have to delete the weight data record in question:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df$Weight[which(Data_df_base$Weight == 420)] &amp;lt;- NA
summary(Data_df$Weight)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##   19.38   27.89   30.63   29.33   32.23   36.66       1
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;hist(Data_df$Weight, breaks = 100)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;4_Data-Handling-and-Assumptions---Making-the-Most-of-Your-Data_files/figure-html/ColContWeight-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We finally fixed it!&lt;/p&gt;
&lt;h3 id=&#34;height&#34;&gt;Height&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;numeric&lt;/code&gt; (height is a continuous metric)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-8&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Height records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Height)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Height)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    1.35   13.52   14.52   15.39   16.22  135.40
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, some of our data don&amp;rsquo;t behave the way the should (a 135.4 or  1.35 cm tall sparrow are just absurd).&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-7&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;Height (or &amp;ldquo;Length&amp;rdquo;) records of &lt;em&gt;Passer domesticus&lt;/em&gt; should fall roughly between 10cm and 22cm. Looking at the data which exceed these thresholds, it is apparent that these are generated simply through misplaced decimal delimiters. So we fix them as follows and use a histogram to check if it worked.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df$Height[which(Data_df$Height &amp;lt; 10)] # decimal point placed wrong here
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.350 1.446
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df$Height[which(Data_df$Height &amp;lt; 10)] &amp;lt;- Data_df$Height[which(Data_df$Height &amp;lt; 10)] * 10 # FIXED IT!
Data_df$Height[which(Data_df$Height &amp;gt; 22)] # decimal point placed wrong here
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 126.7 135.4
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df$Height[which(Data_df$Height &amp;gt; 22)] &amp;lt;- Data_df$Height[which(Data_df$Height &amp;gt; 22)] / 10 # FIXED IT!
summary(Data_df$Height)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   11.09   13.52   14.51   15.20   16.20   21.68
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;hist(Data_df$Height, breaks = 100)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;4_Data-Handling-and-Assumptions---Making-the-Most-of-Your-Data_files/figure-html/ColContHeight-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We finally fixed it!&lt;/p&gt;
&lt;h3 id=&#34;wing-chord&#34;&gt;Wing Chord&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;numeric&lt;/code&gt; (wing chord is a continuous metric)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-9&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Wing Chord records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Wing.Chord)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Wing.Chord)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   6.410   6.840   7.050   7.337   7.400   9.000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, they do behave just like we&amp;rsquo;d expect them to.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-8&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;We don&amp;rsquo;t need to fix anything here.&lt;/p&gt;
&lt;h3 id=&#34;colour&#34;&gt;Colour&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;factor&lt;/code&gt; (three levels: black, grey, brown)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-10&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Colour records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Colour)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Colour)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1068 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Some of the colour records are very odd.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-9&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;The colour records &amp;ldquo;Bright black&amp;rdquo; and &amp;ldquo;Grey with black spots&amp;rdquo; should be &amp;ldquo;Grey&amp;rdquo;. Someone clearly got too eager on the assignment of colours here. The fix is as easy as identifying the data records which are &amp;ldquo;too precise&amp;rdquo; and overwrite them with the correct assignment:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df$Colour[which(Data_df$Colour == &amp;quot;Bright black&amp;quot;)] &amp;lt;- &amp;quot;Grey&amp;quot;
Data_df$Colour[which(Data_df$Colour == &amp;quot;Grey with black spots&amp;quot;)] &amp;lt;- &amp;quot;Grey&amp;quot;
Data_df$Colour &amp;lt;- droplevels(factor(Data_df$Colour)) # drop unused factor levels
summary(Data_df$Colour) # FIXED IT!
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Black Brown  Grey 
##   356   298   414
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We finally fixed it!&lt;/p&gt;
&lt;h3 id=&#34;sex&#34;&gt;Sex&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;factor&lt;/code&gt; (two levels: male and female)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-11&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Climate records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Sex)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Sex)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1068 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, they do behave just like we&amp;rsquo;d expect them to.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-10&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;We don&amp;rsquo;t need to fix anything here.&lt;/p&gt;
&lt;h3 id=&#34;nesting-site&#34;&gt;Nesting Site&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;factor&lt;/code&gt; (two levels: shrub and tree)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-12&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Nesting Site records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Nesting.Site)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Nesting.Site)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1068 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;fixing-problems-11&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;One individual is recording to be nesting on the ground. This is something house sparrows don&amp;rsquo;t do. Therefore, we have to assume that this individual is not even a &lt;em&gt;Passer domesticus&lt;/em&gt; to begin with.&lt;/p&gt;
&lt;p&gt;The only way to solve this is to remove all observations pertaining to this individual:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df &amp;lt;- Data_df[-which(Data_df$Nesting.Site == &amp;quot;Ground&amp;quot;), ]
summary(Data_df$Nesting.Site)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1067 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We just deleted a data record. This affects the flock size of the flock it belongs to (basically, this column contains hard-coded values) which we are going to deal with later.&lt;br&gt;
Still, there are manually entered &lt;code&gt;NA&lt;/code&gt; records present which we have to get rid of. These can be fixed easily without altering column classes and simply making use of logic by indexing their dependencies on other column values. The nesting site for a data record where sex reads &amp;ldquo;Male&amp;rdquo; has to be &lt;code&gt;NA&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df$Nesting.Site[which(Data_df$Sex == &amp;quot;Male&amp;quot;)] &amp;lt;- NA
Data_df$Nesting.Site &amp;lt;- droplevels(factor(Data_df$Nesting.Site)) # drop unused factor levels
summary(Data_df$Nesting.Site) # FIXED IT!
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Shrub  Tree  NA&#39;s 
##   292   231   544
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;nesting-height&#34;&gt;Nesting Height&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;numeric&lt;/code&gt; (continuous records in two clusters corresponding to shrubs and trees)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-13&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Nesting Height records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Nesting.Height)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Nesting.Height)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1067 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are obviously some issues here.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-12&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;Nesting height is a clear example of a variable that should be recorded as &lt;code&gt;numeric&lt;/code&gt; and yet our data frame currently stores them as character.&lt;/p&gt;
&lt;p&gt;Our first approach to fixing this, again, is using the &lt;code&gt;as.numeric()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(as.numeric(Data_df$Nesting.Height))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##   11.78   42.34   64.85  480.59  951.38 1950.86     544
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Clearly, something went horribly wrong here. When taking a closer look, the number of 1s is artificially inflated. This is due to the &lt;code&gt;NA&lt;/code&gt;s contained within the data set. These are currently stored as characters since they have been entered into the Excel sheet itself. The &lt;code&gt;as.numeric()&lt;/code&gt; function transforms these into 1s.&lt;/p&gt;
&lt;p&gt;One way of circumventing this issue is to combine the &lt;code&gt;as.numeric()&lt;/code&gt; function with the &lt;code&gt;as.character()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df$Nesting.Height &amp;lt;- as.numeric(as.character(Data_df$Nesting.Height))
summary(Data_df$Nesting.Height)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##   11.78   42.34   64.85  480.59  951.38 1950.86     544
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This quite clearly fixed our problems.&lt;/p&gt;
&lt;!-- As can be seen in the histograms below there are now far less erroneously small values. --&gt;
&lt;!-- ```{r plottingpanesNestingHeight, fig.height=2.75} --&gt;
&lt;!-- par(mfrow=c(1,2)) # plotting panes as 1 by 2 --&gt;
&lt;!-- hist(as.numeric(Data_df_base$Nesting.Height), main = &#34;Numeric(Data)&#34;, breaks = 100) --&gt;
&lt;!-- hist(as.numeric(as.character(Data_df_base$Nesting.Height)), main = &#34;Numeric(Character(Data))&#34;, breaks = 100) --&gt;
&lt;!-- ``` --&gt;
&lt;h3 id=&#34;number-of-eggs&#34;&gt;Number of Eggs&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;numeric&lt;/code&gt; (no a priori knowledge of levels)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-14&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Number of Eggs records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Number.of.Eggs)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Number.of.Eggs)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1067 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One very out of the ordinary record is to be seen.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-13&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;Number of eggs is another variable which should be recorded as &lt;code&gt;numeric&lt;/code&gt; and yet is currently stored as character.&lt;/p&gt;
&lt;p&gt;Our first approach to fixing this, again, is using the &lt;code&gt;as.numeric()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(as.numeric(Data_df$Number.of.Eggs))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##   0.000   2.000   3.000   3.746   4.000  10.000     544
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, this didn&amp;rsquo;t do the trick. The number of 1s might be inflated and we expect exactly 544 (number of males) &lt;code&gt;NA&lt;/code&gt;s since number of eggs have only been recorded for female house sparrows.&lt;/p&gt;
&lt;p&gt;We already know that improperly stored &lt;code&gt;NA&lt;/code&gt; records are prone to causing an inflation of data records of value 1. We also remember that head and tail of our data frame hold different types of &lt;code&gt;NA&lt;/code&gt; records. Let&amp;rsquo;s find out who entered &lt;code&gt;NA&lt;/code&gt;s correctly:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;unique(Data_df$Site[which(is.na(Data_df$Egg.Weight))])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## character(0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code above identifies the sites at which proper &lt;code&gt;NA&lt;/code&gt; recording has been done. The Falkland Isle team did it right (&lt;code&gt;NA&lt;/code&gt; fields in Excel were left blank). Fixing this is actually a bit more challenging and so we do the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# make everything into characters
Data_df$Number.of.Eggs &amp;lt;- as.character(Data_df$Number.of.Eggs)
# writing character NA onto actual NAs
Data_df$Number.of.Eggs[which(is.na(Data_df$Number.of.Eggs))] &amp;lt;- &amp;quot;  NA&amp;quot;
# make all character NAs into proper NAs
Data_df$Number.of.Eggs[Data_df$Number.of.Eggs == &amp;quot;  NA&amp;quot;] &amp;lt;- NA
# make everything numeric
Data_df$Number.of.Eggs &amp;lt;- as.numeric(as.character(Data_df$Number.of.Eggs))
summary(Data_df$Number.of.Eggs)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##   0.000   2.000   3.000   3.746   4.000  10.000     544
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We did it!&lt;/p&gt;
&lt;h3 id=&#34;egg-weight&#34;&gt;Egg Weight&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;numeric&lt;/code&gt; (another weight measurement that needs to be continuous)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-15&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Egg Weight records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Egg.Weight)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Egg.Weight)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1067 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;fixing-problems-14&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;Egg weight should be recorded as &lt;code&gt;numeric&lt;/code&gt; and yet is currently stored as character. Our first approach to fixing this, again, is using the &lt;code&gt;as.numeric()&lt;/code&gt; function again.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(as.numeric(Data_df$Egg.Weight))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##   1.580   2.340   2.670   2.619   2.890   3.590     590
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Something is wrong here. Not enough &lt;code&gt;NA&lt;/code&gt;s are recorded. We expect exactly 590 &lt;code&gt;NA&lt;/code&gt;s (Number of males + Number of Females with zero eggs). Additionally, there are way too many 1s.
Our problem, again, lies with the way the &lt;code&gt;NA&lt;/code&gt;s have been entered into the data set from the beginning and so we use the following fix again.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# make everything into characters
Data_df$Egg.Weight &amp;lt;- as.character(Data_df$Egg.Weight)
# writing character NA onto actual NAs
Data_df$Egg.Weight[which(is.na(Data_df$Egg.Weight))] &amp;lt;- &amp;quot;  NA&amp;quot;
# make all character NAs into proper NAs
Data_df$Egg.Weight[Data_df$Egg.Weight == &amp;quot;  NA&amp;quot;] &amp;lt;- NA
# make everything numeric
Data_df$Egg.Weight &amp;lt;- as.numeric(as.character(Data_df$Egg.Weight))
summary(Data_df$Egg.Weight)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##   1.580   2.340   2.670   2.619   2.890   3.590     590
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;flock&#34;&gt;Flock&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;factor&lt;/code&gt; (each sparrow was assigned to one particular flock)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-16&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Flock records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Flock)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Flock)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1067 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, they do behave just like we&amp;rsquo;d expect them to.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-15&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;We don&amp;rsquo;t need to fix anything here.&lt;/p&gt;
&lt;h3 id=&#34;home-range&#34;&gt;Home Range&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;factor&lt;/code&gt; (three levels: small, medium, large)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-17&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Home Range records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Home.Range)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Home.Range)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1067 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, they do behave just like we&amp;rsquo;d expect them to.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-16&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;We don&amp;rsquo;t need to fix anything here.&lt;/p&gt;
&lt;h3 id=&#34;flock-size&#34;&gt;Flock Size&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;numeric&lt;/code&gt; (continuous measurement of how many sparrows are in each flock - measured as integers)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-18&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Flock Size records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Flock.Size)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;integer&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Flock.Size)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    7.00   16.00   19.00   25.81   31.00   58.00
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, they do behave just like we&amp;rsquo;d expect them to.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-17&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;We don&amp;rsquo;t need to fix anything here.&lt;/p&gt;
&lt;h3 id=&#34;predator-presence&#34;&gt;Predator Presence&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;factor&lt;/code&gt; (two levels: yes and no)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-19&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Predator Presence records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Predator.Presence)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Predator.Presence)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1067 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, they do behave just like we&amp;rsquo;d expect them to.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-18&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;We don&amp;rsquo;t need to fix anything here.&lt;/p&gt;
&lt;h3 id=&#34;predator-type&#34;&gt;Predator Type&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;factor&lt;/code&gt; (three levels: Avian, Non-Avian, and &lt;code&gt;NA&lt;/code&gt;)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-20&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Predator Type records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Predator.Type)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Predator.Type)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1067 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Something doesn&amp;rsquo;t sit well here.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-19&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;Someone got overly eager when recording Predator Type and specified the presence of a hawk instead of taking down &amp;ldquo;Avian&amp;rdquo;. We fix this as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df$Predator.Type[which(Data_df$Predator.Type == &amp;quot;Hawk&amp;quot;)] &amp;lt;- &amp;quot;Avian&amp;quot;
summary(Data_df$Predator.Type)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1067 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This fixed it  but there are still manually entered &lt;code&gt;NA&lt;/code&gt; records present which we have to get rid of. These can be fixed easily without altering column classes and simply making use of logic by indexing their dependencies on other column values. The predator type for a data record where predator presence reads &amp;ldquo;No&amp;rdquo; has to be &lt;code&gt;NA&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df$Predator.Type[which(Data_df$Predator.Presence == &amp;quot;No&amp;quot;)] &amp;lt;- NA
Data_df$Predator.Type &amp;lt;- droplevels(factor(Data_df$Predator.Type)) # drop unused factor levels
summary(Data_df$Predator.Type) # FIXED IT!
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Avian Non-Avian      NA&#39;s 
##       490       220       357
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;redundant-data&#34;&gt;Redundant Data&lt;/h3&gt;
&lt;p&gt;Our data contains redundant columns (i.e.: columns whose data is present in another column already). These are (1) Flock Size (data contained in Flock column) and (2) Site (data contained in Index column). The fix to this is as easy as removing the columns in question.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df &amp;lt;- within(Data_df, rm(Flock.Size, Site))
dim(Data_df)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1067   18
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Fixed it!&lt;/p&gt;
&lt;p&gt;By doing so, we have gotten rid of our flock size problem stemming from the deletion of a data record. You could also argue that the columns &lt;code&gt;Site&lt;/code&gt; and &lt;code&gt;Index&lt;/code&gt; are redundant. We could arguably keep both for quality-of-life when interpreting our results (make use of &lt;code&gt;Sites&lt;/code&gt;) and coding (make use of &lt;code&gt;Index&lt;/code&gt;).&lt;/p&gt;
&lt;h2 id=&#34;saving-the-fixed-data-set&#34;&gt;Saving The Fixed Data Set&lt;/h2&gt;
&lt;p&gt;We fixed out entire data set! The data set is now ready for use.&lt;/p&gt;
&lt;p&gt;Keep in mind that the data set I provided you with was relatively clean and real-world messy data sets can be far more difficult to clean up.&lt;/p&gt;
&lt;p&gt;Before going forth, we need to save it. &lt;strong&gt;Attention:&lt;/strong&gt; don&amp;rsquo;t overwrite your initial data file!&lt;/p&gt;
&lt;h3 id=&#34;final-check&#34;&gt;Final Check&lt;/h3&gt;
&lt;p&gt;Before exporting you may want to ensure that everything is in order and do a final round of data inspection. This can be achieved by running the automated &lt;code&gt;summary()&lt;/code&gt; command from earlier again as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;for (i in 1:dim(Data_df)[2]) {
  print(colnames(Data_df)[i])
  print(summary(Data_df[, i]))
  print(&amp;quot;------------------------------------------------------&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Index&amp;quot;
##    Length     Class      Mode 
##      1067 character character 
## [1] &amp;quot;------------------------------------------------------&amp;quot;
## [1] &amp;quot;Latitude&amp;quot;
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  -51.75    4.00   10.50   13.63   31.00   70.00 
## [1] &amp;quot;------------------------------------------------------&amp;quot;
## [1] &amp;quot;Longitude&amp;quot;
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  -97.00  -88.75  -53.00  -28.47   -2.00  135.00 
## [1] &amp;quot;------------------------------------------------------&amp;quot;
## [1] &amp;quot;Climate&amp;quot;
##    Length     Class      Mode 
##      1067 character character 
## [1] &amp;quot;------------------------------------------------------&amp;quot;
## [1] &amp;quot;Population.Status&amp;quot;
##    Length     Class      Mode 
##      1067 character character 
## [1] &amp;quot;------------------------------------------------------&amp;quot;
## [1] &amp;quot;Weight&amp;quot;
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##   19.38   27.87   30.61   29.32   32.24   36.66       1 
## [1] &amp;quot;------------------------------------------------------&amp;quot;
## [1] &amp;quot;Height&amp;quot;
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   11.09   13.52   14.51   15.20   16.20   21.68 
## [1] &amp;quot;------------------------------------------------------&amp;quot;
## [1] &amp;quot;Wing.Chord&amp;quot;
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   6.410   6.840   7.050   7.337   7.400   9.000 
## [1] &amp;quot;------------------------------------------------------&amp;quot;
## [1] &amp;quot;Colour&amp;quot;
## Black Brown  Grey 
##   356   298   413 
## [1] &amp;quot;------------------------------------------------------&amp;quot;
## [1] &amp;quot;Sex&amp;quot;
##    Length     Class      Mode 
##      1067 character character 
## [1] &amp;quot;------------------------------------------------------&amp;quot;
## [1] &amp;quot;Nesting.Site&amp;quot;
## Shrub  Tree  NA&#39;s 
##   292   231   544 
## [1] &amp;quot;------------------------------------------------------&amp;quot;
## [1] &amp;quot;Nesting.Height&amp;quot;
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##   11.78   42.34   64.85  480.59  951.38 1950.86     544 
## [1] &amp;quot;------------------------------------------------------&amp;quot;
## [1] &amp;quot;Number.of.Eggs&amp;quot;
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##   0.000   2.000   3.000   3.746   4.000  10.000     544 
## [1] &amp;quot;------------------------------------------------------&amp;quot;
## [1] &amp;quot;Egg.Weight&amp;quot;
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##   1.580   2.340   2.670   2.619   2.890   3.590     590 
## [1] &amp;quot;------------------------------------------------------&amp;quot;
## [1] &amp;quot;Flock&amp;quot;
##    Length     Class      Mode 
##      1067 character character 
## [1] &amp;quot;------------------------------------------------------&amp;quot;
## [1] &amp;quot;Home.Range&amp;quot;
##    Length     Class      Mode 
##      1067 character character 
## [1] &amp;quot;------------------------------------------------------&amp;quot;
## [1] &amp;quot;Predator.Presence&amp;quot;
##    Length     Class      Mode 
##      1067 character character 
## [1] &amp;quot;------------------------------------------------------&amp;quot;
## [1] &amp;quot;Predator.Type&amp;quot;
##     Avian Non-Avian      NA&#39;s 
##       490       220       357 
## [1] &amp;quot;------------------------------------------------------&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Everything checks out. Let&amp;rsquo;s save our final data frame.&lt;/p&gt;
&lt;h3 id=&#34;exporting-the-altered-data&#34;&gt;Exporting The Altered Data&lt;/h3&gt;
&lt;p&gt;Since Excel is readily available for viewing data outside of R, I like to save my final data set in excel format as can be seen below. Additionally, I recommend saving your final data frame as an RDS file. These are &lt;code&gt;R&lt;/code&gt; specific data files which you will not be able to alter outside of &lt;code&gt;R&lt;/code&gt; thus saving yourself from accidentally changing records when only trying to view your data. On top of that, RDS files take up less space than either Excel or TXT files do.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# saving in excel sheet
write.csv(Data_df, file = paste(Dir.Data, &amp;quot;/SparrowData_FIXED.csv&amp;quot;, sep=&amp;quot;&amp;quot;))
# saving as R data frame object
saveRDS(Data_df, file = paste(Dir.Data, &amp;quot;/SparrowData.rds&amp;quot;, sep=&amp;quot;&amp;quot;)) 
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;sessioninfo&#34;&gt;SessionInfo&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.2.3 (2023-03-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Big Sur ... 10.16
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] dplyr_1.1.0
## 
## loaded via a namespace (and not attached):
##  [1] bslib_0.4.2       compiler_4.2.3    pillar_1.8.1      jquerylib_0.1.4   highr_0.10        R.methodsS3_1.8.2 R.utils_2.12.2    tools_4.2.3       digest_0.6.31     jsonlite_1.8.4   
## [11] evaluate_0.20     lifecycle_1.0.3   tibble_3.2.0      R.cache_0.16.0    pkgconfig_2.0.3   rlang_1.0.6       cli_3.6.0         rstudioapi_0.14   yaml_2.3.7        blogdown_1.16    
## [21] xfun_0.37         fastmap_1.1.1     styler_1.9.1      knitr_1.42        generics_0.1.3    vctrs_0.5.2       sass_0.4.5        tidyselect_1.2.0  glue_1.6.2        R6_2.5.1         
## [31] fansi_1.0.4       rmarkdown_2.20    bookdown_0.33     purrr_1.0.1       magrittr_2.0.3    htmltools_0.5.4   utf8_1.2.3        cachem_1.0.7      R.oo_1.25.0
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 04 (Extra Material)</title>
      <link>https://www.erikkusch.com/courses/rethinking/chapter-04b/</link>
      <pubDate>Fri, 15 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/rethinking/chapter-04b/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to additional exercises from previous versions of the end of chapter 4 in 
&lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Satistical Rethinking 2&lt;/a&gt; by Richard McElreath. I have created these notes as a part of my ongoing involvement in the 
&lt;a href=&#34;https://www.erikkusch.com/project/aubayes/&#34;&gt;AU Bayes Study Group&lt;/a&gt;. Much of my inspiration for these solutions, where necessary, has been obtained from 
&lt;a href=&#34;https://gregor-mathes.netlify.app/2021/01/01/rethinking-chapter-4/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gregor Mathes&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;practice-1&#34;&gt;Practice 1&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Refit model &lt;code&gt;m4.3&lt;/code&gt; from the chapter but omit the mean weight &lt;code&gt;xbar&lt;/code&gt;. Compare the new modelâs posterior to that of the original model. In particular, look at the covariance among the parameters. What is difference?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; 
Let&amp;rsquo;s firstly refit the model &lt;code&gt;m4.3&lt;/code&gt; using the code on pages 100 &amp;amp; 101:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rethinking)
data(Howell1)
d &amp;lt;- Howell1
d2 &amp;lt;- d[d$age &amp;gt;= 18, ]
# define the average weight, x-bar
xbar &amp;lt;- mean(d2$weight)
# fit original model
m4.3 &amp;lt;- quap(alist(
  height ~ dnorm(mu, sigma),
  mu &amp;lt;- a + b * (weight - xbar),
  a ~ dnorm(178, 20),
  b ~ dlnorm(0, 1),
  sigma ~ dunif(0, 50)
), data = d2)
# fit reduced model
m4.3_reduced &amp;lt;- quap(alist(
  height ~ dnorm(mu, sigma),
  mu &amp;lt;- a + b * weight,
  a ~ dnorm(178, 20),
  b ~ dlnorm(0, 1),
  sigma ~ dunif(0, 50)
), data = d2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do we compare these models and their posteriors? Here, I want to look at three things:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Covariances between parameters estimates&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;round(vcov(m4.3), digits = 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           a     b sigma
## a     0.073 0.000 0.000
## b     0.000 0.002 0.000
## sigma 0.000 0.000 0.037
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;round(vcov(m4.3_reduced), digits = 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            a      b sigma
## a      3.601 -0.078 0.009
## b     -0.078  0.002 0.000
## sigma  0.009  0.000 0.037
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we can see, the &lt;strong&gt;covariances increase&lt;/strong&gt; quite a bit when omitting &lt;code&gt;xbar&lt;/code&gt; and this &lt;strong&gt;not centring&lt;/strong&gt;.&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Summaries of each parameter in the posterior&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(extract.samples(m4.3))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        a               b              sigma      
##  Min.   :153.6   Min.   :0.7505   Min.   :4.324  
##  1st Qu.:154.4   1st Qu.:0.8738   1st Qu.:4.947  
##  Median :154.6   Median :0.9027   Median :5.076  
##  Mean   :154.6   Mean   :0.9023   Mean   :5.076  
##  3rd Qu.:154.8   3rd Qu.:0.9307   3rd Qu.:5.205  
##  Max.   :155.7   Max.   :1.0443   Max.   :5.773
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(extract.samples(m4.3_reduced))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        a               b              sigma      
##  Min.   :108.2   Min.   :0.7290   Min.   :4.434  
##  1st Qu.:113.2   1st Qu.:0.8632   1st Qu.:4.945  
##  Median :114.5   Median :0.8911   Median :5.071  
##  Mean   :114.5   Mean   :0.8911   Mean   :5.072  
##  3rd Qu.:115.8   3rd Qu.:0.9195   3rd Qu.:5.199  
##  Max.   :121.7   Max.   :1.0403   Max.   :5.833
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Between the two models, neither $\beta$ (&lt;code&gt;b&lt;/code&gt;) nor $\sigma$ (&lt;code&gt;sigma&lt;/code&gt;) differ greatly. However, our posterior estimate of $\alpha$ (&lt;code&gt;a&lt;/code&gt;) is quite a bit lower in the reduced model than it is in the original model.
This is down to the interpretation of the $\alpha$ parameter itself. In the original model, $\alpha$ denotes the average height of a person at the mean weight in the data set. Since we removed the &lt;code&gt;xbar&lt;/code&gt; component in the reduced model, $\alpha$ now identifies the average height of a person of weight $0kg$ - a nonsense metric.&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Predictions and Intervals&lt;br&gt;
Here, I have written a function that takes a model object, data, and some additional arguments to automate plot generation:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot.predictions &amp;lt;- function(X, Y, data, model, main) {
  XOrig &amp;lt;- X
  X &amp;lt;- data[, colnames(data) == X]
  Y &amp;lt;- data[, colnames(data) == Y]
  plot(Y ~ X, col = col.alpha(rangi2, 0.8), main = main)
  # Estimate and plot the quap regression line and 97% HPDI for the mean
  weight.seq &amp;lt;- seq(from = min(X), to = max(X), length.out = 1000)
  predict_df &amp;lt;- data.frame(XOrig = weight.seq)
  colnames(predict_df) &amp;lt;- XOrig
  mu &amp;lt;- link(model, data = predict_df)
  mu.mean &amp;lt;- apply(mu, 2, mean)
  mu.HPDI &amp;lt;- apply(mu, 2, HPDI, prob = 0.97)
  lines(weight.seq, mu.mean)
  shade(mu.HPDI, weight.seq)
  # Estimate and plot the 97% HPDI for the predicted heights
  predict_ls &amp;lt;- list(weight = weight.seq)
  names(predict_ls) &amp;lt;- XOrig
  sim.height &amp;lt;- sim(model, data = predict_ls)
  height.HPDI &amp;lt;- apply(sim.height, 2, HPDI, prob = 0.97)
  shade(height.HPDI, weight.seq)
}

plot.predictions(X = &amp;quot;weight&amp;quot;, Y = &amp;quot;height&amp;quot;, data = d2, model = m4.3, main = &amp;quot;Original Model&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-15-statistical-rethinking-chapter-04b_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot.predictions(X = &amp;quot;weight&amp;quot;, Y = &amp;quot;height&amp;quot;, data = d2, model = m4.3_reduced, main = &amp;quot;Reduced Model&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-15-statistical-rethinking-chapter-04b_files/figure-html/unnamed-chunk-4-2.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So. Does centring or not change the predictions of our model? No, it does not. At least in this case.&lt;/p&gt;
&lt;h2 id=&#34;practice-2&#34;&gt;Practice 2&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; In the chapter, we used 15 knots with the cherry blossom spline. Increase the number of knots and observe what happens to the resulting spline. Then adjust also the width of the prior on the weights - change the standard deviation of the prior and watch what happens. What do you think the combination of knot number and the prior on the weights controls?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; 
Again, I start with code from the book - pages 118, 120 &amp;amp; 122 to be precise - and implement it into a function for easy changing of model specifications:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rethinking)
library(splines)
data(cherry_blossoms)
d &amp;lt;- cherry_blossoms
d2 &amp;lt;- d[complete.cases(d$temp), ] # complete cases on temp

cherry_spline &amp;lt;- function(n_Knots, StdV) {
  # knot list
  knot_list &amp;lt;- quantile(d2$year, probs = seq(0, 1, length.out = n_Knots))[-c(1, n_Knots)]
  # basis function
  B &amp;lt;- bs(d2$year,
    knots = knot_list,
    degree = 3, intercept = TRUE
  )
  # Run quap model
  m4.7 &amp;lt;- quap(alist(
    T ~ dnorm(mu, sigma),
    mu &amp;lt;- a + B %*% w,
    a ~ dnorm(6, 10),
    w ~ dnorm(0, StdV),
    sigma ~ dexp(1)
  ),
  data = list(T = d2$temp, B = B, StdV = StdV),
  start = list(w = rep(0, ncol(B)))
  )
  # get 97% posterior interval for mean and plot
  mu &amp;lt;- link(m4.7)
  mu_PI &amp;lt;- apply(mu, 2, PI, 0.97)
  plot(d2$year, d2$temp,
    col = col.alpha(rangi2, 0.3), pch = 16,
    main = paste(&amp;quot;Knots:&amp;quot;, n_Knots, &amp;quot;-&amp;quot;, &amp;quot;Prior Weight:&amp;quot;, StdV)
  )
  shade(mu_PI, d2$year, col = col.alpha(&amp;quot;black&amp;quot;, 0.5))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s start by &lt;strong&gt;increasing the number of knots&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cherry_spline(n_Knots = 15, StdV = 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-15-statistical-rethinking-chapter-04b_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cherry_spline(n_Knots = 20, StdV = 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-15-statistical-rethinking-chapter-04b_files/figure-html/unnamed-chunk-6-2.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cherry_spline(n_Knots = 30, StdV = 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-15-statistical-rethinking-chapter-04b_files/figure-html/unnamed-chunk-6-3.png&#34; width=&#34;1440&#34; /&gt;
The more knots we use, the more flexible the resulting function becomes. It fits the data better, but might overfit if we try to do predictions.&lt;/p&gt;
&lt;p&gt;Now, we &lt;strong&gt;change the prior weights&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cherry_spline(n_Knots = 15, StdV = 1) # base standard deviation
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-15-statistical-rethinking-chapter-04b_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cherry_spline(n_Knots = 15, StdV = .1) # decreased standard deviation
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-15-statistical-rethinking-chapter-04b_files/figure-html/unnamed-chunk-7-2.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cherry_spline(n_Knots = 15, StdV = 100) # increased standard deviation
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-15-statistical-rethinking-chapter-04b_files/figure-html/unnamed-chunk-7-3.png&#34; width=&#34;1440&#34; /&gt;
As I &lt;strong&gt;decrease the standard deviation for the prior or the weights&lt;/strong&gt;, I see that the resulting function becomes &lt;strong&gt;less flexible&lt;/strong&gt;. I expected our function to become less flexible as I lower the &lt;code&gt;StdV&lt;/code&gt; parameter since a lower standard deviation here will increase the weights and thus give each base function more of say in determining the overall function globally, making the result smoother.&lt;/p&gt;
&lt;h2 id=&#34;practice-3&#34;&gt;Practice 3&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Return to &lt;code&gt;data(cherry_blossoms)&lt;/code&gt; and model the association between blossom date (&lt;code&gt;doy&lt;/code&gt;) and March temperature (&lt;code&gt;temp&lt;/code&gt;). Note that there are many missing values in both variables. You may consider a linear model, a polynomial, or a spline on temperature. How well does temperature rend predict the blossom trend?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rethinking)
library(splines)
data(cherry_blossoms)
d &amp;lt;- cherry_blossoms[, 2:3]
d2 &amp;lt;- na.omit(d)
d2$temps &amp;lt;- scale(d2$temp)
with(d2, plot(temps, doy,
  xlab = &amp;quot;Centred Temperature in March&amp;quot;, ylab = &amp;quot;Day in Year&amp;quot;
))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-15-statistical-rethinking-chapter-04b_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There is a seemingly negative relationship here, but there is also a lot of noise. I expect polynomial or spline approaches to capture too much of that noise and opt for a simple linear regression instead:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# define average temp
xbar &amp;lt;- mean(d2$temp)

# fit modell
cherry_linear &amp;lt;- quap(
  alist(
    doy ~ dnorm(mu, sigma),
    mu &amp;lt;- a + b * (temp - xbar),
    a ~ dnorm(115, 30),
    b ~ dnorm(-2, 5),
    sigma ~ dunif(0, 50)
  ),
  data = d2
)

# output
precis(cherry_linear)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean        sd       5.5%      94.5%
## a     104.921713 0.2106637 104.585032 105.258394
## b      -2.990211 0.3078719  -3.482249  -2.498172
## sigma   5.910003 0.1489654   5.671927   6.148078
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With average temperatures in March, cherries blossom on day 105 of the year. With every increase of 1Â°C in temperature in March, cherries blossom - on average - 3 earlier. Our PI shows that we are pretty certain of this relationship. Let&amp;rsquo;s plot this to finish:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot.predictions(X = &amp;quot;temp&amp;quot;, Y = &amp;quot;doy&amp;quot;, data = d2, model = cherry_linear, main = &amp;quot;Cherry Blossoms&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-15-statistical-rethinking-chapter-04b_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;1440&#34; /&gt;
Off, that&amp;rsquo;s quite some uncertainty there. I guess we aren&amp;rsquo;t doing a tremendous job at predicting cherry blossom dates depending on temperature in March with this model.&lt;/p&gt;
&lt;h2 id=&#34;practice-4&#34;&gt;Practice 4&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Simulate the prior predictive distribution for the cherry blossom spline in the chapter. Adjust the prior on the weights and observe what happens. What do you think the prior on the weight is doing?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I haven&amp;rsquo;t solved this myself (yet). In the meantime, you can consult the answer provided by 
&lt;a href=&#34;https://gregor-mathes.netlify.app/2021/01/01/rethinking-chapter-4/#question-4h6&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gregor Mathes&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;practice-5&#34;&gt;Practice 5&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; The cherry blossom spline in the chapter used an intercept a, but technically it doesnât require one. The first basis function could substitute for the intercept. Try refitting the cherry blossom spline without the intercept. What else about the model do you need to change to make this work?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rethinking)
library(splines)
data(cherry_blossoms)
d &amp;lt;- cherry_blossoms
d2 &amp;lt;- d[complete.cases(d$temp), ] # complete cases on temp

n_Knots &amp;lt;- 15
# knot list
knot_list &amp;lt;- quantile(d2$year, probs = seq(0, 1, length.out = n_Knots))[-c(1, n_Knots)]
# basis function
B &amp;lt;- bs(d2$year,
  knots = knot_list,
  degree = 3, intercept = FALSE
)
# Run quap model
m4.7 &amp;lt;- quap(alist(
  T ~ dnorm(mu, sigma),
  mu &amp;lt;- B %*% w,
  a ~ dnorm(6, 10),
  w ~ dnorm(0, 1),
  sigma ~ dexp(1)
),
data = list(T = d2$temp, B = B),
start = list(w = rep(0, ncol(B)))
)
# get 97% posterior interval for mean and plot
mu &amp;lt;- link(m4.7)
mu_PI &amp;lt;- apply(mu, 2, PI, 0.97)
plot(d2$year, d2$temp,
  col = col.alpha(rangi2, 0.3), pch = 16,
  main = &amp;quot;No Intercept&amp;quot;
)
shade(mu_PI, d2$year, col = col.alpha(&amp;quot;black&amp;quot;, 0.5))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-15-statistical-rethinking-chapter-04b_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We need to change the deterministic formula in the model as well as the creation of basis functions by setting &lt;code&gt;Intercept = FALSE&lt;/code&gt; in the &lt;code&gt;bs()&lt;/code&gt; function call.&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.5 (2021-03-31)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19043)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.1252  LC_CTYPE=English_United Kingdom.1252    LC_MONETARY=English_United Kingdom.1252 LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.1252    
## 
## attached base packages:
## [1] splines   parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] rethinking_2.13      rstan_2.21.2         ggplot2_3.3.6        StanHeaders_2.21.0-7
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.7         mvtnorm_1.1-1      lattice_0.20-41    prettyunits_1.1.1  ps_1.6.0           assertthat_0.2.1   digest_0.6.27      utf8_1.2.1         V8_3.4.1           R6_2.5.0          
## [11] backports_1.2.1    stats4_4.0.5       evaluate_0.14      coda_0.19-4        highr_0.9          blogdown_1.3       pillar_1.6.0       rlang_0.4.11       curl_4.3.2         callr_3.7.0       
## [21] jquerylib_0.1.4    R.utils_2.10.1     R.oo_1.24.0        rmarkdown_2.7      styler_1.4.1       stringr_1.4.0      loo_2.4.1          munsell_0.5.0      compiler_4.0.5     xfun_0.22         
## [31] pkgconfig_2.0.3    pkgbuild_1.2.0     shape_1.4.5        htmltools_0.5.1.1  tidyselect_1.1.0   tibble_3.1.1       gridExtra_2.3      bookdown_0.22      codetools_0.2-18   matrixStats_0.61.0
## [41] fansi_0.4.2        crayon_1.4.1       dplyr_1.0.5        withr_2.4.2        MASS_7.3-53.1      R.methodsS3_1.8.1  grid_4.0.5         jsonlite_1.7.2     gtable_0.3.0       lifecycle_1.0.0   
## [51] DBI_1.1.1          magrittr_2.0.1     scales_1.1.1       RcppParallel_5.1.2 cli_3.0.0          stringi_1.5.3      bslib_0.2.4        ellipsis_0.3.2     generics_0.1.0     vctrs_0.3.7       
## [61] rematch2_2.1.2     tools_4.0.5        R.cache_0.14.0     glue_1.4.2         purrr_0.3.4        processx_3.5.1     yaml_2.2.1         inline_0.3.17      colorspace_2.0-0   knitr_1.33        
## [71] sass_0.3.1
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Data Visualisation</title>
      <link>https://www.erikkusch.com/courses/an-introduction-to-biostatistics/data-visualisation/</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/an-introduction-to-biostatistics/data-visualisation/</guid>
      <description>&lt;h2 id=&#34;theory&#34;&gt;Theory&lt;/h2&gt;
&lt;p&gt;These are the solutions to the exercises contained within the handout to Data Visualisation which walks you through the basics of data visualisation in &lt;code&gt;R&lt;/code&gt;using &lt;code&gt;ggplot2&lt;/code&gt;. The plots presented here are using data from the &lt;code&gt;iris&lt;/code&gt; data set supplied through the &lt;code&gt;datasets&lt;/code&gt; package. Keep in mind that there is probably a myriad of other ways to reach the same conclusions as presented in these solutions.&lt;/p&gt;
&lt;p&gt;I have prepared some &lt;a href=&#34;https://www.erikkusch.com/courses/an-introduction-to-biostatistics/05---Data-Visualisation_Handout.html&#34; target=&#34;_blank&#34;&gt; Lecture Slides &lt;/a&gt; for this session.&lt;/p&gt;
&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;p&gt;This practical makes use of R-internal data so you don&amp;rsquo;t need to download anything extra today.&lt;/p&gt;
&lt;h2 id=&#34;packages&#34;&gt;Packages&lt;/h2&gt;
&lt;p&gt;Recall the exercise that went along with the last seminar (Descriptive Statistics) where we learnt the difference between a basic and advanced preamble for package loading in &lt;code&gt;R&lt;/code&gt;. Here (and in future exercises) I will only supply you with the advanced version of the preamble.&lt;/p&gt;
&lt;p&gt;Now let&amp;rsquo;s load the &lt;code&gt;ggplot2&lt;/code&gt; package into our &lt;code&gt;R&lt;/code&gt; session so we&amp;rsquo;ll be able to use its functionality for data visualisation as well as the &lt;code&gt;datasets&lt;/code&gt; package to get the &lt;code&gt;iris&lt;/code&gt; data set.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# function to load packages and install them if they haven&#39;t been installed yet
install.load.package &amp;lt;- function(x) {
  if (!require(x, character.only = TRUE))
    install.packages(x)
  require(x, character.only = TRUE)
}
# packages to load/install if necessary
package_vec &amp;lt;- c(&amp;quot;ggplot2&amp;quot;, &amp;quot;datasets&amp;quot;)
# applying function install.load.package to all packages specified in package_vec
sapply(package_vec, install.load.package)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggplot2
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  ggplot2 datasets 
##     TRUE     TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;loading-r-internal-data-sets-iris&#34;&gt;Loading &lt;code&gt;R&lt;/code&gt;-internal data sets (&lt;code&gt;iris&lt;/code&gt;)&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;iris&lt;/code&gt; data set is included in the &lt;code&gt;datasets&lt;/code&gt; package in &lt;code&gt;R&lt;/code&gt;. An &lt;code&gt;R&lt;/code&gt;-internal data set is loaded through the command &lt;code&gt;data()&lt;/code&gt;. Take note that you do not have to assign this command&amp;rsquo;s output to a new object (via &lt;code&gt;&amp;lt;-&lt;/code&gt;). Instead, the dataset is loaded to your current environment by its name (iris, in this case). Keep in mind that this &lt;strong&gt;can override&lt;/strong&gt; objects of the same name that are already present in your current session of &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(&amp;quot;iris&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;inspect-the-data-set&#34;&gt;Inspect the data set&lt;/h2&gt;
&lt;p&gt;Since we know that &lt;code&gt;iris&lt;/code&gt; is a dataset, we can be reasonably sure that this object will be complex enough to warrant using the &lt;code&gt;str()&lt;/code&gt; function for inspection:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(iris)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &#39;data.frame&#39;:	150 obs. of  5 variables:
##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...
##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...
##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...
##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...
##  $ Species     : Factor w/ 3 levels &amp;quot;setosa&amp;quot;,&amp;quot;versicolor&amp;quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;iris&lt;/code&gt; dataset contains four measurements (&lt;code&gt;Sepal.Length&lt;/code&gt;, &lt;code&gt;Sepal.Width&lt;/code&gt;, &lt;code&gt;Petal.Length&lt;/code&gt;, &lt;code&gt;Petal.Width&lt;/code&gt;) for 150 flowers representing three species of iris (&lt;em&gt;Iris setosa&lt;/em&gt;, &lt;em&gt;versicolor&lt;/em&gt; and &lt;em&gt;virginica&lt;/em&gt;).&lt;/p&gt;
&lt;h2 id=&#34;boxplot-of-petallength-by-species&#34;&gt;&lt;strong&gt;Boxplot&lt;/strong&gt; of &lt;code&gt;Petal.Length&lt;/code&gt; by &lt;code&gt;Species&lt;/code&gt;&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(iris, # the data set
       aes(x=Species, y=Petal.Length) # aesthetics
       ) + geom_boxplot() + # this is the end of the bare minimum plot
  theme_bw() + labs(title=&amp;quot;Petal Length of three different species of Iris&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;05---Data-Visualisation_files/figure-html/PlottingExercise1-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;THis boxplot shows us exactly how the distributions of petal length measurements of our three species of Iris are differing from one another. Despite the obvious trend in the data, &lt;strong&gt;be sure not to report results through figures alone!&lt;/strong&gt; We will find out how to test whether the pattern we can observe here holds up to scrutiny at a later point in time of our seminars.&lt;/p&gt;
&lt;h2 id=&#34;scatterplot-of-petallength-and-petalwidth&#34;&gt;&lt;strong&gt;Scatterplot&lt;/strong&gt; of &lt;code&gt;Petal.Length&lt;/code&gt; and &lt;code&gt;Petal.Width&lt;/code&gt;&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(iris, # the data set
       aes(x=Petal.Width, y=Petal.Length) # aesthetics
       ) + geom_point() + # this is the end of the bare minimum plot
  theme_bw() + labs(title=&amp;quot;Petal Width and Petal Length of three different species of Iris&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;05---Data-Visualisation_files/figure-html/PlottingExercise2-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;scatterplot-of-petallength-and-petalwidth-grouped-by-species&#34;&gt;&lt;strong&gt;Scatterplot&lt;/strong&gt; of &lt;code&gt;Petal.Length&lt;/code&gt; and &lt;code&gt;Petal.Width&lt;/code&gt; grouped by &lt;code&gt;Species&lt;/code&gt;&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(iris, # the data set
       aes(x=Petal.Width, y=Petal.Length, colour = Species) # aesthetics
       ) + geom_point() + # this is the end of the bare minimum plot
  theme_bw() + labs(title=&amp;quot;Petal Width and Petal Length of three different species of Iris&amp;quot;) + 
  theme(legend.justification=c(1,0), legend.position=c(1,0)) + # legend inside
  scale_color_discrete(name=&amp;quot;Iris Species&amp;quot;)  # Change legend title
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;05---Data-Visualisation_files/figure-html/PlottingExercise3-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;relationship-of-sepallength-and-sepalwidth&#34;&gt;Relationship of &lt;code&gt;Sepal.Length&lt;/code&gt; and &lt;code&gt;Sepal.Width&lt;/code&gt;&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(iris, # the data set
       aes(x=Sepal.Width, y=Sepal.Length) # aesthetics
       ) + geom_point() + geom_smooth() + # this is the end of the bare minimum plot
  theme_bw() + labs(title=&amp;quot;Petal Width and Petal Length of three different species of Iris&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;05---Data-Visualisation_files/figure-html/PlottingExercise4-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;relationship-of-sepallength-and-sepalwidth-grouped-by-species&#34;&gt;Relationship of &lt;code&gt;Sepal.Length&lt;/code&gt; and &lt;code&gt;Sepal.Width&lt;/code&gt; (grouped by &lt;code&gt;Species&lt;/code&gt;)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(iris, # the data set
       aes(x=Sepal.Width, y=Sepal.Length, colour = Species) # aesthetics
       ) + geom_point() + geom_smooth() + # this is the end of the bare minimum plot
  theme_bw() + labs(title=&amp;quot;Petal Width and Petal Length of three different species of Iris&amp;quot;) + 
  theme(legend.justification=c(1,0), legend.position=c(1,0)) + # legend inside
  scale_color_discrete(name=&amp;quot;Iris Species&amp;quot;)  # Change legend title
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;05---Data-Visualisation_files/figure-html/PlottingExercise5-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data Visualisation</title>
      <link>https://www.erikkusch.com/courses/biostat101/data-visualisation/</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/biostat101/data-visualisation/</guid>
      <description>&lt;h2 id=&#34;theory&#34;&gt;Theory&lt;/h2&gt;
&lt;p&gt;These are the solutions to the exercises contained within the handout to Data Visualisation which walks you through the basics of data visualisation in &lt;code&gt;R&lt;/code&gt; using &lt;code&gt;ggplot2&lt;/code&gt;. The plots presented here are using data from the &lt;code&gt;iris&lt;/code&gt; data set supplied through the &lt;code&gt;datasets&lt;/code&gt; package. Keep in mind that there is probably a myriad of other ways to reach the same conclusions as presented in these solutions. I have prepared some slides for this session: &lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/biostat101/05---Data-Visualisation_Handout.html&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://www.erikkusch.com/courses/biostat101/05---BioStat101_featured.png&#34; width=&#34;900&#34; margin-top = &#34;0&#34;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;p&gt;This practical makes use of R-internal data so you don&amp;rsquo;t need to download anything extra today.&lt;/p&gt;
&lt;h2 id=&#34;packages&#34;&gt;Packages&lt;/h2&gt;
&lt;p&gt;Recall the exercise that went along with the last seminar (Descriptive Statistics) where we learnt the difference between a basic and advanced preamble for package loading in &lt;code&gt;R&lt;/code&gt;. Here (and in future exercises) I will only supply you with the advanced version of the preamble.&lt;/p&gt;
&lt;p&gt;Now let&amp;rsquo;s load the &lt;code&gt;ggplot2&lt;/code&gt; package into our &lt;code&gt;R&lt;/code&gt; session so we&amp;rsquo;ll be able to use its functionality for data visualisation as well as the &lt;code&gt;datasets&lt;/code&gt; package to get the &lt;code&gt;iris&lt;/code&gt; data set.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# function to load packages and install them if they haven&#39;t been installed yet
install.load.package &amp;lt;- function(x) {
  if (!require(x, character.only = TRUE))
    install.packages(x)
  require(x, character.only = TRUE)
}
# packages to load/install if necessary
package_vec &amp;lt;- c(&amp;quot;ggplot2&amp;quot;, &amp;quot;datasets&amp;quot;)
# applying function install.load.package to all packages specified in package_vec
sapply(package_vec, install.load.package)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggplot2
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  ggplot2 datasets 
##     TRUE     TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;loading-r-internal-data-sets-iris&#34;&gt;Loading &lt;code&gt;R&lt;/code&gt;-internal data sets (&lt;code&gt;iris&lt;/code&gt;)&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;iris&lt;/code&gt; data set is included in the &lt;code&gt;datasets&lt;/code&gt; package in &lt;code&gt;R&lt;/code&gt;. An &lt;code&gt;R&lt;/code&gt;-internal data set is loaded through the command &lt;code&gt;data()&lt;/code&gt;. Take note that you do not have to assign this command&amp;rsquo;s output to a new object (via &lt;code&gt;&amp;lt;-&lt;/code&gt;). Instead, the dataset is loaded to your current environment by its name (iris, in this case). Keep in mind that this &lt;strong&gt;can override&lt;/strong&gt; objects of the same name that are already present in your current session of &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(&amp;quot;iris&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;inspect-the-data-set&#34;&gt;Inspect the data set&lt;/h2&gt;
&lt;p&gt;Since we know that &lt;code&gt;iris&lt;/code&gt; is a dataset, we can be reasonably sure that this object will be complex enough to warrant using the &lt;code&gt;str()&lt;/code&gt; function for inspection:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(iris)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &#39;data.frame&#39;:	150 obs. of  5 variables:
##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...
##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...
##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...
##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...
##  $ Species     : Factor w/ 3 levels &amp;quot;setosa&amp;quot;,&amp;quot;versicolor&amp;quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;iris&lt;/code&gt; dataset contains four measurements (&lt;code&gt;Sepal.Length&lt;/code&gt;, &lt;code&gt;Sepal.Width&lt;/code&gt;, &lt;code&gt;Petal.Length&lt;/code&gt;, &lt;code&gt;Petal.Width&lt;/code&gt;) for 150 flowers representing three species of iris (&lt;em&gt;Iris setosa&lt;/em&gt;, &lt;em&gt;versicolor&lt;/em&gt; and &lt;em&gt;virginica&lt;/em&gt;).&lt;/p&gt;
&lt;h2 id=&#34;boxplot-of-petallength-by-species&#34;&gt;&lt;strong&gt;Boxplot&lt;/strong&gt; of &lt;code&gt;Petal.Length&lt;/code&gt; by &lt;code&gt;Species&lt;/code&gt;&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(iris, # the data set
       aes(x=Species, y=Petal.Length) # aesthetics
       ) + geom_boxplot() + # this is the end of the bare minimum plot
  theme_bw() + labs(title=&amp;quot;Petal Length of three different species of Iris&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;05---Data-Visualisation_files/figure-html/PlottingExercise1-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;THis boxplot shows us exactly how the distributions of petal length measurements of our three species of Iris are differing from one another. Despite the obvious trend in the data, &lt;strong&gt;be sure not to report results through figures alone!&lt;/strong&gt; We will find out how to test whether the pattern we can observe here holds up to scrutiny at a later point in time of our seminars.&lt;/p&gt;
&lt;h2 id=&#34;scatterplot-of-petallength-and-petalwidth&#34;&gt;&lt;strong&gt;Scatterplot&lt;/strong&gt; of &lt;code&gt;Petal.Length&lt;/code&gt; and &lt;code&gt;Petal.Width&lt;/code&gt;&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(iris, # the data set
       aes(x=Petal.Width, y=Petal.Length) # aesthetics
       ) + geom_point() + # this is the end of the bare minimum plot
  theme_bw() + labs(title=&amp;quot;Petal Width and Petal Length of three different species of Iris&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;05---Data-Visualisation_files/figure-html/PlottingExercise2-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;scatterplot-of-petallength-and-petalwidth-grouped-by-species&#34;&gt;&lt;strong&gt;Scatterplot&lt;/strong&gt; of &lt;code&gt;Petal.Length&lt;/code&gt; and &lt;code&gt;Petal.Width&lt;/code&gt; grouped by &lt;code&gt;Species&lt;/code&gt;&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(iris, # the data set
       aes(x=Petal.Width, y=Petal.Length, colour = Species) # aesthetics
       ) + geom_point() + # this is the end of the bare minimum plot
  theme_bw() + labs(title=&amp;quot;Petal Width and Petal Length of three different species of Iris&amp;quot;) + 
  theme(legend.justification=c(1,0), legend.position=c(1,0)) + # legend inside
  scale_color_discrete(name=&amp;quot;Iris Species&amp;quot;)  # Change legend title
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;05---Data-Visualisation_files/figure-html/PlottingExercise3-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;relationship-of-sepallength-and-sepalwidth&#34;&gt;Relationship of &lt;code&gt;Sepal.Length&lt;/code&gt; and &lt;code&gt;Sepal.Width&lt;/code&gt;&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(iris, # the data set
       aes(x=Sepal.Width, y=Sepal.Length) # aesthetics
       ) + geom_point() + geom_smooth() + # this is the end of the bare minimum plot
  theme_bw() + labs(title=&amp;quot;Petal Width and Petal Length of three different species of Iris&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &#39;loess&#39; and formula = &#39;y ~ x&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;05---Data-Visualisation_files/figure-html/PlottingExercise4-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;relationship-of-sepallength-and-sepalwidth-grouped-by-species&#34;&gt;Relationship of &lt;code&gt;Sepal.Length&lt;/code&gt; and &lt;code&gt;Sepal.Width&lt;/code&gt; (grouped by &lt;code&gt;Species&lt;/code&gt;)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(iris, # the data set
       aes(x=Sepal.Width, y=Sepal.Length, colour = Species) # aesthetics
       ) + geom_point() + geom_smooth() + # this is the end of the bare minimum plot
  theme_bw() + labs(title=&amp;quot;Petal Width and Petal Length of three different species of Iris&amp;quot;) + 
  theme(legend.justification=c(1,0), legend.position=c(1,0)) + # legend inside
  scale_color_discrete(name=&amp;quot;Iris Species&amp;quot;)  # Change legend title
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &#39;loess&#39; and formula = &#39;y ~ x&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;05---Data-Visualisation_files/figure-html/PlottingExercise5-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Downloading Data with rgbif</title>
      <link>https://www.erikkusch.com/courses/gbif/datadownload/</link>
      <pubDate>Sun, 21 May 2023 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/gbif/datadownload/</guid>
      <description>&lt;div class=&#34;alert alert-normal&#34;&gt;
  &lt;div&gt;
    &lt;details&gt;
  &lt;summary&gt;Preamble, Package-Loading, and GBIF API Credential Registering (click here):&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Custom install &amp;amp; load function
install.load.package &amp;lt;- function(x) {
  if (!require(x, character.only = TRUE)) {
    install.packages(x, repos = &amp;quot;http://cran.us.r-project.org&amp;quot;)
  }
  require(x, character.only = TRUE)
}
## names of packages we want installed (if not installed yet) and loaded
package_vec &amp;lt;- c(
  &amp;quot;rgbif&amp;quot;,
  &amp;quot;knitr&amp;quot;, # for rmarkdown table visualisations
  &amp;quot;rio&amp;quot; # for dwc import
)
## executing install &amp;amp; load for each package
sapply(package_vec, install.load.package)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## rgbif knitr   rio 
##  TRUE  TRUE  TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;options(gbif_user = &amp;quot;my gbif username&amp;quot;)
options(gbif_email = &amp;quot;my registred gbif e-mail&amp;quot;)
options(gbif_pwd = &amp;quot;my gbif password&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt; 
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;First, we need to register the correct &lt;strong&gt;usageKey&lt;/strong&gt; for &lt;em&gt;Lagopus muta&lt;/em&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sp_name &amp;lt;- &amp;quot;Lagopus muta&amp;quot;
sp_backbone &amp;lt;- name_backbone(name = sp_name)
sp_key &amp;lt;- sp_backbone$usageKey
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Second, this is the specific data we are interested in obtaining:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;occ_final &amp;lt;- occ_search(
  taxonKey = sp_key,
  country = &amp;quot;NO&amp;quot;,
  year = &amp;quot;2000,2020&amp;quot;,
  basisOfRecord = &amp;quot;HUMAN_OBSERVATION&amp;quot;,
  occurrenceStatus = &amp;quot;PRESENT&amp;quot;
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we are ready to obtain GBIF mediated data we may want to use in downstream analyses and applications. This can be done in one of two ways - either via:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Near-Instant Download - via &lt;code&gt;occ_data(...)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Asynchronous Download - via &lt;code&gt;occ_download(...)&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let&amp;rsquo;s explore both of these in turn.&lt;/p&gt;
&lt;h1 id=&#34;near-instant-download&#34;&gt;Near-Instant Download&lt;/h1&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    Near-instant downloads should only ever be used for initial data exploration and data stream/model building and &lt;strong&gt;not for final publications&lt;/strong&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Near-instant downloads do not require a GBIF account to be set-up and are the quickest way to obtain GBIF mediated data, but also the more limited functionality since near-instant downloads allow the user to only obtain 100,000 records per query and do not create citable DOIs for easy reference and accreditation of GBIF mediated data.&lt;/p&gt;
&lt;p&gt;Let me demonstrate how these can be executed nevertheless.&lt;/p&gt;
&lt;p&gt;As a matter of fact, we have already executed one of these through our call to &lt;code&gt;occ_search(...)&lt;/code&gt; at the top of this page. While this function does download data via the &lt;code&gt;...$data&lt;/code&gt; output it provides, let&amp;rsquo;s showcase instead how we could use the &lt;code&gt;occ_data(...)&lt;/code&gt; function to arrive at the same result:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;NI_occ &amp;lt;- occ_data(
  taxonKey = sp_key,
  country = &amp;quot;NO&amp;quot;,
  year = &amp;quot;2000,2020&amp;quot;,
  basisOfRecord = &amp;quot;HUMAN_OBSERVATION&amp;quot;,
  occurrenceStatus = &amp;quot;PRESENT&amp;quot;
)
knitr::kable(head(NI_occ$data))
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;key&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;scientificName&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;decimalLatitude&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;decimalLongitude&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;issues&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;datasetKey&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;publishingOrgKey&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;installationKey&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;hostingOrganizationKey&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;publishingCountry&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;protocol&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;lastCrawled&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;lastParsed&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;crawlId&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;basisOfRecord&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;individualCount&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;occurrenceStatus&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;taxonKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;kingdomKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;phylumKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;classKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;orderKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;familyKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;genusKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;speciesKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;acceptedTaxonKey&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;acceptedScientificName&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;kingdom&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;phylum&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;order&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;family&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;genus&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;species&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;genericName&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;specificEpithet&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;taxonRank&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;taxonomicStatus&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;iucnRedListCategory&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;continent&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;stateProvince&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;year&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;month&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;day&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;eventDate&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;startDayOfYear&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;endDayOfYear&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;lastInterpreted&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;license&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;isSequenced&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;isInCluster&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;recordedBy&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;geodeticDatum&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;class&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;countryCode&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;country&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;gbifRegion&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;publishedByGbifRegion&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;identifier&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;catalogNumber&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;vernacularName&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;institutionCode&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;taxonConceptID&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;locality&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;gbifID&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;collectionCode&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;occurrenceID&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;coordinateUncertaintyInMeters&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;modified&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;dynamicProperties&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;municipality&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;associatedReferences&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;county&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;locationID&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;fieldNotes&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;eventTime&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;behavior&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;sex&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;identificationVerificationStatus&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;identificationRemarks&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;dateIdentified&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;references&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;datasetName&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;identifiedBy&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;rightsHolder&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;http://unknown.org/nick&#34;&gt;http://unknown.org/nick&lt;/a&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;verbatimEventDate&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;verbatimLocality&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;taxonID&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;http://unknown.org/captive&#34;&gt;http://unknown.org/captive&lt;/a&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;identificationID&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;datasetID&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;footprintWKT&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;lifeStage&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;samplingProtocol&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;habitat&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;eventType&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;networkKeys&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;elevation&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;elevationAccuracy&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;organismQuantity&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;organismQuantityType&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;sampleSizeUnit&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;sampleSizeValue&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;eventID&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;informationWithheld&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;dataGeneralizations&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;samplingEffort&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;type&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;ownerInstitutionCode&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;occurrenceRemarks&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;name&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;3219111975&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta (Montin, 1781)&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;58.46553&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;8.895254&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;cdc&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;4fa7b334-ce0d-4e88-aaae-2e0c138d049e&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;e2e717bf-551a-4917-bdc9-4fa0f342c530&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;7182d304-b0a2-404b-baba-2086a325c221&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;e2e717bf-551a-4917-bdc9-4fa0f342c530&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NO&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;DWC_ARCHIVE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2024-04-15T22:36:22.136+00:00&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2024-04-17T08:30:11.331+00:00&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;20&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;HUMAN_OBSERVATION&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;PRESENT&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;44&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;212&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;723&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;9331&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2473369&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta (Montin, 1781)&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Animalia&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Chordata&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Galliformes&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Phasianidae&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SPECIES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;ACCEPTED&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;LC&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EUROPE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Aust-Agder&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2020&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2020-01-05&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2024-04-17T08:30:11.331+00:00&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;http://creativecommons.org/licenses/by/4.0/legalcode&#34;&gt;http://creativecommons.org/licenses/by/4.0/legalcode&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;obsr1460143&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;WGS84&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Aves&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NO&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Norway&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EUROPE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EUROPE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;OBS854553845&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;OBS854553845&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Rock Ptarmigan&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CLO&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;avibase-79B161B7&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Botstangen&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;3219111975&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EBIRD&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;URN:catalog:CLO:EBIRD:OBS854553845&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NULL&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta (Montin, 1781)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2560812506&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta (Montin, 1781)&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;60.86029&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;8.513532&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;cdc,cdround,gass84&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;b124e1e0-4755-430f-9eab-894f25a9b59c&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;d3978a37-635a-4ae3-bb85-7b4d41bc0b88&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;7bdf9f6d-317a-45ec-8bb7-7ff61345d6a6&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;d3978a37-635a-4ae3-bb85-7b4d41bc0b88&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NO&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EML&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2024-10-29T15:12:40.936+00:00&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2024-10-29T19:14:46.578+00:00&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;382&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;HUMAN_OBSERVATION&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;18&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;PRESENT&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;44&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;212&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;723&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;9331&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2473369&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta (Montin, 1781)&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Animalia&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Chordata&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Galliformes&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Phasianidae&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SPECIES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;ACCEPTED&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;LC&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EUROPE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Viken&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2020&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;26&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2020-01-26&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;26&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;26&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2024-10-29T19:14:46.578+00:00&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;http://creativecommons.org/licenses/by/4.0/legalcode&#34;&gt;http://creativecommons.org/licenses/by/4.0/legalcode&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Magnus Larsson&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;WGS84&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Aves&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NO&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Norway&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EUROPE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EUROPE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;urn:uuid:089a889f-e894-433c-a466-9e00708d212c&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;23369915&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;nof&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Hemsedal Skisenter, Hemsedal, Vi&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2560812506&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;so2-birds&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;urn:uuid:089a889f-e894-433c-a466-9e00708d212c&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;300&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2020-01-28T08:37:41.000+00:00&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;{&amp;ldquo;Activity&amp;rdquo;:&amp;ldquo;Stationary&amp;rdquo;}&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Hemsedal&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.artsobservasjoner.no/Sighting/23369915&#34;&gt;https://www.artsobservasjoner.no/Sighting/23369915&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Viken&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;394857&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Activity: Stationary.&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;11.0/11.17&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;stationary&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NULL&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta (Montin, 1781)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2544028549&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta (Montin, 1781)&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;58.46407&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;8.895621&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;cdc,cdround,gass84&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;b124e1e0-4755-430f-9eab-894f25a9b59c&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;d3978a37-635a-4ae3-bb85-7b4d41bc0b88&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;7bdf9f6d-317a-45ec-8bb7-7ff61345d6a6&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;d3978a37-635a-4ae3-bb85-7b4d41bc0b88&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NO&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EML&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2024-10-29T15:12:40.936+00:00&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2024-10-29T19:08:50.532+00:00&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;382&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;HUMAN_OBSERVATION&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;PRESENT&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;44&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;212&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;723&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;9331&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2473369&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta (Montin, 1781)&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Animalia&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Chordata&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Galliformes&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Phasianidae&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SPECIES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;ACCEPTED&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;LC&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EUROPE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Agder&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2020&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2020-01-05&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2024-10-29T19:08:50.532+00:00&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;http://creativecommons.org/licenses/by/4.0/legalcode&#34;&gt;http://creativecommons.org/licenses/by/4.0/legalcode&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Trond Nilsen&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;WGS84&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Aves&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NO&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Norway&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EUROPE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EUROPE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;urn:uuid:0bd9ed37-74f0-4bef-bd31-f9b6f45c5430&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;23266188&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;nof&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;BlÃ¥mannen, Botne, Arendal, Ag&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2544028549&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;so2-birds&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;urn:uuid:0bd9ed37-74f0-4bef-bd31-f9b6f45c5430&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;79&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2020-01-05T17:57:17.000+00:00&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Arendal&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.artsobservasjoner.no/Sighting/23266188&#34;&gt;https://www.artsobservasjoner.no/Sighting/23266188&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Agder&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;461661&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;8.67/9.33&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NULL&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta (Montin, 1781)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2549610691&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta (Montin, 1781)&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;59.49676&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;8.990860&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;cdc,cdround,gass84&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;b124e1e0-4755-430f-9eab-894f25a9b59c&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;d3978a37-635a-4ae3-bb85-7b4d41bc0b88&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;7bdf9f6d-317a-45ec-8bb7-7ff61345d6a6&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;d3978a37-635a-4ae3-bb85-7b4d41bc0b88&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NO&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EML&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2024-10-29T15:12:40.936+00:00&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2024-10-29T19:09:45.170+00:00&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;382&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;HUMAN_OBSERVATION&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;6&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;PRESENT&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;44&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;212&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;723&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;9331&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2473369&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta (Montin, 1781)&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Animalia&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Chordata&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Galliformes&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Phasianidae&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SPECIES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;ACCEPTED&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;LC&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EUROPE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Vestfold og Telemark&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2020&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;10&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2020-01-10&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;10&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;10&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2024-10-29T19:09:45.170+00:00&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;http://creativecommons.org/licenses/by/4.0/legalcode&#34;&gt;http://creativecommons.org/licenses/by/4.0/legalcode&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Ola Nordsteien|Hanne LÃ¸vberg&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;WGS84&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Aves&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NO&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Norway&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EUROPE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EUROPE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;urn:uuid:25c5ecea-3187-4436-ab51-dc8438e78bb8&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;23288768&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;nof&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SkÃ¥rÃ¥fjell, Lifjell, Midt-Telemark, Vt&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2549610691&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;so2-birds&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;urn:uuid:25c5ecea-3187-4436-ab51-dc8438e78bb8&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;300&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2020-01-10T23:48:45.000+00:00&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;{&amp;ldquo;Activity&amp;rdquo;:&amp;ldquo;Forage&amp;rdquo;}&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Midt-Telemark&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.artsobservasjoner.no/Sighting/23288768&#34;&gt;https://www.artsobservasjoner.no/Sighting/23288768&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Vestfold og Telemark&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;359743&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Activity: Forage.&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;feeding&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NULL&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta (Montin, 1781)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2560817283&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta (Montin, 1781)&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;69.77555&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;18.822903&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;cdc,cdround,gass84&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;b124e1e0-4755-430f-9eab-894f25a9b59c&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;d3978a37-635a-4ae3-bb85-7b4d41bc0b88&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;7bdf9f6d-317a-45ec-8bb7-7ff61345d6a6&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;d3978a37-635a-4ae3-bb85-7b4d41bc0b88&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NO&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EML&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2024-10-29T15:12:40.936+00:00&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2024-10-29T19:15:36.858+00:00&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;382&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;HUMAN_OBSERVATION&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;PRESENT&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;44&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;212&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;723&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;9331&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2473369&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta (Montin, 1781)&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Animalia&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Chordata&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Galliformes&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Phasianidae&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SPECIES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;ACCEPTED&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;LC&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EUROPE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Troms og Finnmark&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2020&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;26&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2020-01-26&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;26&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;26&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2024-10-29T19:15:36.858+00:00&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;http://creativecommons.org/licenses/by/4.0/legalcode&#34;&gt;http://creativecommons.org/licenses/by/4.0/legalcode&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Ole-Morten Toften&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;WGS84&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Aves&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NO&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Norway&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EUROPE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EUROPE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;urn:uuid:2f576cfd-4387-4473-875c-b6ce9fa7113b&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;23359018&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;nof&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lyfjorddalen, KvalÃ¸ya, TromsÃ¸, Tf&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2560817283&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;so2-birds&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;urn:uuid:2f576cfd-4387-4473-875c-b6ce9fa7113b&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;300&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2020-01-26T13:07:00.000+00:00&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;{&amp;ldquo;Activity&amp;rdquo;:&amp;ldquo;Display/SongOutsideBreeding&amp;rdquo;}&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;TromsÃ¸&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.artsobservasjoner.no/Sighting/23359018&#34;&gt;https://www.artsobservasjoner.no/Sighting/23359018&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Troms og Finnmark&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;371536&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Activity: Display/SongOutsideBreeding.&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;8.77&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;stationary&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;MALE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NULL&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta (Montin, 1781)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2544037194&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta (Montin, 1781)&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;62.57669&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;11.387036&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;cdc,cdround,gass84&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;b124e1e0-4755-430f-9eab-894f25a9b59c&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;d3978a37-635a-4ae3-bb85-7b4d41bc0b88&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;7bdf9f6d-317a-45ec-8bb7-7ff61345d6a6&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;d3978a37-635a-4ae3-bb85-7b4d41bc0b88&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NO&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EML&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2024-10-29T15:12:40.936+00:00&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2024-10-29T19:10:26.365+00:00&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;382&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;HUMAN_OBSERVATION&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;PRESENT&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;44&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;212&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;723&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;9331&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2473369&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta (Montin, 1781)&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Animalia&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Chordata&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Galliformes&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Phasianidae&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SPECIES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;ACCEPTED&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;LC&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EUROPE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;TrÃ¸ndelag&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2020&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2020-01-04&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2024-10-29T19:10:26.365+00:00&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;http://creativecommons.org/licenses/by/4.0/legalcode&#34;&gt;http://creativecommons.org/licenses/by/4.0/legalcode&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Norvald GrÃ¸nning&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;WGS84&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Aves&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NO&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Norway&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EUROPE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EUROPE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;urn:uuid:3a154636-f513-46de-a3d6-6c2dfdeb3283&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;23267493&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;nof&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;RÃ¸ros by, RÃ¸ros, TÃ¸&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2544037194&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;so2-birds&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;urn:uuid:3a154636-f513-46de-a3d6-6c2dfdeb3283&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;300&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2021-09-22T11:26:11.000+00:00&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;{&amp;ldquo;ValidationStatus&amp;rdquo;:&amp;ldquo;Approved Documented&amp;rdquo;}&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;RÃ¸ros&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.artsobservasjoner.no/Sighting/23267493&#34;&gt;https://www.artsobservasjoner.no/Sighting/23267493&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;TrÃ¸ndelag&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;355523&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Validationstatus: Approved Documented&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;validated&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Validator: Andreas Winnem&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NULL&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta (Montin, 1781)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The near-instantaneous nature of this type of download is largely due to the hard limit on how much data you may obtain. For our example, the 100,000 record limit is not an issue as we are dealing with only 10606 records being available to us, but queries to GBIF mediated records can easily and quickly exceed this limit.&lt;/p&gt;
&lt;h1 id=&#34;asynchronous-download&#34;&gt;Asynchronous Download&lt;/h1&gt;
&lt;div class=&#34;alert alert-success&#34;&gt;
  &lt;div&gt;
    This is how you should obtain data &lt;strong&gt;for publication-level research and reports&lt;/strong&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;To avoid the reproducibility and data richness limitations of near-instant downloads, we must make a more formal download query to the GBIF API and await processing of our data request - an asynchronous download. To do so, we need to do three things in order:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Specify the data we want and request processing and download from GBIF&lt;/li&gt;
&lt;li&gt;Download the data once it is processed&lt;/li&gt;
&lt;li&gt;Load the downloaded data into &lt;code&gt;R&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;alert alert-info&#34;&gt;
  &lt;div&gt;
    Time between registering a download request and retrieval of data may vary according to how busy the GBIF API and servers are as well as the size and complexity of the data requested. GBIF will only handle a maximum of 3 download request per user simultaneously.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Let&amp;rsquo;s tackle these step-by-step.&lt;/p&gt;
&lt;h2 id=&#34;data-request-at-gbif&#34;&gt;Data Request at GBIF&lt;/h2&gt;
&lt;p&gt;To start this process of obtaining GBIF-mediated occurrence records, we first need to make a request to GBIF to start processing of the data we require. This is done with the &lt;code&gt;occ_download(...)&lt;/code&gt; function. Making a data request at GBIF via the &lt;code&gt;occ_download(...)&lt;/code&gt; function comes with two important considerations:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;occ_download(...)&lt;/code&gt; specific syntax&lt;/li&gt;
&lt;li&gt;data query metadata&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;syntax-and-query-through-occ_download&#34;&gt;Syntax and Query through &lt;code&gt;occ_download()&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;occ_download(...)&lt;/code&gt; function - while powerful - requires us to confront a new set of syntax which translates the download request as we have seen it so far into a form which the GBIF API can understand. To do so, we use a host of &lt;code&gt;rgbif&lt;/code&gt; functions built around the GBIF predicate DSL (domain specific language). These functions (with a few exceptions which you can see by calling the documentation - &lt;code&gt;?download_predicate_dsl&lt;/code&gt;) all take two arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;key&lt;/code&gt; - this is a core term which we want to target for our request&lt;/li&gt;
&lt;li&gt;&lt;code&gt;value&lt;/code&gt; - this is the value for the core term which we are interested in&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finally, the relevant functions and how the relate &lt;code&gt;key&lt;/code&gt; to &lt;code&gt;value&lt;/code&gt; are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;pred(...)&lt;/code&gt;: equals&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pred_lt(...)&lt;/code&gt;: lessThan&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pred_lte(...)&lt;/code&gt;: lessThanOrEquals&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pred_gt(...)&lt;/code&gt;: greaterThan&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pred_gte(...)&lt;/code&gt;: greaterThanOrEquals&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pred_like(...)&lt;/code&gt;: like&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pred_within(...)&lt;/code&gt;: within (only for geospatial queries, and only accepts a WKT string)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pred_notnull(...)&lt;/code&gt;: isNotNull (only for geospatial queries, and only accepts a WKT string)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pred_isnull(...)&lt;/code&gt;: isNull (only for stating that you want a key to be null)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pred_and(...)&lt;/code&gt;: and (accepts multiple individual predicates, separating them by either &amp;ldquo;and&amp;rdquo; or &amp;ldquo;or&amp;rdquo;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pred_or(...)&lt;/code&gt;: or (accepts multiple individual predicates, separating them by either &amp;ldquo;and&amp;rdquo; or &amp;ldquo;or&amp;rdquo;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pred_not(...)&lt;/code&gt;: not (negates whatever predicate is passed in)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pred_in(...)&lt;/code&gt;: in (accepts a single key but many values; stating that you want to search for all the values)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let us use these to query the data we are interested in:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;res &amp;lt;- occ_download(
  pred(&amp;quot;taxonKey&amp;quot;, sp_key),
  pred(&amp;quot;basisOfRecord&amp;quot;, &amp;quot;HUMAN_OBSERVATION&amp;quot;),
  pred(&amp;quot;country&amp;quot;, &amp;quot;NO&amp;quot;),
  pred(&amp;quot;hasCoordinate&amp;quot;, TRUE),
  pred_gte(&amp;quot;year&amp;quot;, 2000),
  pred_lte(&amp;quot;year&amp;quot;, 2020)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that the download request has been made with GBIF, you need to wait for GBIF to process your query and return the desired data to you. You will find an overview of all your requested and processed downloads in your personal 
&lt;a href=&#34;https://www.gbif.org/user/download&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;download tab&lt;/a&gt; on the GBIF webportal.&lt;/p&gt;
&lt;h3 id=&#34;data-query-metadata&#34;&gt;Data Query Metadata&lt;/h3&gt;
&lt;p&gt;To keep track of all your data queries, each call to &lt;code&gt;occ_download(...)&lt;/code&gt; returns a set of metadata for your download request as part of its object structure. This can be assessed as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;occ_download_meta(res)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;&amp;lt;gbif download metadata&amp;gt;&amp;gt;
##   Status: SUCCEEDED
##   DOI: 10.15468/dl.vjazpv
##   Format: DWCA
##   Download key: 0010612-241024112534372
##   Created: 2024-10-29T13:25:46.576+00:00
##   Modified: 2024-10-29T13:27:15.574+00:00
##   Download link: https://api.gbif.org/v1/occurrence/download/request/0010612-241024112534372.zip
##   Total records: 10609
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It can even be subset for relevant parts of it like so:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;occ_download_meta(res)$downloadLink
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;https://api.gbif.org/v1/occurrence/download/request/0010612-241024112534372.zip&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is done according to the underlying structure of the metadata:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(occ_download_meta(res), max.level = 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## List of 12
##  $ key           : chr &amp;quot;0010612-241024112534372&amp;quot;
##  $ doi           : chr &amp;quot;10.15468/dl.vjazpv&amp;quot;
##  $ license       : chr &amp;quot;http://creativecommons.org/licenses/by-nc/4.0/legalcode&amp;quot;
##  $ request       :List of 5
##  $ created       : chr &amp;quot;2024-10-29T13:25:46.576+00:00&amp;quot;
##  $ modified      : chr &amp;quot;2024-10-29T13:27:15.574+00:00&amp;quot;
##  $ eraseAfter    : chr &amp;quot;2025-04-29T13:25:46.526+00:00&amp;quot;
##  $ status        : chr &amp;quot;SUCCEEDED&amp;quot;
##  $ downloadLink  : chr &amp;quot;https://api.gbif.org/v1/occurrence/download/request/0010612-241024112534372.zip&amp;quot;
##  $ size          : int 2842371
##  $ totalRecords  : int 10609
##  $ numberDatasets: int 18
##  - attr(*, &amp;quot;class&amp;quot;)= chr &amp;quot;occ_download_meta&amp;quot;
##  - attr(*, &amp;quot;format&amp;quot;)= chr &amp;quot;DWCA&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; You will find that the number of records in our query made via &lt;code&gt;occ_download(...)&lt;/code&gt; here and &lt;code&gt;occ_search(...)&lt;/code&gt; above are different. This is due to different ways data ranges are handled.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;occ_download_meta(res)$totalRecords
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10609
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;data-download&#34;&gt;Data Download&lt;/h2&gt;
&lt;p&gt;Now that the download request is registered with GBIF, it is time to actually obtain the data you have queried. To facilitate this, there are two distinct ways to obtain the data itself:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Webportal&lt;/li&gt;
&lt;li&gt;&lt;code&gt;rgbif&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Personally, I can not recommend any of these methods over the &lt;code&gt;rgbif&lt;/code&gt;-internal download method. However, in case you might prefer other options, I will quickly show how these work, too.&lt;/p&gt;
&lt;h3 id=&#34;gbif-portal-download-method&#34;&gt;GBIF Portal Download Method&lt;/h3&gt;
&lt;p&gt;At the most basic level, you may obtain data through the 
&lt;a href=&#34;https://www.gbif.org/user/download&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GBIF portal&lt;/a&gt; where you will find an overview of your download requests. Any individual download request can be investigated more deeply by clicking the &amp;ldquo;SHOW&amp;rdquo; button:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.erikkusch.com/courses/gbif/gbif-download1.jpeg&#34; width=&#34;100%&#34;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;There, you will see a breakdown of your download request and also whether it is still being processed or already finished:
&lt;img src=&#34;https://www.erikkusch.com/courses/gbif/gbif-download2.jpeg&#34; width=&#34;100%&#34;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Note that the DOI differs here from the other images and the data product we are working with ultimately. This is because I had to requeue the download to get this screenshot.&lt;/p&gt;
&lt;p&gt;Upon completion of your request on the GBIF end, a &amp;ldquo;DOWNLOAD&amp;rdquo; button will appear on the specific download request overview page:
&lt;img src=&#34;https://www.erikkusch.com/courses/gbif/gbif-download3.jpeg&#34; width=&#34;100%&#34;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For requests of few data points you will usually have to be quite quick to see this in action as GBIF is often pretty fast and getting data ready for you.&lt;/p&gt;
&lt;h3 id=&#34;rgbif-download-method&#34;&gt;&lt;code&gt;rgbif&lt;/code&gt; Download Method&lt;/h3&gt;
&lt;div class=&#34;alert alert-success&#34;&gt;
  &lt;div&gt;
    This is the &lt;strong&gt;preferred method&lt;/strong&gt; for downloading data you have requested from GBIF.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;To download our data programmatically, we need to execute roughly the same steps as we did above:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Wait for data to be ready for download&lt;/li&gt;
&lt;li&gt;Download the data&lt;/li&gt;
&lt;li&gt;Load the data into &lt;code&gt;R&lt;/code&gt; (we didn&amp;rsquo;t do this in the above)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This is done with the following three corresponding functions:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## 1. Check GBIF whether data is ready, this function will finish running when done and return metadata
res_meta &amp;lt;- occ_download_wait(res, status_ping = 5, curlopts = list(), quiet = FALSE)
## 2. Download the data as .zip (can specify a path)
res_get &amp;lt;- occ_download_get(res)
## 3. Load the data into R
res_data &amp;lt;- occ_download_import(res_get)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at the data we just loaded:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dim(res_data)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10609   223
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;knitr::kable(head(res_data))
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right&#34;&gt;gbifID&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;accessRights&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;bibliographicCitation&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;language&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;license&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;modified&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;publisher&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;references&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;rightsHolder&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;type&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;institutionID&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;collectionID&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;datasetID&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;institutionCode&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;collectionCode&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;datasetName&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;ownerInstitutionCode&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;basisOfRecord&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;informationWithheld&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;dataGeneralizations&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;dynamicProperties&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;occurrenceID&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;catalogNumber&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;recordNumber&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;recordedBy&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;recordedByID&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;individualCount&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;organismQuantity&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;organismQuantityType&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;sex&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;lifeStage&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;reproductiveCondition&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;caste&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;behavior&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;vitality&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;establishmentMeans&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;degreeOfEstablishment&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;pathway&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;georeferenceVerificationStatus&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;occurrenceStatus&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;preparations&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;disposition&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;associatedOccurrences&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;associatedReferences&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;associatedSequences&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;associatedTaxa&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;otherCatalogNumbers&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;occurrenceRemarks&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;organismID&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;organismName&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;organismScope&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;associatedOrganisms&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;previousIdentifications&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;organismRemarks&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;materialEntityID&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;materialEntityRemarks&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;verbatimLabel&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;materialSampleID&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;eventID&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;parentEventID&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;eventType&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;fieldNumber&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;eventDate&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;eventTime&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;startDayOfYear&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;endDayOfYear&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;year&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;month&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;day&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;verbatimEventDate&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;habitat&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;samplingProtocol&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;sampleSizeValue&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;sampleSizeUnit&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;samplingEffort&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;fieldNotes&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;eventRemarks&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;locationID&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;higherGeographyID&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;higherGeography&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;continent&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;waterBody&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;islandGroup&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;island&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;countryCode&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;stateProvince&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;county&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;municipality&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;locality&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;verbatimLocality&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;verbatimElevation&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;verticalDatum&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;verbatimDepth&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;minimumDistanceAboveSurfaceInMeters&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;maximumDistanceAboveSurfaceInMeters&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;locationAccordingTo&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;locationRemarks&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;decimalLatitude&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;decimalLongitude&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;coordinateUncertaintyInMeters&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;coordinatePrecision&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;pointRadiusSpatialFit&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;verbatimCoordinateSystem&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;verbatimSRS&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;footprintWKT&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;footprintSRS&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;footprintSpatialFit&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;georeferencedBy&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;georeferencedDate&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;georeferenceProtocol&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;georeferenceSources&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;georeferenceRemarks&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;geologicalContextID&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;earliestEonOrLowestEonothem&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;latestEonOrHighestEonothem&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;earliestEraOrLowestErathem&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;latestEraOrHighestErathem&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;earliestPeriodOrLowestSystem&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;latestPeriodOrHighestSystem&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;earliestEpochOrLowestSeries&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;latestEpochOrHighestSeries&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;earliestAgeOrLowestStage&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;latestAgeOrHighestStage&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;lowestBiostratigraphicZone&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;highestBiostratigraphicZone&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;lithostratigraphicTerms&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;group&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;formation&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;member&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;bed&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;identificationID&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;verbatimIdentification&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;identificationQualifier&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;typeStatus&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;identifiedBy&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;identifiedByID&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;dateIdentified&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;identificationReferences&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;identificationVerificationStatus&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;identificationRemarks&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;taxonID&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;scientificNameID&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;acceptedNameUsageID&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;parentNameUsageID&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;originalNameUsageID&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;nameAccordingToID&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;namePublishedInID&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;taxonConceptID&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;scientificName&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;acceptedNameUsage&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;parentNameUsage&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;originalNameUsage&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;nameAccordingTo&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;namePublishedIn&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;namePublishedInYear&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;higherClassification&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;kingdom&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;phylum&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;class&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;order&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;superfamily&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;family&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;subfamily&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;tribe&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;subtribe&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;genus&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;genericName&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;subgenus&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;infragenericEpithet&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;specificEpithet&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;infraspecificEpithet&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;cultivarEpithet&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;taxonRank&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;verbatimTaxonRank&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;vernacularName&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;nomenclaturalCode&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;taxonomicStatus&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;nomenclaturalStatus&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;taxonRemarks&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;datasetKey&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;publishingCountry&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;lastInterpreted&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;elevation&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;elevationAccuracy&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;depth&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;depthAccuracy&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;distanceFromCentroidInMeters&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;issue&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;mediaType&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;hasCoordinate&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;hasGeospatialIssues&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;taxonKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;acceptedTaxonKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;kingdomKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;phylumKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;classKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;orderKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;familyKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;genusKey&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;subgenusKey&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;speciesKey&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;species&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;acceptedScientificName&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;verbatimScientificName&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;typifiedName&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;protocol&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;lastParsed&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;lastCrawled&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;repatriated&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;relativeOrganismQuantity&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;projectId&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;isSequenced&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;gbifRegion&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;publishedByGbifRegion&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;level0Gid&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;level0Name&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;level1Gid&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;level1Name&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;level2Gid&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;level2Name&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;level3Gid&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;level3Name&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;iucnRedListCategory&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;979205774&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CC_BY_4_0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CLO&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EBIRD&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;HUMAN_OBSERVATION&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;URN:catalog:CLO:EBIRD:OBS238252603&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;OBS238252603&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;obsr498576&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;PRESENT&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2011-05-12&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;132&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;132&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2011&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;12&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EUROPE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NO&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Oppland&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Fisketjerni&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;61.37563&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;8.840089&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;avibase-79B161B7&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta (Montin, 1781)&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Animalia&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Chordata&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Aves&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Galliformes&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Phasianidae&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SPECIES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Rock Ptarmigan&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;ACCEPTED&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;4fa7b334-ce0d-4e88-aaae-2e0c138d049e&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NO&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2024-04-17T08:24:53.730Z&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CONTINENT_DERIVED_FROM_COORDINATES;TAXON_MATCH_TAXON_CONCEPT_ID_IGNORED&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;TRUE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;44&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;212&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;723&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;9331&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2473369&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta (Montin, 1781)&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;DWC_ARCHIVE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2024-04-17T08:24:53.730Z&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2024-04-15T22:36:22.136Z&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EUROPE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EUROPE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NOR&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Norway&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NOR.11_1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Oppland&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NOR.11.16_1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Ãystre Slidre&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;LC&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;977864468&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CC_BY_4_0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CLO&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EBIRD&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;HUMAN_OBSERVATION&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;URN:catalog:CLO:EBIRD:OBS227313790&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;OBS227313790&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;obsr419757&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;PRESENT&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2007-06-09&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;160&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;160&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2007&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;6&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;9&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EUROPE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NO&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Finnmark&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Syltefjord&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;70.54530&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;30.054518&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;avibase-79B161B7&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta (Montin, 1781)&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Animalia&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Chordata&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Aves&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Galliformes&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Phasianidae&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SPECIES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Rock Ptarmigan&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;ACCEPTED&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;4fa7b334-ce0d-4e88-aaae-2e0c138d049e&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NO&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2024-04-17T08:23:18.063Z&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CONTINENT_DERIVED_FROM_COORDINATES;TAXON_MATCH_TAXON_CONCEPT_ID_IGNORED&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;TRUE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;44&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;212&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;723&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;9331&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2473369&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta (Montin, 1781)&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;DWC_ARCHIVE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2024-04-17T08:23:18.063Z&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2024-04-15T22:36:22.136Z&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EUROPE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EUROPE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NOR&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Norway&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NOR.5_1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Finnmark&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NOR.5.2_1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;BÃ¥tsfjord&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;LC&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;977848277&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CC_BY_4_0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CLO&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EBIRD&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;HUMAN_OBSERVATION&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;URN:catalog:CLO:EBIRD:OBS227314225&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;OBS227314225&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;obsr419757&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;PRESENT&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2007-06-09&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;160&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;160&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2007&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;6&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;9&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EUROPE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NO&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Finnmark&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Julahaugen&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;70.52452&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;29.125786&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;avibase-79B161B7&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta (Montin, 1781)&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Animalia&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Chordata&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Aves&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Galliformes&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Phasianidae&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SPECIES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Rock Ptarmigan&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;ACCEPTED&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;4fa7b334-ce0d-4e88-aaae-2e0c138d049e&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NO&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2024-04-17T08:23:56.208Z&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CONTINENT_DERIVED_FROM_COORDINATES;TAXON_MATCH_TAXON_CONCEPT_ID_IGNORED&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;TRUE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;44&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;212&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;723&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;9331&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2473369&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta (Montin, 1781)&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;DWC_ARCHIVE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2024-04-17T08:23:56.208Z&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2024-04-15T22:36:22.136Z&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EUROPE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EUROPE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NOR&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Norway&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NOR.5_1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Finnmark&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NOR.5.3_1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;BerlevÃ¥g&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;LC&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;976560970&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CC_BY_4_0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CLO&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EBIRD&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;HUMAN_OBSERVATION&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;URN:catalog:CLO:EBIRD:OBS223362772&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;OBS223362772&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;obsr226921&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;PRESENT&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2010-05-29&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;149&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;149&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2010&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;29&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EUROPE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NO&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SÃ¸r-TrÃ¸ndelag&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Norway, Kongsvold&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;62.30637&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;9.609218&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;avibase-79B161B7&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta (Montin, 1781)&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Animalia&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Chordata&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Aves&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Galliformes&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Phasianidae&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SPECIES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Rock Ptarmigan&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;ACCEPTED&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;4fa7b334-ce0d-4e88-aaae-2e0c138d049e&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NO&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2024-04-17T09:50:27.030Z&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CONTINENT_DERIVED_FROM_COORDINATES;TAXON_MATCH_TAXON_CONCEPT_ID_IGNORED&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;TRUE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;44&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;212&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;723&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;9331&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2473369&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta (Montin, 1781)&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;DWC_ARCHIVE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2024-04-17T09:50:27.030Z&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2024-04-15T22:36:22.136Z&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EUROPE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EUROPE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NOR&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Norway&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NOR.15_1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SÃ¸r-TrÃ¸ndelag&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NOR.15.13_1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Oppdal&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;LC&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;956725853&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CC_BY_4_0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CLO&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EBIRD&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;HUMAN_OBSERVATION&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;URN:catalog:CLO:EBIRD:OBS201175760&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;OBS201175760&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;obsr98173&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;PRESENT&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2013-06-12&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;163&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;163&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2013&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;6&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;12&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EUROPE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NO&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Nordland&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Arctic Circle  - E6&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;66.55537&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;15.315628&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;avibase-79B161B7&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta (Montin, 1781)&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Animalia&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Chordata&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Aves&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Galliformes&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Phasianidae&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SPECIES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Rock Ptarmigan&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;ACCEPTED&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;4fa7b334-ce0d-4e88-aaae-2e0c138d049e&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NO&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2024-04-17T08:23:05.601Z&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CONTINENT_DERIVED_FROM_COORDINATES;TAXON_MATCH_TAXON_CONCEPT_ID_IGNORED&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;TRUE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;44&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;212&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;723&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;9331&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2473369&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta (Montin, 1781)&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;DWC_ARCHIVE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2024-04-17T08:23:05.601Z&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2024-04-15T22:36:22.136Z&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EUROPE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EUROPE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NOR&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Norway&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NOR.10_1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Nordland&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NOR.10.28_1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Rana&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;LC&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;922124083&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CC_BY_4_0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;naturgucker&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;naturgucker&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;HUMAN_OBSERVATION&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1450641070&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;-1963296749&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;PRESENT&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2011-08-04T00:00&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;216&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;216&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2011&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;8&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EUROPE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NO&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Ferienhaus Brekkeseter&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;61.89373&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;9.020678&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;250&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta (Montin, 1781)&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Animalia&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Chordata&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Aves&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Galliformes&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Phasianidae&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SPECIES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;ACCEPTED&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;6ac3f774-d9fb-4796-b3e9-92bf6c81c084&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;DE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2024-03-15T23:26:53.443Z&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;COORDINATE_ROUNDED;GEODETIC_DATUM_ASSUMED_WGS84;CONTINENT_DERIVED_FROM_COORDINATES;MULTIMEDIA_URI_INVALID&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;TRUE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;44&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;212&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;723&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;9331&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2473369&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta (Montin, 1781)&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;BIOCASE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2024-03-15T23:26:53.443Z&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2024-03-15T21:43:29.197Z&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;TRUE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EUROPE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;EUROPE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NOR&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Norway&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NOR.11_1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Oppland&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NOR.11.23_1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;VÃ¥gÃ¥&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;LC&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Note that the structure of this data set is different to that obtained with &lt;code&gt;occ_search(...)&lt;/code&gt; hence enforcing that you &lt;strong&gt;should always stage downloads whose data are used in publications with &lt;code&gt;occ_download(...)&lt;/code&gt;!&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;alert alert-success&#34;&gt;
  &lt;div&gt;
    You now have the means of &lt;strong&gt;obtaining GBIF mediated data&lt;/strong&gt;. Next, you will need to work with that data locally.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h1 id=&#34;session-info&#34;&gt;Session Info&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.4.0 (2024-04-24 ucrt)
## Platform: x86_64-w64-mingw32/x64
## Running under: Windows 11 x64 (build 22631)
## 
## Matrix products: default
## 
## 
## locale:
## [1] LC_COLLATE=Norwegian BokmÃ¥l_Norway.utf8  LC_CTYPE=Norwegian BokmÃ¥l_Norway.utf8   
## [3] LC_MONETARY=Norwegian BokmÃ¥l_Norway.utf8 LC_NUMERIC=C                            
## [5] LC_TIME=Norwegian BokmÃ¥l_Norway.utf8    
## 
## time zone: Europe/Oslo
## tzcode source: internal
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] rio_1.2.3   knitr_1.48  rgbif_3.8.1
## 
## loaded via a namespace (and not attached):
##  [1] styler_1.10.3     sass_0.4.9        utf8_1.2.4        generics_0.1.3   
##  [5] xml2_1.3.6        blogdown_1.19     stringi_1.8.4     httpcode_0.3.0   
##  [9] digest_0.6.37     magrittr_2.0.3    evaluate_0.24.0   grid_4.4.0       
## [13] bookdown_0.40     fastmap_1.2.0     R.oo_1.26.0       R.cache_0.16.0   
## [17] plyr_1.8.9        jsonlite_1.8.8    R.utils_2.12.3    whisker_0.4.1    
## [21] crul_1.5.0        urltools_1.7.3    httr_1.4.7        purrr_1.0.2      
## [25] fansi_1.0.6       scales_1.3.0      oai_0.4.0         lazyeval_0.2.2   
## [29] jquerylib_0.1.4   cli_3.6.3         rlang_1.1.4       triebeard_0.4.1  
## [33] R.methodsS3_1.8.2 bit64_4.0.5       munsell_0.5.1     cachem_1.1.0     
## [37] yaml_2.3.10       tools_4.4.0       dplyr_1.1.4       colorspace_2.1-1 
## [41] ggplot2_3.5.1     curl_5.2.2        vctrs_0.6.5       R6_2.5.1         
## [45] lifecycle_1.0.4   stringr_1.5.1     bit_4.0.5         pkgconfig_2.0.3  
## [49] pillar_1.9.0      bslib_0.8.0       gtable_0.3.6      data.table_1.16.0
## [53] glue_1.7.0        Rcpp_1.0.13       xfun_0.47         tibble_3.2.1     
## [57] tidyselect_1.2.1  htmltools_0.5.8.1 rmarkdown_2.28    compiler_4.4.0
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Classifications</title>
      <link>https://www.erikkusch.com/courses/excursions-into-biostatistics/classifications-order-from-chaos/</link>
      <pubDate>Sat, 27 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/excursions-into-biostatistics/classifications-order-from-chaos/</guid>
      <description>&lt;h2 id=&#34;theory&#34;&gt;Theory&lt;/h2&gt;
&lt;p&gt;These are exercises and solutions meant as a compendium to my talk on Model Selection and Model Building.&lt;/p&gt;
&lt;p&gt;I have prepared some &lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/Excursions-into-Biostatistics/Classifications---Order-from-Chaos.html&#34; target=&#34;_blank&#34;&gt; Lecture Slides &lt;/a&gt; for this session.&lt;/p&gt;
&lt;h2 id=&#34;our-resarch-project&#34;&gt;Our Resarch Project&lt;/h2&gt;
&lt;p&gt;Today, we are looking at a big (and entirely fictional) data base of the common house sparrow (&lt;em&gt;Passer domesticus&lt;/em&gt;). In particular, we are interested in the &lt;strong&gt;Evolution of &lt;em&gt;Passer domesticus&lt;/em&gt; in Response to Climate Change&lt;/strong&gt; which was previously explained &lt;a href=&#34;https://www.erikkusch.com/courses/excursions-into-biostatistics/research-project/&#34; target=&#34;_blank&#34;&gt; here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;the-data&#34;&gt;The Data&lt;/h3&gt;
&lt;p&gt;I have created a large data set for this exercise which is available &lt;a href=&#34;https://github.com/ErikKusch/Homepage/raw/master/content/courses/excursions-into-biostatistics/Data.rar&#34; target=&#34;_blank&#34;&gt; here&lt;/a&gt; and we previously cleaned up so that is now usable &lt;a href=&#34;https://www.erikkusch.com/courses/excursions-into-biostatistics/data-handling-and-data-assumptions/&#34; target=&#34;_blank&#34;&gt; here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;reading-the-data-into-r&#34;&gt;Reading the Data into &lt;code&gt;R&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s start by reading the data into &lt;code&gt;R&lt;/code&gt; and taking an initial look at it:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Sparrows_df &amp;lt;- readRDS(file.path(&amp;quot;Data&amp;quot;, &amp;quot;SparrowDataClimate.rds&amp;quot;))
head(Sparrows_df)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Index Latitude Longitude     Climate Population.Status Weight Height Wing.Chord Colour    Sex Nesting.Site Nesting.Height Number.of.Eggs Egg.Weight Flock Home.Range Predator.Presence Predator.Type
## 1    SI       60       100 Continental            Native  34.05  12.87       6.67  Brown   Male         &amp;lt;NA&amp;gt;             NA             NA         NA     B      Large               Yes         Avian
## 2    SI       60       100 Continental            Native  34.86  13.68       6.79   Grey   Male         &amp;lt;NA&amp;gt;             NA             NA         NA     B      Large               Yes         Avian
## 3    SI       60       100 Continental            Native  32.34  12.66       6.64  Black Female        Shrub          35.60              1       3.21     C      Large               Yes         Avian
## 4    SI       60       100 Continental            Native  34.78  15.09       7.00  Brown Female        Shrub          47.75              0         NA     E      Large               Yes         Avian
## 5    SI       60       100 Continental            Native  35.01  13.82       6.81   Grey   Male         &amp;lt;NA&amp;gt;             NA             NA         NA     B      Large               Yes         Avian
## 6    SI       60       100 Continental            Native  32.36  12.67       6.64  Brown Female        Shrub          32.47              1       3.17     E      Large               Yes         Avian
##       TAvg      TSD
## 1 269.9596 15.71819
## 2 269.9596 15.71819
## 3 269.9596 15.71819
## 4 269.9596 15.71819
## 5 269.9596 15.71819
## 6 269.9596 15.71819
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;hypotheses&#34;&gt;Hypotheses&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s remember our hypotheses:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Sparrow Morphology&lt;/strong&gt; is determined by:&lt;br&gt;
A. &lt;em&gt;Climate Conditions&lt;/em&gt; with sparrows in stable, warm environments fairing better than those in colder, less stable ones.&lt;br&gt;
B. &lt;em&gt;Competition&lt;/em&gt; with sparrows in small flocks doing better than those in big flocks.&lt;br&gt;
C. &lt;em&gt;Predation&lt;/em&gt; with sparrows under pressure of predation doing worse than those without.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sites&lt;/strong&gt;  accurately represent &lt;strong&gt;sparrow morphology&lt;/strong&gt;. This may mean:&lt;br&gt;
A. &lt;em&gt;Population status&lt;/em&gt; as inferred through morphology.&lt;br&gt;
B. &lt;em&gt;Site index&lt;/em&gt; as inferred through morphology.&lt;br&gt;
C. &lt;em&gt;Climate&lt;/em&gt; as inferred through morphology.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Quite obviously, &lt;strong&gt;hypothesis 2&lt;/strong&gt; is the only one lending itself well to classification exercises. In fact, what we want to answer is the question: &lt;em&gt;&amp;ldquo;Can we successfully classify populations at different sites according to their morphological expressions?&amp;quot;&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&#34;r-environment&#34;&gt;&lt;code&gt;R&lt;/code&gt; Environment&lt;/h2&gt;
&lt;p&gt;For this exercise, we will need the following packages:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.load.package &amp;lt;- function(x) {
  if (!require(x, character.only = TRUE)) {
    install.packages(x, repos = &amp;quot;http://cran.us.r-project.org&amp;quot;)
  }
  require(x, character.only = TRUE)
}
package_vec &amp;lt;- c(
  &amp;quot;ggplot2&amp;quot;, # for visualisation
  &amp;quot;mclust&amp;quot;, # for k-means clustering,
  &amp;quot;vegan&amp;quot;, # for distance matrices in hierarchical clustering
  &amp;quot;rpart&amp;quot;, # for decision trees
  &amp;quot;rpart.plot&amp;quot;, # for plotting decision trees
  &amp;quot;randomForest&amp;quot;, # for randomForest classifier
  &amp;quot;car&amp;quot;, # check multicollinearity
  &amp;quot;MASS&amp;quot; # for ordinal logistic regression
)
sapply(package_vec, install.load.package)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      ggplot2       mclust        vegan        rpart   rpart.plot randomForest          car         MASS 
##         TRUE         TRUE         TRUE         TRUE         TRUE         TRUE         TRUE         TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the above function is way more sophisticated than the usual &lt;code&gt;install.packages()&lt;/code&gt; &amp;amp; &lt;code&gt;library()&lt;/code&gt; approach since it automatically detects which packages require installing and only install these thus not overwriting already installed packages.&lt;/p&gt;
&lt;h2 id=&#34;logistic-regression&#34;&gt;Logistic Regression&lt;/h2&gt;
&lt;p&gt;Remember the &lt;strong&gt;Assumptions of Logistic Regression&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Absence of influential outliers&lt;/li&gt;
&lt;li&gt;Absence of multi-collinearity&lt;/li&gt;
&lt;li&gt;Predictor Variables and log odds are related in a linear fashion&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;binary-logistic-regression&#34;&gt;Binary Logistic Regression&lt;/h3&gt;
&lt;p&gt;Binary Logistic regression only accommodates binary outcomes. This leaves only one of our hypotheses open for investigation - &lt;strong&gt;2.A.&lt;/strong&gt; &lt;em&gt;Population Status&lt;/em&gt; - since this is the only response variable boasting two levels.&lt;/p&gt;
&lt;p&gt;To reduce the effect of as many confounding variables as possible, I reduce the data set to just those observations belonging to our station in Siberia and Manitoba. Both are located at very similar latitudes. They really only differ in their climate condition and the population status:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;LogReg_df &amp;lt;- Sparrows_df[Sparrows_df$Index == &amp;quot;MA&amp;quot; | Sparrows_df$Index == &amp;quot;SI&amp;quot;, c(&amp;quot;Population.Status&amp;quot;, &amp;quot;Weight&amp;quot;, &amp;quot;Height&amp;quot;, &amp;quot;Wing.Chord&amp;quot;)]
LogReg_df$PS &amp;lt;- as.numeric(LogReg_df$Population.Status) - 1 # make climate numeric for model
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;initial-model--collinearity&#34;&gt;Initial Model &amp;amp; Collinearity&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s start with the biggest model we can build here and then assess if our assumptions are met:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;H2_LogReg_mod &amp;lt;- glm(PS ~ Weight + Height + Wing.Chord,
  data = LogReg_df,
  family = binomial(link = &amp;quot;logit&amp;quot;),
)
summary(H2_LogReg_mod)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = PS ~ Weight + Height + Wing.Chord, family = binomial(link = &amp;quot;logit&amp;quot;), 
##     data = LogReg_df)
## 
## Deviance Residuals: 
##        Min          1Q      Median          3Q         Max  
## -2.657e-05  -2.110e-08  -2.110e-08   2.110e-08   2.855e-05  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&amp;gt;|z|)
## (Intercept)  1.557e+03  3.312e+07   0.000    1.000
## Weight       7.242e+01  3.735e+04   0.002    0.998
## Height       2.153e+01  1.061e+06   0.000    1.000
## Wing.Chord  -6.247e+02  6.928e+06   0.000    1.000
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1.8437e+02  on 132  degrees of freedom
## Residual deviance: 6.8926e-09  on 129  degrees of freedom
## AIC: 8
## 
## Number of Fisher Scoring iterations: 25
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Well&amp;hellip; nothing here is significant. Let&amp;rsquo;s see what the culprit might be. With morphological traits, you are often looking at a whole set of collinearity, so let&amp;rsquo;s start by investigating that:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;vif(H2_LogReg_mod)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Weight      Height  Wing.Chord 
##    9.409985 6550.394451 6342.683550
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A Variance Inflation Factor (VIF) value of $\geq5-10$ is seen as identifying problematic collinearity. Quite obviously, this is the case. We need to throw away some predictors. I only want to keep &lt;code&gt;Weight&lt;/code&gt;.&lt;/p&gt;
&lt;h4 id=&#34;weight-model-and-further-assumptions&#34;&gt;&lt;code&gt;Weight&lt;/code&gt; Model and Further Assumptions&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s run a simplified model that just used &lt;code&gt;Weight&lt;/code&gt; as a predictor:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;H2_LogReg_mod &amp;lt;- glm(PS ~ Weight,
  data = LogReg_df,
  family = binomial(link = &amp;quot;logit&amp;quot;)
)
summary(H2_LogReg_mod)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = PS ~ Weight, family = binomial(link = &amp;quot;logit&amp;quot;), 
##     data = LogReg_df)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.1980  -0.5331  -0.1235   0.5419   1.9067  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept) -46.3244     7.8319  -5.915 3.32e-09 ***
## Weight        1.4052     0.2374   5.920 3.23e-09 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 184.37  on 132  degrees of freedom
## Residual deviance: 105.08  on 131  degrees of freedom
## AIC: 109.08
## 
## Number of Fisher Scoring iterations: 5
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A significant effect, huzzah! We still need to test for our assumptions, however. Checking for &lt;strong&gt;multicollinearity&lt;/strong&gt; makes no sense since we only use one predictor, so we can skip that.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Linear Relationship&lt;/strong&gt; between predictor(s) and log-odds of the output can be assessed as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;probabilities &amp;lt;- predict(H2_LogReg_mod, type = &amp;quot;response&amp;quot;) # predict model response on original data
LogReg_df$Probs &amp;lt;- probabilities # safe probabilities to data frame
LogReg_df$LogOdds &amp;lt;- log(probabilities / (1 - probabilities)) # calculate log-odds
## Plot Log-Odds vs. Predictor
ggplot(data = LogReg_df, aes(x = Weight, y = LogOdds)) +
  geom_point() +
  geom_smooth(method = &amp;quot;lm&amp;quot;, se = TRUE) +
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;5_Classifications---Order-from-Chaos_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That is clearly linear relationship!&lt;/p&gt;
&lt;p&gt;Moving on to our final assumption, we want to assess whether there are influential &lt;strong&gt;Outliers&lt;/strong&gt;. For this, we want to look at the &lt;em&gt;Cook&amp;rsquo;s distance&lt;/em&gt; as well as the &lt;em&gt;standardised residuals&lt;/em&gt; per observation:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Cook&#39;s distance
plot(H2_LogReg_mod, which = 4, id.n = 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;5_Classifications---Order-from-Chaos_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Standardises Residuals
Outlier_df &amp;lt;- data.frame(
  Residuals = resid(H2_LogReg_mod),
  Index = 1:nrow(LogReg_df),
  Outcome = factor(LogReg_df$PS)
)
Outlier_df$Std.Resid &amp;lt;- scale(Outlier_df$Residuals)
# Plot Residuals
ggplot(Outlier_df, aes(Outcome, Std.Resid)) +
  geom_boxplot() +
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;5_Classifications---Order-from-Chaos_files/figure-html/unnamed-chunk-8-2.png&#34; width=&#34;1440&#34; /&gt;
Both of these plots do not highlight any worrying influential outliers. An influential outliers would manifest with a prominent standardises residual ($|Std.Resid|\sim3$)/Cook&amp;rsquo;s distance.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s finally plot what the model predicts:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(data = LogReg_df, aes(x = Weight, y = LogReg_df$PS)) +
  geom_point() +
  theme_bw() +
  geom_smooth(
    data = LogReg_df, aes(x = Weight, y = Probs),
    method = &amp;quot;glm&amp;quot;,
    method.args = list(family = &amp;quot;binomial&amp;quot;),
    se = TRUE
  ) +
  labs(y = &amp;quot;Introduced Population&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;5_Classifications---Order-from-Chaos_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;ordinal-logistic-regression&#34;&gt;Ordinal Logistic Regression&lt;/h3&gt;
&lt;p&gt;Ordinal Logistic regression allows for multiple levels of the response variable so long as they are on an ordinal scale. Here, we could test all of our above hypotheses. However, I&amp;rsquo;d like to stick with &lt;strong&gt;2.C.&lt;/strong&gt; &lt;em&gt;Climate&lt;/em&gt; for this example.&lt;/p&gt;
&lt;p&gt;Again, to reduce the effect of as many confounding variables as possible, I reduce the data set to just those observations belonging to our station in Siberia, Manitoba, and also the United Kingdom this time. All three are located at very similar latitudes. They really only differ in their climate condition and the population status:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;LogReg_df &amp;lt;- Sparrows_df[Sparrows_df$Index == &amp;quot;UK&amp;quot; | Sparrows_df$Index == &amp;quot;MA&amp;quot; | Sparrows_df$Index == &amp;quot;SI&amp;quot;, c(&amp;quot;Climate&amp;quot;, &amp;quot;Weight&amp;quot;, &amp;quot;Height&amp;quot;, &amp;quot;Wing.Chord&amp;quot;)]
LogReg_df$CL &amp;lt;- factor(as.numeric(LogReg_df$Climate) - 1) # make climate factored numeric for model
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;initial-model--collinearity-1&#34;&gt;Initial Model &amp;amp; Collinearity&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s start with the biggest model we can build here and then assess if our assumptions are met:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;H2_LogReg_mod &amp;lt;- polr(CL ~ Weight + Height + Wing.Chord,
  data = LogReg_df,
  Hess = TRUE
)
summary_table &amp;lt;- coef(summary(H2_LogReg_mod))
pval &amp;lt;- pnorm(abs(summary_table[, &amp;quot;t value&amp;quot;]), lower.tail = FALSE) * 2
summary_table &amp;lt;- cbind(summary_table, &amp;quot;p value&amp;quot; = round(pval, 6))
summary_table
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                   Value Std. Error      t value p value
## Weight       -0.4595719 0.09750018    -4.713549   2e-06
## Height       25.0808034 0.19522606   128.470573   0e+00
## Wing.Chord -164.1103857 0.51246129  -320.239573   0e+00
## 0|1        -788.2133893 0.11008589 -7159.985419   0e+00
## 1|2        -786.8019284 0.18747890 -4196.749302   0e+00
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Well&amp;hellip; a lot here is significant. We identified &lt;strong&gt;multicollinearity&lt;/strong&gt; as a problem earlier. Let&amp;rsquo;s investigate that again:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;vif(H2_LogReg_mod)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Weight     Height Wing.Chord 
##   431.6796   294.6353   536.5452
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Horrible!. A Variance Inflation Factor (VIF) value of $\geq5-10$ is seen as identifying problematic collinearity. Quite obviously, this is the case. We need to throw away some predictors. I only want to keep &lt;code&gt;Weight&lt;/code&gt;.&lt;/p&gt;
&lt;h4 id=&#34;weight-model-and-further-assumptions-1&#34;&gt;&lt;code&gt;Weight&lt;/code&gt; Model and Further Assumptions&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s run a simplified model that just used &lt;code&gt;Weight&lt;/code&gt; as a predictor:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;H2_LogReg_mod &amp;lt;- polr(CL ~ Weight,
  data = LogReg_df,
  Hess = TRUE
)
summary_table &amp;lt;- coef(summary(H2_LogReg_mod))
pval &amp;lt;- pnorm(abs(summary_table[, &amp;quot;t value&amp;quot;]), lower.tail = FALSE) * 2
summary_table &amp;lt;- cbind(summary_table, &amp;quot;p value&amp;quot; = round(pval, 6))
summary_table
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               Value Std. Error      t value  p value
## Weight -0.020768177  0.0761669 -0.272666718 0.785109
## 0|1    -1.354848455  2.5131706 -0.539099272 0.589818
## 1|2     0.009549511  2.5112093  0.003802754 0.996966
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Well&amp;hellip; this model doesn&amp;rsquo;t help us at all in understanding climate through morphology of our sparrows. Let&amp;rsquo;s abandon this and move on to classification methods which are much better suited to this task.&lt;/p&gt;
&lt;h2 id=&#34;k-means-clustering&#34;&gt;K-Means Clustering&lt;/h2&gt;
&lt;p&gt;K-Means clustering is incredibly potent in identifying a number of appropriate clusters, their attributes, and sort observations into appropriate clusters.&lt;/p&gt;
&lt;h3 id=&#34;population-status-classifier&#34;&gt;Population Status Classifier&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s start with understanding population status through morphological traits:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Morph_df &amp;lt;- Sparrows_df[, c(&amp;quot;Weight&amp;quot;, &amp;quot;Height&amp;quot;, &amp;quot;Wing.Chord&amp;quot;, &amp;quot;Population.Status&amp;quot;)]
H2_PS_mclust &amp;lt;- Mclust(Morph_df[-4], G = length(unique(Morph_df[, 4])))
plot(H2_PS_mclust, what = &amp;quot;uncertainty&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;5_Classifications---Order-from-Chaos_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we can see, K-means clustering is able to really neatly identify two groups in our data. But do they actually belong do the right groups of &lt;code&gt;Population.Status&lt;/code&gt;? We&amp;rsquo;ll find out in &lt;a href=&#34;https://www.erikkusch.com/courses/excursions-into-biostatistics/excursion-into-biostatistics/&#34; target=&#34;_blank&#34;&gt; Model Selection and Validation&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;site-classifier&#34;&gt;Site Classifier&lt;/h3&gt;
&lt;p&gt;On to our site index classification. Running the k-means clustering algorithm returns:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Morph_df &amp;lt;- Sparrows_df[, c(&amp;quot;Weight&amp;quot;, &amp;quot;Height&amp;quot;, &amp;quot;Wing.Chord&amp;quot;, &amp;quot;Index&amp;quot;)]
H2_Index_mclust &amp;lt;- Mclust(Morph_df[-4], G = length(unique(Morph_df[, 4])))
plot(H2_Index_mclust, what = &amp;quot;uncertainty&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;5_Classifications---Order-from-Chaos_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s a pretty bad classification. I would not place trust in these clusters seeing how much they overlap.&lt;/p&gt;
&lt;h3 id=&#34;climate-classifier&#34;&gt;Climate Classifier&lt;/h3&gt;
&lt;p&gt;Lastly, turning to our climate classification using k-means classification:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Morph_df &amp;lt;- Sparrows_df[, c(&amp;quot;Weight&amp;quot;, &amp;quot;Height&amp;quot;, &amp;quot;Wing.Chord&amp;quot;, &amp;quot;Climate&amp;quot;)]
H2_Climate_mclust &amp;lt;- Mclust(Morph_df[-4], G = length(unique(Morph_df[, 4])))
plot(H2_Climate_mclust, what = &amp;quot;uncertainty&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;5_Classifications---Order-from-Chaos_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;1440&#34; /&gt;
These clusters are decent although there is quite a bit of overlap between the blue and red cluster.&lt;/p&gt;
&lt;h3 id=&#34;optimal-model&#34;&gt;Optimal Model&lt;/h3&gt;
&lt;p&gt;K-means clustering is also able to identify the most &amp;ldquo;appropriate&amp;rdquo; number of clusters given the data and uncertainty of classification:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Morph_df &amp;lt;- Sparrows_df[, c(&amp;quot;Weight&amp;quot;, &amp;quot;Height&amp;quot;, &amp;quot;Wing.Chord&amp;quot;)]
dataBIC &amp;lt;- mclustBIC(Morph_df)
summary(dataBIC) # show summary of top-ranking models
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Best BIC values:
##             VVV,7     EVV,7     EVV,8
## BIC      63.39237 -304.1895 -336.0531
## BIC diff  0.00000 -367.5819 -399.4455
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(dataBIC)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;5_Classifications---Order-from-Chaos_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;G &amp;lt;- as.numeric(strsplit(names(summary(dataBIC))[1], &amp;quot;,&amp;quot;)[[1]][2])
H2_Opt_mclust &amp;lt;- Mclust(Morph_df, # data for the cluster model
  G = G # BIC index for model to be built
)
H2_Opt_mclust[[&amp;quot;parameters&amp;quot;]][[&amp;quot;mean&amp;quot;]] # mean values of clusters
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                 [,1]      [,2]     [,3]      [,4]      [,5]      [,6]      [,7]
## Weight     34.830000 32.677280 33.63023 31.354892 30.146417 22.585240 22.796014
## Height     13.641765 13.570427 14.20721 14.317070 14.085826 18.847550 19.036621
## Wing.Chord  6.787059  6.780954  6.99186  7.044881  6.965047  8.576106  8.609035
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(H2_Opt_mclust, what = &amp;quot;uncertainty&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;5_Classifications---Order-from-Chaos_files/figure-html/unnamed-chunk-17-2.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here, K-means clustering would have us settle on 7 clusters. That does not coincide with anything we could really test for at this point. COnclusively, this model goes into the category of &amp;ldquo;Nice to have, but ultimately useless here&amp;rdquo;.&lt;/p&gt;
&lt;h3 id=&#34;summary-of-k-means-clustering&#34;&gt;Summary of K-Means Clustering&lt;/h3&gt;
&lt;p&gt;So what do we take from this? Well&amp;hellip; Population status was well explained all morphological traits and so would in turn also do a good job of being a proxy for the other when doing mixed regression models, for example. Hence, we might want to include this variable in future &lt;a href=&#34;https://www.erikkusch.com/courses/excursions-into-biostatistics/regressions-correlations-for-the-advanced/&#34; target=&#34;_blank&#34;&gt; Regression Models&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;hierarchical-clustering&#34;&gt;Hierarchical Clustering&lt;/h2&gt;
&lt;p&gt;Moving on to hierarchical clustering, we luckily only need to create a few trees to start with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Morph_df &amp;lt;- Sparrows_df[, c(&amp;quot;Weight&amp;quot;, &amp;quot;Height&amp;quot;, &amp;quot;Wing.Chord&amp;quot;)] # selecting morphology data
dist_mat &amp;lt;- dist(Morph_df) # distance matrix
## Hierarchical clustering using different linkages
H2_Hierachical_clas1 &amp;lt;- hclust(dist_mat, method = &amp;quot;complete&amp;quot;)
H2_Hierachical_clas2 &amp;lt;- hclust(dist_mat, method = &amp;quot;single&amp;quot;)
H2_Hierachical_clas3 &amp;lt;- hclust(dist_mat, method = &amp;quot;average&amp;quot;)
## Plotting Hierarchies
par(mfrow = c(1, 3))
plot(H2_Hierachical_clas1, main = &amp;quot;complete&amp;quot;)
plot(H2_Hierachical_clas2, main = &amp;quot;single&amp;quot;)
plot(H2_Hierachical_clas3, main = &amp;quot;average&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;5_Classifications---Order-from-Chaos_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here, you can see that the type of linkage employed by your hierarchical approach is very important as to how the hierarchy ends up looking like. For now, we run with all of them.&lt;/p&gt;
&lt;h3 id=&#34;population-status-classifier-1&#34;&gt;Population Status Classifier&lt;/h3&gt;
&lt;p&gt;For our population status classifier, let&amp;rsquo;s obtain our data and cluster number we are after:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Morph_df &amp;lt;- Sparrows_df[, c(&amp;quot;Weight&amp;quot;, &amp;quot;Height&amp;quot;, &amp;quot;Wing.Chord&amp;quot;, &amp;quot;Population.Status&amp;quot;)]
G &amp;lt;- length(unique(Morph_df[, 4]))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can look at how well our different Hierarchies fair at explaining these categories when cut at the point where the same number of categories is present in the tree:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;clusterCut &amp;lt;- cutree(H2_Hierachical_clas1, k = G) # cut tree
table(clusterCut, Morph_df$Population.Status) # assess fit
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           
## clusterCut Introduced Native
##          1        682    134
##          2        250      0
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;clusterCut &amp;lt;- cutree(H2_Hierachical_clas2, k = G) # cut tree
table(clusterCut, Morph_df$Population.Status) # assess fit
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           
## clusterCut Introduced Native
##          1        682    134
##          2        250      0
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;clusterCut &amp;lt;- cutree(H2_Hierachical_clas3, k = G) # cut tree
table(clusterCut, Morph_df$Population.Status) # assess fit
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           
## clusterCut Introduced Native
##          1        682    134
##          2        250      0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Interestingly enough, no matter the linkage, all of these approaches get Introduced and Native populations confused in the first group, but not the second.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s look at the decisions that we could make when following a decision tree for this example:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;H2_PS_decision &amp;lt;- rpart(Population.Status ~ ., data = Morph_df)
rpart.plot(H2_PS_decision)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;5_Classifications---Order-from-Chaos_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Following this decision tree we first ask &lt;em&gt;&amp;ldquo;Is our sparrow lighter than 35g?&amp;quot;&lt;/em&gt;. If the answer is yes, we move to the left and ask the question &lt;em&gt;&amp;ldquo;Is the wing span of our sparrow greater/equal than 6.9cm?&amp;quot;&lt;/em&gt;. If the answer is yes, we move to the left and assign this sparrow to an introduced population status. 62% of all observations are in this node and to 2% we believe that this node might actually be a Native node. All other nodes are explained accordingly. More about their interpretation can be found in this 
&lt;a href=&#34;https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=&amp;amp;ved=2ahUKEwizk67jmJDvAhUnMuwKHbaiD90QFjAAegQIARAD&amp;amp;url=http%3A%2F%2Fwww.milbo.org%2Frpart-plot%2Fprp.pdf&amp;amp;usg=AOvVaw2DpMfeZC2yVdRaYZBXBA8K&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Manual&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;site-classifier-1&#34;&gt;Site Classifier&lt;/h3&gt;
&lt;p&gt;Moving on to the site index classifier, we need our data and number of clusters:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Morph_df &amp;lt;- Sparrows_df[, c(&amp;quot;Weight&amp;quot;, &amp;quot;Height&amp;quot;, &amp;quot;Wing.Chord&amp;quot;, &amp;quot;Index&amp;quot;)]
G &amp;lt;- length(unique(Morph_df[, 4]))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looking at our different outputs:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;clusterCut &amp;lt;- cutree(H2_Hierachical_clas1, k = G) # cut tree
table(clusterCut, Morph_df$Index) # assess fit
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           
## clusterCut  AU  BE  FG  FI  LO  MA  NU  RE  SA  SI  UK
##         1   24   0   0  21   0  15  17   0   0  22  13
##         2   17   0   0   5   3   7   6   0   0  31   5
##         3   19   0   0  29  12  22  21   0   0  13  25
##         4   24  26   0   2  33   5   7  32  16   0  12
##         5    3   0   0  12   4  18  13   0   0   0  13
##         6    0  60   0   0  20   0   0  49  77   0   0
##         7    0  19   0   0   9   0   0  14  21   0   0
##         8    0   0  80   0   0   0   0   0   0   0   0
##         9    0   0 138   0   0   0   0   0   0   0   0
##         10   0   0  16   0   0   0   0   0   0   0   0
##         11   0   0  16   0   0   0   0   0   0   0   0
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;clusterCut &amp;lt;- cutree(H2_Hierachical_clas2, k = G) # cut tree
table(clusterCut, Morph_df$Index) # assess fit
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           
## clusterCut  AU  BE  FG  FI  LO  MA  NU  RE  SA  SI  UK
##         1    0   0   0   0   0   0   0   0   0  28   0
##         2   87 102   0  69  80  67  64  95 112  32  68
##         3    0   0   0   0   0   0   0   0   0   4   0
##         4    0   0   0   0   0   0   0   0   0   2   0
##         5    0   0   0   0   1   0   0   0   0   0   0
##         6    0   1   0   0   0   0   0   0   0   0   0
##         7    0   2   0   0   0   0   0   0   0   0   0
##         8    0   0 122   0   0   0   0   0   0   0   0
##         9    0   0 126   0   0   0   0   0   0   0   0
##         10   0   0   2   0   0   0   0   0   0   0   0
##         11   0   0   0   0   0   0   0   0   2   0   0
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;clusterCut &amp;lt;- cutree(H2_Hierachical_clas3, k = G) # cut tree
table(clusterCut, Morph_df$Index) # assess fit
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           
## clusterCut  AU  BE  FG  FI  LO  MA  NU  RE  SA  SI  UK
##         1   44   0   0  15  14  15  22   0   0  45  19
##         2   42  31   0  50  50  49  40  27   0  12  44
##         3    1   0   0   0   0   0   0   0   0   5   0
##         4    0   0   0   0   0   0   0   0   0   4   0
##         5    0   6   0   4   9   3   2   1   0   0   5
##         6    0  34   0   0   0   0   0  35  81   0   0
##         7    0  21   0   0   8   0   0  27  23   0   0
##         8    0  13   0   0   0   0   0   5  10   0   0
##         9    0   0 106   0   0   0   0   0   0   0   0
##         10   0   0 134   0   0   0   0   0   0   0   0
##         11   0   0  10   0   0   0   0   0   0   0   0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now see clearly how different linkages have a major impact in determining how our hierarchy groups different observations. I won&amp;rsquo;t go into interpretations here to save time and energy since these outputs are so busy.&lt;/p&gt;
&lt;p&gt;Our decision tree is also excrutiatingly busy:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;H2_Index_decision &amp;lt;- rpart(Index ~ ., data = Morph_df)
rpart.plot(H2_Index_decision)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;5_Classifications---Order-from-Chaos_files/figure-html/unnamed-chunk-24-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;climate-classifier-1&#34;&gt;Climate Classifier&lt;/h3&gt;
&lt;p&gt;Back over to our climate classifier:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Morph_df &amp;lt;- Sparrows_df[, c(&amp;quot;Weight&amp;quot;, &amp;quot;Height&amp;quot;, &amp;quot;Wing.Chord&amp;quot;, &amp;quot;Climate&amp;quot;)]
G &amp;lt;- length(unique(Morph_df[, 4]))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s look at how the different linkages impact our results:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;clusterCut &amp;lt;- cutree(H2_Hierachical_clas1, k = G) # cut tree
table(clusterCut, Morph_df$Climate) # assess fit
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           
## clusterCut Coastal Continental Semi-Coastal
##          1     577         105           60
##          2      19          48            7
##          3     250           0            0
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;clusterCut &amp;lt;- cutree(H2_Hierachical_clas2, k = G) # cut tree
table(clusterCut, Morph_df$Climate) # assess fit
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           
## clusterCut Coastal Continental Semi-Coastal
##          1     595         153           67
##          2       1           0            0
##          3     250           0            0
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;clusterCut &amp;lt;- cutree(H2_Hierachical_clas3, k = G) # cut tree
table(clusterCut, Morph_df$Climate) # assess fit
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           
## clusterCut Coastal Continental Semi-Coastal
##          1     596         153           67
##          2     240           0            0
##          3      10           0            0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All of our linkage types have problems discerning Coastal types. I wager that is because of a ton of confounding effects which drive morphological traits in addition to climate types.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s another look at a decision tree:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;H2_Climate_decision &amp;lt;- rpart(Climate ~ ., data = Morph_df)
rpart.plot(H2_Climate_decision)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;5_Classifications---Order-from-Chaos_files/figure-html/unnamed-chunk-27-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;summary-of-hierarchical-clustering&#34;&gt;Summary of Hierarchical Clustering&lt;/h3&gt;
&lt;p&gt;We have seen that site indices may hold some explanatory power regarding sparrow morphology, but the picture is very complex. We may want to keep them in mind as random effects for future models (don&amp;rsquo;t worry if that doesn&amp;rsquo;t mean much to you yet).&lt;/p&gt;
&lt;h2 id=&#34;random-forest&#34;&gt;Random Forest&lt;/h2&gt;
&lt;p&gt;Random Forests are one of the most powerful classification methods and I love them. They are incredibly powerful, accurate, and easy to use. Unfortunately, they are black-box algorithms (you don&amp;rsquo;t know what&amp;rsquo;s happening in them exactly in numerical terms) and they require observed outcomes. That&amp;rsquo;s not a problem for us with this research project!&lt;/p&gt;
&lt;h3 id=&#34;population-status-classifier-2&#34;&gt;Population Status Classifier&lt;/h3&gt;
&lt;p&gt;Running our random for model for population statuses:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(42) # set seed because the process is random
H2_PS_RF &amp;lt;- tuneRF(
  x = Sparrows_df[, c(&amp;quot;Weight&amp;quot;, &amp;quot;Height&amp;quot;, &amp;quot;Wing.Chord&amp;quot;)], # variables which to use for clustering
  y = Sparrows_df$Population.Status, # correct cluster assignment
  strata = Sparrows_df$Population.Status, # stratified sampling
  doBest = TRUE, # run the best overall tree
  ntreeTry = 20000, # consider this number of trees
  improve = 0.0000001, # improvement if this is exceeded
  trace = FALSE, plot = FALSE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## -0.08235294 1e-07
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Works perfectly.&lt;/p&gt;
&lt;p&gt;Random forests give us access to &lt;em&gt;confusion matrices&lt;/em&gt; which tell us about classification accuracy:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;H2_PS_RF[[&amp;quot;confusion&amp;quot;]]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            Introduced Native class.error
## Introduced        902     30  0.03218884
## Native             55     79  0.41044776
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Evidently, we are good at predicting Introduced population status, but Native population status is almost as random as a coin toss.&lt;/p&gt;
&lt;p&gt;Which variables give us the most information when establishing these groups?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;varImpPlot(H2_PS_RF)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;5_Classifications---Order-from-Chaos_files/figure-html/unnamed-chunk-30-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Well look who it is. &lt;code&gt;Weight&lt;/code&gt; comes out as the most important variable once again!&lt;/p&gt;
&lt;h3 id=&#34;site-classifier-2&#34;&gt;Site Classifier&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s run a random forest analysis for our site indices:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(42) # set seed because the process is random
H2_Index_RF &amp;lt;- tuneRF(
  x = Sparrows_df[, c(&amp;quot;Weight&amp;quot;, &amp;quot;Height&amp;quot;, &amp;quot;Wing.Chord&amp;quot;)], # variables which to use for clustering
  y = Sparrows_df$Index, # correct cluster assignment
  strata = Sparrows_df$Index, # stratified sampling
  doBest = TRUE, # run the best overall tree
  ntreeTry = 20000, # consider this number of trees
  improve = 0.0000001, # improvement if this is exceeded
  trace = FALSE, plot = FALSE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 0.01630435 1e-07 
## 0 1e-07
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;H2_Index_RF[[&amp;quot;confusion&amp;quot;]]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    AU  BE  FG FI LO MA NU RE  SA SI UK class.error
## AU 77   0   0  2  8  0  0  0   0  0  0  0.11494253
## BE  0 102   0  0  0  0  0  0   3  0  0  0.02857143
## FG  0   0 250  0  0  0  0  0   0  0  0  0.00000000
## FI  0   0   0 33  0 21  0  0   0  0 15  0.52173913
## LO  9   0   0  0 69  0  0  2   0  0  1  0.14814815
## MA  0   0   0 17  0 26  2  0   0  0 22  0.61194030
## NU  0   0   0  0  0  7 44  0   0  7  6  0.31250000
## RE  0   4   0  0  3  0  0 87   1  0  0  0.08421053
## SA  0   5   0  0  0  0  0  0 109  0  0  0.04385965
## SI  0   0   0  0  0  1  7  0   0 58  0  0.12121212
## UK  0   0   0 14  0 25  1  0   0  0 28  0.58823529
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;varImpPlot(H2_Index_RF)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;5_Classifications---Order-from-Chaos_files/figure-html/unnamed-chunk-31-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Except for Manitoba and the UK (which are often mistaken for each other), morphology (and mostly &lt;code&gt;Weight&lt;/code&gt;) explains station indices quite adequately.&lt;/p&gt;
&lt;h3 id=&#34;climate-classifier-2&#34;&gt;Climate Classifier&lt;/h3&gt;
&lt;p&gt;Lastly, we turn to our climate classifier again:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(42) # set seed because the process is random
H2_Climate_RF &amp;lt;- tuneRF(
  x = Sparrows_df[, c(&amp;quot;Weight&amp;quot;, &amp;quot;Height&amp;quot;, &amp;quot;Wing.Chord&amp;quot;)], # variables which to use for clustering
  y = Sparrows_df$Climate, # correct cluster assignment
  strata = Sparrows_df$Climate, # stratified sampling
  doBest = TRUE, # run the best overall tree
  ntreeTry = 20000, # consider this number of trees
  improve = 0.0000001, # improvement if this is exceeded
  trace = FALSE, plot = FALSE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 0.05172414 1e-07 
## -0.02727273 1e-07
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;H2_Climate_RF[[&amp;quot;confusion&amp;quot;]]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              Coastal Continental Semi-Coastal class.error
## Coastal          797          16           33  0.05791962
## Continental       15         137            1  0.10457516
## Semi-Coastal      47           0           20  0.70149254
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;varImpPlot(H2_Climate_RF)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;5_Classifications---Order-from-Chaos_files/figure-html/unnamed-chunk-32-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Oof. We get semi-coastal habitats almost completely wrong. The other climate conditions are explained well through morphology, though.&lt;/p&gt;
&lt;h2 id=&#34;final-models&#34;&gt;Final Models&lt;/h2&gt;
&lt;p&gt;In our upcoming &lt;a href=&#34;https://www.erikkusch.com/courses/excursions-into-biostatistics/excursion-into-biostatistics/&#34; target=&#34;_blank&#34;&gt; Model Selection and Validation&lt;/a&gt; Session, we will look into how to compare and validate models. We now need to select some models we have created here today and want to carry forward to said session.&lt;/p&gt;
&lt;p&gt;Personally, I am quite enamoured with our models &lt;code&gt;H2_PS_mclust&lt;/code&gt; (k-means clustering of population status), &lt;code&gt;H2_PS_RF&lt;/code&gt; (random forest of population status), and &lt;code&gt;H2_Index_RF&lt;/code&gt; (random forest of site indices). Let&amp;rsquo;s save these as a separate object ready to be loaded into our &lt;code&gt;R&lt;/code&gt; environment in the coming session:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;save(H2_PS_mclust, H2_PS_RF, H2_Index_RF, file = file.path(&amp;quot;Data&amp;quot;, &amp;quot;H2_Models.RData&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;sessioninfo&#34;&gt;SessionInfo&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.2.3 (2023-03-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Big Sur ... 10.16
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] MASS_7.3-58.2        car_3.1-1            carData_3.0-5        randomForest_4.7-1.1 rpart.plot_3.1.1     rpart_4.1.19         vegan_2.6-4          lattice_0.20-45      permute_0.9-7       
## [10] mclust_6.0.0         ggplot2_3.4.1       
## 
## loaded via a namespace (and not attached):
##  [1] styler_1.9.1      tidyselect_1.2.0  xfun_0.37         bslib_0.4.2       purrr_1.0.1       splines_4.2.3     colorspace_2.1-0  vctrs_0.5.2       generics_0.1.3    htmltools_0.5.4  
## [11] yaml_2.3.7        mgcv_1.8-42       utf8_1.2.3        rlang_1.0.6       R.oo_1.25.0       jquerylib_0.1.4   pillar_1.8.1      glue_1.6.2        withr_2.5.0       R.utils_2.12.2   
## [21] R.cache_0.16.0    lifecycle_1.0.3   munsell_0.5.0     blogdown_1.16     gtable_0.3.1      R.methodsS3_1.8.2 evaluate_0.20     labeling_0.4.2    knitr_1.42        fastmap_1.1.1    
## [31] parallel_4.2.3    fansi_1.0.4       highr_0.10        scales_1.2.1      cachem_1.0.7      jsonlite_1.8.4    abind_1.4-5       farver_2.1.1      digest_0.6.31     bookdown_0.33    
## [41] dplyr_1.1.0       grid_4.2.3        cli_3.6.0         tools_4.2.3       magrittr_2.0.3    sass_0.4.5        tibble_3.2.0      cluster_2.1.4     pkgconfig_2.0.3   Matrix_1.5-3     
## [51] rmarkdown_2.20    rstudioapi_0.14   R6_2.5.1          nlme_3.1-162      compiler_4.2.3
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 05</title>
      <link>https://www.erikkusch.com/courses/rethinking/chapter-05/</link>
      <pubDate>Thu, 21 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/rethinking/chapter-05/</guid>
      <description>&lt;h1 id=&#34;the-many-variables--the-spurious-waffles&#34;&gt;The Many Variables &amp;amp; The Spurious Waffles&lt;/h1&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/rethinking/5__22-01-2021_SUMMARY_-Multiple-Regression.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides Chapter 5&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to the exercises at the end of chapter 5 in 
&lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Satistical Rethinking 2&lt;/a&gt; by Richard McElreath. I have created these notes as a part of my ongoing involvement in the 
&lt;a href=&#34;https://www.erikkusch.com/project/aubayes/&#34;&gt;AU Bayes Study Group&lt;/a&gt;. Much of my inspiration for these solutions, where necessary, has been obtained from 
&lt;a href=&#34;https://jmgirard.com/statistical-rethinking-ch5/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jeffrey Girard&lt;/a&gt;, and This 
&lt;a href=&#34;https://github.com/jffist/statistical-rethinking-solutions/blob/master/ch05_hw.R&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub Repository&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;easy-exercises&#34;&gt;Easy Exercises&lt;/h2&gt;
&lt;!-- Despite it not being specifically asked of me, I decide to show some examples in `R`even for the easy exercises and need some packages: --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- rm(list=ls()) --&gt;
&lt;!-- library(rethinking) --&gt;
&lt;!-- library(ggplot2) --&gt;
&lt;!-- library(GGally) --&gt;
&lt;!-- library(dagitty) --&gt;
&lt;!-- ``` --&gt;
&lt;h3 id=&#34;practice-e1&#34;&gt;Practice E1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Which of the linear models below are multiple regressions?&lt;/p&gt;
&lt;p&gt;(1) $Î¼_i=Î±+Î²x_i$&lt;br&gt;
(2) $Î¼_i=Î²_xx_i+Î²_zz_i$&lt;br&gt;
(3) $Î¼_i=Î±+Î²(x_iâz_i)$&lt;br&gt;
(4) $Î¼_i=Î±+Î²_xx_i+Î²_zz_i$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  &lt;em&gt;2 and 4 are multiple regressions.&lt;/em&gt;&lt;br&gt;
Model 1 does only considers one predictor variable ($x_i$) and can thus not be a multiple regression. Models 2 and 4 contain multiple predictors with ($x_i$ and $z_i$) with separate slope parameters ($\beta_x$ and $\beta_z$) and thus qualify to be considered multiple regressions. The presence or absence of an intercept parameter ($\alpha$) does not change this interpretation.&lt;/p&gt;
&lt;p&gt;Model 3 is a tad out there. While it only contains one slope parameter ($\beta$), it does make use of two variables ($x_i$ and $z_i$). It can be rewritten as $Î¼_i=Î±+Î²x_iâÎ²z_i$. Now, the notation is in line with models 2 and 4, but has a fixed slope for both. For that reason, I do not think that it is a multiple regression.&lt;/p&gt;
&lt;h3 id=&#34;practice-e2&#34;&gt;Practice E2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Write down a multiple regression to evaluate the claim: &lt;em&gt;Animal diversity is linearly related to latitude, but only after controlling for plant diversity&lt;/em&gt;. You just need to write down the model definition.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; $Div_ {Animals} = \alpha + \beta_{Lat}Lat + \beta_{Plants}Div_{Plants}$&lt;/p&gt;
&lt;h3 id=&#34;practice-e3&#34;&gt;Practice E3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Write down a multiple regression to evaluate the claim: &lt;em&gt;Neither amount of funding nor size of laboratory is by itself a good predictor of time to PhD degree; but together these variables are both positively associated with time to degree&lt;/em&gt;. Write down the model definition and indicate which side of zero each slope parameter should be on.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; $T = \alpha + \beta_FF + \beta_SS$&lt;br&gt;
with $T$ (time to PhD), $F$ (funding), and $S$ (size of laboratory).&lt;/p&gt;
&lt;p&gt;On a side-note, I am a bit unclear as to whether the question should really state that they have a &amp;ldquo;positive&amp;rdquo; effect as this indicates, by intuition, a decrease in time to PhD, but in statistical terms, denote the exact opposite.&lt;/p&gt;
&lt;p&gt;Since the combined effect of both slopes is supposed to be positive, the individual slopes must both be positive (or negative, depending on what you understand by &amp;ldquo;positive effect on time&amp;rdquo;).&lt;/p&gt;
&lt;!-- I decided to simulate one case in `R`: --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- N &lt;- 1e3 # number of samples --&gt;
&lt;!-- rho &lt;- .9 # controls correlation between predictors --&gt;
&lt;!-- set.seed(42) --&gt;
&lt;!-- F &lt;- rnorm(n = N, mean = 10, sd = 1) # funding --&gt;
&lt;!-- S &lt;- rnorm(n = N, mean = -rho*F, sd = sqrt(1-rho^2)) # size --&gt;
&lt;!-- d &lt;- data.frame(F=F, S=S,  --&gt;
&lt;!--                 T=rnorm(n = N, mean = F + S, sd = 3) # T dependant F and S --&gt;
&lt;!--                 ) --&gt;
&lt;!-- ggpairs(d) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- Now that I have the data, I want to run individual models for $T$ as predicted by $S$ and $F$, respectively, to make sure neither are a good predictor in isolation: --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- ### Size Model --&gt;
&lt;!-- modE3_S &lt;- quap( --&gt;
&lt;!--   alist( --&gt;
&lt;!--     T ~ dnorm(mu, sigma), --&gt;
&lt;!--     mu &lt;- a + bS*S, --&gt;
&lt;!--     a ~ dnorm(0,10), --&gt;
&lt;!--     bS ~ dnorm(0, 1), --&gt;
&lt;!--     sigma ~ dunif(0, 10) --&gt;
&lt;!--   ), --&gt;
&lt;!--   data = d --&gt;
&lt;!-- )  --&gt;
&lt;!-- precis(modE3_S) --&gt;
&lt;!-- ### Funding Model --&gt;
&lt;!-- modE3_F &lt;- quap( --&gt;
&lt;!--   alist( --&gt;
&lt;!--     T ~ dnorm(mu, sigma), --&gt;
&lt;!--     mu &lt;- a + bF*F, --&gt;
&lt;!--     a ~ dnorm(0,10), --&gt;
&lt;!--     bF ~ dnorm(0, 1), --&gt;
&lt;!--     sigma ~ dunif(0, 10) --&gt;
&lt;!--   ), --&gt;
&lt;!--   data = d --&gt;
&lt;!-- )  --&gt;
&lt;!-- precis(modE3_F) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- Cool. Neither are good predictors by themselves. How about if we combine them? --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- modE3_FS &lt;- quap( --&gt;
&lt;!--   alist( --&gt;
&lt;!--     T ~ dnorm(mu, sigma), --&gt;
&lt;!--     mu &lt;- a + bF*F + bS*S, --&gt;
&lt;!--     a ~ dnorm(0,10), --&gt;
&lt;!--     bF ~ dnorm(0, 1), --&gt;
&lt;!--     bS ~ dnorm(0, 1), --&gt;
&lt;!--     sigma ~ dunif(0, 10) --&gt;
&lt;!--   ), --&gt;
&lt;!--   data = d --&gt;
&lt;!-- )  --&gt;
&lt;!-- precis(modE3_FS) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- Ok. In a shared model, both funding and size of laboratory are still pretty useless as predictors in my simulated data. Maybe I can get a clearer picture of this when plotting the models: --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- par(mfrow = c(1,2)) --&gt;
&lt;!-- ## SIZE --&gt;
&lt;!-- plot(T~S, d, col = col.alpha(&#34;blue&#34;,0.5), main = &#34;Time vs. Size&#34;) --&gt;
&lt;!-- ## Single Effect --&gt;
&lt;!-- S.seq &lt;- seq(-14, -4, by=0.1) --&gt;
&lt;!-- modE3_S.Link &lt;- link(modE3_S, data=list(S=S.seq), n=1000) --&gt;
&lt;!-- modE3_S.mean &lt;- apply(modE3_S.Link, 2, mean) --&gt;
&lt;!-- modE3_S.pi &lt;- apply(modE3_S.Link, 2, PI) --&gt;
&lt;!-- lines(S.seq, modE3_S.mean, col=&#39;red&#39;) --&gt;
&lt;!-- shade(modE3_S.pi, S.seq, col = col.alpha(&#34;red&#34;,0.15)) --&gt;
&lt;!-- ## Combined Effect --&gt;
&lt;!-- modE3_FS_S &lt;- link(modE3_FS, data=data.frame(S=S.seq, F=mean(d$F)), n=1000) --&gt;
&lt;!-- modE3_FS_S.mean &lt;- apply(modE3_FS_S, 2, mean) --&gt;
&lt;!-- modE3_FS_S.pi &lt;- apply(modE3_FS_S, 2, PI) --&gt;
&lt;!-- lines(S.seq, modE3_FS_S.mean, col=&#39;green&#39;) --&gt;
&lt;!-- shade(modE3_FS_S.pi, S.seq, col = col.alpha(&#34;green&#34;,0.5)) --&gt;
&lt;!-- ## FUNDING --&gt;
&lt;!-- plot(T~F, d, col = col.alpha(&#34;blue&#34;,0.5), main = &#34;Time vs. Funding&#34;) --&gt;
&lt;!-- # Single Effect --&gt;
&lt;!-- F.seq &lt;- seq(7, 14, by=0.1) --&gt;
&lt;!-- modE3_F.Link &lt;- link(modE3_F, data=list(F=F.seq), n=1000) --&gt;
&lt;!-- modE3_F.mean &lt;- apply(modE3_F.Link, 2, mean) --&gt;
&lt;!-- modE3_F.pi &lt;- apply(modE3_F.Link, 2, PI) --&gt;
&lt;!-- lines(F.seq, modE3_F.mean, col=&#39;red&#39;) --&gt;
&lt;!-- shade(modE3_F.pi, F.seq, col = col.alpha(&#34;red&#34;,0.15)) --&gt;
&lt;!-- ## Combined Effect --&gt;
&lt;!-- modE3_FS_F &lt;- link(modE3_FS, data=data.frame(S=mean(d$S), F=F.seq), n=1000) --&gt;
&lt;!-- modE3_FS_F.mean &lt;- apply(modE3_FS_F, 2, mean) --&gt;
&lt;!-- modE3_FS_F.pi &lt;- apply(modE3_FS_F, 2, PI) --&gt;
&lt;!-- lines(F.seq, modE3_FS_F.mean, col=&#39;green&#39;) --&gt;
&lt;!-- shade(modE3_FS_F.pi, F.seq, col = col.alpha(&#34;green&#34;,0.5)) --&gt;
&lt;!-- ``` --&gt;
&lt;h3 id=&#34;practice-e4&#34;&gt;Practice E4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Suppose you have a single categorical predictor with 4 levels (unique values), labelled $A$, $B$, $C$ and $D$. Let $A_i$ be an indicator variable that is 1 where case $i$ is in category $A$. Also suppose $B_i$, $C_i$, and $D_i$ for the other categories. Now which of the following linear models are inferentially equivalent ways to include the categorical variable in a regression? Models are inferentially equivalent when itâs possible to compute one posterior distribution from the posterior distribution of another model.&lt;/p&gt;
&lt;p&gt;(1) $Î¼_i=Î±+Î²_AA_i+Î²_BB_i+Î²_DD_i$&lt;br&gt;
(2) $Î¼_i=Î±+Î²_AA_i+Î²_BB_i+Î²_CC_i+Î²_DD_i$&lt;br&gt;
(3) $Î¼_i=Î±+Î²_BB_i+Î²_CC_i+Î²_DD_i$&lt;br&gt;
(4) $Î¼_i=Î±_AA_i+Î±_BB_i+Î±_CC_i+Î±_DD_i$&lt;br&gt;
(5) $Î¼_i=Î±_A(1âBiâCiâDi)+Î±_BB_i+Î±_CC_i+Î±_DD_i$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; &lt;em&gt;Model 1 and 3-5 are inferentially equivalent.&lt;/em&gt;&lt;br&gt;
Models 1 and 3 both make use of 3 of the 4 total indicator variables which means that we can always derive the the parameter estimates for the 4th indicator variable from a combination of the three parameter estimates present as well as the intercept. Model 4 is akin to an index variable approach which is inferentially the same as an indicator approach (Models 1 and 3). Model 5 is the same as Model 4, so long as we assume that each observation has to belong to one of the four indicator variables.&lt;/p&gt;
&lt;h2 id=&#34;medium-exercises&#34;&gt;Medium Exercises&lt;/h2&gt;
&lt;p&gt;Time to get into &lt;code&gt;R&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rm(list = ls())
library(rethinking)
library(ggplot2)
library(GGally)
library(dagitty)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-m1&#34;&gt;Practice M1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Invent your own example of a spurious correlation. An outcome variable should be correlated with both predictor variables. But when both predictors are entered in the same model, the correlation between the outcome and one of the predictors should mostly vanish (or at least be greatly reduced).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Here, I follow the example in the Overthinking box on page 138. I create an example in which I consider a relationship between standardised vegetative height, standardised air temperature that vanishes once standardised elevation enters the picture.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(42)
N &amp;lt;- 1e2
Elev &amp;lt;- rnorm(n = N, mean = 0, sd = 1)
VegHeight &amp;lt;- rnorm(n = 100, mean = -Elev, sd = 1)
AirTemp &amp;lt;- rnorm(n = N, mean = Elev, sd = 2)
d &amp;lt;- data.frame(Elev, VegHeight, AirTemp)
ggpairs(d)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-21-statistical-rethinking-chapter-05_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;First, I need to show that there even is a (spurious) association between vegetation height (&lt;code&gt;VegHeight&lt;/code&gt;) and air temperature (&lt;code&gt;AirTemp&lt;/code&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m &amp;lt;- quap(
  alist(
    VegHeight ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bAT * AirTemp,
    a ~ dnorm(0, 1),
    bAT ~ dnorm(0, 1),
    sigma ~ dunif(0, 2)
  ),
  data = d
)
precis(m)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean         sd       5.5%       94.5%
## a     -0.1163981 0.13088036 -0.3255702  0.09277397
## bAT   -0.1334516 0.06168793 -0.2320409 -0.03486242
## sigma  1.3201363 0.09334783  1.1709484  1.46932412
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, let&amp;rsquo;s see what happens when I add elevation data (&lt;code&gt;Elev&lt;/code&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m &amp;lt;- quap(
  alist(
    VegHeight ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bAT * AirTemp + bEL * Elev,
    a ~ dnorm(0, 1),
    bAT ~ dnorm(0, 1),
    bEL ~ dnorm(0, 1),
    sigma ~ dunif(0, 2)
  ),
  data = d
)
precis(m)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              mean         sd        5.5%      94.5%
## a     -0.08752358 0.08934620 -0.23031606  0.0552689
## bAT    0.03291017 0.04470657 -0.03853957  0.1043599
## bEL   -0.98905981 0.09190458 -1.13594107 -0.8421785
## sigma  0.89659689 0.06340391  0.79526519  0.9979286
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Conclusively, according to our simulated data and the multiple regression analysis, increasing elevation leads to decreased vegetation height directly. Since the bivariate effect of decreasing air temperature leading to decreases in vegetation height vanishes when we include elevation data, we deem this association to be spurious.&lt;/p&gt;
&lt;h3 id=&#34;practice-m2&#34;&gt;Practice M2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Invent your own example of a masked relationship. An outcome variable should be correlated with both predictor variables, but in opposite directions. And the two predictor variables should be correlated with one another.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Again, I take inspiration from an Overthinking box. This time from page 156. Here, I create an example of of species richness (&lt;code&gt;S&lt;/code&gt;) as driven by human footprint (&lt;code&gt;H&lt;/code&gt;), and latitude centred on the equator (&lt;code&gt;L&lt;/code&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;N &amp;lt;- 1e2
rho &amp;lt;- 0.6
L &amp;lt;- rnorm(n = N, mean = 0, sd = 1)
H &amp;lt;- rnorm(n = N, mean = rho * L, sd = sqrt(1 - rho^2))
S &amp;lt;- rnorm(n = N, mean = L - H, sd = 1)
d &amp;lt;- data.frame(S, L, H)
ggpairs(d)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-21-statistical-rethinking-chapter-05_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start with some bivariate models:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Latitude centred on equator
m1 &amp;lt;- quap(
  alist(
    S ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bL * L,
    a ~ dnorm(0, 1),
    bL ~ dnorm(0, 1),
    sigma ~ dunif(0, 2)
  ),
  data = d
)
precis(m1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean         sd        5.5%     94.5%
## a     0.09656591 0.12275009 -0.09961245 0.2927443
## bL    0.26165086 0.13796469  0.04115663 0.4821451
## sigma 1.23678173 0.08745475  1.09701214 1.3765513
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Human footprint
m2 &amp;lt;- quap(
  alist(
    S ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bH * H,
    a ~ dnorm(0, 1),
    bH ~ dnorm(0, 1),
    sigma ~ dunif(0, 2)
  ),
  data = d
)
precis(m2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              mean        sd       5.5%      94.5%
## a      0.07758373 0.1209797 -0.1157652  0.2709327
## bH    -0.30689555 0.1147838 -0.4903422 -0.1234489
## sigma  1.21600759 0.0859864  1.0785847  1.3534305
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, let&amp;rsquo;s combine these into one big multiple regression:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m3 &amp;lt;- quap(
  alist(
    S ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bH * H + bL * L,
    a ~ dnorm(0, 1),
    bH ~ dnorm(0, 1),
    bL ~ dnorm(0, 1),
    sigma ~ dunif(0, 2)
  ),
  data = d
)
precis(m3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              mean         sd       5.5%      94.5%
## a      0.03603387 0.10567367 -0.1328531  0.2049208
## bH    -0.78399406 0.13167742 -0.9944400 -0.5735481
## bL     0.86622135 0.15576555  0.6172779  1.1151648
## sigma  1.05757879 0.07482312  0.9379970  1.1771606
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Both associations became stronger! To link this back to reality, equatoward position has been linked to increased species richness for a long time (through many hypotheses I won&amp;rsquo;t go into here), there is also a global pattern of reduced human footprint around the equator (although this may change&amp;hellip; looking at you, Brazil). Human footprint itself has been unequivocally linked with a decrease in species richness.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s look at this in plotted form:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(coeftab(m3, m2, m1), par = c(&amp;quot;bH&amp;quot;, &amp;quot;bL&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-21-statistical-rethinking-chapter-05_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In a Directed Acyclic Graph, this works out to:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dag &amp;lt;- dagitty(&amp;quot;dag { L -&amp;gt; S L -&amp;gt; H H -&amp;gt; S}&amp;quot;)
coordinates(dag) &amp;lt;- list(x = c(L = 0, S = 1, H = 2), y = c(L = 0, S = 1, H = 0))
drawdag(dag)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-21-statistical-rethinking-chapter-05_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;practice-m3&#34;&gt;Practice M3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; It is sometimes observed that the best predictor of fire risk is the presence of firefighters - States and localities with many firefighters also have more fires. Presumably firefighters do not &lt;em&gt;cause&lt;/em&gt; fires. Nevertheless, this is not a spurious correlation. Instead fires cause firefighters. Consider the same reversal of causal inference in the context of the divorce and marriage data. How might a high divorce rate cause a higher marriage rate? Can you think of a way to evaluate this relationship, using multiple regression?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Divorces introduce un-married individuals to the population. These individuals have already demonstrated a readiness to get married in the first place and so might spike marriage rates via re-marrying. I would test for this by running a multiple regression in which I regress marriage rate on divorce rate and re-marriage rate (this excludes first-marriages). As long as divorce no longer predicts marriage rate once re-marriage rate is known, our hypothesis would be true.&lt;/p&gt;
&lt;h3 id=&#34;practice-m4&#34;&gt;Practice M4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; In the divorce data, States with high numbers of Mormons (members of The Church of Jesus Christ of Latter-day Saints, LDS) have much lower divorce rates than the regression models expected. Find a list of LDS population by State and use those numbers as a predictor variable, predicting divorce rate using marriage rate, median age at marriage, and percent LDS population (possibly standardised). You may want to consider transformations of the raw percent LDS variable.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  Here, I use the percentage values of LDS as obtained through Wikipedia by 
&lt;a href=&#34;https://jmgirard.com/statistical-rethinking-ch5/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jeffrey Girard&lt;/a&gt;. Since there is a large skew in these data due to states with large LDS populations, I apply a log-transformation before standardising:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(&amp;quot;WaffleDivorce&amp;quot;)
d &amp;lt;- WaffleDivorce
d$LDS &amp;lt;- c(0.0077, 0.0453, 0.0610, 0.0104, 0.0194, 0.0270, 0.0044, 0.0057, 0.0041, 0.0075, 0.0082, 0.0520, 0.2623, 0.0045, 0.0067, 0.0090, 0.0130, 0.0079, 0.0064, 0.0082, 0.0072, 0.0040, 0.0045, 0.0059, 0.0073, 0.0116, 0.0480, 0.0130, 0.0065, 0.0037, 0.0333, 0.0041, 0.0084, 0.0149, 0.0053, 0.0122, 0.0372, 0.0040, 0.0039, 0.0081, 0.0122, 0.0076, 0.0125, 0.6739, 0.0074, 0.0113, 0.0390, 0.0093, 0.0046, 0.1161)
d$logLDS &amp;lt;- log(d$LDS)
d$logLDS.s &amp;lt;- (d$logLDS - mean(d$logLDS)) / sd(d$logLDS)
par(mfrow = c(1, 3))
hist(d$LDS)
hist(d$logLDS)
hist(d$logLDS.s)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-21-statistical-rethinking-chapter-05_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now I am ready to build the model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m &amp;lt;- map(
  alist(
    Divorce ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bm * Marriage + ba * MedianAgeMarriage + bl * logLDS.s,
    a ~ dnorm(10, 20),
    bm ~ dnorm(0, 10),
    ba ~ dnorm(0, 10),
    bl ~ dnorm(0, 10),
    sigma ~ dunif(0, 5)
  ),
  data = d
)
precis(m)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              mean         sd      5.5%      94.5%
## a     35.43051636 6.77505687 24.602667 46.2583658
## bm     0.05343619 0.08261404 -0.078597  0.1854694
## ba    -1.02939820 0.22468646 -1.388491 -0.6703058
## bl    -0.60777261 0.29055419 -1.072134 -0.1434109
## sigma  1.37864526 0.13836923  1.157505  1.5997860
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While marriage rate is not a powerful predictor of divorce rate, both age at marriage and percentage of LDS population are strongly negatively associated with divorce rates.&lt;/p&gt;
&lt;h3 id=&#34;practice-m5&#34;&gt;Practice M5&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; One way to reason through multiple causation hypotheses is to imagine detailed mechanisms through which predictor variables may influence outcomes. For example, it is sometimes argued that the price of gasoline (predictor variable) is positively associated with lower obesity rates (outcome variable). However, there are at least two important mechanisms by which the price of gas could reduce obesity. First, it could lead to less driving and therefore more exercise. Second, it could lead to less driving, which leads to less eating out, which leads to less consumption of huge restaurant meals. Can you outline one or more multiple regressions that address these two mechanisms? Assume you can have any predictor data you need.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; $Î¼_i=Î±+Î²_GG_i+Î²_EE_i+Î²_RR_i$. I propose we run a multiple regression containing a variable $E$ denoting frequency of exercising, and another variable $R$ which captures the frequency of restaurant visits. We use these alongside the variable $G$ (gasoline price) to model obesity rates.&lt;/p&gt;
&lt;h2 id=&#34;hard-exercises&#34;&gt;Hard Exercises&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Disclaimer:&lt;/strong&gt; All three exercises below use the same data, data(foxes) (part of rethinking). The urban fox (&lt;em&gt;Vulpes vulpes&lt;/em&gt;) is a successful exploiter of human habitat. Since urban foxes move in packs and defend territories, data on habitat quality and population density is also included. The data frame has five columns:&lt;/p&gt;
&lt;p&gt;(1) &lt;code&gt;group&lt;/code&gt;: Number of the social group the individual fox belongs to&lt;br&gt;
(2) &lt;code&gt;avgfood&lt;/code&gt;: The average amount of food available in the territory&lt;br&gt;
(3) &lt;code&gt;groupsize&lt;/code&gt;: The number of foxes in the social group&lt;br&gt;
(4) &lt;code&gt;area&lt;/code&gt;: Size of the territory&lt;br&gt;
(5) &lt;code&gt;weight&lt;/code&gt;: Body weight of the individual fox&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(&amp;quot;foxes&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-h1&#34;&gt;Practice H1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Fit two bivariate Gaussian regressions, using &lt;code&gt;quap&lt;/code&gt;: (1) body weight as a linear function of territory size (&lt;code&gt;area&lt;/code&gt;), and (2) body weight as a linear function of &lt;code&gt;groupsize&lt;/code&gt;. Plot the results of these regressions, displaying the MAP regression line and the 95% interval of the mean. Is either variable important for predicting fox body weight?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Let&amp;rsquo;s start with the models:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Area
d &amp;lt;- foxes
ma &amp;lt;- quap(
  alist(
    weight ~ dnorm(mu, sigma),
    mu &amp;lt;- a + ba * area,
    a ~ dnorm(5, 5),
    ba ~ dnorm(0, 5),
    sigma ~ dunif(0, 5)
  ),
  data = d
)
precis(ma)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean         sd       5.5%     94.5%
## a     4.45430638 0.38955723  3.8317187 5.0768941
## ba    0.02385824 0.11803080 -0.1647778 0.2124943
## sigma 1.17868417 0.07738415  1.0550093 1.3023590
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;area.seq &amp;lt;- seq(from = min(d$area), to = max(d$area), length.out = 1e4)
mu &amp;lt;- link(ma, data = data.frame(area = area.seq))
mu.PI &amp;lt;- apply(mu, 2, PI, prob = 0.95)
plot(weight ~ area, data = d, col = rangi2)
abline(ma)
shade(mu.PI, area.seq)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-21-statistical-rethinking-chapter-05_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Group Size
mg &amp;lt;- quap(
  alist(
    weight ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bg * groupsize,
    a ~ dnorm(5, 5),
    bg ~ dnorm(0, 5),
    sigma ~ dunif(0, 5)
  ),
  data = d
)
precis(mg)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean         sd       5.5%       94.5%
## a      5.0675829 0.32418282  4.5494762  5.58568969
## bg    -0.1238161 0.07038361 -0.2363027 -0.01132946
## sigma  1.1635303 0.07638932  1.0414454  1.28561522
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;groupsize.seq &amp;lt;- seq(from = min(d$groupsize), to = max(d$groupsize), length.out = 1e4)
mu &amp;lt;- link(mg, data = data.frame(groupsize = groupsize.seq))
mu.PI &amp;lt;- apply(mu, 2, PI, prob = 0.95)
plot(weight ~ groupsize, data = d, col = rangi2)
abline(mg)
shade(mu.PI, groupsize.seq)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-21-statistical-rethinking-chapter-05_files/figure-html/unnamed-chunk-13-2.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;According to these two bivariate models, neither &lt;code&gt;area&lt;/code&gt; nor &lt;code&gt;groupsize&lt;/code&gt; have strong associations with &lt;code&gt;weight&lt;/code&gt; of which we could be certain.&lt;/p&gt;
&lt;h3 id=&#34;practice-h2&#34;&gt;Practice H2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Now fit a multiple linear regression with &lt;code&gt;weight&lt;/code&gt; as the outcome and both &lt;code&gt;area&lt;/code&gt; and &lt;code&gt;groupsize&lt;/code&gt; as predictor variables. Plot the predictions of the model for each predictor, holding the other predictor constant at its mean. What does this model say about the importance of each variable? Why do you get different results than you got in the exercise just above?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; First, we run the model itself:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mag &amp;lt;- quap(
  alist(
    weight ~ dnorm(mu, sigma),
    mu &amp;lt;- a + ba * area + bg * groupsize,
    a ~ dnorm(5, 5),
    ba ~ dnorm(0, 5),
    bg ~ dnorm(0, 5),
    sigma ~ dunif(0, 5)
  ),
  data = d
)
precis(mag)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean         sd       5.5%      94.5%
## a      4.4541696 0.36977162  3.8632031  5.0451360
## ba     0.6159473 0.19978363  0.2966545  0.9352402
## bg    -0.4318471 0.12066677 -0.6246959 -0.2389982
## sigma  1.1184516 0.07342985  1.0010965  1.2358067
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, I want to actually have a look at the underlying data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggpairs(d[, 3:5])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-21-statistical-rethinking-chapter-05_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;groupsize&lt;/code&gt; and &lt;code&gt;area&lt;/code&gt; are pretty heavily associated it seems. Finally, we establish counterfactual plots:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Fixing Group Size
G.avg &amp;lt;- mean(d$groupsize)
A.seq &amp;lt;- seq(from = 0, to = 6, length.out = 1e4)
pred.data &amp;lt;- data.frame(
  groupsize = G.avg,
  area = A.seq
)
mu &amp;lt;- link(mag, data = pred.data)
mu.mean &amp;lt;- apply(mu, 2, mean)
mu.PI &amp;lt;- apply(mu, 2, PI, prob = 0.95)
A.sim &amp;lt;- sim(mag, data = pred.data, n = 1e4)
A.PI &amp;lt;- apply(A.sim, 2, PI)
plot(weight ~ area, data = d, type = &amp;quot;n&amp;quot;)
mtext(&amp;quot;groupsize = 4.345&amp;quot;)
lines(A.seq, mu.mean)
shade(mu.PI, A.seq)
shade(A.PI, A.seq)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-21-statistical-rethinking-chapter-05_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Fixing Area
A.avg &amp;lt;- mean(d$area)
G.seq &amp;lt;- seq(from = 1, to = 10, length.out = 1e4)
pred.data &amp;lt;- data.frame(
  groupsize = G.seq,
  area = A.avg
)
mu &amp;lt;- link(mag, data = pred.data)
mu.mean &amp;lt;- apply(mu, 2, mean)
mu.PI &amp;lt;- apply(mu, 2, PI, prob = 0.95)
G.sim &amp;lt;- sim(mag, data = pred.data, n = 1e4)
G.PI &amp;lt;- apply(G.sim, 2, PI)
plot(weight ~ groupsize, data = d, type = &amp;quot;n&amp;quot;)
mtext(&amp;quot;area = 3.169&amp;quot;)
lines(G.seq, mu.mean)
shade(mu.PI, G.seq)
shade(G.PI, G.seq)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-21-statistical-rethinking-chapter-05_files/figure-html/unnamed-chunk-16-2.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is a clear example of a masking relationship. When considered in isolation (bivariate models) neither &lt;code&gt;groupsize&lt;/code&gt; nor &lt;code&gt;area&lt;/code&gt; show clear associations with &lt;code&gt;weight&lt;/code&gt;. However, as soon as we use a multiple regression, we find that &lt;code&gt;weight&lt;/code&gt; declines as &lt;code&gt;groupsize&lt;/code&gt; increases, while &lt;code&gt;area&lt;/code&gt; has the opposite effect. These effects cancel each other out in bivariate settings.&lt;/p&gt;
&lt;h3 id=&#34;practice-h3&#34;&gt;Practice H3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Finally, consider the &lt;code&gt;avgfood&lt;/code&gt; variable. Fit two more multiple regressions: (1) body weight as an additive function of &lt;code&gt;avgfood&lt;/code&gt; and &lt;code&gt;groupsize&lt;/code&gt;, and (2) body weight as an additive function of all three variables, &lt;code&gt;avgfood&lt;/code&gt; and &lt;code&gt;groupsize&lt;/code&gt; and &lt;code&gt;area&lt;/code&gt;. Compare the results of these models to the previous models you&amp;rsquo;ve fit, in the first two exercises.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; First, we require the model with two variables:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m &amp;lt;- quap(
  alist(
    weight ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bf * avgfood + bg * groupsize,
    a ~ dnorm(5, 5),
    bf ~ dnorm(0, 5),
    bg ~ dnorm(0, 5),
    sigma ~ dunif(0, 5)
  ),
  data = d
)
precis(m)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean         sd       5.5%      94.5%
## a      4.1806405 0.42501186  3.5013895  4.8598915
## bf     3.6052545 1.17683527  1.7244445  5.4860646
## bg    -0.5433458 0.15271144 -0.7874082 -0.2992834
## sigma  1.1166611 0.07332213  0.9994782  1.2338441
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we need the model with three variables:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m &amp;lt;- quap(
  alist(
    weight ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bf * avgfood + bg * groupsize + ba * area,
    a ~ dnorm(5, 5),
    bf ~ dnorm(0, 5),
    bg ~ dnorm(0, 5),
    ba ~ dnorm(0, 5),
    sigma ~ dunif(0, 5)
  ),
  data = d
)
precis(m)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean         sd        5.5%      94.5%
## a      4.1010815 0.42308541  3.42490933  4.7772537
## bf     2.3024285 1.39359133  0.07520038  4.5296566
## bg    -0.5926614 0.15385399 -0.83854977 -0.3467730
## ba     0.4017410 0.23609446  0.02441642  0.7790655
## sigma  1.1044452 0.07252194  0.98854116  1.2203493
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-a&#34;&gt;Part A&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Is &lt;code&gt;avgfood&lt;/code&gt; or &lt;code&gt;area&lt;/code&gt; a better predictor of body weight? If you had to choose one or the other to include in a model, which would it be? Support your assessment with any tables or plots you choose.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; According to intuition, I would prefer &lt;code&gt;avgfood&lt;/code&gt; because it is directly and obviously linked to a gain in calories and thus &lt;code&gt;weight&lt;/code&gt; whereas &lt;code&gt;area&lt;/code&gt; is a fairly indirect relationship. However, I still want to test this in &lt;code&gt;R&lt;/code&gt;. Let&amp;rsquo;s first look at the raw data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggpairs(d[, -1])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-21-statistical-rethinking-chapter-05_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;avgfood&lt;/code&gt; and &lt;code&gt;area&lt;/code&gt; are strikingly correlated with one another. To chose the most informative of these two, I build models of &lt;code&gt;weight&lt;/code&gt; solely dependant on standardised records of both (so the slope estimates are comparable in their magnitude) as well as &lt;code&gt;groupsize&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Average Food
d$avgfood.s &amp;lt;- (d$avgfood - mean(d$avgfood)) / sd(d$avgfood)
m &amp;lt;- quap(
  alist(
    weight ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bf * avgfood.s + bg * groupsize,
    a ~ dnorm(5, 5),
    bf ~ dnorm(0, 5),
    bg ~ dnorm(0, 5),
    sigma ~ dunif(0, 5)
  ),
  data = d
)
precis(m)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean         sd       5.5%      94.5%
## a      6.9566906 0.67995576  5.8699900  8.0433913
## bf     0.7450529 0.23844151  0.3639773  1.1261284
## bg    -0.5588002 0.15473498 -0.8060966 -0.3115038
## sigma  1.1166194 0.07331182  0.9994530  1.2337859
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Area
d$area.s &amp;lt;- (d$area - mean(d$area)) / sd(d$area)
m &amp;lt;- quap(
  alist(
    weight ~ dnorm(mu, sigma),
    mu &amp;lt;- a + ba * area.s + bg * groupsize,
    a ~ dnorm(5, 5),
    ba ~ dnorm(0, 5),
    bg ~ dnorm(0, 5),
    sigma ~ dunif(0, 5)
  ),
  data = d
)
precis(m)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean         sd       5.5%      94.5%
## a      6.3903420 0.53148096  5.5409327  7.2397512
## ba     0.5682683 0.18494820  0.2726854  0.8638512
## bg    -0.4283914 0.12001993 -0.6202064 -0.2365764
## sigma  1.1184575 0.07343099  1.0011006  1.2358144
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Given these results, I prefer the stronger relationship between &lt;code&gt;weight&lt;/code&gt; and &lt;code&gt;avgfood&lt;/code&gt; over that relying on &lt;code&gt;area&lt;/code&gt; and would chose a model using only &lt;code&gt;avgfood&lt;/code&gt;.&lt;/p&gt;
&lt;h4 id=&#34;part-b&#34;&gt;Part B&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; When both &lt;code&gt;avgfood&lt;/code&gt; or &lt;code&gt;area&lt;/code&gt; are in the same model, their effects are reduced (closer to zero) and their standard errors are larger than when they are included in separate models. Can you explain this result?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; I am pretty sure that this is our old friend &lt;em&gt;multicollinearity&lt;/em&gt; rearing its ugly head. Since the two are highly correlated, their respective effects become less when controlling for either in the same model.&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.5 (2021-03-31)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19043)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.1252  LC_CTYPE=English_United Kingdom.1252    LC_MONETARY=English_United Kingdom.1252 LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.1252    
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] dagitty_0.3-1        GGally_2.1.2         rethinking_2.13      rstan_2.21.2         ggplot2_3.3.6        StanHeaders_2.21.0-7
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.7         mvtnorm_1.1-1      lattice_0.20-41    prettyunits_1.1.1  ps_1.6.0           assertthat_0.2.1   digest_0.6.27      utf8_1.2.1         V8_3.4.1           plyr_1.8.6        
## [11] R6_2.5.0           backports_1.2.1    stats4_4.0.5       evaluate_0.14      coda_0.19-4        highr_0.9          blogdown_1.3       pillar_1.6.0       rlang_0.4.11       curl_4.3.2        
## [21] callr_3.7.0        jquerylib_0.1.4    R.utils_2.10.1     R.oo_1.24.0        rmarkdown_2.7      styler_1.4.1       labeling_0.4.2     stringr_1.4.0      loo_2.4.1          munsell_0.5.0     
## [31] compiler_4.0.5     xfun_0.22          pkgconfig_2.0.3    pkgbuild_1.2.0     shape_1.4.5        htmltools_0.5.1.1  tidyselect_1.1.0   tibble_3.1.1       gridExtra_2.3      bookdown_0.22     
## [41] codetools_0.2-18   matrixStats_0.61.0 reshape_0.8.8      fansi_0.4.2        crayon_1.4.1       dplyr_1.0.5        withr_2.4.2        MASS_7.3-53.1      R.methodsS3_1.8.1  grid_4.0.5        
## [51] jsonlite_1.7.2     gtable_0.3.0       lifecycle_1.0.0    DBI_1.1.1          magrittr_2.0.1     scales_1.1.1       RcppParallel_5.1.2 cli_3.0.0          stringi_1.5.3      farver_2.1.0      
## [61] bslib_0.2.4        ellipsis_0.3.2     generics_0.1.0     vctrs_0.3.7        boot_1.3-27        rematch2_2.1.2     RColorBrewer_1.1-2 tools_4.0.5        R.cache_0.14.0     glue_1.4.2        
## [71] purrr_0.3.4        processx_3.5.1     yaml_2.2.1         inline_0.3.17      colorspace_2.0-0   knitr_1.33         sass_0.3.1
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Inferential Statistics, Hypotheses And Our Research Project</title>
      <link>https://www.erikkusch.com/courses/an-introduction-to-biostatistics/06-inferential-statistics-hypotheses-and-our-research-project/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/an-introduction-to-biostatistics/06-inferential-statistics-hypotheses-and-our-research-project/</guid>
      <description>&lt;p&gt;This is a purely theoretical session and there is no practical exercise attached.&lt;/p&gt;
&lt;p&gt;I have prepared some &lt;a href=&#34;https://www.erikkusch.com/courses/an-introduction-to-biostatistics/06---Inferential-Statistics,-Hypotheses-And-Our-Research-Project_Handout.html&#34; target=&#34;_blank&#34;&gt; Lecture Slides &lt;/a&gt; for this session.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Inferential Statistics, Hypotheses And Our Research Project</title>
      <link>https://www.erikkusch.com/courses/biostat101/06-inferential-statistics-hypotheses-and-our-research-project/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/biostat101/06-inferential-statistics-hypotheses-and-our-research-project/</guid>
      <description>&lt;p&gt;I have prepared some slides for this session: &lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/biostat101/06---Inferential-Statistics,-Hypotheses-And-Our-Research-Project_Handout.html&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://www.erikkusch.com/courses/biostat101/06---BioStat101_featured.png&#34; width=&#34;900&#34; margin-top = &#34;0&#34;/&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Handling Data with rgbif</title>
      <link>https://www.erikkusch.com/courses/gbif/datacontrol/</link>
      <pubDate>Sun, 21 May 2023 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/gbif/datacontrol/</guid>
      <description>&lt;div class=&#34;alert alert-normal&#34;&gt;
  &lt;div&gt;
    &lt;details&gt;
  &lt;summary&gt;Preamble, Package-Loading, and GBIF API Credential Registering (click here):&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Custom install &amp;amp; load function
install.load.package &amp;lt;- function(x) {
  if (!require(x, character.only = TRUE)) {
    install.packages(x, repos = &amp;quot;http://cran.us.r-project.org&amp;quot;)
  }
  require(x, character.only = TRUE)
}
## names of packages we want installed (if not installed yet) and loaded
package_vec &amp;lt;- c(
  &amp;quot;rgbif&amp;quot;,
  &amp;quot;knitr&amp;quot;, # for rmarkdown table visualisations
  &amp;quot;sp&amp;quot;, # for spatialobject creation
  &amp;quot;sf&amp;quot;, # an alternative spatial object library
  &amp;quot;ggplot2&amp;quot;, # for visualistion
  &amp;quot;raster&amp;quot;, # for setting and reading CRS
  &amp;quot;rnaturalearth&amp;quot; # for shapefiles of naturalearth
)
## executing install &amp;amp; load for each package
sapply(package_vec, install.load.package)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         rgbif         knitr            sp            sf       ggplot2        raster 
##          TRUE          TRUE          TRUE          TRUE          TRUE          TRUE 
## rnaturalearth 
##          TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;options(gbif_user = &amp;quot;my gbif username&amp;quot;)
options(gbif_email = &amp;quot;my registred gbif e-mail&amp;quot;)
options(gbif_pwd = &amp;quot;my gbif password&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt; 
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;First, we obtain and load the data we are interested in like such:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Download query
res &amp;lt;- occ_download(
  pred(&amp;quot;taxonKey&amp;quot;, sp_key),
  pred_in(&amp;quot;basisOfRecord&amp;quot;, c(&amp;quot;HUMAN_OBSERVATION&amp;quot;)),
  pred(&amp;quot;country&amp;quot;, &amp;quot;NO&amp;quot;),
  pred(&amp;quot;hasCoordinate&amp;quot;, TRUE),
  pred_gte(&amp;quot;year&amp;quot;, 2000),
  pred_lte(&amp;quot;year&amp;quot;, 2020)
)
# Downloading and Loading
res_meta &amp;lt;- occ_download_wait(res, status_ping = 5, curlopts = list(), quiet = FALSE)
res_get &amp;lt;- occ_download_get(res)
res_data &amp;lt;- occ_download_import(res_get)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this data in our &lt;code&gt;R&lt;/code&gt; environment, we are ready to explore the data itself.&lt;/p&gt;
&lt;div class=&#34;alert alert-info&#34;&gt;
  &lt;div&gt;
    There are &lt;strong&gt;a myriad&lt;/strong&gt; of things you can do to and with GBIF mediated data - here, we focus only on a few data handling steps.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;initial-data-handling&#34;&gt;Initial Data Handling&lt;/h2&gt;
&lt;p&gt;Before working with the data you obtained via GBIF, it is usually good practice to first check that all data is as expected/in order and then either reduce the dataset further to fit quality standards and extract the relevant variables for your application.&lt;/p&gt;
&lt;h3 id=&#34;common-data-considerations--issues&#34;&gt;Common Data Considerations &amp;amp; Issues&lt;/h3&gt;
&lt;p&gt;Common data considerations and quality flags are largely related to geolocations (but other quality markers do exist). These can be used as limiting factors in data discovery, when querying downloads as well as after a download is done and the data is loaded. Within the GBIF Portal, these options are presented in a side-bar like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.erikkusch.com/courses/gbif/qualityflags.png&#34; width=&#34;100%&#34;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As a matter of fact, we have already used the functionality by which to control for data quality markers when carrying out data discovery (&lt;code&gt;occ_search(...)&lt;/code&gt;) and data download queries (&lt;code&gt;occ_download(...)&lt;/code&gt;) by matching Darwin Core Terms like basisOfRecord or hasCoordinate.&lt;/p&gt;
&lt;p&gt;For this exercise, let&amp;rsquo;s focus on some data markers that are contained in our already downloaded data set which we may want to use for further limiting of our data set for subsequent analyses. To do so, let&amp;rsquo;s consider the &lt;code&gt;coordinateUncertaintyInMeters&lt;/code&gt; field by visualising the values we have obtained for each record in our occurrence data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(res_data, aes(x = coordinateUncertaintyInMeters)) +
  geom_histogram(bins = 1e2) +
  theme_bw() +
  scale_y_continuous(trans = &amp;quot;log10&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;rgbif-datacontrol_files/figure-html/uncertaintyinmetre-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The y-axis on the above plot is log-transformed and 439 of the underlying records do not report a value for &lt;code&gt;coordinateUncertaintyInMeters&lt;/code&gt; thus being removed from the above visualisation.&lt;/p&gt;
&lt;p&gt;What we find is that there exists considerable variation in confidence of individual occurrence locations and we probably want to remove those records which are assigned a certain level of &lt;code&gt;coordinateUncertaintyInMeters&lt;/code&gt;. Let&amp;rsquo;s say 200 metres (after all, we are dealing with a mobile organism):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;preci_data &amp;lt;- res_data[which(res_data$coordinateUncertaintyInMeters &amp;lt; 200), ]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This quality control leaves us with 2721 &lt;em&gt;Lagopus muta&lt;/em&gt; records. A significant drop in data points which may well change our analyses and their outcomes drastically.&lt;/p&gt;
&lt;h3 id=&#34;extract-a-subset-of-cata-columns&#34;&gt;Extract a Subset of Cata-Columns&lt;/h3&gt;
&lt;p&gt;GBIF mediated data comes with a lot of attributes. These can be assessed readily via the Darwin Core or, within &lt;code&gt;R&lt;/code&gt; via: &lt;code&gt;colnames(...)&lt;/code&gt; (here with &lt;code&gt;...&lt;/code&gt;  = &lt;code&gt;res_data&lt;/code&gt;). Rarely will we need all of them for our analyses. For now, we will simply subset the data for a smaller set of columns which are often relevant for end-users:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data_subset &amp;lt;- preci_data[
  ,
  c(&amp;quot;scientificName&amp;quot;, &amp;quot;decimalLongitude&amp;quot;, &amp;quot;decimalLatitude&amp;quot;, &amp;quot;basisOfRecord&amp;quot;, &amp;quot;year&amp;quot;, &amp;quot;month&amp;quot;, &amp;quot;day&amp;quot;, &amp;quot;eventDate&amp;quot;, &amp;quot;countryCode&amp;quot;, &amp;quot;municipality&amp;quot;, &amp;quot;stateProvince&amp;quot;, &amp;quot;taxonKey&amp;quot;, &amp;quot;species&amp;quot;, &amp;quot;catalogNumber&amp;quot;, &amp;quot;hasGeospatialIssues&amp;quot;, &amp;quot;hasCoordinate&amp;quot;, &amp;quot;mediaType&amp;quot;, &amp;quot;datasetKey&amp;quot;)
]
knitr::kable(head(data_subset))
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;scientificName&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;decimalLongitude&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;decimalLatitude&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;basisOfRecord&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;year&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;month&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;day&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;eventDate&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;countryCode&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;municipality&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;stateProvince&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;taxonKey&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;species&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;catalogNumber&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;hasGeospatialIssues&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;hasCoordinate&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;mediaType&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;datasetKey&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta (Montin, 1781)&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;7.679194&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;59.81584&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;HUMAN_OBSERVATION&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2008&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2008-05-03&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NO&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Vinje&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Telemark&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;34904134&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;TRUE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;b124e1e0-4755-430f-9eab-894f25a9b59c&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta (Montin, 1781)&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;28.961315&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;70.46989&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;HUMAN_OBSERVATION&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2017&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;6&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2017-06-05&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NO&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Finnmark&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;TRUE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;8a863029-f435-446a-821e-275f4f641165&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta (Montin, 1781)&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;23.689751&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;70.68865&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;HUMAN_OBSERVATION&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2015&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;6&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;22&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2015-06-22&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NO&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Finnmark&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;TRUE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;StillImage&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;8a863029-f435-446a-821e-275f4f641165&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta (Montin, 1781)&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;8.628497&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;60.65505&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;HUMAN_OBSERVATION&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2000&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;6&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2000-06-05&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NO&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Buskerud&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;TRUE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;StillImage&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;8a863029-f435-446a-821e-275f4f641165&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta (Montin, 1781)&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;24.720209&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;71.10924&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;HUMAN_OBSERVATION&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2019&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;7&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;18&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2019-07-18T10:41&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NO&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Finnmark&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;214152489&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;TRUE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;StillImage&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;50c9509d-22c7-4a22-a47d-8c48425ef4a7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta (Montin, 1781)&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;24.729996&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;71.11087&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;HUMAN_OBSERVATION&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2017&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;7&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;14&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2017-07-14T10:26&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NO&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Finnmark&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5227679&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Lagopus muta&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;214152361&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;TRUE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;StillImage&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;50c9509d-22c7-4a22-a47d-8c48425ef4a7&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;explore-the-occurrence-data&#34;&gt;Explore the Occurrence Data&lt;/h2&gt;
&lt;p&gt;Now that we have the data we might use for analyses ready, we can explore what the data itself contains.&lt;/p&gt;
&lt;h3 id=&#34;data-contents&#34;&gt;Data Contents&lt;/h3&gt;
&lt;p&gt;Here are a few overviews of &lt;em&gt;Lagopus muta&lt;/em&gt; abundances across different data attributes:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(data_subset$year)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 
##    4    4   12    6    7   23    7   17   34   59   87  119  291  144  147  181  254  255 
## 2018 2019 2020 
##  372  382  316
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(data_subset$stateProvince)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##                                     Agder           Aust-Agder             Buskerud 
##                  149                  116                    2                   15 
##             Finnmark              Hedmark            Hordaland            Innlandet 
##                  127                  284                  269                  122 
##      MÃ¸re og Romsdal                 Nord       Nord-TrÃ¸ndelag             Nordland 
##                   70                    8                  160                  309 
##              Oppland             Rogaland     Sogn og Fjordane                  SÃ¸r 
##                   77                   36                    3                    8 
##        SÃ¸r-TrÃ¸ndelag             Telemark                Troms    Troms og Finnmark 
##                  150                    2                   40                  128 
##            TrÃ¸ndelag           Vest-Agder Vestfold og Telemark             Vestland 
##                  405                   36                   25                  134 
##                Viken              Ãstfold 
##                   45                    1
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(data_subset$mediaType)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##            StillImage 
##       2606        115
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;spatial-data-handling&#34;&gt;Spatial Data Handling&lt;/h3&gt;
&lt;p&gt;Most use-cases of GBIF make use of the geolocation references for data records either implicitly or explicitly. It is thus vital to be able to handle GBIF mediated data for spatial analyses. There exist plenty workshop (like 
&lt;a href=&#34;https://pjbartlein.github.io/REarthSysSci/geospatial.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this one&lt;/a&gt;) already for this topic so I will be brief.&lt;/p&gt;
&lt;h4 id=&#34;make-spatialpointsdataframe&#34;&gt;Make &lt;code&gt;SpatialPointsDataFrame&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;First, we can use the &lt;code&gt;sf&lt;/code&gt; package to create &lt;code&gt;SpatialPoints&lt;/code&gt; from our geo-referenced occurrence data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;options(digits = 8) ## set 8 digits (ie. all digits, not decimals) for the type cast as.double to keep decimals
data_subset &amp;lt;- as.data.frame(data_subset)
data_subset$lon &amp;lt;- as.double(data_subset$decimalLongitude) ## cast lon from char to double
data_subset$lat &amp;lt;- as.double(data_subset$decimalLatitude) ## cast lat from char to double
data_sf &amp;lt;- st_as_sf(data_subset, coords = c(&amp;quot;lon&amp;quot;, &amp;quot;lat&amp;quot;), remove = FALSE)
st_crs(data_sf) &amp;lt;- CRS(&amp;quot;+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs +towgs84=0,0,0&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This data format lends itself well for analysing where occurrence have been recorded in relation to study parameters of choice (e.g., climatic conditions, land-use, political boundaries, etc.).&lt;/p&gt;
&lt;h4 id=&#34;spatialpoints-and-polygons&#34;&gt;&lt;code&gt;SpatialPoints&lt;/code&gt; and Polygons&lt;/h4&gt;
&lt;p&gt;In first instance, &lt;code&gt;SpatialPoints&lt;/code&gt; can easily be used to create initial visualisations of spatial patterns:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## get background map
NO_shp &amp;lt;- rnaturalearth::ne_countries(country = &amp;quot;Norway&amp;quot;, scale = &amp;quot;medium&amp;quot;, returnclass = &amp;quot;sf&amp;quot;)[, 1]
## make plot
ggplot() +
  geom_sf(data = NO_shp) +
  geom_sf(data = data_sf[, 1]) +
  theme_bw() +
  labs(title = &amp;quot;Occurrences of Lagopus muta recorded by human observations between 2000 and 2022&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;rgbif-datacontrol_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h4 id=&#34;the-coordinate-reference-system-crs&#34;&gt;The Coordinate Reference System (CRS)&lt;/h4&gt;
&lt;p&gt;Each spatial object in &lt;code&gt;R&lt;/code&gt; is assigned a  
&lt;a href=&#34;https://www.earthdatascience.org/courses/earth-analytics/spatial-data-r/intro-to-coordinate-reference-systems/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Coordinate Reference System (CRS)&lt;/a&gt; which details how geolocational values are to be understood. For an overview of different CRSs, see 
&lt;a href=&#34;https://epsg.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;R&lt;/code&gt;, we can assess the CRS of most spatial objects as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;st_crs(data_sf)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Coordinate Reference System:
##   User input: BOUNDCRS[
##     SOURCECRS[
##         GEOGCRS[&amp;quot;unknown&amp;quot;,
##             DATUM[&amp;quot;World Geodetic System 1984&amp;quot;,
##                 ELLIPSOID[&amp;quot;WGS 84&amp;quot;,6378137,298.257223563,
##                     LENGTHUNIT[&amp;quot;metre&amp;quot;,1]],
##                 ID[&amp;quot;EPSG&amp;quot;,6326]],
##             PRIMEM[&amp;quot;Greenwich&amp;quot;,0,
##                 ANGLEUNIT[&amp;quot;degree&amp;quot;,0.0174532925199433],
##                 ID[&amp;quot;EPSG&amp;quot;,8901]],
##             CS[ellipsoidal,2],
##                 AXIS[&amp;quot;longitude&amp;quot;,east,
##                     ORDER[1],
##                     ANGLEUNIT[&amp;quot;degree&amp;quot;,0.0174532925199433,
##                         ID[&amp;quot;EPSG&amp;quot;,9122]]],
##                 AXIS[&amp;quot;latitude&amp;quot;,north,
##                     ORDER[2],
##                     ANGLEUNIT[&amp;quot;degree&amp;quot;,0.0174532925199433,
##                         ID[&amp;quot;EPSG&amp;quot;,9122]]]]],
##     TARGETCRS[
##         GEOGCRS[&amp;quot;WGS 84&amp;quot;,
##             DATUM[&amp;quot;World Geodetic System 1984&amp;quot;,
##                 ELLIPSOID[&amp;quot;WGS 84&amp;quot;,6378137,298.257223563,
##                     LENGTHUNIT[&amp;quot;metre&amp;quot;,1]]],
##             PRIMEM[&amp;quot;Greenwich&amp;quot;,0,
##                 ANGLEUNIT[&amp;quot;degree&amp;quot;,0.0174532925199433]],
##             CS[ellipsoidal,2],
##                 AXIS[&amp;quot;latitude&amp;quot;,north,
##                     ORDER[1],
##                     ANGLEUNIT[&amp;quot;degree&amp;quot;,0.0174532925199433]],
##                 AXIS[&amp;quot;longitude&amp;quot;,east,
##                     ORDER[2],
##                     ANGLEUNIT[&amp;quot;degree&amp;quot;,0.0174532925199433]],
##             ID[&amp;quot;EPSG&amp;quot;,4326]]],
##     ABRIDGEDTRANSFORMATION[&amp;quot;Transformation from unknown to WGS84&amp;quot;,
##         METHOD[&amp;quot;Geocentric translations (geog2D domain)&amp;quot;,
##             ID[&amp;quot;EPSG&amp;quot;,9603]],
##         PARAMETER[&amp;quot;X-axis translation&amp;quot;,0,
##             ID[&amp;quot;EPSG&amp;quot;,8605]],
##         PARAMETER[&amp;quot;Y-axis translation&amp;quot;,0,
##             ID[&amp;quot;EPSG&amp;quot;,8606]],
##         PARAMETER[&amp;quot;Z-axis translation&amp;quot;,0,
##             ID[&amp;quot;EPSG&amp;quot;,8607]]]] 
##   wkt:
## BOUNDCRS[
##     SOURCECRS[
##         GEOGCRS[&amp;quot;unknown&amp;quot;,
##             DATUM[&amp;quot;World Geodetic System 1984&amp;quot;,
##                 ELLIPSOID[&amp;quot;WGS 84&amp;quot;,6378137,298.257223563,
##                     LENGTHUNIT[&amp;quot;metre&amp;quot;,1]],
##                 ID[&amp;quot;EPSG&amp;quot;,6326]],
##             PRIMEM[&amp;quot;Greenwich&amp;quot;,0,
##                 ANGLEUNIT[&amp;quot;degree&amp;quot;,0.0174532925199433],
##                 ID[&amp;quot;EPSG&amp;quot;,8901]],
##             CS[ellipsoidal,2],
##                 AXIS[&amp;quot;longitude&amp;quot;,east,
##                     ORDER[1],
##                     ANGLEUNIT[&amp;quot;degree&amp;quot;,0.0174532925199433,
##                         ID[&amp;quot;EPSG&amp;quot;,9122]]],
##                 AXIS[&amp;quot;latitude&amp;quot;,north,
##                     ORDER[2],
##                     ANGLEUNIT[&amp;quot;degree&amp;quot;,0.0174532925199433,
##                         ID[&amp;quot;EPSG&amp;quot;,9122]]]]],
##     TARGETCRS[
##         GEOGCRS[&amp;quot;WGS 84&amp;quot;,
##             DATUM[&amp;quot;World Geodetic System 1984&amp;quot;,
##                 ELLIPSOID[&amp;quot;WGS 84&amp;quot;,6378137,298.257223563,
##                     LENGTHUNIT[&amp;quot;metre&amp;quot;,1]]],
##             PRIMEM[&amp;quot;Greenwich&amp;quot;,0,
##                 ANGLEUNIT[&amp;quot;degree&amp;quot;,0.0174532925199433]],
##             CS[ellipsoidal,2],
##                 AXIS[&amp;quot;geodetic latitude (Lat)&amp;quot;,north,
##                     ORDER[1],
##                     ANGLEUNIT[&amp;quot;degree&amp;quot;,0.0174532925199433]],
##                 AXIS[&amp;quot;geodetic longitude (Lon)&amp;quot;,east,
##                     ORDER[2],
##                     ANGLEUNIT[&amp;quot;degree&amp;quot;,0.0174532925199433]],
##             ID[&amp;quot;EPSG&amp;quot;,4326]]],
##     ABRIDGEDTRANSFORMATION[&amp;quot;Transformation from unknown to WGS84&amp;quot;,
##         METHOD[&amp;quot;Geocentric translations (geog2D domain)&amp;quot;,
##             ID[&amp;quot;EPSG&amp;quot;,9603]],
##         PARAMETER[&amp;quot;X-axis translation&amp;quot;,0,
##             ID[&amp;quot;EPSG&amp;quot;,8605]],
##         PARAMETER[&amp;quot;Y-axis translation&amp;quot;,0,
##             ID[&amp;quot;EPSG&amp;quot;,8606]],
##         PARAMETER[&amp;quot;Z-axis translation&amp;quot;,0,
##             ID[&amp;quot;EPSG&amp;quot;,8607]]]]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;st_crs(NO_shp)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Coordinate Reference System:
##   User input: WGS 84 
##   wkt:
## GEOGCRS[&amp;quot;WGS 84&amp;quot;,
##     DATUM[&amp;quot;World Geodetic System 1984&amp;quot;,
##         ELLIPSOID[&amp;quot;WGS 84&amp;quot;,6378137,298.257223563,
##             LENGTHUNIT[&amp;quot;metre&amp;quot;,1]]],
##     PRIMEM[&amp;quot;Greenwich&amp;quot;,0,
##         ANGLEUNIT[&amp;quot;degree&amp;quot;,0.0174532925199433]],
##     CS[ellipsoidal,2],
##         AXIS[&amp;quot;latitude&amp;quot;,north,
##             ORDER[1],
##             ANGLEUNIT[&amp;quot;degree&amp;quot;,0.0174532925199433]],
##         AXIS[&amp;quot;longitude&amp;quot;,east,
##             ORDER[2],
##             ANGLEUNIT[&amp;quot;degree&amp;quot;,0.0174532925199433]],
##     ID[&amp;quot;EPSG&amp;quot;,4326]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When dealing with data in specific areas of the world or wanting to match occurrence data to other products with specific CRSs, it may be prudent to reproject the &lt;code&gt;SpatialPoints&lt;/code&gt; occurrence data object.  We can use &lt;code&gt;sf::st_transform)&lt;/code&gt; to do so (this is reprojecting to the same CRS the data is already in):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sf::st_transform(data_sf, CRS(&amp;quot;+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs +towgs84=0,0,0&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Simple feature collection with 2721 features and 20 fields
## Geometry type: POINT
## Dimension:     XY
## Bounding box:  xmin: 5.488913 ymin: 58.065259 xmax: 31.020815 ymax: 71.170652
## Geodetic CRS:  BOUNDCRS[
##     SOURCECRS[
##         GEOGCRS[&amp;quot;unknown&amp;quot;,
##             DATUM[&amp;quot;World Geodetic System 1984&amp;quot;,
##                 ELLIPSOID[&amp;quot;WGS 84&amp;quot;,6378137,298.257223563,
##                     LENGTHUNIT[&amp;quot;metre&amp;quot;,1]],
##                 ID[&amp;quot;EPSG&amp;quot;,6326]],
##             PRIMEM[&amp;quot;Greenwich&amp;quot;,0,
##                 ANGLEUNIT[&amp;quot;degree&amp;quot;,0.0174532925199433],
##                 ID[&amp;quot;EPSG&amp;quot;,8901]],
##             CS[ellipsoidal,2],
##                 AXIS[&amp;quot;longitude&amp;quot;,east,
##                     ORDER[1],
##                     ANGLEUNIT[&amp;quot;degree&amp;quot;,0.0174532925199433,
##                         ID[&amp;quot;EPSG&amp;quot;,9122]]],
##                 AXIS[&amp;quot;latitude&amp;quot;,north,
##                     ORDER[2],
##                     ANGLEUNIT[&amp;quot;degree&amp;quot;,0.0174532925199433,
##                         ID[&amp;quot;EPSG&amp;quot;,9122]]]]],
##     TARGETCRS[
##         GEOGCRS[&amp;quot;WGS 84&amp;quot;,
##             DATUM[&amp;quot;World Geodetic System 1984&amp;quot;,
##                 ELLIPSOID[&amp;quot;WGS 84&amp;quot;,6378137,298.257223563,
##                     LENGTHUNIT[&amp;quot;metre&amp;quot;,1]]],
##             PRIMEM[&amp;quot;Greenwich&amp;quot;,0,
##                 ANGLEUNIT[&amp;quot;degree&amp;quot;,0.0174532925199433]],
##             CS[ellipsoidal,2],
##                 AXIS[&amp;quot;latitude&amp;quot;,north,
##                     ORDER[1],
##                     ANGLEUNIT[&amp;quot;degree&amp;quot;,0.0174532925199433]],
##                 AXIS[&amp;quot;longitude&amp;quot;,east,
##                     ORDER[2],
##                     ANGLEUNIT[&amp;quot;degree&amp;quot;,0.0174532925199433]],
##             ID[&amp;quot;EPSG&amp;quot;,4326]]],
##     ABRIDGEDTRANSFORMATION[&amp;quot;Transformation from unknown to WGS84&amp;quot;,
##         METHOD[&amp;quot;Geocentric translations (geog2D domain)&amp;quot;,
##             ID[&amp;quot;EPSG&amp;quot;,9603]],
##         PARAMETER[&amp;quot;X-axis translation&amp;quot;,0,
##             ID[&amp;quot;EPSG&amp;quot;,8605]],
##         PARAMETER[&amp;quot;Y-axis translation&amp;quot;,0,
##             ID[&amp;quot;EPSG&amp;quot;,8606]],
##         PARAMETER[&amp;quot;Z-axis translation&amp;quot;,0,
##             ID[&amp;quot;EPSG&amp;quot;,8607]]]]
## First 10 features:
##                 scientificName decimalLongitude decimalLatitude     basisOfRecord year
## 1  Lagopus muta (Montin, 1781)         7.679194       59.815843 HUMAN_OBSERVATION 2008
## 2  Lagopus muta (Montin, 1781)        28.961315       70.469890 HUMAN_OBSERVATION 2017
## 3  Lagopus muta (Montin, 1781)        23.689751       70.688646 HUMAN_OBSERVATION 2015
## 4  Lagopus muta (Montin, 1781)         8.628497       60.655046 HUMAN_OBSERVATION 2000
## 5  Lagopus muta (Montin, 1781)        24.720209       71.109243 HUMAN_OBSERVATION 2019
## 6  Lagopus muta (Montin, 1781)        24.729996       71.110875 HUMAN_OBSERVATION 2017
## 7  Lagopus muta (Montin, 1781)        14.563245       67.088208 HUMAN_OBSERVATION 2017
## 8  Lagopus muta (Montin, 1781)         9.291491       62.433117 HUMAN_OBSERVATION 2020
## 9  Lagopus muta (Montin, 1781)         9.466869       62.722573 HUMAN_OBSERVATION 2004
## 10 Lagopus muta (Montin, 1781)        11.736154       62.914320 HUMAN_OBSERVATION 2002
##    month day        eventDate countryCode municipality stateProvince taxonKey
## 1      5   3       2008-05-03          NO        Vinje      Telemark  5227679
## 2      6   5       2017-06-05          NO                   Finnmark  5227679
## 3      6  22       2015-06-22          NO                   Finnmark  5227679
## 4      6   5       2000-06-05          NO                   Buskerud  5227679
## 5      7  18 2019-07-18T10:41          NO                   Finnmark  5227679
## 6      7  14 2017-07-14T10:26          NO                   Finnmark  5227679
## 7      6  17       2017-06-17          NO       Beiarn      Nordland  5227679
## 8      7  31       2020-07-31          NO       Oppdal    TrÃ¸ndelag  5227679
## 9      9  11       2004-09-11          NO       Oppdal    TrÃ¸ndelag  5227679
## 10     7  27       2002-07-27          NO                 TrÃ¸ndelag  5227679
##         species  catalogNumber hasGeospatialIssues hasCoordinate  mediaType
## 1  Lagopus muta       34904134               FALSE          TRUE           
## 2  Lagopus muta                              FALSE          TRUE           
## 3  Lagopus muta                              FALSE          TRUE StillImage
## 4  Lagopus muta                              FALSE          TRUE StillImage
## 5  Lagopus muta      214152489               FALSE          TRUE StillImage
## 6  Lagopus muta      214152361               FALSE          TRUE StillImage
## 7  Lagopus muta       34314978               FALSE          TRUE           
## 8  Lagopus muta       33833573               FALSE          TRUE StillImage
## 9  Lagopus muta       32243615               FALSE          TRUE           
## 10 Lagopus muta BA00022793-106               FALSE          TRUE           
##                              datasetKey       lon       lat                    geometry
## 1  b124e1e0-4755-430f-9eab-894f25a9b59c  7.679194 59.815843  POINT (7.679194 59.815843)
## 2  8a863029-f435-446a-821e-275f4f641165 28.961315 70.469890  POINT (28.961315 70.46989)
## 3  8a863029-f435-446a-821e-275f4f641165 23.689751 70.688646 POINT (23.689751 70.688646)
## 4  8a863029-f435-446a-821e-275f4f641165  8.628497 60.655046  POINT (8.628497 60.655046)
## 5  50c9509d-22c7-4a22-a47d-8c48425ef4a7 24.720209 71.109243 POINT (24.720209 71.109243)
## 6  50c9509d-22c7-4a22-a47d-8c48425ef4a7 24.729996 71.110875 POINT (24.729996 71.110875)
## 7  b124e1e0-4755-430f-9eab-894f25a9b59c 14.563245 67.088208 POINT (14.563245 67.088208)
## 8  b124e1e0-4755-430f-9eab-894f25a9b59c  9.291491 62.433117  POINT (9.291491 62.433117)
## 9  b124e1e0-4755-430f-9eab-894f25a9b59c  9.466869 62.722573  POINT (9.466869 62.722573)
## 10 492d63a8-4978-4bc7-acd8-7d0e3ac0e744 11.736154 62.914320  POINT (11.736154 62.91432)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;classifying-spatial-data&#34;&gt;Classifying Spatial Data&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s say, for example, we want to quantify abundances of &lt;em&gt;Lagopus muta&lt;/em&gt; across political regions in Norway:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Obtain sf object
NO_municip &amp;lt;- rnaturalearth::ne_states(country = &amp;quot;Norway&amp;quot;, returnclass = &amp;quot;sf&amp;quot;) # get shapefiles for Norwegian states
NO_municip &amp;lt;- sf::st_crop(NO_municip, extent(4.5, 31.5, 50, 71.5)) # crop shapefile to continental Norway
## Identify overlap of points and polygons
cover_sf &amp;lt;- st_intersects(NO_municip, data_sf)
names(cover_sf) &amp;lt;- NO_municip$name
## report abundances
abundances_municip &amp;lt;- unlist(lapply(cover_sf, length))
knitr::kable(t(sort(abundances_municip, decreasing = TRUE)))
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right&#34;&gt;SÃ¸r-TrÃ¸ndelag&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Hordaland&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Hedmark&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Nord-TrÃ¸ndelag&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Nordland&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Oppland&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Finnmark&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Troms&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Vest-Agder&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Buskerud&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;MÃ¸re og Romsdal&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Rogaland&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Telemark&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Sogn og Fjordane&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Aust-Agder&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Akershus&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Ãstfold&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Oslo&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Vestfold&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;489&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;361&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;348&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;340&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;286&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;168&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;160&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;132&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;122&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;79&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;78&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;33&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;28&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;27&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;25&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Looks like there are hotspots for &lt;em&gt;Lagopus muta&lt;/em&gt; in SÃ¸rTrÃ¸ndelag and Hordaland - could this be sampling bias or effects of bioclimatic niche preferences and local environmental conditions? Questions like these you will be able to answer with additional analyses which are beyond the scope of this workshop.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s visualise these abundances:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;NO_municip$abundances &amp;lt;- abundances_municip
ggplot(data = NO_municip) +
  geom_sf(aes(fill = abundances)) +
  scale_fill_viridis_c() +
  labs(x = &amp;quot;Longitude&amp;quot;, y = &amp;quot;Latitude&amp;quot;, col = &amp;quot;Abundance&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;rgbif-datacontrol_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Finally, let&amp;rsquo;s consider wanting to identify for each data record and attach to the data itself which state it falls into. We can do so as follows (not necessarily the most elegant way:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## create a dataframe of occurrence records by rownumber in original data (data_subset) and state-membership
cover_ls &amp;lt;- lapply(names(cover_sf), FUN = function(x) {
  if (length(cover_sf[[x]]) == 0) {
    points &amp;lt;- NA
  } else {
    points &amp;lt;- cover_sf[[x]]
  }
  data.frame(
    municip = x,
    points = points
  )
})
cover_df &amp;lt;- na.omit(do.call(rbind, cover_ls))
## attach state-membership to original data, NAs for points without state-membership
data_subset$municip &amp;lt;- NA
data_subset$municip[cover_df$points] &amp;lt;- cover_df$municip
## visualise the result
ggplot(data = NO_municip) +
  geom_sf(fill = &amp;quot;white&amp;quot;) +
  geom_point(
    data = data_subset, size = 1,
    aes(x = decimalLongitude, y = decimalLatitude, col = municip)
  ) +
  scale_colour_viridis_d() +
  labs(x = &amp;quot;Longitude&amp;quot;, y = &amp;quot;Latitude&amp;quot;, col = &amp;quot;Municipality&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;rgbif-datacontrol_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s say we feed these data into an analysis which runs to completion and we want to report on our findings. What&amp;rsquo;s next? Citing the data we used.&lt;/p&gt;
&lt;div class=&#34;alert alert-success&#34;&gt;
  &lt;div&gt;
    Now that you can &lt;strong&gt;handle GBIF data locally&lt;/strong&gt;, you are ready to pipe these data into your specific analysis tools. Lastly, you only need to cite the data you used.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.4.0 (2024-04-24 ucrt)
## Platform: x86_64-w64-mingw32/x64
## Running under: Windows 11 x64 (build 22631)
## 
## Matrix products: default
## 
## 
## locale:
## [1] C
## 
## time zone: Europe/Oslo
## tzcode source: internal
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] rnaturalearth_1.0.1 raster_3.6-26       ggplot2_3.5.1       sf_1.0-17          
## [5] sp_2.1-4            knitr_1.48          rgbif_3.8.1        
## 
## loaded via a namespace (and not attached):
##  [1] gtable_0.3.6                  xfun_0.47                    
##  [3] bslib_0.8.0                   lattice_0.22-6               
##  [5] vctrs_0.6.5                   tools_4.4.0                  
##  [7] generics_0.1.3                curl_5.2.2                   
##  [9] tibble_3.2.1                  proxy_0.4-27                 
## [11] fansi_1.0.6                   highr_0.11                   
## [13] pkgconfig_2.0.3               R.oo_1.26.0                  
## [15] KernSmooth_2.23-22            data.table_1.16.0            
## [17] lifecycle_1.0.4               R.cache_0.16.0               
## [19] farver_2.1.2                  compiler_4.4.0               
## [21] stringr_1.5.1                 munsell_0.5.1                
## [23] terra_1.7-78                  codetools_0.2-20             
## [25] htmltools_0.5.8.1             class_7.3-22                 
## [27] sass_0.4.9                    yaml_2.3.10                  
## [29] lazyeval_0.2.2                pillar_1.9.0                 
## [31] jquerylib_0.1.4               whisker_0.4.1                
## [33] R.utils_2.12.3                classInt_0.4-10              
## [35] cachem_1.1.0                  wk_0.9.4                     
## [37] rnaturalearthdata_1.0.0       styler_1.10.3                
## [39] tidyselect_1.2.1              digest_0.6.37                
## [41] stringi_1.8.4                 dplyr_1.1.4                  
## [43] purrr_1.0.2                   bookdown_0.40                
## [45] labeling_0.4.3                fastmap_1.2.0                
## [47] grid_4.4.0                    colorspace_2.1-1             
## [49] cli_3.6.3                     magrittr_2.0.3               
## [51] triebeard_0.4.1               crul_1.5.0                   
## [53] utf8_1.2.4                    e1071_1.7-16                 
## [55] withr_3.0.1                   scales_1.3.0                 
## [57] bit64_4.0.5                   oai_0.4.0                    
## [59] rmarkdown_2.28                httr_1.4.7                   
## [61] bit_4.0.5                     blogdown_1.19                
## [63] rnaturalearthhires_1.0.0.9000 R.methodsS3_1.8.2            
## [65] evaluate_0.24.0               viridisLite_0.4.2            
## [67] s2_1.1.7                      urltools_1.7.3               
## [69] rlang_1.1.4                   Rcpp_1.0.13                  
## [71] httpcode_0.3.0                glue_1.7.0                   
## [73] DBI_1.2.3                     xml2_1.3.6                   
## [75] jsonlite_1.8.8                R6_2.5.1                     
## [77] plyr_1.8.9                    units_0.8-5
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Regressions</title>
      <link>https://www.erikkusch.com/courses/excursions-into-biostatistics/regressions-correlations-for-the-advanced/</link>
      <pubDate>Sat, 27 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/excursions-into-biostatistics/regressions-correlations-for-the-advanced/</guid>
      <description>&lt;h2 id=&#34;theory&#34;&gt;Theory&lt;/h2&gt;
&lt;p&gt;These are exercises and solutions meant as a compendium to my talk on Model Selection and Model Building.&lt;/p&gt;
&lt;p&gt;I have prepared some &lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/Excursions-into-Biostatistics/Regressions---Correlations-for-the-Advanced.html&#34; target=&#34;_blank&#34;&gt; Lecture Slides &lt;/a&gt; for this session.&lt;/p&gt;
&lt;h2 id=&#34;our-resarch-project&#34;&gt;Our Resarch Project&lt;/h2&gt;
&lt;p&gt;Today, we are looking at a big (and entirely fictional) data base of the common house sparrow (&lt;em&gt;Passer domesticus&lt;/em&gt;). In particular, we are interested in the &lt;strong&gt;Evolution of &lt;em&gt;Passer domesticus&lt;/em&gt; in Response to Climate Change&lt;/strong&gt; which was previously explained &lt;a href=&#34;https://www.erikkusch.com/courses/excursions-into-biostatistics/research-project/&#34; target=&#34;_blank&#34;&gt; here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;the-data&#34;&gt;The Data&lt;/h3&gt;
&lt;p&gt;I have created a large data set for this exercise which is available &lt;a href=&#34;https://github.com/ErikKusch/Homepage/raw/master/content/courses/excursions-into-biostatistics/Data.rar&#34; target=&#34;_blank&#34;&gt; here&lt;/a&gt; and we previously cleaned up so that is now usable &lt;a href=&#34;https://www.erikkusch.com/courses/excursions-into-biostatistics/data-handling-and-data-assumptions/&#34; target=&#34;_blank&#34;&gt; here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;reading-the-data-into-r&#34;&gt;Reading the Data into &lt;code&gt;R&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s start by reading the data into &lt;code&gt;R&lt;/code&gt; and taking an initial look at it:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Sparrows_df &amp;lt;- readRDS(file.path(&amp;quot;Data&amp;quot;, &amp;quot;SparrowDataClimate.rds&amp;quot;))
head(Sparrows_df)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Index Latitude Longitude     Climate Population.Status Weight Height Wing.Chord Colour    Sex Nesting.Site Nesting.Height Number.of.Eggs Egg.Weight Flock Home.Range Predator.Presence Predator.Type
## 1    SI       60       100 Continental            Native  34.05  12.87       6.67  Brown   Male         &amp;lt;NA&amp;gt;             NA             NA         NA     B      Large               Yes         Avian
## 2    SI       60       100 Continental            Native  34.86  13.68       6.79   Grey   Male         &amp;lt;NA&amp;gt;             NA             NA         NA     B      Large               Yes         Avian
## 3    SI       60       100 Continental            Native  32.34  12.66       6.64  Black Female        Shrub          35.60              1       3.21     C      Large               Yes         Avian
## 4    SI       60       100 Continental            Native  34.78  15.09       7.00  Brown Female        Shrub          47.75              0         NA     E      Large               Yes         Avian
## 5    SI       60       100 Continental            Native  35.01  13.82       6.81   Grey   Male         &amp;lt;NA&amp;gt;             NA             NA         NA     B      Large               Yes         Avian
## 6    SI       60       100 Continental            Native  32.36  12.67       6.64  Brown Female        Shrub          32.47              1       3.17     E      Large               Yes         Avian
##       TAvg      TSD
## 1 269.9596 15.71819
## 2 269.9596 15.71819
## 3 269.9596 15.71819
## 4 269.9596 15.71819
## 5 269.9596 15.71819
## 6 269.9596 15.71819
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;hypotheses&#34;&gt;Hypotheses&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s remember our hypotheses:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Sparrow Morphology&lt;/strong&gt; is determined by:&lt;br&gt;
A. &lt;em&gt;Climate Conditions&lt;/em&gt; with sparrows in stable, warm environments fairing better than those in colder, less stable ones.&lt;br&gt;
B. &lt;em&gt;Competition&lt;/em&gt; with sparrows in small flocks doing better than those in big flocks.&lt;br&gt;
C. &lt;em&gt;Predation&lt;/em&gt; with sparrows under pressure of predation doing worse than those without.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sites&lt;/strong&gt;  accurately represent &lt;strong&gt;sparrow morphology&lt;/strong&gt;. This may mean:&lt;br&gt;
A. &lt;em&gt;Population status&lt;/em&gt; as inferred through morphology.&lt;br&gt;
B. &lt;em&gt;Site index&lt;/em&gt; as inferred through morphology.&lt;br&gt;
C. &lt;em&gt;Climate&lt;/em&gt; as inferred through morphology.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Quite obviously, &lt;strong&gt;hypothesis 1&lt;/strong&gt; is the only one lending itself well to regression exercises. Since we have three variables that describe sparrow morphology (&lt;code&gt;Weight&lt;/code&gt;, &lt;code&gt;Height&lt;/code&gt;, &lt;code&gt;Wing.Chord&lt;/code&gt;) and multi-response-variable models are definitely above the pay-grade of this material, we need to select one of our morphology variables as our response variable here. Remembering the &lt;a href=&#34;https://www.erikkusch.com/courses/excursions-into-biostatistics/classifications-order-from-chaos/&#34; target=&#34;_blank&#34;&gt; Classification exercise&lt;/a&gt;, we recall that &lt;code&gt;Weight&lt;/code&gt; was the most informative morphological trait so far. Hence, we stick with this one for these exercises.&lt;/p&gt;
&lt;h2 id=&#34;r-environment&#34;&gt;&lt;code&gt;R&lt;/code&gt; Environment&lt;/h2&gt;
&lt;p&gt;For this exercise, we will need the following packages:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.load.package &amp;lt;- function(x) {
  if (!require(x, character.only = TRUE)) {
    install.packages(x, repos = &amp;quot;http://cran.us.r-project.org&amp;quot;)
  }
  require(x, character.only = TRUE)
}
package_vec &amp;lt;- c(
  &amp;quot;ggplot2&amp;quot;, # for visualisation
  &amp;quot;nlme&amp;quot;, # for mixed effect models
  &amp;quot;HLMdiag&amp;quot; # for leverage of mixed effect models
)
sapply(package_vec, install.load.package)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ggplot2    nlme HLMdiag 
##    TRUE    TRUE    TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the above function is way more sophisticated than the usual &lt;code&gt;install.packages()&lt;/code&gt; &amp;amp; &lt;code&gt;library()&lt;/code&gt; approach since it automatically detects which packages require installing and only install these thus not overwriting already installed packages.&lt;/p&gt;
&lt;h2 id=&#34;linear-regression&#34;&gt;Linear Regression&lt;/h2&gt;
&lt;p&gt;Remember the &lt;strong&gt;Assumptions of Linear Regression&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Variable values follow homoscedasticity (equal variance across entire data range)&lt;/li&gt;
&lt;li&gt;Residuals follow normal distribution (normality)&lt;/li&gt;
&lt;li&gt;Absence of influential outliers&lt;/li&gt;
&lt;li&gt;Response and Predictor are related in a linear fashion&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;climate-conditions&#34;&gt;Climate Conditions&lt;/h3&gt;
&lt;h4 id=&#34;weight-as-a-result-of-average-temperature-tavg&#34;&gt;&lt;code&gt;Weight&lt;/code&gt; as a result of average temperature (&lt;code&gt;TAvg&lt;/code&gt;)&lt;/h4&gt;
&lt;p&gt;Before we begin, let&amp;rsquo;s plot the data we want to model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(data = Sparrows_df, aes(y = Weight, x = TAvg)) +
  stat_smooth(method = &amp;quot;lm&amp;quot;) +
  geom_point() +
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;6_Regressions---Correlations-for-the-Advanced_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;1440&#34; /&gt;
I have an inkling that we might run into some issues here, but let&amp;rsquo;s continue for now:&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s build the actual model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;H1_ClimateTavg &amp;lt;- lm(Weight ~ TAvg, data = Sparrows_df)
par(mfrow = c(2, 2))
plot(H1_ClimateTavg)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;6_Regressions---Correlations-for-the-Advanced_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;1440&#34; /&gt;
While the meeting of most of the assumptions here might be debatable, we certainly cannot accept a linear model with residuals this non-normal distributed. Sow hat do we? We try to remove as many confounding effects as possible!&lt;/p&gt;
&lt;p&gt;Remember the plot-locations, climates, and population statues in the data set (go back &lt;a href=&#34;https://www.erikkusch.com/courses/excursions-into-biostatistics/research-project/&#34; target=&#34;_blank&#34;&gt; here&lt;/a&gt; if necessary). How about we look exclusively at stations in the Americas which are all housing introduced sparrow populations and almost exclusively lie in coastal habitats? I&amp;rsquo;ll remove all non-coastal climate sites and only consider the central and North America here. Let&amp;rsquo;s do that:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# everything west of -7Â° is on the Americas
# every that&#39;s coastal climate type is retained
# everything north of 11Â° is central and north America
CentralNorthAm_df &amp;lt;- Sparrows_df[Sparrows_df$Longitude &amp;lt; -7 &amp;amp; Sparrows_df$Climate == &amp;quot;Coastal&amp;quot; &amp;amp; Sparrows_df$Latitude &amp;gt; 11, ]
ggplot(data = CentralNorthAm_df, aes(y = Weight, x = TAvg)) +
  stat_smooth(method = &amp;quot;lm&amp;quot;) +
  geom_point() +
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;6_Regressions---Correlations-for-the-Advanced_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;H1_ClimateTavg &amp;lt;- lm(Weight ~ TAvg, data = CentralNorthAm_df)
par(mfrow = c(2, 2))
plot(H1_ClimateTavg)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;6_Regressions---Correlations-for-the-Advanced_files/figure-html/unnamed-chunk-5-2.png&#34; width=&#34;1440&#34; /&gt;
This looks sensible to me! The scatterplot shows that sparrows are lighter in warmer areas which makes sense to me.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(H1_ClimateTavg)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Weight ~ TAvg, data = CentralNorthAm_df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.1440 -1.0861  0.0219  0.9823  3.9743 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) 45.167990   1.618752  27.903  &amp;lt; 2e-16 ***
## TAvg        -0.049501   0.005635  -8.785 2.63e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.404 on 248 degrees of freedom
## Multiple R-squared:  0.2373,	Adjusted R-squared:  0.2343 
## F-statistic: 77.18 on 1 and 248 DF,  p-value: 2.633e-16
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our model estimates show the same pattern.&lt;/p&gt;
&lt;h4 id=&#34;weight-as-a-result-of-temperature-variability-tsd&#34;&gt;&lt;code&gt;Weight&lt;/code&gt; as a result of temperature variability (&lt;code&gt;TSD&lt;/code&gt;)&lt;/h4&gt;
&lt;p&gt;I&amp;rsquo;ll continue with my North American, coastal subset here:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# everything west of -7Â° is on the Americas
# every that&#39;s coastal climate type is retained
# everything north of 11Â° is central and north America
CentralNorthAm_df &amp;lt;- Sparrows_df[Sparrows_df$Longitude &amp;lt; -7 &amp;amp; Sparrows_df$Climate == &amp;quot;Coastal&amp;quot; &amp;amp; Sparrows_df$Latitude &amp;gt; 11, ]
ggplot(data = CentralNorthAm_df, aes(y = Weight, x = TSD)) +
  stat_smooth(method = &amp;quot;lm&amp;quot;) +
  geom_point() +
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;6_Regressions---Correlations-for-the-Advanced_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;H1_ClimateTSD &amp;lt;- lm(Weight ~ TSD, data = CentralNorthAm_df)
par(mfrow = c(2, 2))
plot(H1_ClimateTSD)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;6_Regressions---Correlations-for-the-Advanced_files/figure-html/unnamed-chunk-7-2.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(H1_ClimateTSD)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Weight ~ TSD, data = CentralNorthAm_df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.7978 -1.0053  0.0002  1.0042  3.5648 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) 29.61266    0.14922  198.45   &amp;lt;2e-16 ***
## TSD          0.22680    0.02069   10.96   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.319 on 248 degrees of freedom
## Multiple R-squared:  0.3263,	Adjusted R-squared:  0.3236 
## F-statistic: 120.1 on 1 and 248 DF,  p-value: &amp;lt; 2.2e-16
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again all assumptions are met and the model itself is very intuitive: The more variable the climate, the heavier the sparrows.&lt;/p&gt;
&lt;h4 id=&#34;weight-as-a-result-of-both-temperature-mean-tavg-and-temperature-variability-tsd&#34;&gt;&lt;code&gt;Weight&lt;/code&gt; as a result of both temperature mean (&lt;code&gt;TAvg&lt;/code&gt;) and temperature variability (&lt;code&gt;TSD&lt;/code&gt;)&lt;/h4&gt;
&lt;p&gt;Naturally, we continue with the same subset of the data as before:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# everything west of -7Â° is on the Americas
# every that&#39;s coastal climate type is retained
# everything north of 11Â° is central and north America
CentralNorthAm_df &amp;lt;- Sparrows_df[Sparrows_df$Longitude &amp;lt; -7 &amp;amp; Sparrows_df$Climate == &amp;quot;Coastal&amp;quot; &amp;amp; Sparrows_df$Latitude &amp;gt; 11, ]
H1_ClimateCont &amp;lt;- lm(Weight ~ TAvg + TSD, data = CentralNorthAm_df)
par(mfrow = c(2, 2))
plot(H1_ClimateCont)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;6_Regressions---Correlations-for-the-Advanced_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(H1_ClimateCont)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Weight ~ TAvg + TSD, data = CentralNorthAm_df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.6593 -1.0263  0.0272  0.9207  3.2359 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) 16.59866    4.68943   3.540 0.000479 ***
## TAvg         0.04215    0.01518   2.777 0.005915 ** 
## TSD          0.38142    0.05931   6.431 6.52e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.302 on 247 degrees of freedom
## Multiple R-squared:  0.3467,	Adjusted R-squared:  0.3414 
## F-statistic: 65.54 on 2 and 247 DF,  p-value: &amp;lt; 2.2e-16
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Interestingly, average temperature has a different (and weaker) effect now than when investigated in isolation. Variability of temperature has become even more important (i.e. stronger effect). So which model should we use? That&amp;rsquo;s exactly what we&amp;rsquo;ll investigate in our session on &lt;a href=&#34;https://www.erikkusch.com/courses/excursions-into-biostatistics/excursion-into-biostatistics/&#34; target=&#34;_blank&#34;&gt; Model Selection and Validation&lt;/a&gt;!&lt;/p&gt;
&lt;h3 id=&#34;competition&#34;&gt;Competition&lt;/h3&gt;
&lt;p&gt;Next, we build models which aim to explain sparrow &lt;code&gt;Weight&lt;/code&gt; through variables pertaining to competition. To do so, I first calculate the size for each flock at each site and append these to each bird:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;FlockSizes &amp;lt;- with(Sparrows_df, table(Flock, Index))
Sparrows_df$Flock.Size &amp;lt;- NA
for (Flock_Iter in rownames(FlockSizes)) { # loop over flocks
  for (Site_Iter in colnames(FlockSizes)) { # loop over sites
    Positions &amp;lt;- Sparrows_df$Index == Site_Iter &amp;amp; Sparrows_df$Flock == Flock_Iter
    Sparrows_df$Flock.Size[Positions] &amp;lt;- FlockSizes[Flock_Iter, Site_Iter]
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With that done, we can now build our models up again like we did with the climate data before.&lt;/p&gt;
&lt;h4 id=&#34;weight-as-a-result-of-home-range-size-homerange&#34;&gt;&lt;code&gt;Weight&lt;/code&gt; as a result of Home Range size (&lt;code&gt;Home.Range&lt;/code&gt;)&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(data = Sparrows_df, aes(x = Home.Range, y = Weight)) +
  geom_boxplot() +
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;6_Regressions---Correlations-for-the-Advanced_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;H1_CompetitionHR &amp;lt;- lm(Weight ~ Home.Range, data = Sparrows_df)
par(mfrow = c(2, 2))
plot(H1_CompetitionHR)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;6_Regressions---Correlations-for-the-Advanced_files/figure-html/unnamed-chunk-10-2.png&#34; width=&#34;1440&#34; /&gt;
Nope. Those residuals have me nope out. We won&amp;rsquo;t use this model.&lt;/p&gt;
&lt;h4 id=&#34;weight-as-a-result-of-flock-size-size-flocksize&#34;&gt;&lt;code&gt;Weight&lt;/code&gt; as a result of Flock Size size (&lt;code&gt;Flock.Size&lt;/code&gt;)&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(data = Sparrows_df, aes(x = Flock.Size, y = Weight)) +
  stat_smooth(method = &amp;quot;lm&amp;quot;) +
  geom_point() +
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;6_Regressions---Correlations-for-the-Advanced_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;H1_CompetitionFS &amp;lt;- lm(Weight ~ Flock.Size, data = Sparrows_df)
par(mfrow = c(2, 2))
plot(H1_CompetitionFS)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;6_Regressions---Correlations-for-the-Advanced_files/figure-html/unnamed-chunk-11-2.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(H1_CompetitionFS)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Weight ~ Flock.Size, data = Sparrows_df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.7421 -1.2163  0.0961  1.2823  4.7176 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) 35.456378   0.114500   309.7   &amp;lt;2e-16 ***
## Flock.Size  -0.237908   0.003831   -62.1   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.893 on 1064 degrees of freedom
## Multiple R-squared:  0.7838,	Adjusted R-squared:  0.7836 
## F-statistic:  3857 on 1 and 1064 DF,  p-value: &amp;lt; 2.2e-16
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that&amp;rsquo;s a neat model! Not only do the diagnostics plot look spot-on (not showing these here), the relationship is strong and clear to see - the bigger the flock, the lighter the sparrow.&lt;/p&gt;
&lt;h4 id=&#34;weight-as-a-result-of-home-range-size-homerange-and-flock-size-size-flocksize&#34;&gt;&lt;code&gt;Weight&lt;/code&gt; as a result of Home Range size (&lt;code&gt;Home.Range&lt;/code&gt;) and Flock Size size (&lt;code&gt;Flock.Size&lt;/code&gt;)&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(data = Sparrows_df, aes(x = Flock.Size, y = Weight, col = Home.Range)) +
  geom_point() +
  stat_smooth(method = &amp;quot;lm&amp;quot;) +
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;6_Regressions---Correlations-for-the-Advanced_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;H1_CompetitionFULL &amp;lt;- lm(Weight ~ Home.Range * Flock.Size, data = Sparrows_df)
par(mfrow = c(2, 2))
plot(H1_CompetitionFULL)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;6_Regressions---Correlations-for-the-Advanced_files/figure-html/unnamed-chunk-12-2.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(H1_CompetitionFULL)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Weight ~ Home.Range * Flock.Size, data = Sparrows_df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.5599 -1.2261  0.0427  1.2806  4.5553 
## 
## Coefficients:
##                             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)                 33.29279    0.31986 104.086  &amp;lt; 2e-16 ***
## Home.RangeMedium             0.29700    0.89288   0.333    0.739    
## Home.RangeSmall              1.88176    0.35265   5.336 1.16e-07 ***
## Flock.Size                  -0.07337    0.01848  -3.970 7.66e-05 ***
## Home.RangeMedium:Flock.Size -0.03363    0.05787  -0.581    0.561    
## Home.RangeSmall:Flock.Size  -0.16211    0.01896  -8.548  &amp;lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.8 on 1060 degrees of freedom
## Multiple R-squared:  0.8051,	Adjusted R-squared:  0.8042 
## F-statistic:   876 on 5 and 1060 DF,  p-value: &amp;lt; 2.2e-16
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that our model is unsure of what to do with the home ranges when it comes to medium-sized ranges. The scatterplot shows that is is probably due to us not having a lot of samples for medium-sized home-ranges. Aside from that, our model meets all assumptions and produces quite intuitive parameter estimates.&lt;/p&gt;
&lt;h3 id=&#34;predation&#34;&gt;Predation&lt;/h3&gt;
&lt;p&gt;Next, we look at sparrow &lt;code&gt;Weight&lt;/code&gt; through the lens of predation. To do so, we need to recode all &lt;code&gt;NA&lt;/code&gt;s in the &lt;code&gt;Predator.Type&lt;/code&gt; variable into something else for our models ro tun properly. I chose &lt;code&gt;&amp;quot;None&amp;quot;&lt;/code&gt; here to indicate that there is no predation-pressure at these sites:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;levels(Sparrows_df$Predator.Type) &amp;lt;- c(levels(Sparrows_df$Predator.Type), &amp;quot;None&amp;quot;)
Sparrows_df$Predator.Type[is.na(Sparrows_df$Predator.Type)] &amp;lt;- &amp;quot;None&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With that taken care of, we again build our models one-by-one.&lt;/p&gt;
&lt;p&gt;Again, because of issues with normality of residuals, we default to our three sites across Central and North America, which are of coastal climate:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# everything west of -7Â° is on the Americas
# every that&#39;s coastal climate type is retained
# everything north of 11Â° is central and north America
CentralNorthAm_df &amp;lt;- Sparrows_df[Sparrows_df$Longitude &amp;lt; -7 &amp;amp; Sparrows_df$Climate == &amp;quot;Coastal&amp;quot; &amp;amp; Sparrows_df$Latitude &amp;gt; 11, ]
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;weight-as-a-result-of-predatorpresence&#34;&gt;&lt;code&gt;Weight&lt;/code&gt; as a result of &lt;code&gt;Predator.Presence&lt;/code&gt;&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(data = CentralNorthAm_df, aes(x = Predator.Presence, y = Weight)) +
  geom_boxplot() +
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;6_Regressions---Correlations-for-the-Advanced_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;H1_PredationPresence &amp;lt;- lm(Weight ~ Predator.Presence, data = CentralNorthAm_df)
par(mfrow = c(2, 2))
plot(H1_PredationPresence)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;6_Regressions---Correlations-for-the-Advanced_files/figure-html/unnamed-chunk-15-2.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(H1_PredationPresence)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Weight ~ Predator.Presence, data = CentralNorthAm_df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.5051 -1.0701 -0.0396  1.0749  4.2549 
## 
## Coefficients:
##                      Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)           31.4141     0.1752 179.261  &amp;lt; 2e-16 ***
## Predator.PresenceYes  -0.6589     0.2131  -3.092  0.00222 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.577 on 248 degrees of freedom
## Multiple R-squared:  0.03711,	Adjusted R-squared:  0.03323 
## F-statistic: 9.557 on 1 and 248 DF,  p-value: 0.002219
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;According to our model, sparrows under pressure of predation are lighter than those which aren&amp;rsquo;t. Does that make sense? Intuitively, yes, but I would argue that there are too many confounding variable here to be sure.&lt;/p&gt;
&lt;h4 id=&#34;weight-as-a-result-of-predatortype&#34;&gt;&lt;code&gt;Weight&lt;/code&gt; as a result of &lt;code&gt;Predator.Type&lt;/code&gt;&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(data = CentralNorthAm_df, aes(x = Predator.Type, y = Weight)) +
  geom_boxplot() +
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;6_Regressions---Correlations-for-the-Advanced_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;H1_PredationType &amp;lt;- lm(Weight ~ Predator.Type, data = CentralNorthAm_df)
par(mfrow = c(2, 2))
plot(H1_PredationType)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;6_Regressions---Correlations-for-the-Advanced_files/figure-html/unnamed-chunk-16-2.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(H1_PredationType)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Weight ~ Predator.Type, data = CentralNorthAm_df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.6593 -1.0263  0.0272  0.9207  3.2359 
## 
## Coefficients:
##                        Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)             29.9093     0.1270 235.439  &amp;lt; 2e-16 ***
## Predator.TypeNon-Avian   2.2335     0.2064  10.819  &amp;lt; 2e-16 ***
## Predator.TypeNone        1.5047     0.1925   7.817 1.56e-13 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.302 on 247 degrees of freedom
## Multiple R-squared:  0.3467,	Adjusted R-squared:  0.3414 
## F-statistic: 65.54 on 2 and 247 DF,  p-value: &amp;lt; 2.2e-16
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;OK. This clearly shows that there are either some confounds present or that our data shows something very counter-intuitive. Sparrows under nor predation are lighter than sparrows under non-avian predation? That makes no sense to me.&lt;/p&gt;
&lt;h4 id=&#34;weight-as-a-result-of-predatorpresence-and-predatortype&#34;&gt;&lt;code&gt;Weight&lt;/code&gt; as a result of &lt;code&gt;Predator.Presence&lt;/code&gt; and &lt;code&gt;Predator.Type&lt;/code&gt;&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(data = CentralNorthAm_df, aes(x = Predator.Presence, y = Weight, fill = Predator.Type)) +
  geom_boxplot() +
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;6_Regressions---Correlations-for-the-Advanced_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;H1_PredationFULL &amp;lt;- lm(Weight ~ Predator.Presence + Predator.Type, data = Sparrows_df)
par(mfrow = c(2, 2))
plot(H1_PredationFULL)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;6_Regressions---Correlations-for-the-Advanced_files/figure-html/unnamed-chunk-17-2.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(H1_PredationFULL)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Weight ~ Predator.Presence + Predator.Type, data = Sparrows_df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -7.6635 -2.2987 -0.0844  1.9563  9.6165 
## 
## Coefficients: (1 not defined because of singularities)
##                        Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)             30.6145     0.1816  168.60   &amp;lt;2e-16 ***
## Predator.PresenceYes    -3.5709     0.2387  -14.96   &amp;lt;2e-16 ***
## Predator.TypeNon-Avian   5.2809     0.2789   18.94   &amp;lt;2e-16 ***
## Predator.TypeNone            NA         NA      NA       NA    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.431 on 1063 degrees of freedom
## Multiple R-squared:  0.2901,	Adjusted R-squared:  0.2888 
## F-statistic: 217.2 on 2 and 1063 DF,  p-value: &amp;lt; 2.2e-16
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Those residuals don&amp;rsquo;t look good, but that&amp;rsquo;s not what I am after with this model. Immediately, we should notice that our model returns &lt;code&gt;NA&lt;/code&gt; for the parameter estimate of &lt;code&gt;Predator.TypeNone&lt;/code&gt;. Why does that happen? Because &lt;code&gt;Predator.TypeNone&lt;/code&gt; coincides with &lt;code&gt;Predator.PresenceNo&lt;/code&gt; (the &lt;code&gt;Intercept&lt;/code&gt;) of this model and so does not provide any additional information. In fact, &lt;code&gt;Predator.Type&lt;/code&gt; offers all the information of &lt;code&gt;Predator.Presence&lt;/code&gt; and then some! Consequently, including both in a model does not make sense and we can immediately disqualify this model.&lt;/p&gt;
&lt;h3 id=&#34;null--full-model&#34;&gt;Null &amp;amp; Full Model&lt;/h3&gt;
&lt;p&gt;For some comparison further down the line, we need a null model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;H1_Null_Sparrows &amp;lt;- lm(Weight ~ 1, data = Sparrows_df)
H1_Null_CNA &amp;lt;- lm(Weight ~ 1, data = CentralNorthAm_df)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our full model contains all of our aforementioned variables/parameters to the best of our knowledge/intuition at this point. We will get to making this model better later. Don&amp;rsquo;t worry:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;H1_FULL_Sparrows &amp;lt;- lm(Weight ~ Climate + TAvg + TSD + Home.Range * Flock.Size + Predator.Type, data = Sparrows_df)
par(mfrow = c(2, 2))
plot(H1_FULL_Sparrows)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;6_Regressions---Correlations-for-the-Advanced_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(H1_FULL_Sparrows)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Weight ~ Climate + TAvg + TSD + Home.Range * Flock.Size + 
##     Predator.Type, data = Sparrows_df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.2164 -1.0436  0.0939  1.0495  4.7403 
## 
## Coefficients:
##                             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)                 43.65271    4.09482  10.660  &amp;lt; 2e-16 ***
## ClimateContinental           2.28851    0.31524   7.259 7.53e-13 ***
## ClimateSemi-Coastal         -0.79097    0.30663  -2.580  0.01003 *  
## TAvg                        -0.04252    0.01402  -3.034  0.00247 ** 
## TSD                          0.01431    0.04032   0.355  0.72272    
## Home.RangeMedium             1.63110    0.85370   1.911  0.05632 .  
## Home.RangeSmall              2.36146    0.41639   5.671 1.83e-08 ***
## Flock.Size                  -0.03445    0.01896  -1.817  0.06955 .  
## Predator.TypeNon-Avian       0.24253    0.16673   1.455  0.14605    
## Predator.TypeNone            0.75123    0.17268   4.350 1.49e-05 ***
## Home.RangeMedium:Flock.Size -0.10115    0.05614  -1.802  0.07186 .  
## Home.RangeSmall:Flock.Size  -0.16452    0.01990  -8.267 4.09e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.605 on 1054 degrees of freedom
## Multiple R-squared:  0.846,	Adjusted R-squared:  0.8444 
## F-statistic: 526.6 on 11 and 1054 DF,  p-value: &amp;lt; 2.2e-16
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Already at this point, it is interesting to point out how some previously non-significant effects drop out while other become significant due to the inclusion of all parameters in one model.&lt;/p&gt;
&lt;p&gt;Now we also need a full model for our Central-/North-America data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;H1_FULL_CNA &amp;lt;- lm(Weight ~ TAvg + TSD + Home.Range * Flock.Size + Predator.Type, data = CentralNorthAm_df)
par(mfrow = c(2, 2))
plot(H1_FULL_CNA)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;6_Regressions---Correlations-for-the-Advanced_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(H1_FULL_CNA)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Weight ~ TAvg + TSD + Home.Range * Flock.Size + 
##     Predator.Type, data = CentralNorthAm_df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.6979 -0.9364 -0.0501  1.0483  3.3590 
## 
## Coefficients: (2 not defined because of singularities)
##                             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)                 14.18473    6.20215   2.287   0.0231 *  
## TAvg                         0.05336    0.02038   2.618   0.0094 ** 
## TSD                          0.39853    0.07842   5.082 7.48e-07 ***
## Home.RangeMedium            -3.83588    1.99164  -1.926   0.0553 .  
## Home.RangeSmall             -1.50681    1.03368  -1.458   0.1462    
## Flock.Size                  -0.07581    0.05850  -1.296   0.1963    
## Predator.TypeNon-Avian            NA         NA      NA       NA    
## Predator.TypeNone                 NA         NA      NA       NA    
## Home.RangeMedium:Flock.Size  0.29821    0.13273   2.247   0.0256 *  
## Home.RangeSmall:Flock.Size   0.10097    0.06212   1.625   0.1054    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.285 on 242 degrees of freedom
## Multiple R-squared:  0.3765,	Adjusted R-squared:  0.3585 
## F-statistic: 20.88 on 7 and 242 DF,  p-value: &amp;lt; 2.2e-16
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;summary-of-linear-regression&#34;&gt;Summary of Linear Regression&lt;/h3&gt;
&lt;p&gt;To streamline the next few steps of our exercise on &lt;a href=&#34;https://www.erikkusch.com/courses/excursions-into-biostatistics/excursion-into-biostatistics/&#34; target=&#34;_blank&#34;&gt; Model Selection and Validation&lt;/a&gt;, we combine all of our models into a &lt;code&gt;list&lt;/code&gt; object for now:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;H1_ModelSparrows_ls &amp;lt;- list(
  H1_Null_Sparrows,
  H1_CompetitionFS,
  H1_CompetitionFULL,
  H1_FULL_Sparrows
)
names(H1_ModelSparrows_ls) &amp;lt;- c(
  &amp;quot;Null&amp;quot;,
  &amp;quot;Comp_Flock.Size&amp;quot;, &amp;quot;Comp_Full&amp;quot;,
  &amp;quot;Full&amp;quot;
)
H1_ModelCNA_ls &amp;lt;- list(
  H1_Null_CNA,
  H1_ClimateTavg,
  H1_ClimateTSD,
  H1_ClimateCont,
  H1_PredationPresence,
  H1_PredationType,
  H1_FULL_CNA
)
names(H1_ModelCNA_ls) &amp;lt;- c(
  &amp;quot;Null&amp;quot;,
  &amp;quot;Clim_TAvg&amp;quot;, &amp;quot;Clim_TSD&amp;quot;, &amp;quot;Clim_Full&amp;quot;,
  &amp;quot;Pred_Pres&amp;quot;, &amp;quot;Pred_Type&amp;quot;,
  &amp;quot;Full&amp;quot;
)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;mixed-effect-models&#34;&gt;Mixed Effect Models&lt;/h2&gt;
&lt;p&gt;Remember the &lt;strong&gt;Assumptions of Mixed Effect Models&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Variable values follow homoscedasticity (equal variance across entire data range)&lt;/li&gt;
&lt;li&gt;Residuals follow normal distribution (normality)&lt;/li&gt;
&lt;li&gt;Absence of influential outliers&lt;/li&gt;
&lt;li&gt;Response and Predictor are related in a linear fashion&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;climate-conditions-1&#34;&gt;Climate Conditions&lt;/h3&gt;
&lt;p&gt;Here, I create a mixed effect model that is accounting predicting &lt;code&gt;Weight&lt;/code&gt; with a combination &lt;code&gt;TAvg&lt;/code&gt; and &lt;code&gt;TSD&lt;/code&gt; while accounting for random intercepts of &lt;code&gt;Index&lt;/code&gt; and &lt;code&gt;Population.Status&lt;/code&gt;. As we learned in our exercise on &lt;a href=&#34;https://www.erikkusch.com/courses/excursions-into-biostatistics/classifications-order-from-chaos/&#34; target=&#34;_blank&#34;&gt; Classifications&lt;/a&gt;, these two are probably important in controlling for some morphology effects in &lt;code&gt;Weight&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;I create such a model for:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The &lt;strong&gt;entire sparrow data data set&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Only the &lt;strong&gt;Central-/North-America sparrows&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;to make models comparable to our previous basic, linear models.&lt;/p&gt;
&lt;h4 id=&#34;global-model&#34;&gt;Global Model&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;H1_Climate_ME_Sparrows &amp;lt;- lme(Weight ~ TAvg + TSD,
  random = list(
    Population.Status = ~1,
    Index = ~1
  ),
  data = Sparrows_df
)
par(mfrow = c(2, 2))
plot(fitted(H1_Climate_ME_Sparrows), resid(H1_Climate_ME_Sparrows)) # Homogeneity of Variances, values around 0, no pattern --&amp;gt; good!
plot(fitted(H1_Climate_ME_Sparrows), Sparrows_df$Weight) # Linearity, there is a clear pattern here --&amp;gt; bad!
qqnorm(resid(H1_Climate_ME_Sparrows)) # Normality, residuals are normal distributed -&amp;gt; good!
hist(leverage(H1_Climate_ME_Sparrows)[, 1]) # Leverage, not really any outliers --&amp;gt; good!
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;6_Regressions---Correlations-for-the-Advanced_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;1440&#34; /&gt;
Quite evidently, the assumption of linearity is violated. Notice that we would not like to use this model for predictions, but will carry it forward for the purpose of subsequent &lt;a href=&#34;https://www.erikkusch.com/courses/excursions-into-biostatistics/excursion-into-biostatistics/&#34; target=&#34;_blank&#34;&gt; Model Selection and Validation&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(H1_Climate_ME_Sparrows)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed-effects model fit by REML
##   Data: Sparrows_df 
##        AIC      BIC    logLik
##   3557.109 3586.922 -1772.554
## 
## Random effects:
##  Formula: ~1 | Population.Status
##         (Intercept)
## StdDev: 0.001507855
## 
##  Formula: ~1 | Index %in% Population.Status
##         (Intercept) Residual
## StdDev:    2.640311 1.237155
## 
## Fixed effects:  Weight ~ TAvg + TSD 
##                Value Std.Error   DF    t-value p-value
## (Intercept) 37.83099 31.229059 1055  1.2114034  0.2260
## TAvg        -0.03070  0.104602    7 -0.2934692  0.7777
## TSD          0.27536  0.265143    7  1.0385452  0.3336
##  Correlation: 
##      (Intr) TAvg  
## TAvg -0.999       
## TSD  -0.826  0.809
## 
## Standardized Within-Group Residuals:
##          Min           Q1          Med           Q3          Max 
## -2.956660984 -0.746001839  0.004452834  0.722973350  2.617329146 
## 
## Number of Observations: 1066
## Number of Groups: 
##            Population.Status Index %in% Population.Status 
##                            2                           11
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;central-north-american-model&#34;&gt;Central-/North-American Model&lt;/h4&gt;
&lt;p&gt;Accounting for population status in among the Central-/North-American sites makes no sense as all of them are introduced populations.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;H1_Climate_ME_CNA &amp;lt;- lme(Weight ~ TAvg + TSD,
  random = list(Index = ~1),
  data = CentralNorthAm_df
)
par(mfrow = c(2, 2))
plot(fitted(H1_Climate_ME_CNA), resid(H1_Climate_ME_CNA)) # Homogeneity of Variances, values around 0, no pattern --&amp;gt; good!
plot(fitted(H1_Climate_ME_CNA), CentralNorthAm_df$Weight) # Linearity, there is no clear pattern here --&amp;gt; good!
qqnorm(resid(H1_Climate_ME_CNA)) # Normality, residuals are normal distributed -&amp;gt; good!
hist(leverage(H1_Climate_ME_CNA)[, 1]) # Leverage, not really any outliers --&amp;gt; good!
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;6_Regressions---Correlations-for-the-Advanced_files/figure-html/unnamed-chunk-24-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With this model, all of our assumption are met nicely!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(H1_Climate_ME_CNA)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed-effects model fit by REML
##   Data: CentralNorthAm_df 
##        AIC      BIC    logLik
##   863.9566 881.5035 -426.9783
## 
## Random effects:
##  Formula: ~1 | Index
##         (Intercept) Residual
## StdDev:   0.3802608 1.301734
## 
## Fixed effects:  Weight ~ TAvg + TSD 
##                 Value Std.Error  DF   t-value p-value
## (Intercept) 16.598661 13.291655 247 1.2488031  0.2129
## TAvg         0.042146  0.042863   0 0.9832757     NaN
## TSD          0.381424  0.174011   0 2.1919464     NaN
##  Correlation: 
##      (Intr) TAvg  
## TAvg -0.999       
## TSD  -0.953  0.945
## 
## Standardized Within-Group Residuals:
##        Min         Q1        Med         Q3        Max 
## -2.8111222 -0.7883782  0.0208856  0.7072618  2.4858581 
## 
## Number of Observations: 250
## Number of Groups: 3
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;competition-1&#34;&gt;Competition&lt;/h3&gt;
&lt;p&gt;With competition effects as my goal, I can now include climate classification as one of the random effects! Of course, this is with the exception of the Central-/North-American data as all of these sites are of the type &amp;ldquo;Coastal&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Again, we do so for a global and a local model:&lt;/p&gt;
&lt;h4 id=&#34;global-model-1&#34;&gt;Global Model&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;H1_Comp_ME_Sparrows &amp;lt;- lme(Weight ~ Home.Range * Flock.Size,
  random = list(
    Population.Status = ~1,
    Climate = ~1
  ),
  data = Sparrows_df
)
par(mfrow = c(2, 2))
plot(fitted(H1_Comp_ME_Sparrows), resid(H1_Comp_ME_Sparrows)) # Homogeneity of Variances, values around 0, no pattern --&amp;gt; good!
plot(fitted(H1_Comp_ME_Sparrows), Sparrows_df$Weight) # Linearity, there is a clear pattern here --&amp;gt; bad!
qqnorm(resid(H1_Comp_ME_Sparrows)) # Normality, residuals are normal distributed -&amp;gt; good!
hist(leverage(H1_Comp_ME_Sparrows)[, 1]) # Leverage, not really any outliers --&amp;gt; good!
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;6_Regressions---Correlations-for-the-Advanced_files/figure-html/unnamed-chunk-26-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h4 id=&#34;central-north-american-model-1&#34;&gt;Central-/North-American Model&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;H1_Comp_ME_CNA &amp;lt;- lme(Weight ~ Home.Range * Flock.Size,
  random = list(Climate = ~1),
  data = CentralNorthAm_df
)
par(mfrow = c(2, 2))
plot(fitted(H1_Comp_ME_CNA), resid(H1_Comp_ME_CNA)) # Homogeneity of Variances, values around 0, no pattern --&amp;gt; good!
plot(fitted(H1_Comp_ME_CNA), CentralNorthAm_df$Weight) # Linearity, there is no clear pattern here --&amp;gt; good!
qqnorm(resid(H1_Comp_ME_CNA)) # Normality, residuals are normal distributed -&amp;gt; good!
hist(leverage(H1_Comp_ME_CNA)[, 1]) # Leverage, not really any outliers --&amp;gt; good!
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;6_Regressions---Correlations-for-the-Advanced_files/figure-html/unnamed-chunk-27-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;predation-1&#34;&gt;Predation&lt;/h3&gt;
&lt;p&gt;With predation effects, I do the same as with competition effects and set a random intercept for climate classification at each site for a global and a local model:&lt;/p&gt;
&lt;h4 id=&#34;global-model-2&#34;&gt;Global Model&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;H1_Pred_ME_Sparrows &amp;lt;- lme(Weight ~ Predator.Type,
  random = list(
    Population.Status = ~1,
    Climate = ~1
  ),
  data = Sparrows_df
)
par(mfrow = c(2, 2))
plot(fitted(H1_Pred_ME_Sparrows), resid(H1_Pred_ME_Sparrows)) # Homogeneity of Variances, values around 0, no pattern --&amp;gt; good!
plot(fitted(H1_Pred_ME_Sparrows), Sparrows_df$Weight) # Linearity, there is a clear pattern here --&amp;gt; bad!
qqnorm(resid(H1_Pred_ME_Sparrows)) # Normality, residuals are normal distributed -&amp;gt; good!
hist(leverage(H1_Pred_ME_Sparrows)[, 1]) # Leverage, not really any outliers --&amp;gt; good!
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;6_Regressions---Correlations-for-the-Advanced_files/figure-html/unnamed-chunk-28-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h4 id=&#34;central-north-american-model-2&#34;&gt;Central-/North-American Model&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;H1_Pred_ME_CNA &amp;lt;- lme(Weight ~ Predator.Type,
  random = list(Index = ~1),
  data = CentralNorthAm_df
)
par(mfrow = c(2, 2))
plot(fitted(H1_Pred_ME_CNA), resid(H1_Pred_ME_CNA)) # Homogeneity of Variances, values around 0, no pattern --&amp;gt; good!
plot(fitted(H1_Pred_ME_CNA), CentralNorthAm_df$Weight) # Linearity, there is no clear pattern here --&amp;gt; good!
qqnorm(resid(H1_Pred_ME_CNA)) # Normality, residuals are normal distributed -&amp;gt; good!
hist(leverage(H1_Pred_ME_CNA)[, 1]) # Leverage, not really any outliers --&amp;gt; good!
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;6_Regressions---Correlations-for-the-Advanced_files/figure-html/unnamed-chunk-29-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;full-model&#34;&gt;Full Model&lt;/h3&gt;
&lt;h4 id=&#34;global-model-3&#34;&gt;Global Model&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;H1_Full_ME_Sparrows &amp;lt;- lme(Weight ~ Predator.Type + Flock.Size * Home.Range + TAvg + TSD,
  random = list(Population.Status = ~1),
  data = Sparrows_df
)
par(mfrow = c(2, 2))
plot(fitted(H1_Full_ME_Sparrows), resid(H1_Full_ME_Sparrows)) # Homogeneity of Variances, values around 0, no pattern --&amp;gt; good!
plot(fitted(H1_Full_ME_Sparrows), Sparrows_df$Weight) # Linearity, there is a clear pattern here --&amp;gt; bad!
qqnorm(resid(H1_Full_ME_Sparrows)) # Normality, residuals are normal distributed -&amp;gt; good!
hist(leverage(H1_Full_ME_Sparrows)[, 1]) # Leverage, not really any outliers --&amp;gt; good!
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;6_Regressions---Correlations-for-the-Advanced_files/figure-html/unnamed-chunk-30-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h4 id=&#34;central-north-american-model-3&#34;&gt;Central-/North-American Model&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;H1_Full_ME_CNA &amp;lt;- lme(Weight ~ Flock.Size * Home.Range + TAvg + TSD,
  random = list(Index = ~1),
  data = CentralNorthAm_df
)
par(mfrow = c(2, 2))
plot(fitted(H1_Full_ME_CNA), resid(H1_Full_ME_CNA)) # Homogeneity of Variances, values around 0, no pattern --&amp;gt; good!
plot(fitted(H1_Full_ME_CNA), CentralNorthAm_df$Weight) # Linearity, there is no clear pattern here --&amp;gt; good!
qqnorm(resid(H1_Full_ME_CNA)) # Normality, residuals are normal distributed -&amp;gt; good!
hist(leverage(H1_Full_ME_CNA)[, 1]) # Leverage, not really any outliers --&amp;gt; good!
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;6_Regressions---Correlations-for-the-Advanced_files/figure-html/unnamed-chunk-31-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;summary-of-mixed-effect-models&#34;&gt;Summary of Mixed Effect Models&lt;/h3&gt;
&lt;p&gt;Mixed effect models are hard and I have certainly not given them full credit for what they are worth here. I highly suggest giving 
&lt;a href=&#34;https://ourcodingclub.github.io/tutorials/mixed-models/#three&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this&lt;/a&gt; a read if you see yourself using mixed effect models.&lt;/p&gt;
&lt;p&gt;For now, I want to carry along only the full models for our session on &lt;a href=&#34;https://www.erikkusch.com/courses/excursions-into-biostatistics/excursion-into-biostatistics/&#34; target=&#34;_blank&#34;&gt; Model Selection and Validation&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;H1_ModelSparrows_ls$Mixed_Full &amp;lt;- H1_Full_ME_Sparrows
H1_ModelCNA_ls$Mixed_Full &amp;lt;- H1_Full_ME_CNA
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;generalised-linear-models&#34;&gt;Generalised Linear Models&lt;/h2&gt;
&lt;p&gt;For a generalised linear model, we may want to run a logistic regression, which we &lt;a href=&#34;https://www.erikkusch.com/courses/excursions-into-biostatistics/classifications-order-from-chaos/#binary-logistic-regression-1&#34; target=&#34;_blank&#34;&gt; already did&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For a different example, we now turn to &lt;em&gt;poisson models&lt;/em&gt; by trying to understand how &lt;code&gt;Flock.Size&lt;/code&gt; comes about.&lt;/p&gt;
&lt;p&gt;Firstly, we limit our data set to necessary variables and then cut all duplicate rows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Flock_df &amp;lt;- Sparrows_df[, c(&amp;quot;Flock.Size&amp;quot;, &amp;quot;TAvg&amp;quot;, &amp;quot;TSD&amp;quot;, &amp;quot;Index&amp;quot;, &amp;quot;Climate&amp;quot;, &amp;quot;Predator.Type&amp;quot;)]
Flock_df &amp;lt;- unique(Flock_df)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we are ready to build a basic linear model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Flock_lm &amp;lt;- lm(Flock.Size ~ TAvg * TSD * Predator.Type, data = Flock_df)
par(mfrow = c(2, 2))
plot(Flock_lm)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;6_Regressions---Correlations-for-the-Advanced_files/figure-html/unnamed-chunk-34-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Everything looks line but that Scale-Location plot. That one seems to indicate that variance in our &lt;code&gt;Flock.Size&lt;/code&gt; increases as the &lt;code&gt;Flock.Size&lt;/code&gt; itself increases - a very typical example of a poisson distributed variable.&lt;/p&gt;
&lt;p&gt;GLMs to the rescue with the poisson GLM:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;poisson.model &amp;lt;- glm(Flock.Size ~ TAvg * TSD * Predator.Type, data = Flock_df, family = poisson(link = &amp;quot;log&amp;quot;))
par(mfrow = c(2, 2))
plot(poisson.model)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;6_Regressions---Correlations-for-the-Advanced_files/figure-html/unnamed-chunk-35-1.png&#34; width=&#34;1440&#34; /&gt;
We fixed it! Now just for the parameter estimates:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(poisson.model)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = Flock.Size ~ TAvg * TSD * Predator.Type, family = poisson(link = &amp;quot;log&amp;quot;), 
##     data = Flock_df)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -2.02420  -0.88953  -0.09629   0.94122   2.12737  
## 
## Coefficients: (1 not defined because of singularities)
##                                   Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept)                     -13.822674   2.540369  -5.441 5.29e-08 ***
## TAvg                              0.061744   0.008564   7.209 5.62e-13 ***
## TSD                               7.243621   1.056843   6.854 7.18e-12 ***
## Predator.TypeNon-Avian          -48.160439   9.412905  -5.116 3.11e-07 ***
## Predator.TypeNone                10.338971   8.694652   1.189    0.234    
## TAvg:TSD                         -0.026895   0.003936  -6.833 8.34e-12 ***
## TAvg:Predator.TypeNon-Avian       0.171883   0.033327   5.157 2.50e-07 ***
## TAvg:Predator.TypeNone           -0.039510   0.029180  -1.354    0.176    
## TSD:Predator.TypeNon-Avian        0.067442   0.041609   1.621    0.105    
## TSD:Predator.TypeNone            -6.742437   1.177538  -5.726 1.03e-08 ***
## TAvg:TSD:Predator.TypeNon-Avian         NA         NA      NA       NA    
## TAvg:TSD:Predator.TypeNone        0.025061   0.004321   5.800 6.64e-09 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 276.538  on 49  degrees of freedom
## Residual deviance:  53.417  on 39  degrees of freedom
## AIC: 310.71
## 
## Number of Fisher Scoring iterations: 4
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;final-models&#34;&gt;Final Models&lt;/h2&gt;
&lt;p&gt;In our upcoming &lt;a href=&#34;https://www.erikkusch.com/courses/excursions-into-biostatistics/excursion-into-biostatistics/&#34; target=&#34;_blank&#34;&gt; Model Selection and Validation&lt;/a&gt; Session, we will look into how to compare and validate models. We now need to select some models we have created here today and want to carry forward to said session.&lt;/p&gt;
&lt;p&gt;We have already created &lt;code&gt;list&lt;/code&gt; objects for this purpose. Let&amp;rsquo;s save them alongside the data that was used to create them (in the case of the localised models, at least). Let&amp;rsquo;s save these as a separate object ready to be loaded into our &lt;code&gt;R&lt;/code&gt; environment in the coming session:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;save(H1_ModelSparrows_ls, H1_ModelCNA_ls, CentralNorthAm_df, Sparrows_df, file = file.path(&amp;quot;Data&amp;quot;, &amp;quot;H1_Models.RData&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;sessioninfo&#34;&gt;SessionInfo&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.2.3 (2023-03-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Big Sur ... 10.16
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] HLMdiag_0.5.0 nlme_3.1-162  ggplot2_3.4.1
## 
## loaded via a namespace (and not attached):
##  [1] styler_1.9.1      tidyselect_1.2.0  xfun_0.37         bslib_0.4.2       janitor_2.2.0     reshape2_1.4.4    purrr_1.0.1       splines_4.2.3     lattice_0.20-45   snakecase_0.11.0 
## [11] colorspace_2.1-0  vctrs_0.5.2       generics_0.1.3    htmltools_0.5.4   mgcv_1.8-42       yaml_2.3.7        utf8_1.2.3        rlang_1.0.6       R.oo_1.25.0       jquerylib_0.1.4  
## [21] pillar_1.8.1      glue_1.6.2        withr_2.5.0       R.utils_2.12.2    plyr_1.8.8        R.cache_0.16.0    lifecycle_1.0.3   stringr_1.5.0     munsell_0.5.0     blogdown_1.16    
## [31] gtable_0.3.1      R.methodsS3_1.8.2 diagonals_6.4.0   evaluate_0.20     labeling_0.4.2    knitr_1.42        fastmap_1.1.1     fansi_1.0.4       highr_0.10        Rcpp_1.0.10      
## [41] scales_1.2.1      cachem_1.0.7      jsonlite_1.8.4    farver_2.1.1      digest_0.6.31     stringi_1.7.12    bookdown_0.33     dplyr_1.1.0       grid_4.2.3        cli_3.6.0        
## [51] tools_4.2.3       magrittr_2.0.3    sass_0.4.5        tibble_3.2.0      pkgconfig_2.0.3   MASS_7.3-58.2     Matrix_1.5-3      lubridate_1.9.2   timechange_0.2.0  rmarkdown_2.20   
## [61] rstudioapi_0.14   R6_2.5.1          compiler_4.2.3
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 06</title>
      <link>https://www.erikkusch.com/courses/rethinking/chapter-06/</link>
      <pubDate>Thu, 28 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/rethinking/chapter-06/</guid>
      <description>&lt;h1 id=&#34;the-haunted-dag--the-causal-terror&#34;&gt;The Haunted DAG &amp;amp; The Causal Terror&lt;/h1&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/rethinking/6__29-01-2021_SUMMARY_-Confounds.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides Chapter 6&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to the exercises at the end of chapter 6 in 
&lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Satistical Rethinking 2&lt;/a&gt; by Richard McElreath. I have created these notes as a part of my ongoing involvement in the 
&lt;a href=&#34;https://www.erikkusch.com/project/aubayes/&#34;&gt;AU Bayes Study Group&lt;/a&gt;. Much of my inspiration for these solutions, where necessary, has been obtained from 
&lt;a href=&#34;https://sr2-solutions.wjakethompson.com/more-linear-models.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jake Thompson&lt;/a&gt;. The PDF version of 
&lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Satistical Rethinking 2&lt;/a&gt; is lacking some exercises of the print version, it seems. I do not address these here.&lt;/p&gt;
&lt;h2 id=&#34;medium-exercises&#34;&gt;Medium Exercises&lt;/h2&gt;
&lt;p&gt;We are stepping right into &lt;code&gt;R&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rethinking)
library(dagitty)
library(ggdag)
library(ggplot2)
library(tidybayes)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-m1&#34;&gt;Practice M1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Modify the DAG on page 190 to include the variable $V$, an unobserved cause of $C$ and $Y$: variables $C â V â Y$. Reanalyze the DAG. How many paths connect $X$ to $Y$? Which must be closed? Which should you condition on now?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  Let&amp;rsquo;s start by assigning some coordinates and names of our variables which will end up as nodes in our DAG:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dag_coords &amp;lt;- data.frame(
  name = c(&amp;quot;X&amp;quot;, &amp;quot;U&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;C&amp;quot;, &amp;quot;Y&amp;quot;, &amp;quot;V&amp;quot;),
  x = c(1, 1, 2, 2, 3, 3, 3.5),
  y = c(1, 2, 2.5, 1.5, 2, 1, 1.5)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I add the actual path specifications and make a DAG object:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;DAG_m1 &amp;lt;- dagify(Y ~ X + C + V,
  X ~ U,
  U ~ A,
  B ~ U + C,
  C ~ A + V,
  coords = dag_coords
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, I plot the resulting object:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(DAG_m1, aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_point(shape = 1, stroke = 2, color = &amp;quot;black&amp;quot;) +
  geom_dag_text(color = &amp;quot;black&amp;quot;, size = 10) +
  geom_dag_edges(
    edge_color = &amp;quot;black&amp;quot;, edge_width = 2,
    arrow_directed = grid::arrow(
      length = grid::unit(15, &amp;quot;pt&amp;quot;),
      type = &amp;quot;closed&amp;quot;
    )
  ) +
  theme_void()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-28-statistical-rethinking-chapter-06_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The original DAG on page 190 boasted the following two paths which needed closing:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$X &amp;lt;- U &amp;lt;- A -&amp;gt; C -&amp;gt; Y$&lt;/li&gt;
&lt;li&gt;$X &amp;lt;- U -&amp;gt; B &amp;lt;- C -&amp;gt; Y$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These remain unaltered. Through our novel inclusion of $V$, we now have another two paths that require closing between $X$ and $Y$:&lt;br&gt;
3. $X &amp;lt;- U &amp;lt;- A -&amp;gt; C &amp;lt;- V -&amp;gt; Y$&lt;br&gt;
4. $X &amp;lt;- U -&amp;gt; B &amp;lt;- C &amp;lt;- V -&amp;gt; Y$&lt;/p&gt;
&lt;p&gt;Now, how do we close these paths? First of all, paths 2 and 4 are already closed because $B$ acts as a collider. Path 3 is also closed as $C$ acts as a collider (Thanks to 
&lt;a href=&#34;https://twitter.com/ruxandratesloi1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ruxandra Tesloianu&lt;/a&gt; for pointing this out to me). Path 1 requires closing since $A$ is a fork. If we leave it up to &lt;code&gt;R&lt;/code&gt;, we would condition on the following variables to close all paths between $X$ and $Y$:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;adjustmentSets(DAG_m1, exposure = &amp;quot;X&amp;quot;, outcome = &amp;quot;Y&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## { C, V }
## { A }
## { U }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Alright, let&amp;rsquo;s think about this. Our above DAG does not yet know which variables are unobserved. &lt;code&gt;R&lt;/code&gt; suggests we condition on $C$ and $V$. That&amp;rsquo;s going to be impossible since we don&amp;rsquo;t have data for $V$. Next, we are pointed towards conditioning on $A$ as an alternative. That looks alright. Thirdly, we are prompted to consider conditioning on $U$. Again, we don&amp;rsquo;t have data for that. So, we are only left with one option: &lt;strong&gt;Condition on $A$&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Finally, let&amp;rsquo;s actually give &lt;code&gt;R&lt;/code&gt; all the information we have and rerun the &lt;code&gt;adjustmentSets()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;DAG_m1 &amp;lt;- dagitty(&amp;quot;dag { U [unobserved]
                          V [unobserved]
                          X -&amp;gt; Y
                          X &amp;lt;- U &amp;lt;- A -&amp;gt; C -&amp;gt; Y
                          U -&amp;gt; B &amp;lt;- C
                          C &amp;lt;- V -&amp;gt; Y }&amp;quot;)

adjustmentSets(DAG_m1, exposure = &amp;quot;X&amp;quot;, outcome = &amp;quot;Y&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## { A }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Cool. That&amp;rsquo;s exactly the solution we arrived at earlier as well.&lt;/p&gt;
&lt;h2 id=&#34;hard-exercises&#34;&gt;Hard Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-h1&#34;&gt;Practice H1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Use the Waffle House data, &lt;code&gt;data(WaffleDivorce)&lt;/code&gt;, to find the total causal influence of number of Waffle Houses on divorce rate. Justify your model or models with a causal graph.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  Let&amp;rsquo;s start by recreating the DAG on page 191 with some code from page 192 while sprucing it up with some coordinates for our nodes in the DAG:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Define Paths
DAG_h1 &amp;lt;- dagitty(&amp;quot;dag {
    A -&amp;gt; D
    A -&amp;gt; M -&amp;gt; D
    A &amp;lt;- S -&amp;gt; M
    S -&amp;gt; W -&amp;gt; D
  }&amp;quot;)
# Add Coordinates
coordinates(DAG_h1) &amp;lt;- list(
  x = c(A = 1, S = 1, M = 2, W = 3, D = 3),
  y = c(A = 1, S = 3, M = 2, W = 3, D = 1)
)
# Plotting
ggplot(DAG_h1, aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_text(color = &amp;quot;black&amp;quot;, size = 10) +
  geom_dag_edges(
    edge_color = &amp;quot;black&amp;quot;, edge_width = 2,
    arrow_directed = grid::arrow(
      length = grid::unit(15, &amp;quot;pt&amp;quot;),
      type = &amp;quot;closed&amp;quot;
    )
  ) +
  theme_void()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-28-statistical-rethinking-chapter-06_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s check which variables we need to condition on to allow any subsequent model to identify the causal relationship between $W$ and $D$:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;adjustmentSets(DAG_h1, exposure = &amp;quot;W&amp;quot;, outcome = &amp;quot;D&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## { A, M }
## { S }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We could either condition on $A$ and $M$, or condition only on $S$. The latter seems simpler to me, so I&amp;rsquo;ll run with that! On to build that model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Loading Data
data(WaffleDivorce)
d &amp;lt;- WaffleDivorce
## Scaling Relevant Variables
d$D &amp;lt;- scale(d$Divorce)
d$W &amp;lt;- scale(d$WaffleHouses)
d$S &amp;lt;- scale(d$South)
## Specifying and Running Model
MOD_h1 &amp;lt;- quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bS * S + bW * W,
    a ~ dnorm(0, 0.2),
    c(bS, bW) ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d
)
plot(precis(MOD_h1))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-28-statistical-rethinking-chapter-06_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Our model clearly shows that once we know about whether a state is located in the Southern Contiguous U.S., we don&amp;rsquo;t gain additional information about the local divorce rate by learning about the number of Waffle Houses in the area.&lt;/p&gt;
&lt;h3 id=&#34;practice-h2&#34;&gt;Practice H2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Build a series of models to test the implied conditional independencies of the causal graph you used in the previous problem. If any of the tests fail, how do you think the graph needs to be amended? Does the graph need more or fewer arrows? Feel free to nominate variables that aren&amp;rsquo;t in the data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  Let&amp;rsquo;s start by letting &lt;code&gt;R&lt;/code&gt; identify all the implied conditional independecies:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;impliedConditionalIndependencies(DAG_h1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## A _||_ W | S
## D _||_ S | A, M, W
## M _||_ W | S
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are three models we need to build to assertain the conditional independencies here.&lt;/p&gt;
&lt;p&gt;Firstly, I am reloading the data and standardise all variables of interest:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Loading Data
data(WaffleDivorce)
d &amp;lt;- WaffleDivorce
## Scaling Relevant Variables
d$A &amp;lt;- scale(d$MedianAgeMarriage)
d$D &amp;lt;- scale(d$Divorce)
d$M &amp;lt;- scale(d$Marriage)
d$W &amp;lt;- scale(d$WaffleHouses)
d$S &amp;lt;- scale(d$South)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let&amp;rsquo;s get going with the actual models:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A &lt;em&gt;||&lt;/em&gt; W | S&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;MOD_h2a &amp;lt;- quap(
  alist(
    A ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bS * S + bW * W,
    a ~ dnorm(0, 0.2),
    c(bS, bW) ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d
)
plot(precis(MOD_h2a))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-28-statistical-rethinking-chapter-06_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Conditional independence of $A$ of $W$ given $S$ - confirmed!&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;D &lt;em&gt;||&lt;/em&gt; S | A, M, W&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;MOD_h2b &amp;lt;- quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bA * A + bS * S + bM * M + bW * W,
    a ~ dnorm(0, 0.2),
    c(bA, bS, bM, bW) ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d
)
plot(precis(MOD_h2b))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-28-statistical-rethinking-chapter-06_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Conditional independence of $D$ of $S$ given $A$, $M$, and $W$ - confirmed!&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;M &lt;em&gt;||&lt;/em&gt; W | S&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;MOD_h2c &amp;lt;- quap(
  alist(
    M ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bS * S + bW * W,
    a ~ dnorm(0, 0.2),
    c(bS, bW) ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d
)
plot(precis(MOD_h2c))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-28-statistical-rethinking-chapter-06_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Conditional independence of $M$ of $W$ given $S$ - confirmed!&lt;/p&gt;
&lt;p&gt;I finish this exercise by looking at only the relevant posteriors for each model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Plot_df &amp;lt;- data.frame(
  Posteriors = c(
    extract.samples(MOD_h2a, n = 1e4)$bW,
    extract.samples(MOD_h2b, n = 1e4)$bS,
    extract.samples(MOD_h2c, n = 1e4)$bW
  ),
  Name = rep(c(&amp;quot;bw&amp;quot;, &amp;quot;bS&amp;quot;, &amp;quot;bw&amp;quot;), each = 1e4),
  Model = rep(c(&amp;quot;h2_a&amp;quot;, &amp;quot;h2_b&amp;quot;, &amp;quot;h2_c&amp;quot;), each = 1e4)
)

lbls &amp;lt;- c(
  expression(&amp;quot;Model 1:&amp;quot; ~ beta[W]),
  expression(&amp;quot;Model 2:&amp;quot; ~ beta[S]),
  expression(&amp;quot;Model 3:&amp;quot; ~ beta[W])
)

ggplot(Plot_df, aes(y = Model, x = Posteriors)) +
  stat_halfeye() +
  scale_y_discrete(labels = lbls) +
  labs(x = &amp;quot;Parameter Estimate&amp;quot;, y = &amp;quot;Implied Conditional Independency&amp;quot;) +
  theme_bw() +
  geom_vline(xintercept = 0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-28-statistical-rethinking-chapter-06_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.5 (2021-03-31)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19043)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.1252  LC_CTYPE=English_United Kingdom.1252    LC_MONETARY=English_United Kingdom.1252 LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.1252    
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] tidybayes_2.3.1      ggdag_0.2.3          dagitty_0.3-1        rethinking_2.13      rstan_2.21.2         ggplot2_3.3.6        StanHeaders_2.21.0-7
## 
## loaded via a namespace (and not attached):
##  [1] matrixStats_0.61.0   R.cache_0.14.0       tools_4.0.5          backports_1.2.1      bslib_0.2.4          utf8_1.2.1           R6_2.5.0             DBI_1.1.1            colorspace_2.0-0    
## [10] ggdist_2.4.0         withr_2.4.2          tidyselect_1.1.0     gridExtra_2.3        prettyunits_1.1.1    processx_3.5.1       curl_4.3.2           compiler_4.0.5       cli_3.0.0           
## [19] arrayhelpers_1.1-0   labeling_0.4.2       bookdown_0.22        sass_0.3.1           scales_1.1.1         mvtnorm_1.1-1        callr_3.7.0          stringr_1.4.0        digest_0.6.27       
## [28] rmarkdown_2.7        R.utils_2.10.1       pkgconfig_2.0.3      htmltools_0.5.1.1    styler_1.4.1         highr_0.9            rlang_0.4.11         shape_1.4.5          jquerylib_0.1.4     
## [37] farver_2.1.0         generics_0.1.0       svUnit_1.0.6         jsonlite_1.7.2       dplyr_1.0.5          R.oo_1.24.0          distributional_0.2.2 inline_0.3.17        magrittr_2.0.1      
## [46] loo_2.4.1            Rcpp_1.0.7           munsell_0.5.0        fansi_0.4.2          viridis_0.6.0        lifecycle_1.0.0      R.methodsS3_1.8.1    stringi_1.5.3        yaml_2.2.1          
## [55] ggraph_2.0.5         MASS_7.3-53.1        pkgbuild_1.2.0       plyr_1.8.6           grid_4.0.5           ggrepel_0.9.1        forcats_0.5.1        crayon_1.4.1         lattice_0.20-41     
## [64] graphlayouts_0.7.1   knitr_1.33           ps_1.6.0             pillar_1.6.0         igraph_1.2.6         boot_1.3-27          codetools_0.2-18     stats4_4.0.5         glue_1.4.2          
## [73] evaluate_0.14        blogdown_1.3         V8_3.4.1             RcppParallel_5.1.2   vctrs_0.3.7          tweenr_1.0.2         gtable_0.3.0         purrr_0.3.4          polyclip_1.10-0     
## [82] tidyr_1.1.3          rematch2_2.1.2       assertthat_0.2.1     xfun_0.22            ggforce_0.3.3        tidygraph_1.2.0      coda_0.19-4          viridisLite_0.4.0    tibble_3.1.1        
## [91] ellipsis_0.3.2
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Data Handling and Data Mining</title>
      <link>https://www.erikkusch.com/courses/an-introduction-to-biostatistics/data-handling-and-data-mining/</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/an-introduction-to-biostatistics/data-handling-and-data-mining/</guid>
      <description>&lt;h2 id=&#34;theory&#34;&gt;Theory&lt;/h2&gt;
&lt;p&gt;Welcome to our first &amp;ldquo;real&amp;rdquo; practical experience in &lt;code&gt;R&lt;/code&gt;. The following notes present you with an example of how data handling (also known as data cleaning) can be done. Obviously, the possibility for flaws to occur in any given data set are seemingly endless and so the following, tedious procedure should be thought of as less of an recipe of how to fix common flaws in biological data sets but make you aware of how important proper data collection and data entry is.&lt;/p&gt;
&lt;p&gt;I have prepared some &lt;a href=&#34;https://www.erikkusch.com/courses/an-introduction-to-biostatistics/07---Data-Handling-and-Data-Mining_Handout.html&#34; target=&#34;_blank&#34;&gt; Lecture Slides &lt;/a&gt; for this session.&lt;/p&gt;
&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;p&gt;Find the data for this exercise &lt;a href=&#34;https://www.erikkusch.com/courses/an-introduction-to-biostatistics/Data/SparrowData.csv&#34; target=&#34;_blank&#34;&gt; here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;preparing-our-procedure&#34;&gt;Preparing Our Procedure&lt;/h2&gt;
&lt;p&gt;The following three sections are what I consider to be &lt;em&gt;essential&lt;/em&gt; parts of the preamble to any &lt;code&gt;R&lt;/code&gt;-based analysis. I highly recommend clearly indicating these bits in your code.&lt;/p&gt;
&lt;p&gt;More often than not, you will use variations of these code chunks whether you are working on data handling, data exploration or full-fledged statistical analyses.&lt;/p&gt;
&lt;h3 id=&#34;necessary-steps-for-reproducibility&#34;&gt;Necessary Steps For Reproducibility&lt;/h3&gt;
&lt;p&gt;Reproducibility is the be-all and end-all of any statistical analysis, particularly in light of the peer-review process in life sciences.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rm(list=ls()) # clearing environment
Dir.Base &amp;lt;- getwd() # soft-coding our working directory
Dir.Data &amp;lt;- paste(Dir.Base, &amp;quot;Data&amp;quot;, sep=&amp;quot;/&amp;quot;) # soft-coding our data directory 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once you get into highly complex statistical analyses, you may wish to break up chunks of your analysis into separate documents. To ensure that remnants of an earlier analysis or analysis chunk do not influence the results of your current analysis, you may wish to &lt;em&gt;empty&lt;/em&gt; &lt;code&gt;R&lt;/code&gt;&amp;rsquo;s cache (&lt;em&gt;Environment&lt;/em&gt;) before attempting a new analysis. This is achieved via the command &lt;code&gt;rm(list=ls())&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Next, you &lt;em&gt;need&lt;/em&gt; to remember the importance of &lt;em&gt;soft-coding&lt;/em&gt; for the sake of reproducibility. One of the worst offences to the peer-review process in &lt;code&gt;R&lt;/code&gt;-based statistics is the erroneous hard-coding of the working directory. The &lt;code&gt;getwd()&lt;/code&gt; function shown above solves this exact problem. However, for this workaround to function properly you need to open the code document of interest by double-clicking it within its containing folder.&lt;/p&gt;
&lt;p&gt;When using the &lt;code&gt;xlsx&lt;/code&gt; package or any &lt;em&gt;Excel&lt;/em&gt;-reliant process via &lt;code&gt;R&lt;/code&gt;, your code will automatically run a Java process in the background. By default the Java engine is limited as far as RAM allocation goes and tends to fail when faced with enormous data sets. The workaround &lt;code&gt;options(java.parameters = &amp;quot;-Xmx8g&amp;quot;)&lt;/code&gt; gets rid of this issue by allocation 8 GBs of RAM to Java.&lt;/p&gt;
&lt;h3 id=&#34;packages&#34;&gt;Packages&lt;/h3&gt;
&lt;p&gt;Packages are &lt;code&gt;R&lt;/code&gt;&amp;rsquo;s way of giving you access to a seemingly infinite repository of functions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# function to load packages and install them if they haven&#39;t been installed yet
install.load.package &amp;lt;- function(x) {
  if (!require(x, character.only = TRUE))
    install.packages(x)
  require(x, character.only = TRUE)
}
package_vec &amp;lt;- c(&amp;quot;dplyr&amp;quot; # we need this package to fix the most common data errors
                 )
sapply(package_vec, install.load.package)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: dplyr
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &#39;dplyr&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &#39;package:stats&#39;:
## 
##     filter, lag
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &#39;package:base&#39;:
## 
##     intersect, setdiff, setequal, union
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## dplyr 
##  TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the above function is way more sophisticated than the usual &lt;code&gt;install.packages()&lt;/code&gt; + &lt;code&gt;library()&lt;/code&gt; approach since it automatically detects which packages require installing and only install these thus not overwriting already installed packages.&lt;/p&gt;
&lt;h3 id=&#34;loading-the-data&#34;&gt;Loading The Data&lt;/h3&gt;
&lt;p&gt;Loading data is crucial to any analysis in &lt;code&gt;R&lt;/code&gt;. Period.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;R&lt;/code&gt; offers a plethora of approaches to data loading and you will usually be taught the &lt;code&gt;read.table()&lt;/code&gt; command in basic biostatistics courses. However, I have found to prefer the functionality provided by the &lt;code&gt;xlsx&lt;/code&gt; package since most data recording is taking place in Excel. As this package is dependant on the installation of Java and &lt;code&gt;RJava&lt;/code&gt;, we will settle on the base &lt;code&gt;R&lt;/code&gt; function &lt;code&gt;read.csv()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df_base &amp;lt;- read.csv(file = paste(Dir.Data, &amp;quot;/SparrowData.csv&amp;quot;, sep=&amp;quot;&amp;quot;), header = TRUE)
Data_df &amp;lt;- Data_df_base # duplicate and save initial data on a new object
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another trick to have up your sleeve (if your RAM enables you to act on it) is to duplicate your initial data onto a new object once loaded into &lt;code&gt;R&lt;/code&gt;. This will enable you to easily remedy mistakes in data treatment without having to reload your initial data set from the data file.&lt;/p&gt;
&lt;h2 id=&#34;inspecting-the-data&#34;&gt;Inspecting The Data&lt;/h2&gt;
&lt;p&gt;Once the data is loaded into &lt;code&gt;R&lt;/code&gt;, you &lt;em&gt;need to inspect&lt;/em&gt; it to make sure it is ready for use.&lt;/p&gt;
&lt;h3 id=&#34;assessing-a-data-frame-in-r&#34;&gt;Assessing A Data Frame in &lt;code&gt;R&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Most, if not all, data you will ever load into &lt;code&gt;R&lt;/code&gt; will be stored as a &lt;code&gt;data.frame&lt;/code&gt; within &lt;code&gt;R&lt;/code&gt;. Some of the most important functions for inspecting data frames (&amp;ldquo;df&amp;rdquo; in the following) in base &lt;code&gt;R&lt;/code&gt; are the following four:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;dim(df)&lt;/code&gt; returns the dimensions (Rows $\times$ Columns)of the data frame&lt;/li&gt;
&lt;li&gt;&lt;code&gt;head(df)&lt;/code&gt; returns the first 6 rows of the data frame by default (here changed to 4)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tail(df)&lt;/code&gt; returns the last 6 rows of the data frame by default (here changed to 4)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;View(df)&lt;/code&gt; opens nearly any &lt;code&gt;R&lt;/code&gt; object in a separate tab for further inspection. Since we are dealing with an enormous data set here, I will exclude this function for now to save you from printing unnecessary pages.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dim(Data_df)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1068   21
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;head(Data_df, n = 4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   X    Site Index Latitude Longitude     Climate Population.Status Weight
## 1 1 Siberia    SI       60       100 Continental            Native  34,05
## 2 2 Siberia    SI       60       100 Continental            Native  34,86
## 3 3 Siberia    SI       60       100 Continental            Native  32,34
## 4 4 Siberia    SI       60       100 Continental            Native  34,78
##   Height Wing.Chord Colour    Sex Nesting.Site Nesting.Height Number.of.Eggs
## 1  12.87       6.67  Brown   Male         &amp;lt;NA&amp;gt;             NA             NA
## 2  13.68       6.79   Grey   Male         &amp;lt;NA&amp;gt;             NA             NA
## 3  12.66       6.64  Black Female        Shrub           35.6              1
## 4  15.09       7.00  Brown Female        Shrub          47.75              0
##   Egg.Weight Flock Home.Range Flock.Size Predator.Presence Predator.Type
## 1         NA     B      Large         16               Yes         Avian
## 2         NA     B      Large         16               Yes         Avian
## 3       3.21     C      Large         14               Yes         Avian
## 4         NA     E      Large         10               Yes         Avian
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;tail(Data_df, n = 4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         X           Site Index Latitude Longitude Climate Population.Status
## 1065 1065 Falkland Isles    FI   -51.75    -59.17 Coastal        Introduced
## 1066 1066 Falkland Isles    FI   -51.75    -59.17 Coastal        Introduced
## 1067 1067 Falkland Isles    FI   -51.75    -59.17 Coastal        Introduced
## 1068 1068 Falkland Isles    FI   -51.75    -59.17 Coastal        Introduced
##      Weight Height Wing.Chord Colour  Sex Nesting.Site Nesting.Height
## 1065  34.25  15.26       7.04   Grey Male                            
## 1066  31.76  12.78       6.67   Grey Male                            
## 1067  31.48  12.49       6.63  Black Male                            
## 1068  31.94  12.96       6.70   Grey Male                            
##      Number.of.Eggs Egg.Weight Flock Home.Range Flock.Size Predator.Presence
## 1065                               A      Large         19               Yes
## 1066                               A      Large         19               Yes
## 1067                               C      Large         18               Yes
## 1068                               A      Large         19               Yes
##      Predator.Type
## 1065         Avian
## 1066         Avian
## 1067         Avian
## 1068         Avian
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When having an initial look at the results of &lt;code&gt;head(Data_df)&lt;/code&gt; and &lt;code&gt;tail(Data_df)&lt;/code&gt; we can spot two important things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;NA&lt;/code&gt;s in head and tail of our data set are stored differently. This is a common problem with biological data sets and we will deal with this issue extensively in the next few sections of this document.&lt;/li&gt;
&lt;li&gt;Due to our data loading procedure we ended up with a redundant first column that is simply showing the respective row numbers. However, this is unnecessary in &lt;code&gt;R&lt;/code&gt; and so we can delete this column as seen below.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df &amp;lt;- Data_df[,-1] # eliminating the erroneous first column as it is redundant
dim(Data_df) # checking if the elimination went right
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1068   20
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;the-summary-function&#34;&gt;The &lt;code&gt;Summary()&lt;/code&gt; Function&lt;/h3&gt;
&lt;p&gt;As already stated in our seminar series, the &lt;code&gt;summary()&lt;/code&gt; function is &lt;em&gt;invaluable&lt;/em&gt; to data exploration and data inspection. However, it is only partially applicable as it will not work flawlessly on every class of data. Examples of this are shown below.&lt;/p&gt;
&lt;p&gt;The weight data contained within our data frame should be numeric and thus pose no issue to the &lt;code&gt;summary()&lt;/code&gt; function. However, as shown in the next section, it is currently of type character which leads the &lt;code&gt;summary()&lt;/code&gt; function to work improperly.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Weight)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1068 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The height data within our data set, on the other hand, is stored correctly as class numeric. Thus the &lt;code&gt;summary()&lt;/code&gt; function performs flawlessly.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Height)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    1.35   13.52   14.52   15.39   16.22  135.40
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Making data inspection more easy, one may which to automate the use of the &lt;code&gt;summary()&lt;/code&gt; function. However, this only makes sense, when every data column is presenting data in the correct class type. Therefore, we will first fix the column classes and then use the &lt;code&gt;summary()&lt;/code&gt; command.&lt;/p&gt;
&lt;h2 id=&#34;data-cleaning-workflow&#34;&gt;Data Cleaning Workflow&lt;/h2&gt;
&lt;h3 id=&#34;identifying-problems&#34;&gt;Identifying Problems&lt;/h3&gt;
&lt;p&gt;Indentifying most problems in any data set you may ever encounter comes down to mostly two manifestations of inadequate data entry or handling:&lt;/p&gt;
&lt;p&gt;**1. Types/Classes  **&lt;br&gt;
Before even opening a data set, we should know what kind of data classes we expect for every variable (for example, height records as a &lt;code&gt;factor&lt;/code&gt; don&amp;rsquo;t make much sense). Problems with data/variable classes can have lasting influence on your analyses and so we need to test the class for each variable (column) individually. Before we alter any column classes, we will first need to identify columns whose classes need fixing. Doing so is as easy applying the &lt;code&gt;class()&lt;/code&gt; function to the data contained within every column of our data frame separately.&lt;br&gt;
&lt;code&gt;R&lt;/code&gt; offers multiple functions for this but I find the &lt;code&gt;lapply()&lt;/code&gt; function to perform flawlessly as shown below. Since &lt;code&gt;lapply()&lt;/code&gt; returns a &lt;code&gt;list&lt;/code&gt; of class identifiers and these don&amp;rsquo;t translate well to paper, I have opted to transform the list into a named character vector using the &lt;code&gt;unlist()&lt;/code&gt; command. One could also use the &lt;code&gt;str()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;unlist(lapply(Data_df, class))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              Site             Index          Latitude         Longitude 
##       &amp;quot;character&amp;quot;       &amp;quot;character&amp;quot;         &amp;quot;numeric&amp;quot;         &amp;quot;numeric&amp;quot; 
##           Climate Population.Status            Weight            Height 
##       &amp;quot;character&amp;quot;       &amp;quot;character&amp;quot;       &amp;quot;character&amp;quot;         &amp;quot;numeric&amp;quot; 
##        Wing.Chord            Colour               Sex      Nesting.Site 
##         &amp;quot;numeric&amp;quot;       &amp;quot;character&amp;quot;       &amp;quot;character&amp;quot;       &amp;quot;character&amp;quot; 
##    Nesting.Height    Number.of.Eggs        Egg.Weight             Flock 
##       &amp;quot;character&amp;quot;       &amp;quot;character&amp;quot;       &amp;quot;character&amp;quot;       &amp;quot;character&amp;quot; 
##        Home.Range        Flock.Size Predator.Presence     Predator.Type 
##       &amp;quot;character&amp;quot;         &amp;quot;integer&amp;quot;       &amp;quot;character&amp;quot;       &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For further inspection, one may want to combine the information obtained by using the &lt;code&gt;class()&lt;/code&gt; function with either the &lt;code&gt;summary()&lt;/code&gt; function (for all non-numeric records) or the &lt;code&gt;hist&lt;/code&gt; function (particularly useful for numeric records).&lt;/p&gt;
&lt;p&gt;**2. Contents/Values  **&lt;br&gt;
Typos and the like will always lead to some data that simply doesn&amp;rsquo;t make sense given the context of your project. Sometimes, errors like these are salvageable but doing so can be a very difficult process. Before we alter any column contents, we will first need to identify columns whose contents need fixing, however. Doing so is as easy applying an automated version of &lt;code&gt;summary()&lt;/code&gt; to the data contained within every column of our data frame separately after having fixed possibly erroneous data classes.&lt;/p&gt;
&lt;h3 id=&#34;fixing-the-problems&#34;&gt;Fixing The Problems&lt;/h3&gt;
&lt;p&gt;Fixing the problems in our data sets always comes down to altering data classes, altering faulty values or removing them entirely.&lt;br&gt;
To make sure we fix all problems, we may often wish to enlist the &lt;code&gt;summary()&lt;/code&gt; function as well as the &lt;code&gt;hist()&lt;/code&gt; function for data inspection and visualisation.&lt;/p&gt;
&lt;p&gt;Before we alter any column contents, we will first need to identify columns whose contents need fixing.&lt;/p&gt;
&lt;!-- Doing so is as easy applying an automated version of `summary()` to the data contained within every column of our data frame separately which is now possible since we have fixed the column types.   --&gt;
&lt;!-- The code below does exactly that: --&gt;
&lt;!-- ```{r ColContProblems} --&gt;
&lt;!-- for(i in 1:dim(Data_df)[2]){ --&gt;
&lt;!--   print(colnames(Data_df)[i]) --&gt;
&lt;!--   print(summary(Data_df[,i])) --&gt;
&lt;!--   print(&#34;------------------------------------------------------&#34;) --&gt;
&lt;!-- } --&gt;
&lt;!-- ``` --&gt;
&lt;!-- There are some glaring issues her which we will address in the following sections. --&gt;
&lt;h2 id=&#34;our-data&#34;&gt;Our Data&lt;/h2&gt;
&lt;h3 id=&#34;site&#34;&gt;Site&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;factor&lt;/code&gt; (only 11 possible values)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-1&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Site records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Site)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Site)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1068 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, they do behave just like we&amp;rsquo;d expect them to.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;We don&amp;rsquo;t need to fix anything here.&lt;/p&gt;
&lt;h3 id=&#34;index&#34;&gt;Index&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;factor&lt;/code&gt; (only 11 possible values)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-2&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Index records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Index)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Index)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1068 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, they do behave just like we&amp;rsquo;d expect them to. Pay attention that thes shortened index numbers lign up with the numbers of site records!&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-1&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;We don&amp;rsquo;t need to fix anything here.&lt;/p&gt;
&lt;p&gt;\newpage&lt;/p&gt;
&lt;h3 id=&#34;latitude&#34;&gt;Latitude&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;numeric&lt;/code&gt; (Latitude is inherently continuous)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-3&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Latitude records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Latitude)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(Data_df$Latitude) # use this instead of summary due to station-dependency here
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## -51.75    -25  -21.1      4   10.5  17.25     31     54     55     60     70 
##     69     88     95    250    114    105     81     68     68     66     64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, they do behave just like we&amp;rsquo;d expect them to.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-2&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;We don&amp;rsquo;t need to fix anything here.&lt;/p&gt;
&lt;h3 id=&#34;longitude&#34;&gt;Longitude&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;numeric&lt;/code&gt; (Longitude is inherently continuous)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-4&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Longitude records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Longitude)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(Data_df$Longitude) # use this instead of summary due to station-dependency here
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##    -97    -92    -90 -88.75    -67 -59.17    -53     -2   55.6    100    135 
##     68     81     64    105    114     69    250     68     95     66     88
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, they do behave just like we&amp;rsquo;d expect them to.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-3&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;We don&amp;rsquo;t need to fix anything here.&lt;/p&gt;
&lt;h3 id=&#34;climate&#34;&gt;Climate&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;factor&lt;/code&gt; (three levels: coastal, semi-coastal, continental)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-5&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Climate records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Climate)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Climate)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1068 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, they do behave just like we&amp;rsquo;d expect them to.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-4&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;We don&amp;rsquo;t need to fix anything here.&lt;/p&gt;
&lt;h3 id=&#34;population-status&#34;&gt;Population Status&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;factor&lt;/code&gt; (two levels: native, introduced)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-6&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Population Status records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Population.Status)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Population.Status)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1068 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, they do behave just like we&amp;rsquo;d expect them to.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-5&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;We don&amp;rsquo;t need to fix anything here.&lt;/p&gt;
&lt;h3 id=&#34;weight&#34;&gt;Weight&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;numeric&lt;/code&gt; (weight is a continuous metric)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-7&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Weight records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Weight)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Weight)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1068 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Obviously, something is wrong.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-6&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;As seen above, weight records are currently stored as character which they shouldn&amp;rsquo;t. So how do we fix this?&lt;/p&gt;
&lt;p&gt;Firstly, let&amp;rsquo;s try an intuitive &lt;code&gt;as.numeric()&lt;/code&gt; approach which attempts to convert all values contained within a vector into numeric records.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df$Weight &amp;lt;- as.numeric(Data_df_base$Weight)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: NAs introduced by coercion
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Weight)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##   19.38   26.34   30.38   29.40   31.87  420.00      66
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Apparently, this didn&amp;rsquo;t do the trick since weight data values (recorded in g) below 13 and above 40 are highly unlikely for &lt;em&gt;Passer domesticus&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Sometimes, the &lt;code&gt;as.numeric()&lt;/code&gt; can be made more powerful by handing it data of class &lt;code&gt;character&lt;/code&gt;. To do so, simply combine &lt;code&gt;as.numeric()&lt;/code&gt; with &lt;code&gt;as.character()&lt;/code&gt; as shown below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df$Weight &amp;lt;- as.numeric(as.character(Data_df_base$Weight))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: NAs introduced by coercion
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Weight)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##   19.38   26.34   30.38   29.40   31.87  420.00      66
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That still didn&amp;rsquo;t resolve our problem. Weight measurements were taken for all study organisms and so there shouldn&amp;rsquo;t be any &lt;code&gt;NA&lt;/code&gt;s and yet we find 66.&lt;/p&gt;
&lt;p&gt;Interestingly enough this is the exact same number as observations available for Siberia. A closer look at the data frame shows us that weight data for Siberia has been recorded with commas as decimal delimiters whilst the rest of the data set utilises dots.&lt;/p&gt;
&lt;p&gt;Fixing this is not necessarily difficult but it is an erroneous issue for data handling which comes up often and is easy to avoid. Getting rid of the flaws is as simple as using the &lt;code&gt;gsub()&lt;/code&gt; function contained within the &lt;code&gt;dplyr&lt;/code&gt; package.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df$Weight &amp;lt;- as.numeric(gsub(pattern = &amp;quot;,&amp;quot;, replacement = &amp;quot;.&amp;quot;, x = Data_df_base$Weight))
summary(Data_df$Weight)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   19.38   27.90   30.63   29.69   32.24  420.00
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is one data record left hat exceeds the biologically viable span for body weight records of &lt;em&gt;Passer domesticus&lt;/em&gt;. This data record holds the value 420. Since this is unlikely to be a simple mistake of placing the decimal delimiter in the wrong place (both 4.2 and 42 grams are also not feasible weight records for house sparrows), we have to delete the weight data record in question:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df$Weight[which(Data_df_base$Weight == 420)] &amp;lt;- NA 
summary(Data_df$Weight)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##   19.38   27.89   30.63   29.33   32.23   36.66       1
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;hist(Data_df$Weight, breaks = 100)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;07---Data-Handling-and-Data-Mining_files/figure-html/ColContWeight-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We finally fixed it!&lt;/p&gt;
&lt;h3 id=&#34;height&#34;&gt;Height&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;numeric&lt;/code&gt; (height is a continuous metric)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-8&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Height records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Height)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Height)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    1.35   13.52   14.52   15.39   16.22  135.40
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, some of our data don&amp;rsquo;t behave the way the should (a 135.4 or  1.35 cm tall sparrow are just absurd).&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-7&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;Height (or &amp;ldquo;Length&amp;rdquo;) records of &lt;em&gt;Passer domesticus&lt;/em&gt; should fall roughly between 10cm and 22cm. Looking at the data which exceed these thresholds, it is apparent that these are generated simply through misplaced decimal delimiters. So we fix them as follows and use a histogram to check if it worked.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df$Height[which(Data_df$Height &amp;lt; 10)] # decimal point placed wrong here
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.350 1.446
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df$Height[which(Data_df$Height &amp;lt; 10)] &amp;lt;- Data_df$Height[which(Data_df$Height &amp;lt; 10)] * 10 # FIXED IT!
Data_df$Height[which(Data_df$Height &amp;gt; 22)] # decimal point placed wrong here
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 126.7 135.4
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df$Height[which(Data_df$Height &amp;gt; 22)] &amp;lt;- Data_df$Height[which(Data_df$Height &amp;gt; 22)]/10 # FIXED IT!
summary(Data_df$Height)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   11.09   13.52   14.51   15.20   16.20   21.68
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;hist(Data_df$Height, breaks = 100)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;07---Data-Handling-and-Data-Mining_files/figure-html/ColContHeight-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We finally fixed it!&lt;/p&gt;
&lt;h3 id=&#34;wing-chord&#34;&gt;Wing Chord&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;numeric&lt;/code&gt; (wing chord is a continuous metric)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-9&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Wing Chord records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Wing.Chord)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Wing.Chord)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   6.410   6.840   7.050   7.337   7.400   9.000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, they do behave just like we&amp;rsquo;d expect them to.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-8&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;We don&amp;rsquo;t need to fix anything here.&lt;/p&gt;
&lt;h3 id=&#34;colour&#34;&gt;Colour&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;factor&lt;/code&gt; (three levels: black, grey, brown)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-10&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Colour records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Colour)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Colour)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1068 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Some of the colour records are very odd.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-9&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;The colour records &amp;ldquo;Bright black&amp;rdquo; and &amp;ldquo;Grey with black spots&amp;rdquo; should be &amp;ldquo;Grey&amp;rdquo;. Someone clearly got too eager on the assignment of colours here. The fix is as easy as identifying the data records which are &amp;ldquo;too precise&amp;rdquo; and overwrite them with the correct assignment:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df$Colour[which(Data_df$Colour == &amp;quot;Bright black&amp;quot;)] &amp;lt;- &amp;quot;Grey&amp;quot;
Data_df$Colour[which(Data_df$Colour == &amp;quot;Grey with black spots&amp;quot;)] &amp;lt;- &amp;quot;Grey&amp;quot;
Data_df$Colour &amp;lt;- droplevels(factor(Data_df$Colour)) # drop unused factor levels
summary(Data_df$Colour) # FIXED IT!
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Black Brown  Grey 
##   356   298   414
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We finally fixed it!&lt;/p&gt;
&lt;h3 id=&#34;sex&#34;&gt;Sex&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;factor&lt;/code&gt; (two levels: male and female)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-11&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Climate records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Sex)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Sex)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1068 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, they do behave just like we&amp;rsquo;d expect them to.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-10&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;We don&amp;rsquo;t need to fix anything here.&lt;/p&gt;
&lt;h3 id=&#34;nesting-site&#34;&gt;Nesting Site&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;factor&lt;/code&gt; (two levels: shrub and tree)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-12&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Nesting Site records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Nesting.Site)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Nesting.Site)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1068 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;fixing-problems-11&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;One individual is recording to be nesting on the ground. This is something house sparrows don&amp;rsquo;t do. Therefore, we have to assume that this individual is not even a &lt;em&gt;Passer domesticus&lt;/em&gt; to begin with.&lt;/p&gt;
&lt;p&gt;The only way to solve this is to remove all observations pertaining to this individual:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df &amp;lt;- Data_df[-which(Data_df$Nesting.Site == &amp;quot;Ground&amp;quot;), ]
summary(Data_df$Nesting.Site)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1067 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We just deleted a data record. This affects the flock size of the flock it belongs to (basically, this column contains hard-coded values) which we are going to deal with later.&lt;br&gt;
Still, there are manually entered &lt;code&gt;NA&lt;/code&gt; records present which we have to get rid of. These can be fixed easily without altering column classes and simply making use of logic by indexing their dependencies on other column values. The nesting site for a data record where sex reads &amp;ldquo;Male&amp;rdquo; has to be &lt;code&gt;NA&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df$Nesting.Site[which(Data_df$Sex == &amp;quot;Male&amp;quot;)] &amp;lt;- NA 
Data_df$Nesting.Site &amp;lt;- droplevels(factor(Data_df$Nesting.Site)) # drop unused factor levels
summary(Data_df$Nesting.Site)# FIXED IT!
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Shrub  Tree  NA&#39;s 
##   292   231   544
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;nesting-height&#34;&gt;Nesting Height&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;numeric&lt;/code&gt; (continuous records in two clusters corresponding to shrubs and trees)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-13&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Nesting Height records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Nesting.Height)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Nesting.Height)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1067 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are obviously some issues here.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-12&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;Nesting height is a clear example of a variable that should be recorded as &lt;code&gt;numeric&lt;/code&gt; and yet our data frame currently stores them as character.&lt;/p&gt;
&lt;p&gt;Our first approach to fixing this, again, is using the &lt;code&gt;as.numeric()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(as.numeric(Data_df$Nesting.Height))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in summary(as.numeric(Data_df$Nesting.Height)): NAs introduced by
## coercion
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##   11.78   42.34   64.85  480.59  951.38 1950.86     544
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Clearly, something went horribly wrong here. When taking a closer look, the number of 1s is artificially inflated. This is due to the &lt;code&gt;NA&lt;/code&gt;s contained within the data set. These are currently stored as characters since they have been entered into the Excel sheet itself. The &lt;code&gt;as.numeric()&lt;/code&gt; function transforms these into 1s.&lt;/p&gt;
&lt;p&gt;One way of circumventing this issue is to combine the &lt;code&gt;as.numeric()&lt;/code&gt; function with the &lt;code&gt;as.character()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df$Nesting.Height &amp;lt;- as.numeric(as.character(Data_df$Nesting.Height))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: NAs introduced by coercion
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Nesting.Height)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##   11.78   42.34   64.85  480.59  951.38 1950.86     544
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This quite clearly fixed our problems.&lt;/p&gt;
&lt;!-- As can be seen in the histograms below there are now far less erroneously small values. --&gt;
&lt;!-- ```{r plottingpanesNestingHeight, fig.height=2.75} --&gt;
&lt;!-- par(mfrow=c(1,2)) # plotting panes as 1 by 2 --&gt;
&lt;!-- hist(as.numeric(Data_df_base$Nesting.Height), main = &#34;Numeric(Data)&#34;, breaks = 100) --&gt;
&lt;!-- hist(as.numeric(as.character(Data_df_base$Nesting.Height)), main = &#34;Numeric(Character(Data))&#34;, breaks = 100) --&gt;
&lt;!-- ``` --&gt;
&lt;h3 id=&#34;number-of-eggs&#34;&gt;Number of Eggs&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;numeric&lt;/code&gt; (no a priori knowledge of levels)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-14&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Number of Eggs records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Number.of.Eggs)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Number.of.Eggs)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1067 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One very out of the ordinary record is to be seen.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-13&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;Number of eggs is another variable which should be recorded as &lt;code&gt;numeric&lt;/code&gt; and yet is currently stored as character.&lt;/p&gt;
&lt;p&gt;Our first approach to fixing this, again, is using the &lt;code&gt;as.numeric()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(as.numeric(Data_df$Number.of.Eggs))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in summary(as.numeric(Data_df$Number.of.Eggs)): NAs introduced by
## coercion
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##   0.000   2.000   3.000   3.746   4.000  10.000     544
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, this didn&amp;rsquo;t do the trick. The number of 1s might be inflated and we expect exactly 544 (number of males) &lt;code&gt;NA&lt;/code&gt;s since number of eggs have only been recorded for female house sparrows.&lt;/p&gt;
&lt;p&gt;We already know that improperly stored &lt;code&gt;NA&lt;/code&gt; records are prone to causing an inflation of data records of value 1. We also remember that head and tail of our data frame hold different types of &lt;code&gt;NA&lt;/code&gt; records. Let&amp;rsquo;s find out who entered &lt;code&gt;NA&lt;/code&gt;s correctly:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;unique(Data_df$Site[which(is.na(Data_df$Egg.Weight))])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## character(0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code above identifies the sites at which proper &lt;code&gt;NA&lt;/code&gt; recording has been done. The Falkland Isle team did it right (&lt;code&gt;NA&lt;/code&gt; fields in Excel were left blank). Fixing this is actually a bit more challenging and so we do the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# make everything into characters
Data_df$Number.of.Eggs &amp;lt;- as.character(Data_df$Number.of.Eggs) 
# writing character NA onto actual NAs
Data_df$Number.of.Eggs[which(is.na(Data_df$Number.of.Eggs))] &amp;lt;- &amp;quot;  NA&amp;quot;
# make all character NAs into proper NAs
Data_df$Number.of.Eggs[Data_df$Number.of.Eggs == &amp;quot;  NA&amp;quot;] &amp;lt;- NA 
# make everything numeric
Data_df$Number.of.Eggs &amp;lt;- as.numeric(as.character(Data_df$Number.of.Eggs))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: NAs introduced by coercion
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Number.of.Eggs)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##   0.000   2.000   3.000   3.746   4.000  10.000     544
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We did it!&lt;/p&gt;
&lt;h3 id=&#34;egg-weight&#34;&gt;Egg Weight&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;numeric&lt;/code&gt; (another weight measurement that needs to be continuous)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-15&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Egg Weight records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Egg.Weight)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Egg.Weight)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1067 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;fixing-problems-14&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;Egg weight should be recorded as &lt;code&gt;numeric&lt;/code&gt; and yet is currently stored as character. Our first approach to fixing this, again, is using the &lt;code&gt;as.numeric()&lt;/code&gt; function again.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(as.numeric(Data_df$Egg.Weight))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in summary(as.numeric(Data_df$Egg.Weight)): NAs introduced by coercion
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##   1.580   2.340   2.670   2.619   2.890   3.590     590
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Something is wrong here. Not enough &lt;code&gt;NA&lt;/code&gt;s are recorded. We expect exactly 590 &lt;code&gt;NA&lt;/code&gt;s (Number of males + Number of Females with zero eggs). Additionally, there are way too many 1s.
Our problem, again, lies with the way the &lt;code&gt;NA&lt;/code&gt;s have been entered into the data set from the beginning and so we use the following fix again.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# make everything into characters
Data_df$Egg.Weight &amp;lt;- as.character(Data_df$Egg.Weight) 
# writing character NA onto actual NAs
Data_df$Egg.Weight[which(is.na(Data_df$Egg.Weight))] &amp;lt;- &amp;quot;  NA&amp;quot; 
# make all character NAs into proper NAs
Data_df$Egg.Weight[Data_df$Egg.Weight == &amp;quot;  NA&amp;quot;] &amp;lt;- NA 
# make everything numeric
Data_df$Egg.Weight &amp;lt;- as.numeric(as.character(Data_df$Egg.Weight))
summary(Data_df$Egg.Weight)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##   1.580   2.340   2.670   2.619   2.890   3.590     590
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;flock&#34;&gt;Flock&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;factor&lt;/code&gt; (each sparrow was assigned to one particular flock)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-16&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Flock records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Flock)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Flock)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1067 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, they do behave just like we&amp;rsquo;d expect them to.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-15&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;We don&amp;rsquo;t need to fix anything here.&lt;/p&gt;
&lt;h3 id=&#34;home-range&#34;&gt;Home Range&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;factor&lt;/code&gt; (three levels: small, medium, large)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-17&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Home Range records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Home.Range)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Home.Range)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1067 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, they do behave just like we&amp;rsquo;d expect them to.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-16&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;We don&amp;rsquo;t need to fix anything here.&lt;/p&gt;
&lt;h3 id=&#34;flock-size&#34;&gt;Flock Size&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;numeric&lt;/code&gt; (continuous measurement of how many sparrows are in each flock - measured as integers)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-18&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Flock Size records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Flock.Size)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;integer&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Flock.Size)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    7.00   16.00   19.00   25.81   31.00   58.00
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, they do behave just like we&amp;rsquo;d expect them to.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-17&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;We don&amp;rsquo;t need to fix anything here.&lt;/p&gt;
&lt;h3 id=&#34;predator-presence&#34;&gt;Predator Presence&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;factor&lt;/code&gt; (two levels: yes and no)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-19&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Predator Presence records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Predator.Presence)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Predator.Presence)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1067 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, they do behave just like we&amp;rsquo;d expect them to.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-18&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;We don&amp;rsquo;t need to fix anything here.&lt;/p&gt;
&lt;h3 id=&#34;predator-type&#34;&gt;Predator Type&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;factor&lt;/code&gt; (three levels: Avian, Non-Avian, and &lt;code&gt;NA&lt;/code&gt;)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-20&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Predator Type records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Predator.Type)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Predator.Type)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1067 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Something doesn&amp;rsquo;t sit well here.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-19&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;Someone got overly eager when recording Predator Type and specified the presence of a hawk instead of taking down &amp;ldquo;Avian&amp;rdquo;. We fix this as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df$Predator.Type[which(Data_df$Predator.Type == &amp;quot;Hawk&amp;quot;)] &amp;lt;- &amp;quot;Avian&amp;quot;
summary(Data_df$Predator.Type)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1067 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This fixed it  but there are still manually entered &lt;code&gt;NA&lt;/code&gt; records present which we have to get rid of. These can be fixed easily without altering column classes and simply making use of logic by indexing their dependencies on other column values. The predator type for a data record where predator presence reads &amp;ldquo;No&amp;rdquo; has to be &lt;code&gt;NA&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df$Predator.Type[which(Data_df$Predator.Presence == &amp;quot;No&amp;quot;)] &amp;lt;- NA 
Data_df$Predator.Type &amp;lt;- droplevels(factor(Data_df$Predator.Type)) # drop unused factor levels
summary(Data_df$Predator.Type)# FIXED IT!
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Avian Non-Avian      NA&#39;s 
##       490       220       357
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;redundant-data&#34;&gt;Redundant Data&lt;/h3&gt;
&lt;p&gt;Our data contains redundant columns (i.e.: columns whose data is present in another column already). These are (1) Flock Size (data contained in Flock column) and (2) Flock.Size (data contained in Index column). The fix to this is as easy as removing the columns in question.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df &amp;lt;- within(Data_df, rm(Flock.Size, Site))
dim(Data_df)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1067   18
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Fixed it!&lt;/p&gt;
&lt;p&gt;By doing so, we have gotten rid of our flock size problem stemming from the deletion of a data record. You could also argue that the columns &lt;code&gt;Site&lt;/code&gt; and &lt;code&gt;Index&lt;/code&gt; are redundant. We keep both for quality-of-life when interpreting our results (make use of &lt;code&gt;Sites&lt;/code&gt;) and coding (make use os &lt;code&gt;Index&lt;/code&gt;).&lt;/p&gt;
&lt;h2 id=&#34;saving-the-fixed-data-set&#34;&gt;Saving The Fixed Data Set&lt;/h2&gt;
&lt;p&gt;We fixed out entire data set! The data set is now ready for use.&lt;/p&gt;
&lt;p&gt;Keep in mind that the data set I provided you with was relatively clean and real-world messy data sets can be far more difficult to clean up.&lt;/p&gt;
&lt;p&gt;Before going forth, we need to save it. &lt;strong&gt;Attention:&lt;/strong&gt; don&amp;rsquo;t overwrite your initial data file!&lt;/p&gt;
&lt;h3 id=&#34;final-check&#34;&gt;Final Check&lt;/h3&gt;
&lt;p&gt;Before exporting you may want to ensure that everything is in order and do a final round of data inspection. This can be achieved by running the automated &lt;code&gt;summary()&lt;/code&gt; command from earlier again as follows. I am not including the output here to save some space.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;for(i in 1:dim(Data_df)[2]){
  print(colnames(Data_df)[i])
  print(summary(Data_df[,i]))
  print(&amp;quot;------------------------------------------------------&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Everything checks out. Let&amp;rsquo;s save our final data frame.&lt;/p&gt;
&lt;h3 id=&#34;exporting-the-altered-data&#34;&gt;Exporting The Altered Data&lt;/h3&gt;
&lt;p&gt;Since Excel is readily available for viewing data outside of R, I like to save my final data set in excel format as can be seen below. Additionally, I recommend saving your final data frame as an RDS file. These are &lt;code&gt;R&lt;/code&gt; specific data files which you will not be able to alter outside of &lt;code&gt;R&lt;/code&gt; thus saving yourself from accidentally changing records when only trying to view your data. On top of that, RDS files take up less space than either Excel or TXT files do.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# saving in excel sheet
write.csv(Data_df, file = paste(Dir.Data, &amp;quot;/SparrowData_FIXED.csv&amp;quot;, sep=&amp;quot;&amp;quot;))
# saving as R data frame object
saveRDS(Data_df, file = paste(Dir.Data, &amp;quot;/1 - Sparrow_Data_READY.rds&amp;quot;, sep=&amp;quot;&amp;quot;)) 
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Data Handling and Data Mining</title>
      <link>https://www.erikkusch.com/courses/biostat101/data-handling-and-data-mining/</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/biostat101/data-handling-and-data-mining/</guid>
      <description>&lt;h2 id=&#34;theory&#34;&gt;Theory&lt;/h2&gt;
&lt;p&gt;Welcome to our first &amp;ldquo;real&amp;rdquo; practical experience in &lt;code&gt;R&lt;/code&gt;. The following notes present you with an example of how data handling (also known as data cleaning) can be done. Obviously, the possibility for flaws to occur in any given data set are seemingly endless and so the following, tedious procedure should be thought of as less of an recipe of how to fix common flaws in biological data sets but make you aware of how important proper data collection and data entry is. I have prepared some slides for this session: &lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/biostat101/07---Data-Handling-and-Data-Mining_Handout.html&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://www.erikkusch.com/courses/biostat101/07---BioStat101_featured.png&#34; width=&#34;900&#34; margin-top = &#34;0&#34;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;p&gt;Find the data for this exercise &lt;a href=&#34;https://github.com/ErikKusch/Homepage/raw/master/content/courses/biostat101/Data/SparrowData.csv&#34; target=&#34;_blank&#34;&gt; here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;preparing-our-procedure&#34;&gt;Preparing Our Procedure&lt;/h2&gt;
&lt;p&gt;The following three sections are what I consider to be &lt;em&gt;essential&lt;/em&gt; parts of the preamble to any &lt;code&gt;R&lt;/code&gt;-based analysis. I highly recommend clearly indicating these bits in your code.&lt;/p&gt;
&lt;p&gt;More often than not, you will use variations of these code chunks whether you are working on data handling, data exploration or full-fledged statistical analyses.&lt;/p&gt;
&lt;h3 id=&#34;necessary-steps-for-reproducibility&#34;&gt;Necessary Steps For Reproducibility&lt;/h3&gt;
&lt;p&gt;Reproducibility is the be-all and end-all of any statistical analysis, particularly in light of the peer-review process in life sciences.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rm(list=ls()) # clearing environment
Dir.Base &amp;lt;- getwd() # soft-coding our working directory
Dir.Data &amp;lt;- paste(Dir.Base, &amp;quot;Data&amp;quot;, sep=&amp;quot;/&amp;quot;) # soft-coding our data directory 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once you get into highly complex statistical analyses, you may wish to break up chunks of your analysis into separate documents. To ensure that remnants of an earlier analysis or analysis chunk do not influence the results of your current analysis, you may wish to &lt;em&gt;empty&lt;/em&gt; &lt;code&gt;R&lt;/code&gt;&amp;rsquo;s cache (&lt;em&gt;Environment&lt;/em&gt;) before attempting a new analysis. This is achieved via the command &lt;code&gt;rm(list=ls())&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Next, you &lt;em&gt;need&lt;/em&gt; to remember the importance of &lt;em&gt;soft-coding&lt;/em&gt; for the sake of reproducibility. One of the worst offences to the peer-review process in &lt;code&gt;R&lt;/code&gt;-based statistics is the erroneous hard-coding of the working directory. The &lt;code&gt;getwd()&lt;/code&gt; function shown above solves this exact problem. However, for this workaround to function properly you need to open the code document of interest by double-clicking it within its containing folder.&lt;/p&gt;
&lt;p&gt;When using the &lt;code&gt;xlsx&lt;/code&gt; package or any &lt;em&gt;Excel&lt;/em&gt;-reliant process via &lt;code&gt;R&lt;/code&gt;, your code will automatically run a Java process in the background. By default the Java engine is limited as far as RAM allocation goes and tends to fail when faced with enormous data sets. The workaround &lt;code&gt;options(java.parameters = &amp;quot;-Xmx8g&amp;quot;)&lt;/code&gt; gets rid of this issue by allocation 8 GBs of RAM to Java.&lt;/p&gt;
&lt;h3 id=&#34;packages&#34;&gt;Packages&lt;/h3&gt;
&lt;p&gt;Packages are &lt;code&gt;R&lt;/code&gt;&amp;rsquo;s way of giving you access to a seemingly infinite repository of functions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# function to load packages and install them if they haven&#39;t been installed yet
install.load.package &amp;lt;- function(x) {
  if (!require(x, character.only = TRUE))
    install.packages(x)
  require(x, character.only = TRUE)
}
package_vec &amp;lt;- c(&amp;quot;dplyr&amp;quot; # we need this package to fix the most common data errors
                 )
sapply(package_vec, install.load.package)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: dplyr
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &#39;dplyr&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &#39;package:stats&#39;:
## 
##     filter, lag
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &#39;package:base&#39;:
## 
##     intersect, setdiff, setequal, union
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## dplyr 
##  TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the above function is way more sophisticated than the usual &lt;code&gt;install.packages()&lt;/code&gt; + &lt;code&gt;library()&lt;/code&gt; approach since it automatically detects which packages require installing and only install these thus not overwriting already installed packages.&lt;/p&gt;
&lt;h3 id=&#34;loading-the-data&#34;&gt;Loading The Data&lt;/h3&gt;
&lt;p&gt;Loading data is crucial to any analysis in &lt;code&gt;R&lt;/code&gt;. Period.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;R&lt;/code&gt; offers a plethora of approaches to data loading and you will usually be taught the &lt;code&gt;read.table()&lt;/code&gt; command in basic biostatistics courses. However, I have found to prefer the functionality provided by the &lt;code&gt;xlsx&lt;/code&gt; package since most data recording is taking place in Excel. As this package is dependant on the installation of Java and &lt;code&gt;RJava&lt;/code&gt;, we will settle on the base &lt;code&gt;R&lt;/code&gt; function &lt;code&gt;read.csv()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Data_df_base &amp;lt;- read.csv(file = paste(Dir.Data, &amp;quot;/SparrowData.csv&amp;quot;, sep=&amp;quot;&amp;quot;), header = TRUE)
Data_df_base &amp;lt;- read.csv(&amp;quot;https://github.com/ErikKusch/Homepage/raw/master/content/courses/biostat101/Data/SparrowData.csv&amp;quot;, 
                         header = TRUE)
Data_df &amp;lt;- Data_df_base # duplicate and save initial data on a new object
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another trick to have up your sleeve (if your RAM enables you to act on it) is to duplicate your initial data onto a new object once loaded into &lt;code&gt;R&lt;/code&gt;. This will enable you to easily remedy mistakes in data treatment without having to reload your initial data set from the data file.&lt;/p&gt;
&lt;h2 id=&#34;inspecting-the-data&#34;&gt;Inspecting The Data&lt;/h2&gt;
&lt;p&gt;Once the data is loaded into &lt;code&gt;R&lt;/code&gt;, you &lt;em&gt;need to inspect&lt;/em&gt; it to make sure it is ready for use.&lt;/p&gt;
&lt;h3 id=&#34;assessing-a-data-frame-in-r&#34;&gt;Assessing A Data Frame in &lt;code&gt;R&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Most, if not all, data you will ever load into &lt;code&gt;R&lt;/code&gt; will be stored as a &lt;code&gt;data.frame&lt;/code&gt; within &lt;code&gt;R&lt;/code&gt;. Some of the most important functions for inspecting data frames (&amp;ldquo;df&amp;rdquo; in the following) in base &lt;code&gt;R&lt;/code&gt; are the following four:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;dim(df)&lt;/code&gt; returns the dimensions (Rows $\times$ Columns)of the data frame&lt;/li&gt;
&lt;li&gt;&lt;code&gt;head(df)&lt;/code&gt; returns the first 6 rows of the data frame by default (here changed to 4)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tail(df)&lt;/code&gt; returns the last 6 rows of the data frame by default (here changed to 4)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;View(df)&lt;/code&gt; opens nearly any &lt;code&gt;R&lt;/code&gt; object in a separate tab for further inspection. Since we are dealing with an enormous data set here, I will exclude this function for now to save you from printing unnecessary pages.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dim(Data_df)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1068   21
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;head(Data_df, n = 4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   X    Site Index Latitude Longitude     Climate Population.Status Weight
## 1 1 Siberia    SI       60       100 Continental            Native  34,05
## 2 2 Siberia    SI       60       100 Continental            Native  34,86
## 3 3 Siberia    SI       60       100 Continental            Native  32,34
## 4 4 Siberia    SI       60       100 Continental            Native  34,78
##   Height Wing.Chord Colour    Sex Nesting.Site Nesting.Height Number.of.Eggs
## 1  12.87       6.67  Brown   Male         &amp;lt;NA&amp;gt;             NA             NA
## 2  13.68       6.79   Grey   Male         &amp;lt;NA&amp;gt;             NA             NA
## 3  12.66       6.64  Black Female        Shrub           35.6              1
## 4  15.09       7.00  Brown Female        Shrub          47.75              0
##   Egg.Weight Flock Home.Range Flock.Size Predator.Presence Predator.Type
## 1         NA     B      Large         16               Yes         Avian
## 2         NA     B      Large         16               Yes         Avian
## 3       3.21     C      Large         14               Yes         Avian
## 4         NA     E      Large         10               Yes         Avian
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;tail(Data_df, n = 4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         X           Site Index Latitude Longitude Climate Population.Status
## 1065 1065 Falkland Isles    FI   -51.75    -59.17 Coastal        Introduced
## 1066 1066 Falkland Isles    FI   -51.75    -59.17 Coastal        Introduced
## 1067 1067 Falkland Isles    FI   -51.75    -59.17 Coastal        Introduced
## 1068 1068 Falkland Isles    FI   -51.75    -59.17 Coastal        Introduced
##      Weight Height Wing.Chord Colour  Sex Nesting.Site Nesting.Height
## 1065  34.25  15.26       7.04   Grey Male                            
## 1066  31.76  12.78       6.67   Grey Male                            
## 1067  31.48  12.49       6.63  Black Male                            
## 1068  31.94  12.96       6.70   Grey Male                            
##      Number.of.Eggs Egg.Weight Flock Home.Range Flock.Size Predator.Presence
## 1065                               A      Large         19               Yes
## 1066                               A      Large         19               Yes
## 1067                               C      Large         18               Yes
## 1068                               A      Large         19               Yes
##      Predator.Type
## 1065         Avian
## 1066         Avian
## 1067         Avian
## 1068         Avian
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When having an initial look at the results of &lt;code&gt;head(Data_df)&lt;/code&gt; and &lt;code&gt;tail(Data_df)&lt;/code&gt; we can spot two important things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;NA&lt;/code&gt;s in head and tail of our data set are stored differently. This is a common problem with biological data sets and we will deal with this issue extensively in the next few sections of this document.&lt;/li&gt;
&lt;li&gt;Due to our data loading procedure we ended up with a redundant first column that is simply showing the respective row numbers. However, this is unnecessary in &lt;code&gt;R&lt;/code&gt; and so we can delete this column as seen below.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df &amp;lt;- Data_df[,-1] # eliminating the erroneous first column as it is redundant
dim(Data_df) # checking if the elimination went right
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1068   20
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;the-summary-function&#34;&gt;The &lt;code&gt;Summary()&lt;/code&gt; Function&lt;/h3&gt;
&lt;p&gt;As already stated in our seminar series, the &lt;code&gt;summary()&lt;/code&gt; function is &lt;em&gt;invaluable&lt;/em&gt; to data exploration and data inspection. However, it is only partially applicable as it will not work flawlessly on every class of data. Examples of this are shown below.&lt;/p&gt;
&lt;p&gt;The weight data contained within our data frame should be numeric and thus pose no issue to the &lt;code&gt;summary()&lt;/code&gt; function. However, as shown in the next section, it is currently of type character which leads the &lt;code&gt;summary()&lt;/code&gt; function to work improperly.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Weight)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1068 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The height data within our data set, on the other hand, is stored correctly as class numeric. Thus the &lt;code&gt;summary()&lt;/code&gt; function performs flawlessly.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Height)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    1.35   13.52   14.52   15.39   16.22  135.40
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Making data inspection more easy, one may which to automate the use of the &lt;code&gt;summary()&lt;/code&gt; function. However, this only makes sense, when every data column is presenting data in the correct class type. Therefore, we will first fix the column classes and then use the &lt;code&gt;summary()&lt;/code&gt; command.&lt;/p&gt;
&lt;h2 id=&#34;data-cleaning-workflow&#34;&gt;Data Cleaning Workflow&lt;/h2&gt;
&lt;h3 id=&#34;identifying-problems&#34;&gt;Identifying Problems&lt;/h3&gt;
&lt;p&gt;Indentifying most problems in any data set you may ever encounter comes down to mostly two manifestations of inadequate data entry or handling:&lt;/p&gt;
&lt;p&gt;**1. Types/Classes  **&lt;br&gt;
Before even opening a data set, we should know what kind of data classes we expect for every variable (for example, height records as a &lt;code&gt;factor&lt;/code&gt; don&amp;rsquo;t make much sense). Problems with data/variable classes can have lasting influence on your analyses and so we need to test the class for each variable (column) individually. Before we alter any column classes, we will first need to identify columns whose classes need fixing. Doing so is as easy applying the &lt;code&gt;class()&lt;/code&gt; function to the data contained within every column of our data frame separately.&lt;br&gt;
&lt;code&gt;R&lt;/code&gt; offers multiple functions for this but I find the &lt;code&gt;lapply()&lt;/code&gt; function to perform flawlessly as shown below. Since &lt;code&gt;lapply()&lt;/code&gt; returns a &lt;code&gt;list&lt;/code&gt; of class identifiers and these don&amp;rsquo;t translate well to paper, I have opted to transform the list into a named character vector using the &lt;code&gt;unlist()&lt;/code&gt; command. One could also use the &lt;code&gt;str()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;unlist(lapply(Data_df, class))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              Site             Index          Latitude         Longitude 
##       &amp;quot;character&amp;quot;       &amp;quot;character&amp;quot;         &amp;quot;numeric&amp;quot;         &amp;quot;numeric&amp;quot; 
##           Climate Population.Status            Weight            Height 
##       &amp;quot;character&amp;quot;       &amp;quot;character&amp;quot;       &amp;quot;character&amp;quot;         &amp;quot;numeric&amp;quot; 
##        Wing.Chord            Colour               Sex      Nesting.Site 
##         &amp;quot;numeric&amp;quot;       &amp;quot;character&amp;quot;       &amp;quot;character&amp;quot;       &amp;quot;character&amp;quot; 
##    Nesting.Height    Number.of.Eggs        Egg.Weight             Flock 
##       &amp;quot;character&amp;quot;       &amp;quot;character&amp;quot;       &amp;quot;character&amp;quot;       &amp;quot;character&amp;quot; 
##        Home.Range        Flock.Size Predator.Presence     Predator.Type 
##       &amp;quot;character&amp;quot;         &amp;quot;integer&amp;quot;       &amp;quot;character&amp;quot;       &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For further inspection, one may want to combine the information obtained by using the &lt;code&gt;class()&lt;/code&gt; function with either the &lt;code&gt;summary()&lt;/code&gt; function (for all non-numeric records) or the &lt;code&gt;hist&lt;/code&gt; function (particularly useful for numeric records).&lt;/p&gt;
&lt;p&gt;**2. Contents/Values  **&lt;br&gt;
Typos and the like will always lead to some data that simply doesn&amp;rsquo;t make sense given the context of your project. Sometimes, errors like these are salvageable but doing so can be a very difficult process. Before we alter any column contents, we will first need to identify columns whose contents need fixing, however. Doing so is as easy applying an automated version of &lt;code&gt;summary()&lt;/code&gt; to the data contained within every column of our data frame separately after having fixed possibly erroneous data classes.&lt;/p&gt;
&lt;h3 id=&#34;fixing-the-problems&#34;&gt;Fixing The Problems&lt;/h3&gt;
&lt;p&gt;Fixing the problems in our data sets always comes down to altering data classes, altering faulty values or removing them entirely.&lt;br&gt;
To make sure we fix all problems, we may often wish to enlist the &lt;code&gt;summary()&lt;/code&gt; function as well as the &lt;code&gt;hist()&lt;/code&gt; function for data inspection and visualisation.&lt;/p&gt;
&lt;p&gt;Before we alter any column contents, we will first need to identify columns whose contents need fixing.&lt;/p&gt;
&lt;!-- Doing so is as easy applying an automated version of `summary()` to the data contained within every column of our data frame separately which is now possible since we have fixed the column types.   --&gt;
&lt;!-- The code below does exactly that: --&gt;
&lt;!-- ```{r ColContProblems} --&gt;
&lt;!-- for(i in 1:dim(Data_df)[2]){ --&gt;
&lt;!--   print(colnames(Data_df)[i]) --&gt;
&lt;!--   print(summary(Data_df[,i])) --&gt;
&lt;!--   print(&#34;------------------------------------------------------&#34;) --&gt;
&lt;!-- } --&gt;
&lt;!-- ``` --&gt;
&lt;!-- There are some glaring issues her which we will address in the following sections. --&gt;
&lt;h2 id=&#34;our-data&#34;&gt;Our Data&lt;/h2&gt;
&lt;h3 id=&#34;site&#34;&gt;Site&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;factor&lt;/code&gt; (only 11 possible values)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-1&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Site records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Site)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Site)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1068 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, they do behave just like we&amp;rsquo;d expect them to.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;We don&amp;rsquo;t need to fix anything here.&lt;/p&gt;
&lt;h3 id=&#34;index&#34;&gt;Index&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;factor&lt;/code&gt; (only 11 possible values)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-2&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Index records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Index)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Index)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1068 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, they do behave just like we&amp;rsquo;d expect them to. Pay attention that thes shortened index numbers lign up with the numbers of site records!&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-1&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;We don&amp;rsquo;t need to fix anything here.&lt;/p&gt;
&lt;p&gt;\newpage&lt;/p&gt;
&lt;h3 id=&#34;latitude&#34;&gt;Latitude&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;numeric&lt;/code&gt; (Latitude is inherently continuous)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-3&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Latitude records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Latitude)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(Data_df$Latitude) # use this instead of summary due to station-dependency here
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## -51.75    -25  -21.1      4   10.5  17.25     31     54     55     60     70 
##     69     88     95    250    114    105     81     68     68     66     64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, they do behave just like we&amp;rsquo;d expect them to.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-2&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;We don&amp;rsquo;t need to fix anything here.&lt;/p&gt;
&lt;h3 id=&#34;longitude&#34;&gt;Longitude&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;numeric&lt;/code&gt; (Longitude is inherently continuous)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-4&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Longitude records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Longitude)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(Data_df$Longitude) # use this instead of summary due to station-dependency here
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##    -97    -92    -90 -88.75    -67 -59.17    -53     -2   55.6    100    135 
##     68     81     64    105    114     69    250     68     95     66     88
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, they do behave just like we&amp;rsquo;d expect them to.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-3&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;We don&amp;rsquo;t need to fix anything here.&lt;/p&gt;
&lt;h3 id=&#34;climate&#34;&gt;Climate&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;factor&lt;/code&gt; (three levels: coastal, semi-coastal, continental)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-5&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Climate records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Climate)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Climate)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1068 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, they do behave just like we&amp;rsquo;d expect them to.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-4&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;We don&amp;rsquo;t need to fix anything here.&lt;/p&gt;
&lt;h3 id=&#34;population-status&#34;&gt;Population Status&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;factor&lt;/code&gt; (two levels: native, introduced)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-6&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Population Status records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Population.Status)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Population.Status)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1068 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, they do behave just like we&amp;rsquo;d expect them to.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-5&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;We don&amp;rsquo;t need to fix anything here.&lt;/p&gt;
&lt;h3 id=&#34;weight&#34;&gt;Weight&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;numeric&lt;/code&gt; (weight is a continuous metric)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-7&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Weight records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Weight)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Weight)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1068 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Obviously, something is wrong.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-6&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;As seen above, weight records are currently stored as character which they shouldn&amp;rsquo;t. So how do we fix this?&lt;/p&gt;
&lt;p&gt;Firstly, let&amp;rsquo;s try an intuitive &lt;code&gt;as.numeric()&lt;/code&gt; approach which attempts to convert all values contained within a vector into numeric records.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df$Weight &amp;lt;- as.numeric(Data_df_base$Weight)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: NAs introduced by coercion
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Weight)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##   19.38   26.34   30.38   29.40   31.87  420.00      66
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Apparently, this didn&amp;rsquo;t do the trick since weight data values (recorded in g) below 13 and above 40 are highly unlikely for &lt;em&gt;Passer domesticus&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Sometimes, the &lt;code&gt;as.numeric()&lt;/code&gt; can be made more powerful by handing it data of class &lt;code&gt;character&lt;/code&gt;. To do so, simply combine &lt;code&gt;as.numeric()&lt;/code&gt; with &lt;code&gt;as.character()&lt;/code&gt; as shown below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df$Weight &amp;lt;- as.numeric(as.character(Data_df_base$Weight))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: NAs introduced by coercion
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Weight)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##   19.38   26.34   30.38   29.40   31.87  420.00      66
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That still didn&amp;rsquo;t resolve our problem. Weight measurements were taken for all study organisms and so there shouldn&amp;rsquo;t be any &lt;code&gt;NA&lt;/code&gt;s and yet we find 66.&lt;/p&gt;
&lt;p&gt;Interestingly enough this is the exact same number as observations available for Siberia. A closer look at the data frame shows us that weight data for Siberia has been recorded with commas as decimal delimiters whilst the rest of the data set utilises dots.&lt;/p&gt;
&lt;p&gt;Fixing this is not necessarily difficult but it is an erroneous issue for data handling which comes up often and is easy to avoid. Getting rid of the flaws is as simple as using the &lt;code&gt;gsub()&lt;/code&gt; function contained within the &lt;code&gt;dplyr&lt;/code&gt; package.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df$Weight &amp;lt;- as.numeric(gsub(pattern = &amp;quot;,&amp;quot;, replacement = &amp;quot;.&amp;quot;, x = Data_df_base$Weight))
summary(Data_df$Weight)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   19.38   27.90   30.63   29.69   32.24  420.00
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is one data record left hat exceeds the biologically viable span for body weight records of &lt;em&gt;Passer domesticus&lt;/em&gt;. This data record holds the value 420. Since this is unlikely to be a simple mistake of placing the decimal delimiter in the wrong place (both 4.2 and 42 grams are also not feasible weight records for house sparrows), we have to delete the weight data record in question:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df$Weight[which(Data_df_base$Weight == 420)] &amp;lt;- NA 
summary(Data_df$Weight)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##   19.38   27.89   30.63   29.33   32.23   36.66       1
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;hist(Data_df$Weight, breaks = 100)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;07---Data-Handling-and-Data-Mining_files/figure-html/ColContWeight-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We finally fixed it!&lt;/p&gt;
&lt;h3 id=&#34;height&#34;&gt;Height&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;numeric&lt;/code&gt; (height is a continuous metric)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-8&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Height records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Height)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Height)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    1.35   13.52   14.52   15.39   16.22  135.40
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, some of our data don&amp;rsquo;t behave the way the should (a 135.4 or  1.35 cm tall sparrow are just absurd).&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-7&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;Height (or &amp;ldquo;Length&amp;rdquo;) records of &lt;em&gt;Passer domesticus&lt;/em&gt; should fall roughly between 10cm and 22cm. Looking at the data which exceed these thresholds, it is apparent that these are generated simply through misplaced decimal delimiters. So we fix them as follows and use a histogram to check if it worked.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df$Height[which(Data_df$Height &amp;lt; 10)] # decimal point placed wrong here
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.350 1.446
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df$Height[which(Data_df$Height &amp;lt; 10)] &amp;lt;- Data_df$Height[which(Data_df$Height &amp;lt; 10)] * 10 # FIXED IT!
Data_df$Height[which(Data_df$Height &amp;gt; 22)] # decimal point placed wrong here
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 126.7 135.4
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df$Height[which(Data_df$Height &amp;gt; 22)] &amp;lt;- Data_df$Height[which(Data_df$Height &amp;gt; 22)]/10 # FIXED IT!
summary(Data_df$Height)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   11.09   13.52   14.51   15.20   16.20   21.68
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;hist(Data_df$Height, breaks = 100)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;07---Data-Handling-and-Data-Mining_files/figure-html/ColContHeight-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We finally fixed it!&lt;/p&gt;
&lt;h3 id=&#34;wing-chord&#34;&gt;Wing Chord&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;numeric&lt;/code&gt; (wing chord is a continuous metric)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-9&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Wing Chord records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Wing.Chord)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Wing.Chord)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   6.410   6.840   7.050   7.337   7.400   9.000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, they do behave just like we&amp;rsquo;d expect them to.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-8&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;We don&amp;rsquo;t need to fix anything here.&lt;/p&gt;
&lt;h3 id=&#34;colour&#34;&gt;Colour&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;factor&lt;/code&gt; (three levels: black, grey, brown)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-10&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Colour records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Colour)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Colour)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1068 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Some of the colour records are very odd.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-9&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;The colour records &amp;ldquo;Bright black&amp;rdquo; and &amp;ldquo;Grey with black spots&amp;rdquo; should be &amp;ldquo;Grey&amp;rdquo;. Someone clearly got too eager on the assignment of colours here. The fix is as easy as identifying the data records which are &amp;ldquo;too precise&amp;rdquo; and overwrite them with the correct assignment:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df$Colour[which(Data_df$Colour == &amp;quot;Bright black&amp;quot;)] &amp;lt;- &amp;quot;Grey&amp;quot;
Data_df$Colour[which(Data_df$Colour == &amp;quot;Grey with black spots&amp;quot;)] &amp;lt;- &amp;quot;Grey&amp;quot;
Data_df$Colour &amp;lt;- droplevels(factor(Data_df$Colour)) # drop unused factor levels
summary(Data_df$Colour) # FIXED IT!
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Black Brown  Grey 
##   356   298   414
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We finally fixed it!&lt;/p&gt;
&lt;h3 id=&#34;sex&#34;&gt;Sex&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;factor&lt;/code&gt; (two levels: male and female)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-11&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Climate records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Sex)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Sex)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1068 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, they do behave just like we&amp;rsquo;d expect them to.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-10&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;We don&amp;rsquo;t need to fix anything here.&lt;/p&gt;
&lt;h3 id=&#34;nesting-site&#34;&gt;Nesting Site&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;factor&lt;/code&gt; (two levels: shrub and tree)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-12&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Nesting Site records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Nesting.Site)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Nesting.Site)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1068 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;fixing-problems-11&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;One individual is recording to be nesting on the ground. This is something house sparrows don&amp;rsquo;t do. Therefore, we have to assume that this individual is not even a &lt;em&gt;Passer domesticus&lt;/em&gt; to begin with.&lt;/p&gt;
&lt;p&gt;The only way to solve this is to remove all observations pertaining to this individual:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df &amp;lt;- Data_df[-which(Data_df$Nesting.Site == &amp;quot;Ground&amp;quot;), ]
summary(Data_df$Nesting.Site)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1067 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We just deleted a data record. This affects the flock size of the flock it belongs to (basically, this column contains hard-coded values) which we are going to deal with later.&lt;br&gt;
Still, there are manually entered &lt;code&gt;NA&lt;/code&gt; records present which we have to get rid of. These can be fixed easily without altering column classes and simply making use of logic by indexing their dependencies on other column values. The nesting site for a data record where sex reads &amp;ldquo;Male&amp;rdquo; has to be &lt;code&gt;NA&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df$Nesting.Site[which(Data_df$Sex == &amp;quot;Male&amp;quot;)] &amp;lt;- NA 
Data_df$Nesting.Site &amp;lt;- droplevels(factor(Data_df$Nesting.Site)) # drop unused factor levels
summary(Data_df$Nesting.Site)# FIXED IT!
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Shrub  Tree  NA&#39;s 
##   292   231   544
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;nesting-height&#34;&gt;Nesting Height&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;numeric&lt;/code&gt; (continuous records in two clusters corresponding to shrubs and trees)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-13&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Nesting Height records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Nesting.Height)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Nesting.Height)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1067 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are obviously some issues here.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-12&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;Nesting height is a clear example of a variable that should be recorded as &lt;code&gt;numeric&lt;/code&gt; and yet our data frame currently stores them as character.&lt;/p&gt;
&lt;p&gt;Our first approach to fixing this, again, is using the &lt;code&gt;as.numeric()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(as.numeric(Data_df$Nesting.Height))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in summary(as.numeric(Data_df$Nesting.Height)): NAs introduced by
## coercion
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##   11.78   42.34   64.85  480.59  951.38 1950.86     544
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Clearly, something went horribly wrong here. When taking a closer look, the number of 1s is artificially inflated. This is due to the &lt;code&gt;NA&lt;/code&gt;s contained within the data set. These are currently stored as characters since they have been entered into the Excel sheet itself. The &lt;code&gt;as.numeric()&lt;/code&gt; function transforms these into 1s.&lt;/p&gt;
&lt;p&gt;One way of circumventing this issue is to combine the &lt;code&gt;as.numeric()&lt;/code&gt; function with the &lt;code&gt;as.character()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df$Nesting.Height &amp;lt;- as.numeric(as.character(Data_df$Nesting.Height))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: NAs introduced by coercion
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Nesting.Height)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##   11.78   42.34   64.85  480.59  951.38 1950.86     544
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This quite clearly fixed our problems.&lt;/p&gt;
&lt;!-- As can be seen in the histograms below there are now far less erroneously small values. --&gt;
&lt;!-- ```{r plottingpanesNestingHeight, fig.height=2.75} --&gt;
&lt;!-- par(mfrow=c(1,2)) # plotting panes as 1 by 2 --&gt;
&lt;!-- hist(as.numeric(Data_df_base$Nesting.Height), main = &#34;Numeric(Data)&#34;, breaks = 100) --&gt;
&lt;!-- hist(as.numeric(as.character(Data_df_base$Nesting.Height)), main = &#34;Numeric(Character(Data))&#34;, breaks = 100) --&gt;
&lt;!-- ``` --&gt;
&lt;h3 id=&#34;number-of-eggs&#34;&gt;Number of Eggs&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;numeric&lt;/code&gt; (no a priori knowledge of levels)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-14&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Number of Eggs records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Number.of.Eggs)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Number.of.Eggs)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1067 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One very out of the ordinary record is to be seen.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-13&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;Number of eggs is another variable which should be recorded as &lt;code&gt;numeric&lt;/code&gt; and yet is currently stored as character.&lt;/p&gt;
&lt;p&gt;Our first approach to fixing this, again, is using the &lt;code&gt;as.numeric()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(as.numeric(Data_df$Number.of.Eggs))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in summary(as.numeric(Data_df$Number.of.Eggs)): NAs introduced by
## coercion
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##   0.000   2.000   3.000   3.746   4.000  10.000     544
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, this didn&amp;rsquo;t do the trick. The number of 1s might be inflated and we expect exactly 544 (number of males) &lt;code&gt;NA&lt;/code&gt;s since number of eggs have only been recorded for female house sparrows.&lt;/p&gt;
&lt;p&gt;We already know that improperly stored &lt;code&gt;NA&lt;/code&gt; records are prone to causing an inflation of data records of value 1. We also remember that head and tail of our data frame hold different types of &lt;code&gt;NA&lt;/code&gt; records. Let&amp;rsquo;s find out who entered &lt;code&gt;NA&lt;/code&gt;s correctly:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;unique(Data_df$Site[which(is.na(Data_df$Egg.Weight))])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## character(0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code above identifies the sites at which proper &lt;code&gt;NA&lt;/code&gt; recording has been done. The Falkland Isle team did it right (&lt;code&gt;NA&lt;/code&gt; fields in Excel were left blank). Fixing this is actually a bit more challenging and so we do the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# make everything into characters
Data_df$Number.of.Eggs &amp;lt;- as.character(Data_df$Number.of.Eggs) 
# writing character NA onto actual NAs
Data_df$Number.of.Eggs[which(is.na(Data_df$Number.of.Eggs))] &amp;lt;- &amp;quot;  NA&amp;quot;
# make all character NAs into proper NAs
Data_df$Number.of.Eggs[Data_df$Number.of.Eggs == &amp;quot;  NA&amp;quot;] &amp;lt;- NA 
# make everything numeric
Data_df$Number.of.Eggs &amp;lt;- as.numeric(as.character(Data_df$Number.of.Eggs))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: NAs introduced by coercion
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Number.of.Eggs)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##   0.000   2.000   3.000   3.746   4.000  10.000     544
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We did it!&lt;/p&gt;
&lt;h3 id=&#34;egg-weight&#34;&gt;Egg Weight&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;numeric&lt;/code&gt; (another weight measurement that needs to be continuous)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-15&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Egg Weight records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Egg.Weight)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Egg.Weight)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1067 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;fixing-problems-14&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;Egg weight should be recorded as &lt;code&gt;numeric&lt;/code&gt; and yet is currently stored as character. Our first approach to fixing this, again, is using the &lt;code&gt;as.numeric()&lt;/code&gt; function again.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(as.numeric(Data_df$Egg.Weight))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in summary(as.numeric(Data_df$Egg.Weight)): NAs introduced by coercion
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##   1.580   2.340   2.670   2.619   2.890   3.590     590
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Something is wrong here. Not enough &lt;code&gt;NA&lt;/code&gt;s are recorded. We expect exactly 590 &lt;code&gt;NA&lt;/code&gt;s (Number of males + Number of Females with zero eggs). Additionally, there are way too many 1s.
Our problem, again, lies with the way the &lt;code&gt;NA&lt;/code&gt;s have been entered into the data set from the beginning and so we use the following fix again.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# make everything into characters
Data_df$Egg.Weight &amp;lt;- as.character(Data_df$Egg.Weight) 
# writing character NA onto actual NAs
Data_df$Egg.Weight[which(is.na(Data_df$Egg.Weight))] &amp;lt;- &amp;quot;  NA&amp;quot; 
# make all character NAs into proper NAs
Data_df$Egg.Weight[Data_df$Egg.Weight == &amp;quot;  NA&amp;quot;] &amp;lt;- NA 
# make everything numeric
Data_df$Egg.Weight &amp;lt;- as.numeric(as.character(Data_df$Egg.Weight))
summary(Data_df$Egg.Weight)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##   1.580   2.340   2.670   2.619   2.890   3.590     590
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;flock&#34;&gt;Flock&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;factor&lt;/code&gt; (each sparrow was assigned to one particular flock)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-16&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Flock records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Flock)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Flock)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1067 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, they do behave just like we&amp;rsquo;d expect them to.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-15&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;We don&amp;rsquo;t need to fix anything here.&lt;/p&gt;
&lt;h3 id=&#34;home-range&#34;&gt;Home Range&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;factor&lt;/code&gt; (three levels: small, medium, large)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-17&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Home Range records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Home.Range)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Home.Range)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1067 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, they do behave just like we&amp;rsquo;d expect them to.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-16&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;We don&amp;rsquo;t need to fix anything here.&lt;/p&gt;
&lt;h3 id=&#34;flock-size&#34;&gt;Flock Size&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;numeric&lt;/code&gt; (continuous measurement of how many sparrows are in each flock - measured as integers)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-18&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Flock Size records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Flock.Size)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;integer&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Flock.Size)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    7.00   16.00   19.00   25.81   31.00   58.00
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, they do behave just like we&amp;rsquo;d expect them to.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-17&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;We don&amp;rsquo;t need to fix anything here.&lt;/p&gt;
&lt;h3 id=&#34;predator-presence&#34;&gt;Predator Presence&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;factor&lt;/code&gt; (two levels: yes and no)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-19&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Predator Presence records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Predator.Presence)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Predator.Presence)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1067 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, they do behave just like we&amp;rsquo;d expect them to.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-18&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;We don&amp;rsquo;t need to fix anything here.&lt;/p&gt;
&lt;h3 id=&#34;predator-type&#34;&gt;Predator Type&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Variable Class Expectation:&lt;/strong&gt; &lt;code&gt;factor&lt;/code&gt; (three levels: Avian, Non-Avian, and &lt;code&gt;NA&lt;/code&gt;)&lt;/p&gt;
&lt;h4 id=&#34;identifying-problems-20&#34;&gt;Identifying Problems&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses our Predator Type records for our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals and check whether they behave as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(Data_df$Predator.Type)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(Data_df$Predator.Type)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1067 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Something doesn&amp;rsquo;t sit well here.&lt;/p&gt;
&lt;h4 id=&#34;fixing-problems-19&#34;&gt;Fixing Problems&lt;/h4&gt;
&lt;p&gt;Someone got overly eager when recording Predator Type and specified the presence of a hawk instead of taking down &amp;ldquo;Avian&amp;rdquo;. We fix this as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df$Predator.Type[which(Data_df$Predator.Type == &amp;quot;Hawk&amp;quot;)] &amp;lt;- &amp;quot;Avian&amp;quot;
summary(Data_df$Predator.Type)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Length     Class      Mode 
##      1067 character character
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This fixed it  but there are still manually entered &lt;code&gt;NA&lt;/code&gt; records present which we have to get rid of. These can be fixed easily without altering column classes and simply making use of logic by indexing their dependencies on other column values. The predator type for a data record where predator presence reads &amp;ldquo;No&amp;rdquo; has to be &lt;code&gt;NA&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df$Predator.Type[which(Data_df$Predator.Presence == &amp;quot;No&amp;quot;)] &amp;lt;- NA 
Data_df$Predator.Type &amp;lt;- droplevels(factor(Data_df$Predator.Type)) # drop unused factor levels
summary(Data_df$Predator.Type)# FIXED IT!
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Avian Non-Avian      NA&#39;s 
##       490       220       357
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;redundant-data&#34;&gt;Redundant Data&lt;/h3&gt;
&lt;p&gt;Our data contains redundant columns (i.e.: columns whose data is present in another column already). These are (1) Flock Size (data contained in Flock column) and (2) Flock.Size (data contained in Index column). The fix to this is as easy as removing the columns in question.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df &amp;lt;- within(Data_df, rm(Flock.Size, Site))
dim(Data_df)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1067   18
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Fixed it!&lt;/p&gt;
&lt;p&gt;By doing so, we have gotten rid of our flock size problem stemming from the deletion of a data record. You could also argue that the columns &lt;code&gt;Site&lt;/code&gt; and &lt;code&gt;Index&lt;/code&gt; are redundant. We keep both for quality-of-life when interpreting our results (make use of &lt;code&gt;Sites&lt;/code&gt;) and coding (make use os &lt;code&gt;Index&lt;/code&gt;).&lt;/p&gt;
&lt;h2 id=&#34;saving-the-fixed-data-set&#34;&gt;Saving The Fixed Data Set&lt;/h2&gt;
&lt;p&gt;We fixed out entire data set! The data set is now ready for use.&lt;/p&gt;
&lt;p&gt;Keep in mind that the data set I provided you with was relatively clean and real-world messy data sets can be far more difficult to clean up.&lt;/p&gt;
&lt;p&gt;Before going forth, we need to save it. &lt;strong&gt;Attention:&lt;/strong&gt; don&amp;rsquo;t overwrite your initial data file!&lt;/p&gt;
&lt;h3 id=&#34;final-check&#34;&gt;Final Check&lt;/h3&gt;
&lt;p&gt;Before exporting you may want to ensure that everything is in order and do a final round of data inspection. This can be achieved by running the automated &lt;code&gt;summary()&lt;/code&gt; command from earlier again as follows. I am not including the output here to save some space.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;for(i in 1:dim(Data_df)[2]){
  print(colnames(Data_df)[i])
  print(summary(Data_df[,i]))
  print(&amp;quot;------------------------------------------------------&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Everything checks out. Let&amp;rsquo;s save our final data frame.&lt;/p&gt;
&lt;h3 id=&#34;exporting-the-altered-data&#34;&gt;Exporting The Altered Data&lt;/h3&gt;
&lt;p&gt;Since Excel is readily available for viewing data outside of R, I like to save my final data set in excel format as can be seen below. Additionally, I recommend saving your final data frame as an RDS file. These are &lt;code&gt;R&lt;/code&gt; specific data files which you will not be able to alter outside of &lt;code&gt;R&lt;/code&gt; thus saving yourself from accidentally changing records when only trying to view your data. On top of that, RDS files take up less space than either Excel or TXT files do.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# saving in excel sheet
write.csv(Data_df, file = paste(Dir.Data, &amp;quot;/SparrowData_FIXED.csv&amp;quot;, sep=&amp;quot;&amp;quot;))
# saving as R data frame object
saveRDS(Data_df, file = paste(Dir.Data, &amp;quot;/1 - Sparrow_Data_READY.rds&amp;quot;, sep=&amp;quot;&amp;quot;)) 
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Citing GBIF Data</title>
      <link>https://www.erikkusch.com/courses/gbif/dataciting/</link>
      <pubDate>Sun, 21 May 2023 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/gbif/dataciting/</guid>
      <description>&lt;div class=&#34;alert alert-normal&#34;&gt;
  &lt;div&gt;
    &lt;details&gt;
  &lt;summary&gt;Preamble, Package-Loading, and GBIF API Credential Registering (click here):&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Custom install &amp;amp; load function
install.load.package &amp;lt;- function(x) {
  if (!require(x, character.only = TRUE)) {
    install.packages(x, repos = &amp;quot;http://cran.us.r-project.org&amp;quot;)
  }
  require(x, character.only = TRUE)
}
## names of packages we want installed (if not installed yet) and loaded
package_vec &amp;lt;- c(
  &amp;quot;rgbif&amp;quot;,
  &amp;quot;ggplot2&amp;quot; # for visualisation
)
## executing install &amp;amp; load for each package
sapply(package_vec, install.load.package)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   rgbif ggplot2 
##    TRUE    TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;options(gbif_user = &amp;quot;my gbif username&amp;quot;)
options(gbif_email = &amp;quot;my registred gbif e-mail&amp;quot;)
options(gbif_pwd = &amp;quot;my gbif password&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt; 
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;First, we obtain and load the data use for our publication like such:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Download query
res &amp;lt;- occ_download(
  pred(&amp;quot;taxonKey&amp;quot;, sp_key),
  pred_in(&amp;quot;basisOfRecord&amp;quot;, c(&amp;quot;HUMAN_OBSERVATION&amp;quot;)),
  pred(&amp;quot;country&amp;quot;, &amp;quot;NO&amp;quot;),
  pred(&amp;quot;hasCoordinate&amp;quot;, TRUE),
  pred_gte(&amp;quot;year&amp;quot;, 2000),
  pred_lte(&amp;quot;year&amp;quot;, 2020)
)
# Downloading and Loading
res_meta &amp;lt;- occ_download_wait(res, status_ping = 5, curlopts = list(), quiet = FALSE)
res_get &amp;lt;- occ_download_get(res)
res_data &amp;lt;- occ_download_import(res_get)
# Limiting data according to quality control
preci_data &amp;lt;- res_data[which(res_data$coordinateUncertaintyInMeters &amp;lt; 200), ]
# Subsetting data for desired variables and data markers
data_subset &amp;lt;- preci_data[
  ,
  c(&amp;quot;scientificName&amp;quot;, &amp;quot;decimalLongitude&amp;quot;, &amp;quot;decimalLatitude&amp;quot;, &amp;quot;basisOfRecord&amp;quot;, &amp;quot;year&amp;quot;, &amp;quot;month&amp;quot;, &amp;quot;day&amp;quot;, &amp;quot;eventDate&amp;quot;, &amp;quot;countryCode&amp;quot;, &amp;quot;municipality&amp;quot;, &amp;quot;taxonKey&amp;quot;, &amp;quot;species&amp;quot;, &amp;quot;catalogNumber&amp;quot;, &amp;quot;hasGeospatialIssues&amp;quot;, &amp;quot;hasCoordinate&amp;quot;, &amp;quot;datasetKey&amp;quot;)
]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now all we need to worry about is properly accrediting data sources to make our data usage fair and reproducible.&lt;/p&gt;
&lt;h2 id=&#34;finding-download-citation&#34;&gt;Finding Download Citation&lt;/h2&gt;
&lt;p&gt;Much like previous steps of this workshop, identifying the correct citation for a given download from GBIF can be done via:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;GBIF Portal&lt;/li&gt;
&lt;li&gt;&lt;code&gt;rgbif&lt;/code&gt; functionality&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;gbif-portal&#34;&gt;GBIF Portal&lt;/h3&gt;
&lt;p&gt;To figure out how to reference a GBIF mediated set of data records you may head to the 
&lt;a href=&#34;https://www.gbif.org/user/download&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;download tab&lt;/a&gt; of the GBIF portal where, once in the detailed overview of an individual download job, you will find proper accrediation instructions right at the top:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.erikkusch.com/courses/gbif/gbif-download4.jpeg&#34; width=&#34;100%&#34;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I would like to caution against this practise as the download tab can become a bit cluttered when having a long history of asking GBIF for similar downloads.&lt;/p&gt;
&lt;h3 id=&#34;rgbif&#34;&gt;&lt;code&gt;rgbif&lt;/code&gt;&lt;/h3&gt;
&lt;div class=&#34;alert alert-success&#34;&gt;
  &lt;div&gt;
    This is the &lt;strong&gt;preferred way of figuring out correct GBIF mediated data citation&lt;/strong&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;To avoid the pitfalls of manual data citation discovery, &lt;code&gt;rgbif&lt;/code&gt; download metadata make available to us directly a DOI which can be used for refrencing our data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;paste(&amp;quot;GBIF Occurrence Download&amp;quot;, occ_download_meta(res)$doi, &amp;quot;accessed via GBIF.org on&amp;quot;, Sys.Date())
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;GBIF Occurrence Download 10.15468/dl.vjazpv accessed via GBIF.org on 2024-10-30&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this, you are ready to reference the data you use.&lt;/p&gt;
&lt;h2 id=&#34;derived-datasets&#34;&gt;Derived Datasets&lt;/h2&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    When &lt;strong&gt;additionally&lt;/strong&gt; (heavily) &lt;strong&gt;filtering or subsetting&lt;/strong&gt; a GBIF download locally, it is &lt;strong&gt;best to&lt;/strong&gt; not &lt;strong&gt;cite&lt;/strong&gt; the download itself, but rather a &lt;strong&gt;derived data set&lt;/strong&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Derived datasets are citable records of GBIF-mediated occurrence data derived either from:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a GBIF.org download that has been filtered/reduced significantly, or&lt;/li&gt;
&lt;li&gt;data accessed through a cloud service, e.g. Microsoft AI for Earth (Azure), or&lt;/li&gt;
&lt;li&gt;data obtained by any means for which no DOI was assigned, but one is required (e.g. third-party tools accessing the GBIF search API)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For our purposes, we have used a heavily subsetted data download - just look at the number of records in the original and the subsetted data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(data.frame(
  Records = c(nrow(data_subset), nrow(res_data)),
  Data = c(&amp;quot;Subset&amp;quot;, &amp;quot;Original&amp;quot;)
), aes(y = Data, x = Records)) +
  geom_bar(stat = &amp;quot;identity&amp;quot;, fill = &amp;quot;#4f004e&amp;quot;) +
  theme_bw(base_size = 12)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;rgbif-dataciting_files/figure-html/barsdatasubset-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To correctly reference the underlying data sets mediated by GBIF and contributing to our final dataset, we should register a derived data set. When created, a derived dataset is assigned a unique DOI that can be used to cite the data. To create a derived dataset you will need to authenticate using a GBIF.org account and provide:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a title of the dataset,&lt;/li&gt;
&lt;li&gt;a list of the GBIF datasets (by DOI or datasetKey) from which the data originated, ideally with counts of how many records each dataset contributed,&lt;/li&gt;
&lt;li&gt;a persistent URL of where the extracted dataset can be accessed,&lt;/li&gt;
&lt;li&gt;a description of how the dataset was prepared,&lt;/li&gt;
&lt;li&gt;(optional) the GBIF download DOI, if the dataset is derived from an existing download , and&lt;/li&gt;
&lt;li&gt;(optional) a date for when the derived dataset should be registered if not immediately .&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Arguably, the most important aspect here is the list of GBIF datasets and the counts of data used per dataset. First, let&amp;rsquo;s isolate how many datasets contributed to our original data and the subsetted, final data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Original
length(unique(res_data$datasetKey))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 18
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Subsetted
length(unique(data_subset$datasetKey))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is a signifcant decrease in number of datasets which make up our final data product post-subsetting. Let us visualise how the data records are spread across the individual datasets per data product we have handled:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Originally downloaded abundances per dataset
plot_data &amp;lt;- data.frame(table(res_data$datasetKey))
ggplot(
  plot_data,
  aes(
    x = factor(Var1, levels = plot_data$Var1[order(plot_data$Freq, decreasing = TRUE)]),
    y = Freq
  )
) +
  geom_col(color = &amp;quot;black&amp;quot;, fill = &amp;quot;forestgreen&amp;quot;) +
  labs(
    y = &amp;quot;Abundance&amp;quot;, x = &amp;quot;Dataset&amp;quot;,
    title = paste0(&amp;quot;Originally downloaded abundances per dataset (&amp;quot;, occ_download_meta(res)$doi, &amp;quot;)&amp;quot;)
  ) +
  theme_bw(base_size = 21) +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;rgbif-dataciting_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Subsetted abundances per dataset
plot_subset &amp;lt;- data.frame(table(data_subset$datasetKey))
ggplot(
  plot_subset,
  aes(
    x = factor(Var1, levels = plot_subset$Var1[order(plot_subset$Freq, decreasing = TRUE)]),
    y = Freq
  )
) +
  geom_col(color = &amp;quot;black&amp;quot;, fill = &amp;quot;darkred&amp;quot;) +
  labs(y = &amp;quot;Abundance&amp;quot;, x = &amp;quot;Dataset&amp;quot;, title = &amp;quot;Subsetted abundances per dataset&amp;quot;) +
  theme_bw(base_size = 21) +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;rgbif-dataciting_files/figure-html/unnamed-chunk-3-2.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Subsequently, to prepare a potential query to GBIF to establish a derived data set for us, we can tabulate the counts of records per dataset key as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;knitr::kable(table(data_subset$datasetKey))
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Var1&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Freq&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;09c38deb-8674-446e-8be8-3347f6c094ef&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;492d63a8-4978-4bc7-acd8-7d0e3ac0e744&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;50c9509d-22c7-4a22-a47d-8c48425ef4a7&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;6a948a1c-7e23-4d99-b1c1-ec578d0d3159&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;316&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;6ac3f774-d9fb-4796-b3e9-92bf6c81c084&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;8a863029-f435-446a-821e-275f4f641165&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;114&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;b124e1e0-4755-430f-9eab-894f25a9b59c&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1176&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;b49a2978-0e30-4748-a99f-9301d17ae119&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;906&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;b848f1f3-3955-4725-8ad8-e711e4a9e0ac&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;146&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;c47f13c1-7427-45a0-9f12-237aad351040&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;48&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Finally, we can head to the 
&lt;a href=&#34;https://www.gbif.org/derived-dataset/register&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;registration for derived data sets&lt;/a&gt; at GBIF and supply our information:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.erikkusch.com/courses/gbif/gbif-derived.png&#34; width=&#34;100%&#34;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;alert alert-success&#34;&gt;
  &lt;div&gt;
    You are finished - you should now be able to find relevant data for your requirements on GBIF, download said data, handle and subset it according to your needs, and now how to reference back to the data obtained and used in your final report.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.4.0 (2024-04-24 ucrt)
## Platform: x86_64-w64-mingw32/x64
## Running under: Windows 11 x64 (build 22631)
## 
## Matrix products: default
## 
## 
## locale:
## [1] LC_COLLATE=Norwegian BokmÃ¥l_Norway.utf8  LC_CTYPE=Norwegian BokmÃ¥l_Norway.utf8   
## [3] LC_MONETARY=Norwegian BokmÃ¥l_Norway.utf8 LC_NUMERIC=C                            
## [5] LC_TIME=Norwegian BokmÃ¥l_Norway.utf8    
## 
## time zone: Europe/Oslo
## tzcode source: internal
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] ggplot2_3.5.1 rgbif_3.8.1  
## 
## loaded via a namespace (and not attached):
##  [1] styler_1.10.3     sass_0.4.9        utf8_1.2.4        generics_0.1.3   
##  [5] xml2_1.3.6        blogdown_1.19     stringi_1.8.4     httpcode_0.3.0   
##  [9] digest_0.6.37     magrittr_2.0.3    evaluate_0.24.0   grid_4.4.0       
## [13] bookdown_0.40     fastmap_1.2.0     R.oo_1.26.0       R.cache_0.16.0   
## [17] plyr_1.8.9        jsonlite_1.8.8    R.utils_2.12.3    whisker_0.4.1    
## [21] crul_1.5.0        urltools_1.7.3    httr_1.4.7        purrr_1.0.2      
## [25] fansi_1.0.6       scales_1.3.0      oai_0.4.0         lazyeval_0.2.2   
## [29] jquerylib_0.1.4   cli_3.6.3         rlang_1.1.4       triebeard_0.4.1  
## [33] R.methodsS3_1.8.2 bit64_4.0.5       munsell_0.5.1     withr_3.0.1      
## [37] cachem_1.1.0      yaml_2.3.10       tools_4.4.0       dplyr_1.1.4      
## [41] colorspace_2.1-1  curl_5.2.2        vctrs_0.6.5       R6_2.5.1         
## [45] lifecycle_1.0.4   stringr_1.5.1     bit_4.0.5         pkgconfig_2.0.3  
## [49] pillar_1.9.0      bslib_0.8.0       gtable_0.3.6      data.table_1.16.0
## [53] glue_1.7.0        Rcpp_1.0.13       highr_0.11        xfun_0.47        
## [57] tibble_3.2.1      tidyselect_1.2.1  knitr_1.48        farver_2.1.2     
## [61] htmltools_0.5.8.1 labeling_0.4.3    rmarkdown_2.28    compiler_4.4.0
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Model Selection</title>
      <link>https://www.erikkusch.com/courses/excursions-into-biostatistics/excursion-into-biostatistics/</link>
      <pubDate>Sat, 27 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/excursions-into-biostatistics/excursion-into-biostatistics/</guid>
      <description>&lt;h2 id=&#34;theory&#34;&gt;Theory&lt;/h2&gt;
&lt;p&gt;These are exercises and solutions meant as a compendium to my talk on Model Selection and Model Building.&lt;/p&gt;
&lt;p&gt;I have prepared some I have prepared some &lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/Excursions-into-Biostatistics/Model-Selection-and-Statistical-Significance---Reporting-the-Best-Science.html&#34; target=&#34;_blank&#34;&gt; Lecture Slides &lt;/a&gt; for this session. For a more mathematical look at these concepts, I cannot recommend enough 
&lt;a href=&#34;https://bookdown.org/egarpor/PM-UC3M/lm-ii-modsel.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Eduardo GarcÃ­a PortuguÃ©s&#39; blog&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;r-environment&#34;&gt;&lt;code&gt;R&lt;/code&gt; Environment&lt;/h2&gt;
&lt;p&gt;For this exercise, we will need the following packages:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.load.package &amp;lt;- function(x) {
  if (!require(x, character.only = TRUE)) {
    install.packages(x, repos = &amp;quot;http://cran.us.r-project.org&amp;quot;)
  }
  require(x, character.only = TRUE)
}
package_vec &amp;lt;- c(
  &amp;quot;ggplot2&amp;quot;, # for visualisation
  &amp;quot;leaflet&amp;quot;, # for maps
  &amp;quot;splitstackshape&amp;quot;, # for stratified sampling
  &amp;quot;caret&amp;quot;, # for cross-validation exercises
  &amp;quot;boot&amp;quot;, # for bootstrap parameter estimates
  &amp;quot;tidyr&amp;quot;, # for reshaping data frames
  &amp;quot;tidybayes&amp;quot;, # for visualisation of bootstrap estimates
  &amp;quot;pROC&amp;quot;, # for ROC-curves
  &amp;quot;olsrr&amp;quot;, # for subset selection
  &amp;quot;MASS&amp;quot;, # for stepwise subset selection
  &amp;quot;nlme&amp;quot;, # for mixed effect models
  &amp;quot;mclust&amp;quot;, # for k-means clustering,
  &amp;quot;randomForest&amp;quot;, # for randomForest classifier
  &amp;quot;lmeresampler&amp;quot; # for validation of lmer models
)
sapply(package_vec, install.load.package)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         ggplot2         leaflet splitstackshape           caret            boot           tidyr       tidybayes            pROC           olsrr            MASS            nlme          mclust 
##            TRUE            TRUE            TRUE            TRUE            TRUE            TRUE            TRUE            TRUE            TRUE            TRUE            TRUE            TRUE 
##    randomForest    lmeresampler 
##            TRUE            TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the above function is way more sophisticated than the usual &lt;code&gt;install.packages()&lt;/code&gt; &amp;amp; &lt;code&gt;library()&lt;/code&gt; approach since it automatically detects which packages require installing and only install these thus not overwriting already installed packages.&lt;/p&gt;
&lt;h2 id=&#34;our-resarch-project&#34;&gt;Our Resarch Project&lt;/h2&gt;
&lt;p&gt;Today, we are looking at a big (and entirely fictional) data base of the common house sparrow (&lt;em&gt;Passer domesticus&lt;/em&gt;). In particular, we are interested in the &lt;strong&gt;Evolution of &lt;em&gt;Passer domesticus&lt;/em&gt; in Response to Climate Change&lt;/strong&gt; which was previously explained &lt;a href=&#34;https://www.erikkusch.com/courses/excursions-into-biostatistics/research-project/&#34; target=&#34;_blank&#34;&gt; here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;the-data&#34;&gt;The Data&lt;/h3&gt;
&lt;p&gt;I have created a large data set for this exercise which is available &lt;a href=&#34;https://github.com/ErikKusch/Homepage/raw/master/content/courses/excursions-into-biostatistics/Data.rar&#34; target=&#34;_blank&#34;&gt; here&lt;/a&gt; and we previously cleaned up so that is now usable &lt;a href=&#34;https://www.erikkusch.com/courses/excursions-into-biostatistics/data-handling-and-data-assumptions/&#34; target=&#34;_blank&#34;&gt; here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;reading-the-data-into-r&#34;&gt;Reading the Data into &lt;code&gt;R&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s start by reading the data into &lt;code&gt;R&lt;/code&gt; and taking an initial look at it:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Sparrows_df &amp;lt;- readRDS(file.path(&amp;quot;Data&amp;quot;, &amp;quot;SparrowDataClimate.rds&amp;quot;))
head(Sparrows_df)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Index Latitude Longitude     Climate Population.Status Weight Height Wing.Chord Colour    Sex Nesting.Site Nesting.Height Number.of.Eggs Egg.Weight Flock Home.Range Predator.Presence Predator.Type
## 1    SI       60       100 Continental            Native  34.05  12.87       6.67  Brown   Male         &amp;lt;NA&amp;gt;             NA             NA         NA     B      Large               Yes         Avian
## 2    SI       60       100 Continental            Native  34.86  13.68       6.79   Grey   Male         &amp;lt;NA&amp;gt;             NA             NA         NA     B      Large               Yes         Avian
## 3    SI       60       100 Continental            Native  32.34  12.66       6.64  Black Female        Shrub          35.60              1       3.21     C      Large               Yes         Avian
## 4    SI       60       100 Continental            Native  34.78  15.09       7.00  Brown Female        Shrub          47.75              0         NA     E      Large               Yes         Avian
## 5    SI       60       100 Continental            Native  35.01  13.82       6.81   Grey   Male         &amp;lt;NA&amp;gt;             NA             NA         NA     B      Large               Yes         Avian
## 6    SI       60       100 Continental            Native  32.36  12.67       6.64  Brown Female        Shrub          32.47              1       3.17     E      Large               Yes         Avian
##       TAvg      TSD
## 1 269.9596 15.71819
## 2 269.9596 15.71819
## 3 269.9596 15.71819
## 4 269.9596 15.71819
## 5 269.9596 15.71819
## 6 269.9596 15.71819
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;hypotheses&#34;&gt;Hypotheses&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s remember our hypotheses:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Sparrow Morphology&lt;/strong&gt; is determined by:&lt;br&gt;
A. &lt;em&gt;Climate Conditions&lt;/em&gt; with sparrows in stable, warm environments fairing better than those in colder, less stable ones.&lt;br&gt;
B. &lt;em&gt;Competition&lt;/em&gt; with sparrows in small flocks doing better than those in big flocks.&lt;br&gt;
C. &lt;em&gt;Predation&lt;/em&gt; with sparrows under pressure of predation doing worse than those without.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sites&lt;/strong&gt;  accurately represent &lt;strong&gt;sparrow morphology&lt;/strong&gt;. This may mean:&lt;br&gt;
A. &lt;em&gt;Population status&lt;/em&gt; as inferred through morphology.&lt;br&gt;
B. &lt;em&gt;Site index&lt;/em&gt; as inferred through morphology.&lt;br&gt;
C. &lt;em&gt;Climate&lt;/em&gt; as inferred through morphology.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We have already built some models for these &lt;a href=&#34;https://www.erikkusch.com/courses/excursions-into-biostatistics/classifications-order-from-chaos/&#34; target=&#34;_blank&#34;&gt; here&lt;/a&gt; and &lt;a href=&#34;https://www.erikkusch.com/courses/excursions-into-biostatistics/regressions-correlations-for-the-advanced/&#34; target=&#34;_blank&#34;&gt; here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;candidate-models&#34;&gt;Candidate Models&lt;/h2&gt;
&lt;p&gt;Before we can get started on model selection and validation, we need some actual models. Let&amp;rsquo;s create some. Since the data set contains three variables pertaining to sparrow morphology (i.e. &lt;code&gt;Weight&lt;/code&gt;, &lt;code&gt;Height&lt;/code&gt;, &lt;code&gt;Wing.Chord&lt;/code&gt;) and I don&amp;rsquo;t want this exercise to spiral out of control with models that account for more than one response variable, we need to settle on one as our response variable in the first hypothesis. I am going with &lt;code&gt;Weight&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Additionally, because I am under a bit of time pressure in creating this material, I forego all checking of assumptions on any of the following candidate models as the goal with this material is model selection/validation and not model assumption checking.&lt;/p&gt;
&lt;h3 id=&#34;continuous-models&#34;&gt;Continuous Models&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;load(file = file.path(&amp;quot;Data&amp;quot;, &amp;quot;H1_Models.RData&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This just loaded three objects into &lt;code&gt;R&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;H1_ModelSparrows_ls&lt;/code&gt; - a list of candidate models built for the entire &lt;code&gt;Sparrow_df&lt;/code&gt; data set&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Sparrows_df&lt;/code&gt; - the data frame used to build the global candidate models&lt;/li&gt;
&lt;li&gt;&lt;code&gt;H1_ModelCNA_ls&lt;/code&gt; - a list of candidate models built just for three coastal sites across Central and North America&lt;/li&gt;
&lt;li&gt;&lt;code&gt;CentralNorthAm_df&lt;/code&gt; - the data frame used to build the candidate model for Central and North America&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;global-models&#34;&gt;Global Models&lt;/h4&gt;
&lt;p&gt;Global regression models include:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sapply(H1_ModelSparrows_ls, &amp;quot;[[&amp;quot;, &amp;quot;call&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $Null
## lm(formula = Weight ~ 1, data = Sparrows_df)
## 
## $Comp_Flock.Size
## lm(formula = Weight ~ Flock.Size, data = Sparrows_df)
## 
## $Comp_Full
## lm(formula = Weight ~ Home.Range * Flock.Size, data = Sparrows_df)
## 
## $Full
## lm(formula = Weight ~ Climate + TAvg + TSD + Home.Range * Flock.Size + 
##     Predator.Type, data = Sparrows_df)
## 
## $Mixed_Full
## lme.formula(fixed = Weight ~ Predator.Type + Flock.Size * Home.Range + 
##     TAvg + TSD, data = Sparrows_df, random = list(Population.Status = ~1))
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;local-models&#34;&gt;Local Models&lt;/h4&gt;
&lt;p&gt;Local regression models for the region of Central/North America include:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sapply(H1_ModelCNA_ls, &amp;quot;[[&amp;quot;, &amp;quot;call&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $Null
## lm(formula = Weight ~ 1, data = CentralNorthAm_df)
## 
## $Clim_TAvg
## lm(formula = Weight ~ TAvg, data = CentralNorthAm_df)
## 
## $Clim_TSD
## lm(formula = Weight ~ TSD, data = CentralNorthAm_df)
## 
## $Clim_Full
## lm(formula = Weight ~ TAvg + TSD, data = CentralNorthAm_df)
## 
## $Pred_Pres
## lm(formula = Weight ~ Predator.Presence, data = CentralNorthAm_df)
## 
## $Pred_Type
## lm(formula = Weight ~ Predator.Type, data = CentralNorthAm_df)
## 
## $Full
## lm(formula = Weight ~ TAvg + TSD + Home.Range * Flock.Size + 
##     Predator.Type, data = CentralNorthAm_df)
## 
## $Mixed_Full
## lme.formula(fixed = Weight ~ Flock.Size * Home.Range + TAvg + 
##     TSD, data = CentralNorthAm_df, random = list(Index = ~1))
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;categorical-models&#34;&gt;Categorical Models&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;load(file = file.path(&amp;quot;Data&amp;quot;, &amp;quot;H2_Models.RData&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This just loaded three objects into &lt;code&gt;R&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;H2_PS_mclust&lt;/code&gt; - a k-means classifier aiming to group &lt;code&gt;Population.Status&lt;/code&gt; by &lt;code&gt;Weight&lt;/code&gt;, &lt;code&gt;Height&lt;/code&gt;, and &lt;code&gt;Wing.Chord&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;H2_PS_RF&lt;/code&gt; - a random forest classifier which identifies &lt;code&gt;Population.Status&lt;/code&gt; by &lt;code&gt;Weight&lt;/code&gt;, &lt;code&gt;Height&lt;/code&gt;, and &lt;code&gt;Wing.Chord&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;H2_Index_RF&lt;/code&gt; - a random forest classifier which identifies &lt;code&gt;Index&lt;/code&gt; of sites by &lt;code&gt;Weight&lt;/code&gt;, &lt;code&gt;Height&lt;/code&gt;, and &lt;code&gt;Wing.Chord&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;model-comparisonselection&#34;&gt;Model Comparison/Selection&lt;/h2&gt;
&lt;h3 id=&#34;adjusted-coefficient-of-determination&#34;&gt;(adjusted) Coefficient of Determination&lt;/h3&gt;
&lt;p&gt;The coefficient of determination ($R^2$) measures the proportion of variation in our response (&lt;code&gt;Weight&lt;/code&gt;) that can be explained by regression using our predictor(s). The higher this value, the better. Unfortunately, $R^2$ does not penalize complex models (i.e. those with multiple parameters) while the adjusted $R^2$ does. Extracting these for a model object is as easy as writing:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ExampleModel &amp;lt;- H1_ModelSparrows_ls$Comp_Flock.Size
summary(ExampleModel)$r.squared
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7837715
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(ExampleModel)$adj.r.squared
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7835683
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This tells us that the flock size model explains roughly 0.784% of the variation in the &lt;code&gt;Weight&lt;/code&gt; variable. That is pretty decent.&lt;/p&gt;
&lt;p&gt;To check for all other models, I have written a quick &lt;code&gt;sapply&lt;/code&gt; function that does the extraction for us. Because obtaining (adjusted) $R^2$ requires additional packages, I am excluding these from this analysis:&lt;/p&gt;
&lt;h4 id=&#34;global-regression-models&#34;&gt;Global Regression Models&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;H1_Summary_ls &amp;lt;- sapply(H1_ModelSparrows_ls[-length(H1_ModelSparrows_ls)], summary)
R2_df &amp;lt;- data.frame(
  R2 = sapply(H1_Summary_ls, &amp;quot;[[&amp;quot;, &amp;quot;r.squared&amp;quot;),
  Adj.R2 = sapply(H1_Summary_ls, &amp;quot;[[&amp;quot;, &amp;quot;adj.r.squared&amp;quot;)
)
R2_df
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                        R2    Adj.R2
## Null            0.0000000 0.0000000
## Comp_Flock.Size 0.7837715 0.7835683
## Comp_Full       0.8051421 0.8042229
## Full            0.8460500 0.8444433
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can immediately see that some of our candidate models are doing quite well for themselves.&lt;/p&gt;
&lt;h4 id=&#34;local-regression-models&#34;&gt;Local Regression Models&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;H1_Summary_ls &amp;lt;- sapply(H1_ModelCNA_ls[-length(H1_ModelCNA_ls)], summary)
R2_df &amp;lt;- data.frame(
  R2 = sapply(H1_Summary_ls, &amp;quot;[[&amp;quot;, &amp;quot;r.squared&amp;quot;),
  Adj.R2 = sapply(H1_Summary_ls, &amp;quot;[[&amp;quot;, &amp;quot;adj.r.squared&amp;quot;)
)
R2_df
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                   R2     Adj.R2
## Null      0.00000000 0.00000000
## Clim_TAvg 0.23733707 0.23426182
## Clim_TSD  0.32632351 0.32360707
## Clim_Full 0.34671348 0.34142371
## Pred_Pres 0.03710799 0.03322536
## Pred_Type 0.34671348 0.34142371
## Full      0.37651991 0.35848536
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Oof! None of our locally fitted models did well at explaining the data to begin with. With that identified, we are sure not going to trust them when it comes to predictions and so we are scrapping all of them.&lt;/p&gt;
&lt;p&gt;Consequently, we can generalise our naming conventions a bit and now write:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;H1_Model_ls &amp;lt;- H1_ModelSparrows_ls
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;anova&#34;&gt;Anova&lt;/h3&gt;
&lt;p&gt;Analysis of Variance (Anova) is another tool you will often run into when trying to understand explanatory power of a model. Here, I do something relatively complex to run an anova for all models in our list without having to type them all out. Again,we omit the mixed effect model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;eval(parse(text = paste(&amp;quot;anova(&amp;quot;, paste(&amp;quot;H1_Model_ls[[&amp;quot;, 1:(length(H1_Model_ls) - 1), &amp;quot;]]&amp;quot;, sep = &amp;quot;&amp;quot;, collapse = &amp;quot;,&amp;quot;), &amp;quot;)&amp;quot;)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Variance Table
## 
## Model 1: Weight ~ 1
## Model 2: Weight ~ Flock.Size
## Model 3: Weight ~ Home.Range * Flock.Size
## Model 4: Weight ~ Climate + TAvg + TSD + Home.Range * Flock.Size + Predator.Type
##   Res.Df     RSS Df Sum of Sq        F    Pr(&amp;gt;F)    
## 1   1065 17627.2                                    
## 2   1064  3811.5  1   13815.7 5365.996 &amp;lt; 2.2e-16 ***
## 3   1060  3434.8  4     376.7   36.578 &amp;lt; 2.2e-16 ***
## 4   1054  2713.7  6     721.1   46.678 &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, according to this, all of our models are doing much better in explaining our underlying data when compared to the Null Model.&lt;/p&gt;
&lt;h3 id=&#34;information-criteria&#34;&gt;Information Criteria&lt;/h3&gt;
&lt;p&gt;Personally, I would like to have a model that&amp;rsquo;s good at predicting things instead of &amp;ldquo;just&amp;rdquo; explaining things and so we step into &lt;em&gt;information criteria&lt;/em&gt; next. These aim to provide us with exactly that information: &amp;ldquo;How well will our model predict new data?&amp;rdquo; Information criteria make use of information theory which allows us to make such statements with pretty decent certainty despite not having new data.&lt;/p&gt;
&lt;h4 id=&#34;akaike-information-criterion-aic&#34;&gt;Akaike Information Criterion (AIC)&lt;/h4&gt;
&lt;p&gt;Looking at the AIC:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sapply(H1_Model_ls, AIC)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            Null Comp_Flock.Size       Comp_Full            Full      Mixed_Full 
##        6019.872        4389.378        4286.445        4047.250        4162.779
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our full model is the clear favourite here.&lt;/p&gt;
&lt;h4 id=&#34;bayesian-information-criterion-bic&#34;&gt;Bayesian Information Criterion (BIC)&lt;/h4&gt;
&lt;p&gt;As far as the BIC is concerned:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sapply(H1_Model_ls, BIC)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            Null Comp_Flock.Size       Comp_Full            Full      Mixed_Full 
##        6029.815        4404.293        4321.247        4111.882        4222.326
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our full model wins again!&lt;/p&gt;
&lt;h4 id=&#34;receiver-operator-characteristic-roc&#34;&gt;Receiver-Operator Characteristic (ROC)&lt;/h4&gt;
&lt;p&gt;The Receiver-Operator Characteristic (ROC) shows the trade-off between &lt;em&gt;Sensitivity&lt;/em&gt; (rate of true positives) and &lt;em&gt;Specificity&lt;/em&gt; (rate of true negatives). It also provides an &lt;em&gt;Area under the Curve&lt;/em&gt; which serves as a proxy of classification accuracy.&lt;/p&gt;
&lt;p&gt;First, we establish the ROC-Curve for our classification of &lt;code&gt;Population.Status&lt;/code&gt; given sparrow Morphology and a k-means algorithm:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Mclust_PS.roc &amp;lt;- roc(
  Sparrows_df$Population.Status, # known outcome
  H2_PS_mclust$z[, 1] # probability of assigning one out of two outcomes
)
plot(Mclust_PS.roc)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;7_Model-Selection-and-Statistical-Significance---Reporting-the-Best-Science_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;auc(Mclust_PS.roc)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Area under the curve: 0.6341
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Certainly, we could do better! Let&amp;rsquo;s see what more advanced methods have to offer.&lt;/p&gt;
&lt;p&gt;With that, we turn to random forest:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;RF_PS.roc &amp;lt;- roc(
  Sparrows_df$Population.Status,
  H2_PS_RF$votes[, 1]
)
plot(RF_PS.roc)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;7_Model-Selection-and-Statistical-Significance---Reporting-the-Best-Science_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;auc(RF_PS.roc)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Area under the curve: 0.9274
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now this is doing much better!&lt;/p&gt;
&lt;p&gt;Lastly, we want to look at the site &lt;code&gt;Index&lt;/code&gt; as predicted by sparrow morphology given a random forest algorithm:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;RF_Index.roc &amp;lt;- multiclass.roc(
  Sparrows_df$Index, # known outcome
  H2_Index_RF$votes # matrix of certainties of prediction
)
RF_Index.roc[[&amp;quot;auc&amp;quot;]] # average ROC-AUC
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Multi-class area under the curve: 0.9606
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Plot ROC curve for each binary comparison
rs &amp;lt;- RF_Index.roc[[&amp;quot;rocs&amp;quot;]] ## extract comparisons
plot.roc(rs[[1]][[1]]) # blot first comparison
plot.roc(rs[[1]][[2]], add = TRUE) # plot first comparison, in opposite direction
invisible(capture.output(sapply(2:length(rs), function(i) lines.roc(rs[[i]][[1]], col = i))))
invisible(capture.output(sapply(2:length(rs), function(i) lines.roc(rs[[i]][[2]], col = i))))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;7_Model-Selection-and-Statistical-Significance---Reporting-the-Best-Science_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is certainly busy, but look at that average AUC of almost 1! That is the power of Random Forest.&lt;/p&gt;
&lt;h3 id=&#34;summary-of-model-selection&#34;&gt;Summary of Model Selection&lt;/h3&gt;
&lt;h4 id=&#34;morphology-hypothesis&#34;&gt;Morphology Hypothesis&lt;/h4&gt;
&lt;p&gt;Regarding our morphology hypothesis, we saw that most of our hypothesised effects can be detected. However, some models clearly perform better than others. Usual model selection exercises would have us discard all but the best model (&lt;code&gt;Full&lt;/code&gt;, in this case) and leave the rest never to be spoken of again. Doing so would have us miss a pretty neat opportunity to do some &lt;strong&gt;model comparison&lt;/strong&gt; which can already help us identify which effects to focus on in particular.&lt;/p&gt;
&lt;p&gt;To demonstrate some of this, allow me step into the local regression models:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sapply(H1_ModelCNA_ls, AIC)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Null  Clim_TAvg   Clim_TSD  Clim_Full  Pred_Pres  Pred_Type       Full Mixed_Full 
##   948.7346   882.9998   851.9833   846.2997   941.2811   846.2997   844.6250   875.7659
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;as well as global regression models:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sapply(H1_Model_ls, AIC)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            Null Comp_Flock.Size       Comp_Full            Full      Mixed_Full 
##        6019.872        4389.378        4286.445        4047.250        4162.779
&lt;/code&gt;&lt;/pre&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Climate&lt;/em&gt; - interestingly,  temperature variability is much more informative than average temperature and even adding the two into the same model only marginally improves over the variability-only model. This tells us much about which effects are probably meaningful and which aren&amp;rsquo;t.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Competition&lt;/em&gt; -  The competition models did well across the board, but were aided immensely by adding climate information and accounting for random effects.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Predation&lt;/em&gt; - predation effects were best explained by predation type with only a marginal improvement of adding predator presence. That is because predator type already contains all of the information that is within predator presence.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;What we can do so far, is remove some obviously erroneous models which in this case is the entirety of local regression models.&lt;/p&gt;
&lt;h4 id=&#34;categorisation-hypothesis&#34;&gt;Categorisation Hypothesis&lt;/h4&gt;
&lt;p&gt;As far as the categorisation hypotheses are concerned, we now have confirmation that population status and sparrow morphology are linked quite well.&lt;/p&gt;
&lt;p&gt;We have also learned that random forest is an incredibly powerful method for classification and variable selection.&lt;/p&gt;
&lt;h2 id=&#34;model-validation&#34;&gt;Model Validation&lt;/h2&gt;
&lt;p&gt;So far, we have not introduced our models to any new data. We have looked at &lt;em&gt;explanatory power&lt;/em&gt; with (adjusted) $R^2$, and the Anova. We have also looked at &lt;em&gt;estimates of predictive power&lt;/em&gt; with our information criteria (e.g. AIC, BIC).&lt;/p&gt;
&lt;p&gt;What about actually seeing how robust and accurate our models are? That&amp;rsquo;s what Model Validation is for!&lt;/p&gt;
&lt;h3 id=&#34;cross-validation&#34;&gt;Cross-Validation&lt;/h3&gt;
&lt;p&gt;Before we get started, I remove the Null model from our model list. Doing cross-validation on this does not make any sense because there are no actual predictors in it which could be affected by cross-validation processes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;H1_Model_ls &amp;lt;- H1_Model_ls[-1]
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;training-vs-test-data&#34;&gt;Training vs. Test Data&lt;/h4&gt;
&lt;p&gt;The simplest example of cross-validation is the &lt;em&gt;validation data cross-validation&lt;/em&gt; approach; also known as &lt;strong&gt;Training vs. Test Data&lt;/strong&gt; approach.&lt;/p&gt;
&lt;p&gt;To make use of this approach, we need to (1) randomly split our data, (2) build our models using the training data, and (3) test our models on the test data.&lt;/p&gt;
&lt;p&gt;Since we have highly compartmentalised data at different sites, I am employing a stratified sampling scheme to ensure all of my sites are represented in each data set resulting from the split:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(42) # make randomness reproducible
Stratified_ls &amp;lt;- stratified(Sparrows_df, # what to split
  group = &amp;quot;Index&amp;quot;, # by which group to stratify
  size = .7, # what proportion of each group shall be contained in the training data
  bothSets = TRUE # save both training and test data
)
Train_df &amp;lt;- Stratified_ls$SAMP1 # extract training data
Test_df &amp;lt;- Stratified_ls$SAMP2 # extract test data
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have our training and test data, we are ready to run our pre-specified models on said data and subsequently test it&amp;rsquo;s performance on the test data by predicting with the newly trained model and calculating mean squared test error.&lt;/p&gt;
&lt;p&gt;For a single model, we can do it like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ExampleModel &amp;lt;- H1_ModelSparrows_ls$Comp_Flock.Size # extract Model from list
ExampleModel &amp;lt;- update(ExampleModel, data = Train_df) # train model on training data
Prediction &amp;lt;- predict(ExampleModel, newdata = Test_df) # predict outcome for test data
sum((Test_df$Weight - Prediction)^2) # Mean Squared Error
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1133.996
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since we have multiple models stored in a list, here&amp;rsquo;s a way to do the above for the entire list:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;H1_Train_ls &amp;lt;- sapply(X = H1_Model_ls, FUN = function(x) update(x, data = Train_df))
H1_Test_mat &amp;lt;- sapply(X = H1_Train_ls, FUN = function(x) predict(x, newdata = Test_df))
apply(H1_Test_mat, MARGIN = 2, FUN = function(x) sum((Test_df$Weight - x)^2))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Comp_Flock.Size       Comp_Full            Full      Mixed_Full 
##       1133.9958       1026.2199        816.5166        866.2941
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, our full model comes out on top!&lt;/p&gt;
&lt;p&gt;Unfortunately, this approach is fickle due to the randomness of the data split. How can we make this more robust? Easy. We split many, many times and average our mean squared errors out.&lt;/p&gt;
&lt;p&gt;This bring us to traditional Cross-Validation approaches. Luckily, the complex parts of cross-validation are already offered to us with the &lt;code&gt;caret&lt;/code&gt; package in &lt;code&gt;R&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;leave-one-out-cross-validation-loocv&#34;&gt;Leave-One-Out Cross-Validation (LOOCV)&lt;/h4&gt;
&lt;p&gt;Leave-One-Out Cross-Validation is a method within which we split our data into a training data set with $n-1$ observation and a test data set that contains just $1$ observation. We do training and testing as above on this split and then repeat this procedure until every observation has been left out once.&lt;/p&gt;
&lt;p&gt;For a simple model, this can be done like such:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;train(Weight ~ Climate,
  data = Sparrows_df,
  method = &amp;quot;lm&amp;quot;,
  trControl = trainControl(method = &amp;quot;LOOCV&amp;quot;)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear Regression 
## 
## 1066 samples
##    1 predictor
## 
## No pre-processing
## Resampling: Leave-One-Out Cross-Validation 
## Summary of sample sizes: 1065, 1065, 1065, 1065, 1065, 1065, ... 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   3.628905  0.2036173  2.976221
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice the RMSE (Residual mean squared error). That&amp;rsquo;s what we use to compare models.&lt;/p&gt;
&lt;p&gt;Here, I create a function that automatically rebuilds our models for the LOOCV so we can run this on our list of models later.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;CV_LOOCV &amp;lt;- function(x) {
  if (length(x[[&amp;quot;terms&amp;quot;]][[3]]) == 1) {
    Terms &amp;lt;- paste(x[[&amp;quot;terms&amp;quot;]][[3]], collapse = &amp;quot; + &amp;quot;)
  } else {
    Terms &amp;lt;- paste(x[[&amp;quot;terms&amp;quot;]][[3]][-1], collapse = &amp;quot; + &amp;quot;)
  }
  train(as.formula(paste(&amp;quot;Weight ~&amp;quot;, Terms)),
    data = Sparrows_df,
    method = &amp;quot;lm&amp;quot;,
    trControl = trainControl(method = &amp;quot;LOOCV&amp;quot;)
  )
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unfortunately, this cannot be executed for mixed effect models, so for now, I only run this on all our models except the mixed effect model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Begin &amp;lt;- Sys.time()
H1_LOOCV_ls &amp;lt;- sapply(H1_Model_ls[-length(H1_Model_ls)], CV_LOOCV)
End &amp;lt;- Sys.time()
End - Begin
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Time difference of 9.41841 secs
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sapply(H1_LOOCV_ls, &amp;quot;[[&amp;quot;, &amp;quot;results&amp;quot;)[-1, ]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          Comp_Flock.Size Comp_Full Full     
## RMSE     1.894279        1.865854  1.609296 
## Rsquared 0.7829992       0.7894634 0.8433834
## MAE      1.520181        1.492409  1.279003
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unsurprisingly, our full model has the lowest RMSE (which is the mark of a good model).&lt;/p&gt;
&lt;p&gt;So what about our mixed effect model? Luckily, doing LOOCV by hand isn&amp;rsquo;t all that difficult and so we can still compute a RMSE for LOOCV for our mixed effect model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;RMSE_LOOCV &amp;lt;- rep(NA, nrow(Sparrows_df))
for (Fold_Iter in 1:nrow(Sparrows_df)) {
  Iter_mod &amp;lt;- update(H1_Model_ls$Mixed_Full, data = Sparrows_df[-Fold_Iter, ])
  Prediction &amp;lt;- predict(Iter_mod, newdata = Sparrows_df[Fold_Iter, ])
  RMSE_LOOCV[Fold_Iter] &amp;lt;- (Sparrows_df[Fold_Iter, ]$Weight - Prediction)^2
}
mean(RMSE_LOOCV)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.757373
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ouh&amp;hellip; that is quite worse than out other models. Curious. This goes to show how much less robust a more complex model can be.&lt;/p&gt;
&lt;h4 id=&#34;k-fold-cross-validation-k-fold-cv&#34;&gt;k-Fold Cross-Validation (k-fold CV)&lt;/h4&gt;
&lt;p&gt;k-Fold Cross-Validation uses the same concept as all of the previous cross-validation methods, but at less of a computational cost than LOOCV and more robustly than the training/test data approach:&lt;/p&gt;
&lt;p&gt;Again, I write a function for this and run it on my list of models without the mixed effect model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;CV_kFold &amp;lt;- function(x) {
  if (length(x[[&amp;quot;terms&amp;quot;]][[3]]) == 1) {
    Terms &amp;lt;- paste(x[[&amp;quot;terms&amp;quot;]][[3]], collapse = &amp;quot; + &amp;quot;)
  } else {
    Terms &amp;lt;- paste(x[[&amp;quot;terms&amp;quot;]][[3]][-1], collapse = &amp;quot; + &amp;quot;)
  }
  train(as.formula(paste(&amp;quot;Weight ~&amp;quot;, Terms)),
    data = Sparrows_df,
    method = &amp;quot;lm&amp;quot;,
    trControl = trainControl(method = &amp;quot;cv&amp;quot;, number = 15)
  )
}
Begin &amp;lt;- Sys.time()
H1_kFold_ls &amp;lt;- sapply(H1_Model_ls[-length(H1_Model_ls)], CV_kFold)
End &amp;lt;- Sys.time()
End - Begin
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Time difference of 0.3413441 secs
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sapply(H1_kFold_ls, &amp;quot;[[&amp;quot;, &amp;quot;results&amp;quot;)[-1, ]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            Comp_Flock.Size Comp_Full  Full      
## RMSE       1.889439        1.859135   1.603168  
## Rsquared   0.7882333       0.7942782  0.8465977 
## MAE        1.519962        1.491493   1.277595  
## RMSESD     0.1408563       0.1520344  0.1491081 
## RsquaredSD 0.03375562      0.03153792 0.03034729
## MAESD      0.1382304       0.1122565  0.1150599
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Full model performs best still and see how much quicker that was done!&lt;/p&gt;
&lt;h3 id=&#34;bootstrap&#34;&gt;Bootstrap&lt;/h3&gt;
&lt;p&gt;On to the Bootstrap. God, I love the boostrap.&lt;/p&gt;
&lt;p&gt;The idea here is to run a model multiple times on a random sample of the underlying data and then store all of the estimates or the parameters as well as avaerage out the RMSE:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;BootStrap &amp;lt;- function(x) {
  if (length(x[[&amp;quot;terms&amp;quot;]][[3]]) == 1) {
    Terms &amp;lt;- paste(x[[&amp;quot;terms&amp;quot;]][[3]], collapse = &amp;quot; + &amp;quot;)
  } else {
    Terms &amp;lt;- paste(x[[&amp;quot;terms&amp;quot;]][[3]][-1], collapse = &amp;quot; + &amp;quot;)
  }
  train(as.formula(paste(&amp;quot;Weight ~&amp;quot;, Terms)),
    data = Sparrows_df,
    method = &amp;quot;lm&amp;quot;,
    trControl = trainControl(method = &amp;quot;boot&amp;quot;, number = 100)
  )
}
Begin &amp;lt;- Sys.time()
H1_BootStrap_ls &amp;lt;- sapply(H1_Model_ls[-length(H1_Model_ls)], BootStrap)
End &amp;lt;- Sys.time()
End - Begin
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Time difference of 1.308739 secs
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sapply(H1_BootStrap_ls, &amp;quot;[[&amp;quot;, &amp;quot;results&amp;quot;)[-1, ]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            Comp_Flock.Size Comp_Full  Full      
## RMSE       1.893652        1.871873   1.622079  
## Rsquared   0.7835412       0.7896534  0.8425108 
## MAE        1.520457        1.498216   1.288087  
## RMSESD     0.04675792      0.05178669 0.04885059
## RsquaredSD 0.01243994      0.01318599 0.01092356
## MAESD      0.04287091      0.04531032 0.03910463
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The full model is still doing great, of course.&lt;/p&gt;
&lt;p&gt;But what about our mixed effect model? Luckily, there is a function that can do bootstrapping for us on our &lt;code&gt;lme&lt;/code&gt; objects:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Bootstrap mixed model
Mixed_boot &amp;lt;- lmeresampler::bootstrap(H1_Model_ls[[length(H1_Model_ls)]], .f = fixef, type = &amp;quot;parametric&amp;quot;, B = 3e3)
Mixed_boot
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Bootstrap type: parametric 
## 
## Number of resamples: 3000 
## 
##                           term      observed      rep.mean          se          bias
## 1                  (Intercept)  2.212717e+01 22.0672111291 2.853845096 -0.0599612233
## 2       Predator.TypeNon-Avian  6.626664e-01  0.6580310750 0.161081567 -0.0046353000
## 3            Predator.TypeNone  2.694373e-02  0.0212966961 0.152739708 -0.0056470340
## 4                   Flock.Size  1.497092e-05  0.0005839265 0.019216411  0.0005689556
## 5             Home.RangeMedium  1.261878e+00  1.2675791500 0.881426872  0.0057008365
## 6              Home.RangeSmall  3.049068e+00  3.0583903125 0.417796779  0.0093225898
## 7                         TAvg  3.015153e-02  0.0303345903 0.009892483  0.0001830556
## 8                          TSD  1.983744e-01  0.1984962823 0.021196164  0.0001219321
## 9  Flock.Size:Home.RangeMedium -1.208598e-01 -0.1213017777 0.057929474 -0.0004419594
## 10  Flock.Size:Home.RangeSmall -2.110972e-01 -0.2117971129 0.019731749 -0.0006998822
## 
## There were 0 messages, 0 warnings, and 0 errors.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this, we are getting into the heart of the bootstrap. Distributions of our parameter estimates. These give us an amazing understanding of just which parameter values our model sees as plausible given our data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Estimates_df &amp;lt;- data.frame(Mixed_boot[[&amp;quot;replicates&amp;quot;]])
## reshape estimates data frame for plotting
Hist_df &amp;lt;- data.frame(pivot_longer(
  data = Estimates_df,
  cols = colnames(Estimates_df)
))
## plot parameter estimate distributions
ggplot(data = Hist_df, aes(x = value, group = name)) +
  tidybayes::stat_pointinterval() +
  tidybayes::stat_dots() +
  facet_wrap(~name, scales = &amp;quot;free&amp;quot;) +
  labs(
    x = &amp;quot;Parameter Estimate&amp;quot;, y = &amp;quot;Parameter&amp;quot;,
    title = paste(&amp;quot;Bootstrap parameter estimates of&amp;quot;, names(H1_Model_ls[[length(H1_Model_ls)]]), &amp;quot;Model&amp;quot;)
  ) +
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;7_Model-Selection-and-Statistical-Significance---Reporting-the-Best-Science_files/figure-html/unnamed-chunk-30-1.png&#34; width=&#34;1440&#34; /&gt;
As you can see for our mixed effect model, while most parameter estimates are nicely constrained, the Intercept estimate can vary wildly. This is likely to do with our model being very flexible and allowing for a bunch of different combinations of intercepts.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s do the same for our remaining three candidate models:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;BootPlot_ls &amp;lt;- as.list(rep(NA, (length(H1_Model_ls) - 1)))
for (Model_Iter in 1:(length(H1_Model_ls) - 1)) { # loop over all models except the null model
  ## Formula to compute coefficients
  x &amp;lt;- H1_Model_ls[[Model_Iter]]
  if (length(x[[&amp;quot;terms&amp;quot;]][[3]]) == 1) {
    Terms &amp;lt;- as.character(x[[&amp;quot;terms&amp;quot;]][[3]])
  } else {
    Terms &amp;lt;- paste(as.character(x[[&amp;quot;terms&amp;quot;]][[3]])[-1], collapse = as.character(x[[&amp;quot;terms&amp;quot;]][[3]])[1])
  }
  model_coef &amp;lt;- function(data, index) {
    coef(lm(as.formula(paste(&amp;quot;Weight ~&amp;quot;, Terms)), data = data, subset = index))
  }
  ## Bootstrapping
  Boot_test &amp;lt;- boot(data = Sparrows_df, statistic = model_coef, R = 3e3)
  ## set column names of estimates to coefficients
  colnames(Boot_test[[&amp;quot;t&amp;quot;]]) &amp;lt;- names(H1_Model_ls[[Model_Iter]][[&amp;quot;coefficients&amp;quot;]])
  ## make data frame of estimates
  Estimates_df &amp;lt;- data.frame(Boot_test[[&amp;quot;t&amp;quot;]])
  ## reshape estimates data frame for plotting
  Hist_df &amp;lt;- data.frame(pivot_longer(
    data = Estimates_df,
    cols = colnames(Estimates_df)
  ))
  ## plot parameter estimate distributions
  BootPlot_ls[[Model_Iter]] &amp;lt;- ggplot(data = Hist_df, aes(x = value, group = name)) +
    tidybayes::stat_pointinterval() +
    tidybayes::stat_dots() +
    facet_wrap(~name, scales = &amp;quot;free&amp;quot;) +
    labs(
      x = &amp;quot;Parameter Estimate&amp;quot;, y = &amp;quot;Parameter&amp;quot;,
      title = paste(&amp;quot;Bootstrap parameter estimates of&amp;quot;, names(H1_Model_ls)[[Model_Iter]], &amp;quot;Model&amp;quot;),
      subtitle = paste(&amp;quot;Weight ~&amp;quot;, Terms)
    ) +
    theme_bw()
}
BootPlot_ls[[1]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;7_Model-Selection-and-Statistical-Significance---Reporting-the-Best-Science_files/figure-html/unnamed-chunk-31-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;BootPlot_ls[[2]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;7_Model-Selection-and-Statistical-Significance---Reporting-the-Best-Science_files/figure-html/unnamed-chunk-31-2.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;BootPlot_ls[[3]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;7_Model-Selection-and-Statistical-Significance---Reporting-the-Best-Science_files/figure-html/unnamed-chunk-31-3.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;subset-selection&#34;&gt;Subset Selection&lt;/h2&gt;
&lt;p&gt;So far, we have built our own models according to out intuition. Did we test all possible models? No. Should we go back and test all possible models by hand? Hell no! Can we let &lt;code&gt;R&lt;/code&gt; do it for us? You bet we can!&lt;/p&gt;
&lt;h3 id=&#34;best-subset-selection&#34;&gt;Best Subset Selection&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s start with best subset selection. Doing so asks us/&lt;code&gt;R&lt;/code&gt; to establish all possible models and then select the one that performs best according to information criteria. Because our data set contains over 20 variables, including all of our variables would have us establish close to 1 million (you read that right) models. THat is, of course, infeasible.&lt;/p&gt;
&lt;p&gt;Therefore, let&amp;rsquo;s just allow our subset selection to use all variables we have used ourselves thus far (with the exclusion of &lt;code&gt;Index&lt;/code&gt; because it&amp;rsquo;s an amazing, but ultimately useless shorthand):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Reduced_df &amp;lt;- Sparrows_df[, c(&amp;quot;Weight&amp;quot;, &amp;quot;Climate&amp;quot;, &amp;quot;TAvg&amp;quot;, &amp;quot;TSD&amp;quot;, &amp;quot;Population.Status&amp;quot;, &amp;quot;Flock.Size&amp;quot;, &amp;quot;Predator.Type&amp;quot;, &amp;quot;Predator.Presence&amp;quot;)] # reduce data
model &amp;lt;- lm(Weight ~ ., data = Reduced_df) # specify full model
k &amp;lt;- ols_step_best_subset(model) # create all models and select the best
k # show us comparison of best subsets
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                   Best Subsets Regression                                   
## --------------------------------------------------------------------------------------------
## Model Index    Predictors
## --------------------------------------------------------------------------------------------
##      1         Flock.Size                                                                    
##      2         Climate Flock.Size                                                            
##      3         Climate TAvg Flock.Size                                                       
##      4         Climate TAvg Flock.Size Predator.Type                                         
##      5         Climate TAvg TSD Flock.Size Predator.Type                                     
##      6         Climate TAvg TSD Population.Status Flock.Size Predator.Type                   
##      7         Climate TAvg TSD Population.Status Flock.Size Predator.Type Predator.Presence 
## --------------------------------------------------------------------------------------------
## 
##                                                     Subsets Regression Summary                                                    
## ----------------------------------------------------------------------------------------------------------------------------------
##                        Adj.        Pred                                                                                            
## Model    R-Square    R-Square    R-Square      C(p)         AIC       SBIC       SBC         MSEP        FPE       HSP       APC  
## ----------------------------------------------------------------------------------------------------------------------------------
##   1        0.7838      0.7836       0.783    298.7733    4389.3782      NA    4404.2932    3818.6664    3.5890    0.0034    0.2170 
##   2        0.8175      0.8169      0.8163     88.8025    4212.8692      NA    4237.7276    3226.8597    3.0384    0.0029    0.1836 
##   3        0.8227      0.8220      0.8213     58.0852    4184.0693      NA    4213.8994    3137.9149    2.9575    0.0028    0.1787 
##   4        0.8315      0.8305      0.8296      4.5702    4133.6815      NA    4173.4549    2984.6456    2.8183    0.0026    0.1701 
##   5        0.8320      0.8309      0.8298      3.3977    4132.4880      NA    4177.2330    2978.5274    2.8151    0.0026    0.1699 
##   6        0.8320      0.8308      0.8296      5.0000    4134.0870      NA    4183.8036    2980.2214    2.8194    0.0026    0.1702 
##   7        0.8320      0.8308      0.8296      5.0000    4136.0870      NA    4190.7753    2980.2214    2.8194    0.0026    0.1702 
## ----------------------------------------------------------------------------------------------------------------------------------
## AIC: Akaike Information Criteria 
##  SBIC: Sawa&#39;s Bayesian Information Criteria 
##  SBC: Schwarz Bayesian Criteria 
##  MSEP: Estimated error of prediction, assuming multivariate normality 
##  FPE: Final Prediction Error 
##  HSP: Hocking&#39;s Sp 
##  APC: Amemiya Prediction Criteria
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Model 5 (&lt;code&gt;Climate TAvg TSD Flock.Size Predator.Type &lt;/code&gt;) is the one we want to go for here.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s look at visualisation of our different model selection criteria:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(k)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;7_Model-Selection-and-Statistical-Significance---Reporting-the-Best-Science_files/figure-html/unnamed-chunk-33-1.png&#34; width=&#34;1440&#34; /&gt;&lt;img src=&#34;7_Model-Selection-and-Statistical-Significance---Reporting-the-Best-Science_files/figure-html/unnamed-chunk-33-2.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;forward-subset-selection&#34;&gt;Forward Subset Selection&lt;/h3&gt;
&lt;p&gt;Ok. So best subset selection can become intractable given a lot of variables. How about building our models up to be increasingly complex until we hit on gold?&lt;/p&gt;
&lt;p&gt;Unfortunately, doing so does not guarantee finding an optimal model and can easily get stuck, depending on what the model starts off with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;model &amp;lt;- lm(Weight ~ Climate, data = Reduced_df)
step.model &amp;lt;- stepAIC(model,
  direction = &amp;quot;forward&amp;quot;,
  trace = FALSE
)
summary(step.model)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Weight ~ Climate, data = Reduced_df)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -9.020 -2.033  1.050  2.640  6.610 
## 
## Coefficients:
##                     Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)          28.3998     0.1248 227.628  &amp;lt; 2e-16 ***
## ClimateContinental    4.9785     0.3188  15.616  &amp;lt; 2e-16 ***
## ClimateSemi-Coastal   3.3400     0.4606   7.252  7.9e-13 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.629 on 1063 degrees of freedom
## Multiple R-squared:  0.2059,	Adjusted R-squared:  0.2044 
## F-statistic: 137.8 on 2 and 1063 DF,  p-value: &amp;lt; 2.2e-16
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We immediately remain on &lt;code&gt;Climate&lt;/code&gt; as the only predictor in this example.&lt;/p&gt;
&lt;p&gt;What if we start with a true null model?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;model &amp;lt;- lm(Weight ~ 1, data = Reduced_df)
step.model &amp;lt;- stepAIC(model,
  direction = &amp;quot;forward&amp;quot;,
  trace = FALSE
)
summary(step.model)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Weight ~ 1, data = Reduced_df)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -9.944 -1.452  1.291  2.913  7.336 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  29.3243     0.1246   235.3   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.068 on 1065 degrees of freedom
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We even get stuck on our null model!&lt;/p&gt;
&lt;h3 id=&#34;backward-subset-selection&#34;&gt;Backward Subset Selection&lt;/h3&gt;
&lt;p&gt;So what about making our full model simpler?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;model &amp;lt;- lm(Weight ~ ., data = Reduced_df)
step.model &amp;lt;- stepAIC(model,
  direction = &amp;quot;backward&amp;quot;,
  trace = FALSE
)
summary(step.model)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Weight ~ Climate + TAvg + TSD + Flock.Size + Predator.Type, 
##     data = Reduced_df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.2398 -1.1180  0.1215  1.1474  4.9151 
## 
## Coefficients:
##                         Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)            53.505428   3.748484  14.274  &amp;lt; 2e-16 ***
## ClimateContinental      2.978894   0.301131   9.892  &amp;lt; 2e-16 ***
## ClimateSemi-Coastal    -0.640161   0.310970  -2.059   0.0398 *  
## TAvg                   -0.068582   0.012713  -5.395 8.47e-08 ***
## TSD                    -0.069306   0.038900  -1.782   0.0751 .  
## Flock.Size             -0.189607   0.005122 -37.019  &amp;lt; 2e-16 ***
## Predator.TypeNon-Avian  0.379606   0.161332   2.353   0.0188 *  
## Predator.TypeNone       1.258391   0.165347   7.611 6.02e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.673 on 1058 degrees of freedom
## Multiple R-squared:  0.832,	Adjusted R-squared:  0.8309 
## F-statistic: 748.4 on 7 and 1058 DF,  p-value: &amp;lt; 2.2e-16
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Interesting. This time, we have hit on the same model that was identified by the best subset selection above.&lt;/p&gt;
&lt;h3 id=&#34;forward--backward&#34;&gt;Forward &amp;amp; Backward&lt;/h3&gt;
&lt;p&gt;Can we combine the directions of stepwise model selection? Yes, we can:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;model &amp;lt;- lm(Weight ~ ., data = Reduced_df)
step.model &amp;lt;- stepAIC(model,
  direction = &amp;quot;both&amp;quot;,
  trace = FALSE
)
summary(step.model)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Weight ~ Climate + TAvg + TSD + Flock.Size + Predator.Type, 
##     data = Reduced_df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.2398 -1.1180  0.1215  1.1474  4.9151 
## 
## Coefficients:
##                         Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)            53.505428   3.748484  14.274  &amp;lt; 2e-16 ***
## ClimateContinental      2.978894   0.301131   9.892  &amp;lt; 2e-16 ***
## ClimateSemi-Coastal    -0.640161   0.310970  -2.059   0.0398 *  
## TAvg                   -0.068582   0.012713  -5.395 8.47e-08 ***
## TSD                    -0.069306   0.038900  -1.782   0.0751 .  
## Flock.Size             -0.189607   0.005122 -37.019  &amp;lt; 2e-16 ***
## Predator.TypeNon-Avian  0.379606   0.161332   2.353   0.0188 *  
## Predator.TypeNone       1.258391   0.165347   7.611 6.02e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.673 on 1058 degrees of freedom
## Multiple R-squared:  0.832,	Adjusted R-squared:  0.8309 
## F-statistic: 748.4 on 7 and 1058 DF,  p-value: &amp;lt; 2.2e-16
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, we land on our best subset selection model!&lt;/p&gt;
&lt;h3 id=&#34;subset-selection-vs-our-intuition&#34;&gt;Subset Selection vs. Our Intuition&lt;/h3&gt;
&lt;p&gt;Given our best subset selection, we have a very good idea of which model to go for.&lt;/p&gt;
&lt;p&gt;To see how well said model shapes up against our full model, we can simply look at LOOCV:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;train(Weight ~ Climate + TAvg + TSD + Flock.Size + Predator.Type,
  data = Sparrows_df,
  method = &amp;quot;lm&amp;quot;,
  trControl = trainControl(method = &amp;quot;LOOCV&amp;quot;)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear Regression 
## 
## 1066 samples
##    5 predictor
## 
## No pre-processing
## Resampling: Leave-One-Out Cross-Validation 
## Summary of sample sizes: 1065, 1065, 1065, 1065, 1065, 1065, ... 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   1.677673  0.8297908  1.338399
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sapply(H1_LOOCV_ls, &amp;quot;[[&amp;quot;, &amp;quot;results&amp;quot;)[-1, ]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          Comp_Flock.Size Comp_Full Full     
## RMSE     1.894279        1.865854  1.609296 
## Rsquared 0.7829992       0.7894634 0.8433834
## MAE      1.520181        1.492409  1.279003
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And our full model still wins! But why? Didn&amp;rsquo;t we test for all models? Yes, we tested for all additive models, but our Full model contains an interaction terms which the automated functions above just cannot handle, sadly.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s ask a completely different question. Would we have even adopted the best subset selection model if we had thought of it given the assumptions of a linear regression?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;par(mfrow = c(2, 2))
plot(lm(Weight ~ Climate + TAvg + TSD + Flock.Size + Predator.Type, data = Sparrows_df))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;7_Model-Selection-and-Statistical-Significance---Reporting-the-Best-Science_files/figure-html/unnamed-chunk-39-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As it turns out, this is a perfectly reasonable model. It&amp;rsquo;s just not as good as our full model.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 07</title>
      <link>https://www.erikkusch.com/courses/rethinking/chapter-07/</link>
      <pubDate>Thu, 04 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/rethinking/chapter-07/</guid>
      <description>&lt;h1 id=&#34;ulysses-compass&#34;&gt;Ulysses&#39; Compass&lt;/h1&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/rethinking/7__05-02-2021_SUMMARY_-Model-Evaluation.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides Chapter 7&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to the exercises at the end of chapter 7 in 
&lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Satistical Rethinking 2&lt;/a&gt; by Richard McElreath. I have created these notes as a part of my ongoing involvement in the 
&lt;a href=&#34;https://www.erikkusch.com/project/aubayes/&#34;&gt;AU Bayes Study Group&lt;/a&gt;. Much of my inspiration for these solutions, where necessary, has been obtained from 
&lt;a href=&#34;https://jmgirard.com/statistical-rethinking-ch6/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jeffrey Girard&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;r-environment&#34;&gt;&lt;code&gt;R&lt;/code&gt; Environment&lt;/h2&gt;
&lt;p&gt;For today&amp;rsquo;s exercise, I load the following packages:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rethinking)
library(ggplot2)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;easy-exercises&#34;&gt;Easy Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-e1&#34;&gt;Practice E1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; State the three motivating criteria that define information entropy. Try to express each in your own words.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; The principle of information theory is motivated by the three following criteria:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Continuity&lt;/em&gt;. Uncertainty must be measured on a continuous scale of equal intervals to ensure comparability.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Additivity&lt;/em&gt;. Total uncertainty is derived by adding up the uncertainties associated with each prediction.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Scalability&lt;/em&gt;. Uncertainty scales with number of possible outcomes to reflect changes in certainty just by virtue of different numbers of possible outcomes.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;practice-e2&#34;&gt;Practice E2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Suppose a coin is weighted such that, when it is tossed and lands on a table, it comes up heads 70% of the time. What is the entropy of this coin?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  Following the formula 7.1 on page 210:&lt;/p&gt;
&lt;p&gt;$H(p)=â\sum_{i=1}^n p_ilog(p_i) = â(p_Hlog(p_H)+p_Tlog(p_T))$&lt;/p&gt;
&lt;p&gt;with $p_H$ being probability of heads, and $p_T$ being probability of tails, we can simply plug in our values as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;p &amp;lt;- c(0.7, 0.3)
-sum(p * log(p))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6108643
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thus, the entropy is 0.61.&lt;/p&gt;
&lt;h3 id=&#34;practice-e3&#34;&gt;Practice E3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Suppose a four-sided die is loaded such that, when tossed onto a table, it shows â1â 20%, â2â 25%, â3â 25%, and â4â 30% of the time. What is the entropy of this die?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  Again, we use the formula approach as above:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;p &amp;lt;- c(0.2, 0.25, 0.25, 0.3)
-sum(p * log(p))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.376227
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The entropy of our D4 is 1.38.&lt;/p&gt;
&lt;h3 id=&#34;practice-e4&#34;&gt;Practice E4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Suppose another four-sided die is loaded such that it never shows â4â. The other three sides show equally often. What is the entropy of this die?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; By knowing that one side never shows up, we can omit it altogether and are now looking at a perfectly balanced D3 rather than a D4:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;p &amp;lt;- c(1 / 3, 1 / 3, 1 / 3)
-sum(p * log(p))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.098612
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I have never seen a D3 in real-life, but we could easily imagine a D6 where the numbers 1 through 3 show up twice each. The entropy of our D3 is 1.1.&lt;/p&gt;
&lt;h2 id=&#34;medium-exercises&#34;&gt;Medium Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-m1&#34;&gt;Practice M1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Write down and compare the definitions of AIC, and WAIC. Which of these criteria is most general? Which assumptions are required to transform a more general criterion into a less general one?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;AIC&lt;/em&gt; $= D_{train}+2p$; ($D_{train} =$ in-sample deviance, $p =$ number of parameters estimated in the model). It is built on the assumptions that:&lt;br&gt;
A) Priors are flat or overwhelmed by model&lt;br&gt;
B) Posterior distribution is approximately multivariate Gaussian&lt;br&gt;
C) Sample size $&amp;raquo;$ Number of parameters&lt;/li&gt;
&lt;li&gt;&lt;em&gt;WAIC&lt;/em&gt; $= â2(lppd â\sum_i(var_Î¸ log p(y_i|Î¸))$; ($y_i =$ observation $i$, $Î¸ =$ posterior distribution, $lppd(y, Î) =\sum_ilog\frac{1}{S} \sum_Sp(y_i|Î_s) =$ log-pointwise-predictive-density) . It is built on the assumptions that: Sample size $&amp;raquo;$ Number of parameters.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;WAIC is clearly the more general method here as it comes with less assumptions. To transform WAIC into AIC, we need to assume A. and B. of the assumptions of AIC.&lt;/p&gt;
&lt;h3 id=&#34;practice-m2&#34;&gt;Practice M2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Explain the difference between model &lt;em&gt;selection&lt;/em&gt; and model &lt;em&gt;comparison&lt;/em&gt;. What information is lost under model selection?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Model selection and model comparison both use information criteria and/or cross-validation exercises to determine goddess of fit of models to data set (their explanatory power) as well as their accuracy in terms of making inferences (their predictive power). Where they differ is what these approaches do with the models at hand once the desired information is obtained. In &lt;em&gt;model selection&lt;/em&gt; all but the &amp;ldquo;best&amp;rdquo; model is discarded, whereas under &lt;em&gt;model comparison&lt;/em&gt; we use our new-found information to identify relative model accuracy to assess the influences of different parameters in different models. The latter can lead to understanding causal relationships and identification of confounds in the different models.&lt;/p&gt;
&lt;h3 id=&#34;practice-m3&#34;&gt;Practice M3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; When comparing models with an information criterion, why must all models be fit to exactly the same observations? What would happen to the information criterion values, if the models were fit to different numbers of observations? Perform some experiments, if you are not sure.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  Information criteria are based on deviance. Deviance, in turn, is a sum and not a mean product of all observations. All else being equal, a model with more observations returns a higher deviance and thus worse accuracy according to information criteria.&lt;/p&gt;
&lt;h3 id=&#34;practice-m4&#34;&gt;Practice M4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; What happens to the effective number of parameters, as measured by PSIS or WAIC, as a prior becomes more concentrated? Why? Perform some experiments, if you are not sure.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; The effective number of parameters (or &lt;code&gt;&amp;quot;pWAIC&amp;quot;&lt;/code&gt; in the &lt;code&gt;WAIC()&lt;/code&gt; function output), is the penalty term of our regularisation approaches. As priors become more regularising (i.e. more concentrated on certain prior knowledge or assumptions), the effective number of parameters decreases.&lt;/p&gt;
&lt;p&gt;In the case of WAIC, $p_{WAIC}$ is the variance in the log-likelihoods for each observation in the training data. More concentrated priors constrain this likelihood and subsequent measure of variance, thus reducing it.&lt;/p&gt;
&lt;p&gt;As for PSIS, $P_D$ (effective number of parameters) tells us about the flexibility of the model. Increasingly regularised priors decrease the flexibility of the model.&lt;/p&gt;
&lt;h3 id=&#34;practice-m5&#34;&gt;Practice M5&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Provide an informal explanation of why informative priors reduce overfitting.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  Informative priors constrain the model by making it harder for the model to pick up on extreme parameter values and assign them high posterior probabilities.&lt;/p&gt;
&lt;h3 id=&#34;practice-m6&#34;&gt;Practice M6&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Provide an informal explanation of why overly informative priors result in underfitting.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  Informative priors can constrain a model so much that it becomes impossible for the model to change the prior distribution into an accurate posterior distribution given the data. This can especially problematic when we use informative priors that are born under false premises, stem from bad intuition, or are just plain stupid.&lt;/p&gt;
&lt;h2 id=&#34;hard-exercises&#34;&gt;Hard Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-h1&#34;&gt;Practice H1&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://www.erikkusch.com/courses/rethinking/7H1.JPG&#34; align = &#34;right&#34; width = 272/&gt;  &lt;strong&gt;Question:&lt;/strong&gt; In 2007, The Wall Street Journal published an editorial (âWeâre Number One, Alasâ) with a graph of corporate tax rates in 29 countries plotted against tax Revenue. A badly fit curve was drawn in (reconstructed to the right), seemingly by hand, to make the argument that the relationship between tax rate and tax Revenue increases and then declines, such that higher tax rates can actually produce less tax Revenue. I want you to actually fit a curve to these data, found in &lt;code&gt;data(Laffer)&lt;/code&gt;. Consider models that use tax rate to predict tax Revenue. Compare, using WAIC or PSIS, a straight-line model to any curved models you like. What do you conclude about the relationship between tax rate and tax Revenue?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; First, I begin by loading the data and standardising my variables:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# data preparation
data(Laffer)
d &amp;lt;- Laffer
d$Rate &amp;lt;- standardize(d$tax_rate)
d$Revenue &amp;lt;- standardize(d$tax_revenue)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this preparation out of the way, I am ready to run three models: Linear, Quadratic, and Cubic. I could run many more than these, but I wager this will be enough.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# linear model
m7H1a &amp;lt;- quap(
  alist(
    Revenue ~ dnorm(mu, sigma),
    mu &amp;lt;- a + b * Rate,
    a ~ dnorm(0, 0.2),
    b ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d
)

# quadratic model
m7H1b &amp;lt;- quap(
  alist(
    Revenue ~ dnorm(mu, sigma),
    mu &amp;lt;- a + b * Rate + b2 * Rate^2,
    a ~ dnorm(0, 0.2),
    c(b, b2) ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d
)

# cubic model
m7H1c &amp;lt;- quap(
  alist(
    Revenue ~ dnorm(mu, sigma),
    mu &amp;lt;- a + b * Rate + b2 * Rate^2 + b3 * Rate^3,
    a ~ dnorm(0, 0.2),
    c(b, b2, b3) ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d
)

# Comparing Models
comparison &amp;lt;- compare(m7H1a, m7H1b, m7H1c)
comparison
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           WAIC       SE     dWAIC      dSE    pWAIC    weight
## m7H1a 87.98410 21.63416 0.0000000       NA 5.528420 0.4448646
## m7H1b 88.79927 24.58351 0.8151711 3.382774 6.990224 0.2959482
## m7H1c 89.06454 24.19943 1.0804389 3.089764 7.070477 0.2591872
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;According to these WAIC values and their Standard Deviations, I cannot make a clear statement as to which relationship between tax rate and tax revenue should be assumed.&lt;/p&gt;
&lt;p&gt;Let me plot these to make a clearer image of what I mean:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## base sequence for predictions
plot_df &amp;lt;- data.frame(Rate = seq(from = min(d$Rate), to = max(d$Rate), length.out = 1e4))
## Predictions for Linear Model
plot_df$m7H1a &amp;lt;- apply(link(m7H1a, data = plot_df), 2, mean)
plot_df$m7H1aLower &amp;lt;- apply(link(m7H1a, data = plot_df), 2, PI, prob = .95)[1, ]
plot_df$m7H1aUpper &amp;lt;- apply(link(m7H1a, data = plot_df), 2, PI, prob = .95)[2, ]
## Predictions for Quadratic Model
plot_df$m7H1b &amp;lt;- apply(link(m7H1b, data = plot_df), 2, mean)
plot_df$m7H1bLower &amp;lt;- apply(link(m7H1b, data = plot_df), 2, PI, prob = .95)[1, ]
plot_df$m7H1bUpper &amp;lt;- apply(link(m7H1b, data = plot_df), 2, PI, prob = .95)[2, ]
## Predictions for Cubic Model
plot_df$m7H1c &amp;lt;- apply(link(m7H1c, data = plot_df), 2, mean)
plot_df$m7H1cLower &amp;lt;- apply(link(m7H1c, data = plot_df), 2, PI, prob = .95)[1, ]
plot_df$m7H1cUpper &amp;lt;- apply(link(m7H1c, data = plot_df), 2, PI, prob = .95)[2, ]
## Plotting
ggplot(plot_df) +
  geom_point(data = d, aes(x = Rate, y = Revenue), size = 2) +
  geom_line(data = plot_df, aes(y = m7H1a, x = Rate, colour = &amp;quot;Linear&amp;quot;), size = 1.5) +
  geom_ribbon(data = plot_df, aes(ymin = m7H1aLower, ymax = m7H1aUpper, x = Rate), alpha = .1) +
  geom_line(data = plot_df, aes(y = m7H1b, x = Rate, colour = &amp;quot;Quadratic&amp;quot;), size = 1.5) +
  geom_ribbon(data = plot_df, aes(ymin = m7H1bLower, ymax = m7H1bUpper, x = Rate), alpha = .1) +
  geom_line(data = plot_df, aes(y = m7H1c, x = Rate, colour = &amp;quot;Cubic&amp;quot;), size = 1.5) +
  geom_ribbon(data = plot_df, aes(ymin = m7H1cLower, ymax = m7H1cUpper, x = Rate), alpha = .1) +
  theme_bw() +
  scale_colour_discrete(name = &amp;quot;Model&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-28-statistical-rethinking-chapter-07_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I think this highlights quite well just how little difference there is in how the models understand the data. What the Wall Street Journal did there was (quite unsurprisingly) utter trite.&lt;/p&gt;
&lt;h3 id=&#34;practice-h2&#34;&gt;Practice H2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; In the &lt;code&gt;Laffer&lt;/code&gt; data, there is one country with a high tax revenue that is an outlier. Use PSIS and WAIC to measure the importance of this outlier in the models you fit in the previous problem. Then use robust regression with a Studentâs t distribution to revisit the curve fitting problem. How much does a curved relationship depend upon the outlier point?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Using the &lt;code&gt;rethinking&lt;/code&gt; package, we could identify the outlier by a very high WAIC value in the output of &lt;code&gt;WAIC(..., pointwise = TRUE)&lt;/code&gt;, where &lt;code&gt;...&lt;/code&gt; represents our model name. From the plot, we already now that our outlier is the country with the highest tax revenue, so let&amp;rsquo;s remove that one:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# data preparation
data(Laffer)
d &amp;lt;- Laffer
d$Rate &amp;lt;- standardize(d$tax_rate)
d$Revenue &amp;lt;- standardize(d$tax_revenue)
d &amp;lt;- d[d$tax_revenue != max(d$tax_revenue), ] # removing the outlier
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this preparation out of the way, I am ready to run three models again:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# linear model
m7H2a &amp;lt;- quap(
  alist(
    Revenue ~ dnorm(mu, sigma),
    mu &amp;lt;- a + b * Rate,
    a ~ dnorm(0, 0.2),
    b ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d
)

# quadratic model
m7H2b &amp;lt;- quap(
  alist(
    Revenue ~ dnorm(mu, sigma),
    mu &amp;lt;- a + b * Rate + b2 * Rate^2,
    a ~ dnorm(0, 0.2),
    c(b, b2) ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d
)

# cubic model
m7H2c &amp;lt;- quap(
  alist(
    Revenue ~ dnorm(mu, sigma),
    mu &amp;lt;- a + b * Rate + b2 * Rate^2 + b3 * Rate^3,
    a ~ dnorm(0, 0.2),
    c(b, b2, b3) ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d
)
# Comparing Models
comparison &amp;lt;- compare(m7H2a, m7H2b, m7H2c)
comparison
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           WAIC       SE    dWAIC      dSE    pWAIC    weight
## m7H2b 59.61977 9.702003 0.000000       NA 4.010763 0.6272073
## m7H2c 61.37195 9.630274 1.752179 1.215336 4.863606 0.2611742
## m7H2a 63.07215 9.669994 3.452382 4.146202 4.256780 0.1116184
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Well, the models still greatly overlap in their usefulness.&lt;/p&gt;
&lt;p&gt;Next, let&amp;rsquo;s use a robust regression. I was unsure whether to revert back to the data which contains the outlier here. I chose to do so:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Revert back to full data
data(Laffer)
d &amp;lt;- Laffer
d$Rate &amp;lt;- standardize(d$tax_rate)
d$Revenue &amp;lt;- standardize(d$tax_revenue)
# linear model
m7H2aS &amp;lt;- quap(
  alist(
    Revenue ~ dstudent(2, mu, sigma),
    mu &amp;lt;- a + b * Rate,
    a ~ dnorm(0, 0.2),
    b ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d
)

# quadratic model
m7H2bS &amp;lt;- quap(
  alist(
    Revenue ~ dstudent(2, mu, sigma),
    mu &amp;lt;- a + b * Rate + b2 * Rate^2,
    a ~ dnorm(0, 0.2),
    c(b, b2) ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d
)

# cubic model
m7H2cS &amp;lt;- quap(
  alist(
    Revenue ~ dstudent(2, mu, sigma),
    mu &amp;lt;- a + b * Rate + b2 * Rate^2 + b3 * Rate^3,
    a ~ dnorm(0, 0.2),
    c(b, b2, b3) ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d
)
# Comparing Models
comparison &amp;lt;- compare(m7H2aS, m7H2bS, m7H2cS)
comparison
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            WAIC       SE    dWAIC      dSE    pWAIC     weight
## m7H2bS 70.42437 13.97355 0.000000       NA 3.726454 0.69002496
## m7H2cS 72.57488 13.76632 2.150509 1.370840 4.983440 0.23544401
## m7H2aS 74.87539 13.57939 4.451025 4.933044 4.008494 0.07453103
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What happened in comparison to our original models (including the outlier)?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;comparison &amp;lt;- compare(m7H1a, m7H1b, m7H1c, m7H2a, m7H2b, m7H2c, m7H2aS, m7H2bS, m7H2cS)
comparison
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            WAIC        SE     dWAIC       dSE    pWAIC       weight
## m7H2b  58.70723  9.337612  0.000000        NA 3.548011 6.669909e-01
## m7H2c  61.41111  9.567978  2.703873  1.160931 4.897430 1.725764e-01
## m7H2a  61.59824  9.188906  2.891004  3.943353 3.520610 1.571616e-01
## m7H2bS 70.00986 13.878681 11.302627 10.451899 3.521786 2.343072e-03
## m7H2cS 72.56361 13.867102 13.856375 10.523791 4.983524 6.535011e-04
## m7H2aS 74.30030 13.502465 15.593071 10.302465 3.722951 2.742379e-04
## m7H1b  89.68562 25.603534 30.978388 22.459013 7.469330 1.250975e-07
## m7H1a  90.11345 23.731476 31.406222 20.599460 6.639281 1.010056e-07
## m7H1c  90.25121 25.081664 31.543979 21.929293 7.672332 9.428269e-08
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Removing the outlier definitely made our models perform a lot better across the board. So did using a robust regression.&lt;/p&gt;
&lt;h3 id=&#34;practice-h3&#34;&gt;Practice H3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Consider three fictional Polynesian islands. On each there is a Royal Ornithologist charged by the king with surveying the bird population. They have each found the following proportions of 5 important bird species:&lt;/p&gt;
&lt;img src=&#34;https://www.erikkusch.com/courses/rethinking/7H3.JPG&#34; width=&#34;900&#34;/&gt;
&lt;p&gt;Notice that each row sums to 1, all the birds. This problem has two parts. It is not computationally complicated. But it is conceptually tricky. First, compute the entropy of each islandâs bird distribution. Interpret these entropy values. Second, use each islandâs bird distribution to predict the other two. This means to compute the K-L Divergence of each island from the others, treating each island as if it were a statistical model of the other islands. You should end up with 6 different K-L Divergence values. Which island predicts the others best? Why?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; First, let&amp;rsquo;s start with the entropies:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# First Island
p1 &amp;lt;- c(0.2, 0.2, 0.2, 0.2, 0.2)
-sum(p1 * log(p1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.609438
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Second Island
p2 &amp;lt;- c(0.8, 0.1, 0.05, 0.025, 0.025)
-sum(p2 * log(p2))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7430039
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Third Island
p3 &amp;lt;- c(0.05, 0.15, 0.7, 0.05, 0.05)
-sum(p3 * log(p3))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9836003
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Entropy is a measure of &lt;strong&gt;uncertainty&lt;/strong&gt;. The higher the entropy, the more uncertain we are of the probability density distribution at hand. Here, the entropies of the islands are ordered (in increasing order) as Island 2, Island 3, and Island 1. This tells us that there is a lot of certainty (in relative terms) of the proportions assigned to each bird species at Island 2 over the other islands. I posit that this is because of the overwhelming presence of species A in Island 2 whilst all other species presences drop drastically to almost being non-existent on Island 2. In plain terms: &amp;ldquo;We are much more certain of which species to find on an island where we know which species is the sole inhabitant when contrasted with an island where all species are present in equal proportions&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Now, let&amp;rsquo;s move on to the computation of K-L distances also known as &lt;strong&gt;Divergence&lt;/strong&gt;. Divergence is calculated as the average difference in log probability between target (p) and model (q). First things first, I need to identify the model (q). The task states to use information on two islands to obtain K-L distances to one target island. So, to identify the model, I need to average out the proportions on two islands and then compare these to the target island. For each of these pairings, I will obtain two K-L distances since these distances are not reversible in their directionality.&lt;/p&gt;
&lt;p&gt;Let me walk you through my first example of obtaining the Divergence between Island 1 and the Island 2 and 3 taken together:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Average the proportions of Island 2 and 3
(q &amp;lt;- apply(cbind(p2, p3), 1, mean))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.4250 0.1250 0.3750 0.0375 0.0375
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Divergence of Island 1 from combined Islands 2 &amp;amp; 3
(D_pq &amp;lt;- sum(p1 * log(p1 / q)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.4871152
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Divergence of combined Islands 2 &amp;amp; 3 from Island 1
(D_qp &amp;lt;- sum(q * log(q / p1)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3717826
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By using our combined knowledge of Islands 2 &amp;amp; 3 to approximate Island 1, we introduce an additional 0.49 of uncertainty. On the contrary, by using our knowledge of Island 1 to approximate the combination of Islands 2 &amp;amp; 3, we introduce an additional 0.37 of uncertainty.&lt;/p&gt;
&lt;p&gt;Now, I repeat this for the other two target islands:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;output &amp;lt;- data.frame(
  Approximated = D_pq,
  Approximator = D_qp
)
# Target: Island 2
q &amp;lt;- apply(cbind(p1, p3), 1, mean)
D_pq &amp;lt;- sum(p2 * log(p2 / q))
D_qp &amp;lt;- sum(q * log(q / p2))
output &amp;lt;- rbind(output, c(D_pq, D_qp))
# Target: Island 3
q &amp;lt;- apply(cbind(p1, p2), 1, mean)
D_pq &amp;lt;- sum(p3 * log(p3 / q))
D_qp &amp;lt;- sum(q * log(q / p3))
output &amp;lt;- rbind(output, c(D_pq, D_qp))
# output
rownames(output) &amp;lt;- c(&amp;quot;Island 1&amp;quot;, &amp;quot;Island 2&amp;quot;, &amp;quot;Island 3&amp;quot;)
output
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          Approximated Approximator
## Island 1    0.4871152    0.3717826
## Island 2    1.2387437    1.2570061
## Island 3    1.0097143    1.1184060
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As it turns out, the divergence between a &amp;ldquo;safe bet&amp;rdquo; (i.e. Island 1 where all species are equally present) and other systems is much smaller than when comparing heavily skewed probability density distributions.&lt;/p&gt;
&lt;h3 id=&#34;practice-h4&#34;&gt;Practice H4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Recall the marriage, age, and happiness collider bias example from Chapter 6. Run models &lt;code&gt;m6.9&lt;/code&gt; and &lt;code&gt;m6.10&lt;/code&gt; again. Compare these two models using WAIC (or LOO, they will produce identical results). Which model is expected to make better predictions? Which model provides the correct causal inference about the influence of age on happiness? Can you explain why the answers to these two questions disagree?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Remember that in these models, &lt;code&gt;m6.9&lt;/code&gt; shows a negative relationship between age and happiness which we know to be untrue because it conditions on the collider of marriage status which itself, is influenced by age and happiness. &lt;code&gt;m6.10&lt;/code&gt; does not condition on said collider and thus does not find a relationship between age and happiness:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## R code 6.21
d &amp;lt;- sim_happiness(seed = 1977, N_years = 1000)
## R code 6.22
d2 &amp;lt;- d[d$age &amp;gt; 17, ] # only adults
d2$A &amp;lt;- (d2$age - 18) / (65 - 18)
## R code 6.23
d2$mid &amp;lt;- d2$married + 1
m6.9 &amp;lt;- quap(
  alist(
    happiness ~ dnorm(mu, sigma),
    mu &amp;lt;- a[mid] + bA * A,
    a[mid] ~ dnorm(0, 1),
    bA ~ dnorm(0, 2),
    sigma ~ dexp(1)
  ),
  data = d2
)
## R code 6.24
m6.10 &amp;lt;- quap(
  alist(
    happiness ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bA * A,
    a ~ dnorm(0, 1),
    bA ~ dnorm(0, 2),
    sigma ~ dexp(1)
  ),
  data = d2
)
## Comparison
compare(m6.9, m6.10)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           WAIC       SE    dWAIC      dSE    pWAIC       weight
## m6.9  2713.971 37.54465   0.0000       NA 3.738532 1.000000e+00
## m6.10 3101.906 27.74379 387.9347 35.40032 2.340445 5.768312e-85
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;According to these information criteria, model &lt;code&gt;m6.9&lt;/code&gt; is a lot better performing (in terms of out-of-sample deviance). However, we know that model &lt;code&gt;m6.10&lt;/code&gt; provides the true causal relationship between age and happiness: none!&lt;/p&gt;
&lt;p&gt;So why would we want to use model &lt;code&gt;m6.9&lt;/code&gt; given the WAIC above instead of model &lt;code&gt;m6.10&lt;/code&gt;. Because by thinking we should do so, we did model selection instead of model comparison! Argh. I stepped right into that one, didn&amp;rsquo;t I? By comparing these two models we can safely say that some confounding must be taking place. WAIC would have us favour model &lt;code&gt;m6.9&lt;/code&gt; simply because it does a better job at predicting the happiness of out-of-sample individuals. Why is that? Because this model identifies the happiness of the different groups of people: the miserable unmarried as well as the ecstatic married ones. Conditioning on the collider added statistical association and so aids predictive accuracy. Thus, while doing better at &lt;strong&gt;predicting&lt;/strong&gt; model &lt;code&gt;m6.9&lt;/code&gt; fails at finding &lt;strong&gt;causality&lt;/strong&gt;. It is important to highlight that again that a model may be good at predicting things without being causally correct.&lt;/p&gt;
&lt;h3 id=&#34;practice-h5&#34;&gt;Practice H5&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Revisit the urban fox data, &lt;code&gt;data(foxes)&lt;/code&gt;, from the previous chapterâs practice problems. Use WAIC or PSIS based model comparison on five different models, each using weight as the outcome, and containing these sets of predictor variables:&lt;/p&gt;
&lt;p&gt;(1) &lt;code&gt;avgfood + groupsize + area&lt;/code&gt;&lt;br&gt;
(2) &lt;code&gt;avgfood + groupsize&lt;/code&gt;&lt;br&gt;
(3) &lt;code&gt;groupsize + area&lt;/code&gt;&lt;br&gt;
(4) &lt;code&gt;avgfood&lt;/code&gt;&lt;br&gt;
(5) &lt;code&gt;area&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Can you explain the relative differences in WAIC scores, using the fox DAG from last weekâs home-work? Be sure to pay attention to the standard error of the score differences (&lt;code&gt;dSE&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; The previous chapter in the pdf version I am using did not ask any questions about the fox data. I consulted 
&lt;a href=&#34;https://sr2-solutions.wjakethompson.com/overfitting.html#chapter-7&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jake Thomspon&amp;rsquo;s Blog here&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# data loading and prepping
data(foxes)
d &amp;lt;- foxes
d$area &amp;lt;- scale(d$area)
d$avgfood &amp;lt;- scale(d$avgfood)
d$weight &amp;lt;- scale(d$weight)
d$groupsize &amp;lt;- scale(d$groupsize)
## Models
# (1) `avgfood + groupsize + area`
b7h5_1 &amp;lt;- quap(
  alist(
    weight ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bFood * avgfood + bGroup * groupsize + bArea * area,
    a ~ dnorm(0, .2),
    c(bFood, bGroup, bArea) ~ dnorm(0, 5),
    sigma ~ dexp(1)
  ),
  data = d
)
# (2) `avgfood + groupsize`
b7h5_2 &amp;lt;- quap(
  alist(
    weight ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bFood * avgfood + bGroup * groupsize,
    a ~ dnorm(0, .2),
    c(bFood, bGroup) ~ dnorm(0, 5),
    sigma ~ dexp(1)
  ),
  data = d
)
# (3) `groupsize + area`
b7h5_3 &amp;lt;- quap(
  alist(
    weight ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bGroup * groupsize + bArea * area,
    a ~ dnorm(0, .2),
    c(bGroup, bArea) ~ dnorm(0, 5),
    sigma ~ dexp(1)
  ),
  data = d
)
# (4) `avgfood`
b7h5_4 &amp;lt;- quap(
  alist(
    weight ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bFood * avgfood,
    a ~ dnorm(0, .2),
    bFood ~ dnorm(0, 5),
    sigma ~ dexp(1)
  ),
  data = d
)
# (5) `area`
b7h5_5 &amp;lt;- quap(
  alist(
    weight ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bArea * area,
    a ~ dnorm(0, .2),
    bArea ~ dnorm(0, 5),
    sigma ~ dexp(1)
  ),
  data = d
)
## Comparison
compare(b7h5_1, b7h5_2, b7h5_3, b7h5_4, b7h5_5)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            WAIC       SE      dWAIC      dSE    pWAIC      weight
## b7h5_1 323.3416 16.88472  0.0000000       NA 5.187854 0.410865167
## b7h5_2 323.9809 16.81807  0.6393574 3.894043 4.130907 0.298445216
## b7h5_3 324.0666 16.18771  0.7250103 4.207249 3.945774 0.285933698
## b7h5_4 333.5084 13.79238 10.1668387 8.659625 2.454047 0.002546821
## b7h5_5 333.7929 13.79707 10.4513608 8.704578 2.684646 0.002209099
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, the differences of in the WAIC scores all fall well within the 99% intervals of the differences:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(compare(b7h5_1, b7h5_2, b7h5_3, b7h5_4, b7h5_5))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-01-28-statistical-rethinking-chapter-07_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;However, we can see that models &lt;code&gt;b7h5_1&lt;/code&gt;, &lt;code&gt;b7h5_2&lt;/code&gt;, and &lt;code&gt;b7h5_3&lt;/code&gt; are nearly identical in their out-of-sample deviance, as are models &lt;code&gt;b7h5_4&lt;/code&gt; and &lt;code&gt;b7h5_5&lt;/code&gt;. To understand this, we want to look at the DAG that underlies this example:&lt;/p&gt;
&lt;img src=&#34;https://www.erikkusch.com/courses/rethinking/FoxDAG.png&#34;/&gt;
&lt;p&gt;Models &lt;code&gt;b7h5_1&lt;/code&gt;, &lt;code&gt;b7h5_2&lt;/code&gt;, and &lt;code&gt;b7h5_3&lt;/code&gt; all use &lt;code&gt;groupsize&lt;/code&gt; and one of/both &lt;code&gt;area&lt;/code&gt; and/or &lt;code&gt;avgfood&lt;/code&gt;. Consequently, all of these models fair the same in their predictive power because there are no open backdoor paths from either &lt;code&gt;area&lt;/code&gt; or &lt;code&gt;avgfood&lt;/code&gt;, as soon as &lt;code&gt;groupsize&lt;/code&gt; is used in conjunction. In other words, the effect of &lt;code&gt;area&lt;/code&gt; while adjusting for &lt;code&gt;groupsize&lt;/code&gt; is the same as the effect of &lt;code&gt;avgfood&lt;/code&gt; while adjusting for &lt;code&gt;groupsize&lt;/code&gt;, because the effect of &lt;code&gt;area&lt;/code&gt; is routed entirely through &lt;code&gt;avgfood&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Likewise, models &lt;code&gt;b7h5_4&lt;/code&gt; and &lt;code&gt;b7h5_5&lt;/code&gt; are nearly identical because these two only contain &lt;code&gt;area&lt;/code&gt; or &lt;code&gt;avgfood&lt;/code&gt; in isolation and all information of &lt;code&gt;area&lt;/code&gt; onto &lt;code&gt;weight&lt;/code&gt; must pass through &lt;code&gt;avgfood&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.5 (2021-03-31)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19043)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.1252  LC_CTYPE=English_United Kingdom.1252    LC_MONETARY=English_United Kingdom.1252 LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.1252    
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] rethinking_2.13      rstan_2.21.2         ggplot2_3.3.6        StanHeaders_2.21.0-7
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.7         mvtnorm_1.1-1      lattice_0.20-41    prettyunits_1.1.1  ps_1.6.0           assertthat_0.2.1   digest_0.6.27      utf8_1.2.1         V8_3.4.1           R6_2.5.0          
## [11] backports_1.2.1    stats4_4.0.5       evaluate_0.14      coda_0.19-4        highr_0.9          blogdown_1.3       pillar_1.6.0       rlang_0.4.11       curl_4.3.2         callr_3.7.0       
## [21] jquerylib_0.1.4    R.utils_2.10.1     R.oo_1.24.0        rmarkdown_2.7      styler_1.4.1       labeling_0.4.2     stringr_1.4.0      loo_2.4.1          munsell_0.5.0      compiler_4.0.5    
## [31] xfun_0.22          pkgconfig_2.0.3    pkgbuild_1.2.0     shape_1.4.5        htmltools_0.5.1.1  tidyselect_1.1.0   tibble_3.1.1       gridExtra_2.3      bookdown_0.22      codetools_0.2-18  
## [41] matrixStats_0.61.0 fansi_0.4.2        crayon_1.4.1       dplyr_1.0.5        withr_2.4.2        MASS_7.3-53.1      R.methodsS3_1.8.1  grid_4.0.5         jsonlite_1.7.2     gtable_0.3.0      
## [51] lifecycle_1.0.0    DBI_1.1.1          magrittr_2.0.1     scales_1.1.1       RcppParallel_5.1.2 cli_3.0.0          stringi_1.5.3      farver_2.1.0       bslib_0.2.4        ellipsis_0.3.2    
## [61] generics_0.1.0     vctrs_0.3.7        rematch2_2.1.2     tools_4.0.5        R.cache_0.14.0     glue_1.4.2         purrr_0.3.4        processx_3.5.1     yaml_2.2.1         inline_0.3.17     
## [71] colorspace_2.0-0   knitr_1.33         sass_0.3.1
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Nominal Tests</title>
      <link>https://www.erikkusch.com/courses/an-introduction-to-biostatistics/nominal-tests/</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/an-introduction-to-biostatistics/nominal-tests/</guid>
      <description>&lt;h2 id=&#34;theory&#34;&gt;Theory&lt;/h2&gt;
&lt;p&gt;Welcome to our second practical experience in &lt;code&gt;R&lt;/code&gt;. Throughout the following notes, I will introduce you to a couple nominal statistical test approaches that might be useful to you and are often used in biology.&lt;br&gt;
To do so, I will enlist the sparrow data set we handled in our last exercise.&lt;/p&gt;
&lt;p&gt;I have prepared some &lt;a href=&#34;https://www.erikkusch.com/courses/an-introduction-to-biostatistics/08---Nominal-Tests_Handout.html&#34; target=&#34;_blank&#34;&gt; Lecture Slides &lt;/a&gt; for this session.&lt;/p&gt;
&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;p&gt;Find the data for this exercise &lt;a href=&#34;https://www.erikkusch.com/courses/an-introduction-to-biostatistics/Data/1%20-%20Sparrow_Data_READY.rds&#34; target=&#34;_blank&#34;&gt; here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;preparing-our-procedure&#34;&gt;Preparing Our Procedure&lt;/h2&gt;
&lt;p&gt;To ensure others can reproduce our analysis we run the following three lines of code at the beginning of our &lt;code&gt;R&lt;/code&gt; coding file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rm(list=ls()) # clearing environment
Dir.Base &amp;lt;- getwd() # soft-coding our working directory
Dir.Data &amp;lt;- paste(Dir.Base, &amp;quot;Data&amp;quot;, sep=&amp;quot;/&amp;quot;) # soft-coding our data directory 
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;packages&#34;&gt;Packages&lt;/h3&gt;
&lt;p&gt;Using the following, user-defined function, we install/load all the necessary packages into our current &lt;code&gt;R&lt;/code&gt; session.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# function to load packages and install them if they haven&#39;t been installed yet
install.load.package &amp;lt;- function(x) {
  if (!require(x, character.only = TRUE))
    install.packages(x)
  require(x, character.only = TRUE)
}
package_vec &amp;lt;- c(&amp;quot;nonpar&amp;quot;, # needed for Cochran&#39;s Q
                 &amp;quot;ggplot2&amp;quot;) # data visualisation
sapply(package_vec, install.load.package)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: nonpar
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggplot2
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  nonpar ggplot2 
##    TRUE    TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;loading-data&#34;&gt;Loading Data&lt;/h3&gt;
&lt;p&gt;During our last exercise (Data Mining and Data Handling - Fixing The Sparrow Data Set) we saved our clean data set as an RDS file. To load this, we use the &lt;code&gt;readRDS()&lt;/code&gt; command that comes with base &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df_base &amp;lt;- readRDS(file = paste(Dir.Data, &amp;quot;/1 - Sparrow_Data_READY.rds&amp;quot;, sep=&amp;quot;&amp;quot;))
Data_df &amp;lt;- Data_df_base # duplicate and save initial data on a new object
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;binomial-test&#34;&gt;Binomial Test&lt;/h2&gt;
&lt;p&gt;As the name would suggest, a binomial test can only accommodate variables on a binomial scale. A binomial test is used to test whether both values of the binomial variable are present in equal proportions within the data set. The only binomial variables contained within the &lt;em&gt;Passer domesticus&lt;/em&gt; data set are &lt;code&gt;Sex&lt;/code&gt; (Male, Female) and &lt;code&gt;Predator.Presence&lt;/code&gt; (Yes, No). The &lt;code&gt;R&lt;/code&gt; function to carry out a binomial test comes with base &lt;code&gt;R&lt;/code&gt; and is called &lt;code&gt;binom.test()&lt;/code&gt;. The &lt;strong&gt;Null Hypothesis&lt;/strong&gt; we operate on is that &lt;strong&gt;both data values are equally likely to occur&lt;/strong&gt; although one can specify a different expectations using the &lt;code&gt;p = &lt;/code&gt; statement within the &lt;code&gt;binom.test()&lt;/code&gt; function.&lt;/p&gt;
&lt;h3 id=&#34;sexual-dimorphism&#34;&gt;Sexual Dimorphism&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Are the sexes represented in equal proportions?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First, we want to test whether our data has a bias leaning towards either sex of the surveyed sparrows. To do so, we may wish to first convert the binomial data into count records using the &lt;code&gt;table()&lt;/code&gt; command of R as follows. The result of this can then be feed to &lt;code&gt;binom.test()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(Data_df$Sex)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Female   Male 
##    523    544
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;binom.test(table(Data_df$Sex))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Exact binomial test
## 
## data:  table(Data_df$Sex)
## number of successes = 523, number of trials = 1067, p-value = 0.5404
## alternative hypothesis: true probability of success is not equal to 0.5
## 95 percent confidence interval:
##  0.4597580 0.5206151
## sample estimates:
## probability of success 
##              0.4901593
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we can see, there is no skew towards either male or female abundance of individuals of &lt;em&gt;Passer domesticus&lt;/em&gt; and so we have to &lt;strong&gt;accept the null hypothesis&lt;/strong&gt;. Note that, although our data is recorded in terms of Male and Female, the &lt;code&gt;binom.test()&lt;/code&gt; function works with records of success and failure.&lt;/p&gt;
&lt;p&gt;This is to be expected. After all no bias for sex is known in &lt;em&gt;Passer domesticus&lt;/em&gt; and indeed the species does reproduce monogamously so a skew between the sexes wouldn&amp;rsquo;t go anywhere as far as evolution is concerned.&lt;/p&gt;
&lt;h3 id=&#34;predation&#34;&gt;Predation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Are the sites dominated by predators?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Now, let&amp;rsquo;s see if there is a skew towards predators being present at our sites or not. This time, however we make use of a different syntax for the &lt;code&gt;binom.test()&lt;/code&gt; function. We do this for no reason of functionality but simply to show that there are multiple ways to using it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(Data_df$Predator.Presence)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  No Yes 
## 357 710
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;binom.test(x = sum(Data_df$Predator.Presence == &amp;quot;Yes&amp;quot;), n = length(Data_df$Predator.Presence))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Exact binomial test
## 
## data:  sum(Data_df$Predator.Presence == &amp;quot;Yes&amp;quot;) and length(Data_df$Predator.Presence)
## number of successes = 710, number of trials = 1067, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true probability of success is not equal to 0.5
## 95 percent confidence interval:
##  0.6362102 0.6937082
## sample estimates:
## probability of success 
##              0.6654171
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Quite obviously, these proportions aren&amp;rsquo;t as equal as the ones of the sex example. In fact, they exhibit statistically significant proportion sizes within our data set (p $\approx$ 0).&lt;/p&gt;
&lt;p&gt;This is in concordance with what we&amp;rsquo;d expect from the natural world since predation is common in nature after all and so we &lt;strong&gt;reject the null hypothesis&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;mcnemar&#34;&gt;McNemar&lt;/h2&gt;
&lt;p&gt;The McNemar Test (sometimes referred to as McNemar&amp;rsquo;s Chi-Square test because the test statistic has a chi-square distribution) is used when you are interested in finding a change in proportion for paired data. This is very common in repeated sampling analyses.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;null hypothesis&lt;/strong&gt; reads: &lt;strong&gt;Class assignment probabilities do not change within different treatments&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;preparing-the-data&#34;&gt;Preparing The Data&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Do sex ratios change over time?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Unfortunately, our data does not allow for these types of analyses and so we will need to create some additional data here.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s say we wanted to resample the sex ratio of &lt;em&gt;Passer domesticus&lt;/em&gt; in Australia (AU) a year after our initial survey because of an especially hostile winter and we&amp;rsquo;d like to see whether this resulted in an alteration of the sex ratio.&lt;/p&gt;
&lt;p&gt;What is our sex ratio before the winter?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(Data_df$Sex[which(Data_df$Index == &amp;quot;AU&amp;quot;)])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Female   Male 
##     44     44
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Sexes_AU_Now_vec &amp;lt;- Data_df$Sex[which(Data_df$Index == &amp;quot;AU&amp;quot;)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The sex ratio is not skewed. So let&amp;rsquo;s hypothesise about what might happen to the sex ratio when a strong winter hits our population. The sex ratio could either (1) stay the same or (2) change. Although it would make sense to assume that the population would shrink, McNemar tests can&amp;rsquo;t account for that and so we assume that our population size will stay the same and only the sex ratio might change. Let&amp;rsquo;s create some new data for a changed sex ratio that is male biased:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Sexes &amp;lt;- c(&amp;quot;Male&amp;quot;, &amp;quot;Female&amp;quot;) # creating a vector of sexes to sample from
set.seed(42) # making it reproducible
Sexes_AU_Next_vec &amp;lt;- sample(Sexes, sum(Data_df$Index==&amp;quot;AU&amp;quot;), replace = TRUE, prob = c(0.8,0.2))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here&amp;rsquo;s the data we will be testing:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(Sexes_AU_Now_vec) 
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Sexes_AU_Now_vec
## Female   Male 
##     44     44
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(Sexes_AU_Next_vec)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Sexes_AU_Next_vec
## Female   Male 
##     21     67
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;running-the-test&#34;&gt;Running The Test&lt;/h3&gt;
&lt;p&gt;Now let&amp;rsquo;s go on to test the unbiased vs. the male-skewed sex ratio:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mcnemar_matrix_change &amp;lt;- matrix(rbind(table(Sexes_AU_Now_vec), table(Sexes_AU_Next_vec)), 2)
mcnemar.test(mcnemar_matrix_change)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	McNemar&#39;s Chi-squared test with continuity correction
## 
## data:  mcnemar_matrix_change
## McNemar&#39;s chi-squared = 7.4462, df = 1, p-value = 0.006357
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Obviously, with this data we would record a statistically significant change and &lt;strong&gt;reject the null hypothesis&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;making-sense-of-the-results&#34;&gt;Making Sense Of The Results&lt;/h3&gt;
&lt;p&gt;Unfortunately, McNemar only tells us &lt;em&gt;that&lt;/em&gt; there is a difference without any information about the &lt;em&gt;direction&lt;/em&gt; of the difference. For now, we will have to settle on a visualisation of the sexes to shed some light on the difference.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# preparing plotting
plot_df &amp;lt;- data.frame(Data = c(prop.table(mcnemar_matrix_change[1,]),
                               prop.table(mcnemar_matrix_change[2,])),
                      Identifiers = rep(c(&amp;quot;Female&amp;quot;, &amp;quot;Male&amp;quot;), 2),
                      Year = rep(c(&amp;quot;Now&amp;quot;, &amp;quot;Next Year&amp;quot;), each = 2))
# plotting
ggplot(plot_df, aes(x = Year, y = Data, fill = Identifiers)) + geom_bar(stat=&amp;quot;identity&amp;quot;) +
  ggtitle(label = &amp;quot;Abundances of the sexes among study organisms&amp;quot;) + theme_bw() +
  ylab(&amp;quot;Proportion&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;08---Nominal-Tests_files/figure-html/McNemar6-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The above plot is very crude and should only ever be used for data exploration and not for publishing purposes. Clearly, we can see the change in sex ratio towards a male-biased state (blue colour represents males).&lt;/p&gt;
&lt;h2 id=&#34;cochrans-q&#34;&gt;Cochran&amp;rsquo;s Q&lt;/h2&gt;
&lt;p&gt;Cochran&amp;rsquo;s Q is a non parametric test for finding differences in matched sets of three or more frequencies or proportions.&lt;/p&gt;
&lt;p&gt;As such, the Cochran&amp;rsquo;s Q Test is an extension of the McNemar test - the two tests are equal if Cochran&amp;rsquo;s Q is calculated for two groups.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;null hypothesis&lt;/strong&gt; for Cochran&amp;rsquo;s Q postulates an &lt;strong&gt;equal proportion of class assignents for all treatments&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;preparing-the-data-1&#34;&gt;Preparing The Data&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Are colours related to sex or predator parameters?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When exploring our data, we can clearly see a pattern concerning the colour polymorphism of house sparrows arise which is dependant on the value of Predator Presence.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;counts &amp;lt;- table(Data_df$Colour, Data_df$Predator.Presence)
# preparing plotting
plot_df &amp;lt;- data.frame(Data = c(prop.table(counts[,1]), prop.table(counts[,2])),
                      Identifiers = rep(c(&amp;quot;Black&amp;quot;, &amp;quot;Brown&amp;quot;, &amp;quot;Grey&amp;quot;), 2),
                      Predation = rep(c(&amp;quot;No&amp;quot;, &amp;quot;Yes&amp;quot;), each = 3))
# plotting
ggplot(plot_df, aes(x = Predation, y = Data, fill = Identifiers)) + geom_bar(stat=&amp;quot;identity&amp;quot;) + ggtitle(label = &amp;quot;Colour Variations of the common House Sparrow&amp;quot;) + theme_bw() +ylab(&amp;quot;Proportions&amp;quot;) + scale_fill_manual(values=c(&amp;quot;black&amp;quot;, &amp;quot;saddlebrown&amp;quot;, &amp;quot;grey&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;08---Nominal-Tests_files/figure-html/ChochranPrep-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This might lead us to believe that the presence of predators cause an evolutionary change of the plumage colour of &lt;em&gt;Passer domesticus&lt;/em&gt; (we will have a more in-depth look on this in later seminars) and we might even postulate that &amp;ldquo;Black&amp;rdquo; and &amp;ldquo;Grey&amp;rdquo; serve as camouflage.&lt;/p&gt;
&lt;p&gt;Cochran&amp;rsquo;s Q requires data to be delivered as binomial records. Therefore, we prepare colour as a binary variable of &amp;ldquo;Brown&amp;rdquo; and &amp;ldquo;Camouflage&amp;rdquo; (which we postulate to encompass &amp;ldquo;Grey&amp;rdquo; and &amp;ldquo;Black&amp;rdquo;). Since Colour is of type &lt;code&gt;factor&lt;/code&gt; within our data set, we need to take some precautions in changing the data records. Predator Presence and Sex don&amp;rsquo;t need any additional preparation.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Colour 
CochColour &amp;lt;- Data_df_base$Colour
# adding new level to factor list
levels(CochColour) &amp;lt;- c(levels(CochColour), &amp;quot;Camouflage&amp;quot;) 
# defining black and grey to be camouflage
CochColour[which(CochColour == &amp;quot;Grey&amp;quot;)] &amp;lt;- &amp;quot;Camouflage&amp;quot; 
CochColour[which(CochColour == &amp;quot;Black&amp;quot;)] &amp;lt;- &amp;quot;Camouflage&amp;quot; 
# dropping unnecessary factor levels
CochColour &amp;lt;- droplevels(CochColour) 

# Predator Presence
CochPredator.Presence &amp;lt;- factor(Data_df_base$Predator.Presence)

# Sex
CochSex &amp;lt;- factor(Data_df_base$Sex)

# Making vectors into a matrix
CochMatrix &amp;lt;- matrix(c(CochColour, CochPredator.Presence, CochSex), ncol = 3) - 1
colnames(CochMatrix) &amp;lt;- c(&amp;quot;Colour&amp;quot;, &amp;quot;Predator Presence&amp;quot;, &amp;quot;Sex&amp;quot;)
head(CochMatrix)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Colour Predator Presence Sex
## [1,]      0                 1   1
## [2,]      1                 1   1
## [3,]      1                 1   0
## [4,]      0                 1   0
## [5,]      1                 1   1
## [6,]      0                 1   0
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;runing-the-test&#34;&gt;Runing The Test&lt;/h3&gt;
&lt;p&gt;Now let&amp;rsquo;s run our test using the &lt;code&gt;cochrans.q()&lt;/code&gt; function that comes with the &lt;code&gt;nonpar&lt;/code&gt; package:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cochrans.q(CochMatrix)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Cochran&#39;s Q Test 
##  
##  H0: There is no difference in the effectiveness of treatments. 
##  HA: There is a difference in the effectiveness of treatments. 
##  
##  Q = 122.984939759036 
##  
##  Degrees of Freedom = 2 
##  
##  Significance Level = 0.05 
##  The p-value is  0 
##  There is enough evidence to conclude that the effectiveness of at least two treatments differ. 
## 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we can see, the output from this function is extremely user friendly. Additionally, as was to be expected the assignment probabilities for each class in each treatment are not equal thus forcing us to &lt;strong&gt;reject the null hypothesis&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;making-sense-of-the-results-1&#34;&gt;Making Sense Of The Results&lt;/h3&gt;
&lt;p&gt;Where are the differences coming from?&lt;/p&gt;
&lt;p&gt;As you may recall from just a few pages ago, using the binomial test, we can identify the assignment proportions for any binomial variable individually.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Firstly&lt;/em&gt;, let&amp;rsquo;s test the &lt;strong&gt;binary version&lt;/strong&gt; of the &lt;strong&gt;colour&lt;/strong&gt; variable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(CochColour)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## CochColour
##      Brown Camouflage 
##        298        769
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;binom.test(table(CochColour))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Exact binomial test
## 
## data:  table(CochColour)
## number of successes = 298, number of trials = 1067, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true probability of success is not equal to 0.5
## 95 percent confidence interval:
##  0.2525387 0.3072599
## sample estimates:
## probability of success 
##              0.2792877
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Based on this result, we &lt;strong&gt;reject the null hypothesis&lt;/strong&gt; of binary colour records being equally likely to occur.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Secondly&lt;/em&gt;, let&amp;rsquo;s test the &lt;strong&gt;predator presence&lt;/strong&gt; variable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(CochPredator.Presence)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## CochPredator.Presence
##  No Yes 
## 357 710
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;binom.test(table(CochPredator.Presence))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Exact binomial test
## 
## data:  table(CochPredator.Presence)
## number of successes = 357, number of trials = 1067, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true probability of success is not equal to 0.5
## 95 percent confidence interval:
##  0.3062918 0.3637898
## sample estimates:
## probability of success 
##              0.3345829
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Based on this result, we &lt;strong&gt;reject the null hypothesis&lt;/strong&gt; of predator presence records being equally likely to occur.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Lastly&lt;/em&gt;, recall the binomial test run on the &lt;strong&gt;sex&lt;/strong&gt; data records which exhibit an almost even 50/50 split.&lt;/p&gt;
&lt;p&gt;Whilst none of these test give us any idea about the overlap of similar assignments along these variable vectors, a 50/50 split (sex) can never link up comparably with a roughly 30/70 split (predator presence and binary colour). Therefore, we could hypothesize a linkage of predator presence and colour rather than sex and colour morphs.&lt;/p&gt;
&lt;h2 id=&#34;chi-squared&#34;&gt;Chi-Squared&lt;/h2&gt;
&lt;p&gt;The Chi-Squared (also known as $Chi^2$) Test can be regarded as a functional extension of the binomial test and is used to test the similarity of class assignment proportions for a categorical/nominal variable. Unlike the binomial test, however, this test is not constrained to binomial records alone.&lt;/p&gt;
&lt;p&gt;The null hypothesis states that: &lt;strong&gt;Every class assignment contained within a given variable is equally likely&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The Chi-Squared Test can be applied in a one or two sample situation. One sample represents one variable in this setting.&lt;/p&gt;
&lt;h3 id=&#34;one-sample-situation&#34;&gt;One Sample Situation&lt;/h3&gt;
&lt;h4 id=&#34;binary-colour&#34;&gt;Binary Colour&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses the proportions of one variable we have already looked at - the &lt;strong&gt;binary version&lt;/strong&gt; of the &lt;strong&gt;colour&lt;/strong&gt; variable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(CochColour)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## CochColour
##      Brown Camouflage 
##        298        769
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;chisq.test(table(CochColour))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Chi-squared test for given probabilities
## 
## data:  table(CochColour)
## X-squared = 207.91, df = 1, p-value &amp;lt; 2.2e-16
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Based on this result, we &lt;strong&gt;reject the null hypothesis&lt;/strong&gt; of binary colour records being equally likely to occur. Note how the Chi-Squared test returns the same p-value as the binomial test above (within the Cochran&amp;rsquo;s Q section).&lt;/p&gt;
&lt;h4 id=&#34;colour&#34;&gt;Colour&lt;/h4&gt;
&lt;p&gt;Now let&amp;rsquo;s run the same test on the non-binary &lt;strong&gt;colour&lt;/strong&gt; data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(Data_df$Colour)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Black Brown  Grey 
##   356   298   413
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;chisq.test(table(Data_df$Colour))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Chi-squared test for given probabilities
## 
## data:  table(Data_df$Colour)
## X-squared = 18.592, df = 2, p-value = 9.178e-05
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, we &lt;strong&gt;reject the null hypothesis&lt;/strong&gt; thus concluding differing class proportions for every possible class of &amp;ldquo;Colour&amp;rdquo;.&lt;/p&gt;
&lt;h3 id=&#34;two-sample-situation&#34;&gt;Two Sample Situation&lt;/h3&gt;
&lt;p&gt;The two sample Chi-Squared approach lets us identify whether class assignment proportions of one variable differ when they are considered in a dependency of another nominal variable.&lt;/p&gt;
&lt;h4 id=&#34;sexual-dimorphism-1&#34;&gt;Sexual Dimorphism&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Are colours of Passer domesticus related to their sexes?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Firstly&lt;/em&gt;, let&amp;rsquo;s see if males and females share the same likelihoods of being of a certain colour:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(Data_df$Colour, Data_df$Sex)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        
##         Female Male
##   Black    320   36
##   Brown    122  176
##   Grey      81  332
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;chisq.test(table(Data_df$Colour, Data_df$Sex))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Pearson&#39;s Chi-squared test
## 
## data:  table(Data_df$Colour, Data_df$Sex)
## X-squared = 388.63, df = 2, p-value &amp;lt; 2.2e-16
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Clearly, they don&amp;rsquo;t and we &lt;strong&gt;reject the null hypothesis&lt;/strong&gt;.&lt;/p&gt;
&lt;h4 id=&#34;predation-1&#34;&gt;Predation&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Are colours of Passer domesticus related to predator parameters?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Secondly&lt;/em&gt;, we test whether colour proportions change when considering predator presence. Although we partially considered this already in the Cochran&amp;rsquo;s Q section. This time, however, we use a non-binary version of the colour variable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(Data_df$Colour, Data_df$Predator.Presence)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        
##          No Yes
##   Black  64 292
##   Brown 211  87
##   Grey   82 331
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;chisq.test(table(Data_df$Colour, Data_df$Predator.Presence))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Pearson&#39;s Chi-squared test
## 
## data:  table(Data_df$Colour, Data_df$Predator.Presence)
## X-squared = 259.34, df = 2, p-value &amp;lt; 2.2e-16
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The statement holds. Predator presence seems likely to be a driver of the colour polymorphism in &lt;em&gt;Passer domesticus&lt;/em&gt; and we &lt;strong&gt;reject the null hypothesis&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;So what about a possible link of sparrow colour and predator type?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(Data_df$Colour, Data_df$Predator.Type)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        
##         Avian Non-Avian
##   Black   197        95
##   Brown    60        27
##   Grey    233        98
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;chisq.test(table(Data_df$Colour, Data_df$Predator.Type))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Pearson&#39;s Chi-squared test
## 
## data:  table(Data_df$Colour, Data_df$Predator.Type)
## X-squared = 0.62164, df = 2, p-value = 0.7328
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Nope, no link here. We have to &lt;strong&gt;accept the null hypothesis&lt;/strong&gt; and conclude that there may be no causal link of predator type and sparrow colour.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Are nesting sites of Passer domesticus related to predator parameters?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Third&lt;/em&gt;, let&amp;rsquo;s test whether nesting site assignments might differ based on predator presence:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(Data_df$Nesting.Site, Data_df$Predator.Presence)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        
##          No Yes
##   Shrub  87 205
##   Tree   94 137
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;chisq.test(table(Data_df$Nesting.Site, Data_df$Predator.Presence))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Pearson&#39;s Chi-squared test with Yates&#39; continuity correction
## 
## data:  table(Data_df$Nesting.Site, Data_df$Predator.Presence)
## X-squared = 6.2955, df = 1, p-value = 0.0121
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There seems to be a link here and we &lt;strong&gt;reject the null hypothesis&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;So what about a link of predator type and nesting site?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(Data_df$Nesting.Site, Data_df$Predator.Type)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        
##         Avian Non-Avian
##   Shrub   182        23
##   Tree     49        88
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;chisq.test(table(Data_df$Nesting.Site, Data_df$Predator.Type))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Pearson&#39;s Chi-squared test with Yates&#39; continuity correction
## 
## data:  table(Data_df$Nesting.Site, Data_df$Predator.Type)
## X-squared = 102.88, df = 1, p-value &amp;lt; 2.2e-16
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Apparently, there is a really strong one and we &lt;strong&gt;reject the null hypothesis&lt;/strong&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Nominal Tests</title>
      <link>https://www.erikkusch.com/courses/biostat101/nominal-tests/</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/biostat101/nominal-tests/</guid>
      <description>&lt;h2 id=&#34;theory&#34;&gt;Theory&lt;/h2&gt;
&lt;p&gt;Welcome to our second practical experience in &lt;code&gt;R&lt;/code&gt;. Throughout the following notes, I will introduce you to a couple nominal statistical test approaches that might be useful to you and are often used in biology. To do so, I will enlist the sparrow data set we handled in our last exercise. I have prepared some slides for this session: &lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/biostat101/08---Nominal-Tests_Handout.html&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://www.erikkusch.com/courses/biostat101/08---BioStat101_featured.png&#34; width=&#34;900&#34; margin-top = &#34;0&#34;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;p&gt;Find the data for this exercise &lt;a href=&#34;https://github.com/ErikKusch/Homepage/raw/master/content/courses/biostat101/Data/1%20-%20Sparrow_Data_READY.rds&#34; target=&#34;_blank&#34;&gt; here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;preparing-our-procedure&#34;&gt;Preparing Our Procedure&lt;/h2&gt;
&lt;p&gt;To ensure others can reproduce our analysis we run the following three lines of code at the beginning of our &lt;code&gt;R&lt;/code&gt; coding file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rm(list=ls()) # clearing environment
Dir.Base &amp;lt;- getwd() # soft-coding our working directory
Dir.Data &amp;lt;- paste(Dir.Base, &amp;quot;Data&amp;quot;, sep=&amp;quot;/&amp;quot;) # soft-coding our data directory 
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;packages&#34;&gt;Packages&lt;/h3&gt;
&lt;p&gt;Using the following, user-defined function, we install/load all the necessary packages into our current &lt;code&gt;R&lt;/code&gt; session.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# function to load packages and install them if they haven&#39;t been installed yet
install.load.package &amp;lt;- function(x) {
  if (!require(x, character.only = TRUE))
    install.packages(x)
  require(x, character.only = TRUE)
}
package_vec &amp;lt;- c(&amp;quot;nonpar&amp;quot;, # needed for Cochran&#39;s Q
                 &amp;quot;ggplot2&amp;quot;) # data visualisation
sapply(package_vec, install.load.package)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: nonpar
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggplot2
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  nonpar ggplot2 
##    TRUE    TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;loading-data&#34;&gt;Loading Data&lt;/h3&gt;
&lt;p&gt;During our last exercise (Data Mining and Data Handling - Fixing The Sparrow Data Set) we saved our clean data set as an RDS file. To load this, we use the &lt;code&gt;readRDS()&lt;/code&gt; command that comes with base &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df_base &amp;lt;- readRDS(file = paste(Dir.Data, &amp;quot;/1 - Sparrow_Data_READY.rds&amp;quot;, sep=&amp;quot;&amp;quot;))
Data_df &amp;lt;- Data_df_base # duplicate and save initial data on a new object
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;binomial-test&#34;&gt;Binomial Test&lt;/h2&gt;
&lt;p&gt;As the name would suggest, a binomial test can only accommodate variables on a binomial scale. A binomial test is used to test whether both values of the binomial variable are present in equal proportions within the data set. The only binomial variables contained within the &lt;em&gt;Passer domesticus&lt;/em&gt; data set are &lt;code&gt;Sex&lt;/code&gt; (Male, Female) and &lt;code&gt;Predator.Presence&lt;/code&gt; (Yes, No). The &lt;code&gt;R&lt;/code&gt; function to carry out a binomial test comes with base &lt;code&gt;R&lt;/code&gt; and is called &lt;code&gt;binom.test()&lt;/code&gt;. The &lt;strong&gt;Null Hypothesis&lt;/strong&gt; we operate on is that &lt;strong&gt;both data values are equally likely to occur&lt;/strong&gt; although one can specify a different expectations using the &lt;code&gt;p = &lt;/code&gt; statement within the &lt;code&gt;binom.test()&lt;/code&gt; function.&lt;/p&gt;
&lt;h3 id=&#34;sexual-dimorphism&#34;&gt;Sexual Dimorphism&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Are the sexes represented in equal proportions?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First, we want to test whether our data has a bias leaning towards either sex of the surveyed sparrows. To do so, we may wish to first convert the binomial data into count records using the &lt;code&gt;table()&lt;/code&gt; command of R as follows. The result of this can then be feed to &lt;code&gt;binom.test()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(Data_df$Sex)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Female   Male 
##    523    544
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;binom.test(table(Data_df$Sex))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Exact binomial test
## 
## data:  table(Data_df$Sex)
## number of successes = 523, number of trials = 1067, p-value = 0.5404
## alternative hypothesis: true probability of success is not equal to 0.5
## 95 percent confidence interval:
##  0.4597580 0.5206151
## sample estimates:
## probability of success 
##              0.4901593
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we can see, there is no skew towards either male or female abundance of individuals of &lt;em&gt;Passer domesticus&lt;/em&gt; and so we have to &lt;strong&gt;accept the null hypothesis&lt;/strong&gt;. Note that, although our data is recorded in terms of Male and Female, the &lt;code&gt;binom.test()&lt;/code&gt; function works with records of success and failure.&lt;/p&gt;
&lt;p&gt;This is to be expected. After all no bias for sex is known in &lt;em&gt;Passer domesticus&lt;/em&gt; and indeed the species does reproduce monogamously so a skew between the sexes wouldn&amp;rsquo;t go anywhere as far as evolution is concerned.&lt;/p&gt;
&lt;h3 id=&#34;predation&#34;&gt;Predation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Are the sites dominated by predators?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Now, let&amp;rsquo;s see if there is a skew towards predators being present at our sites or not. This time, however we make use of a different syntax for the &lt;code&gt;binom.test()&lt;/code&gt; function. We do this for no reason of functionality but simply to show that there are multiple ways to using it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(Data_df$Predator.Presence)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  No Yes 
## 357 710
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;binom.test(x = sum(Data_df$Predator.Presence == &amp;quot;Yes&amp;quot;), n = length(Data_df$Predator.Presence))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Exact binomial test
## 
## data:  sum(Data_df$Predator.Presence == &amp;quot;Yes&amp;quot;) and length(Data_df$Predator.Presence)
## number of successes = 710, number of trials = 1067, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true probability of success is not equal to 0.5
## 95 percent confidence interval:
##  0.6362102 0.6937082
## sample estimates:
## probability of success 
##              0.6654171
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Quite obviously, these proportions aren&amp;rsquo;t as equal as the ones of the sex example. In fact, they exhibit statistically significant proportion sizes within our data set (p $\approx$ 0).&lt;/p&gt;
&lt;p&gt;This is in concordance with what we&amp;rsquo;d expect from the natural world since predation is common in nature after all and so we &lt;strong&gt;reject the null hypothesis&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;mcnemar&#34;&gt;McNemar&lt;/h2&gt;
&lt;p&gt;The McNemar Test (sometimes referred to as McNemar&amp;rsquo;s Chi-Square test because the test statistic has a chi-square distribution) is used when you are interested in finding a change in proportion for paired data. This is very common in repeated sampling analyses.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;null hypothesis&lt;/strong&gt; reads: &lt;strong&gt;Class assignment probabilities do not change within different treatments&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;preparing-the-data&#34;&gt;Preparing The Data&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Do sex ratios change over time?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Unfortunately, our data does not allow for these types of analyses and so we will need to create some additional data here.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s say we wanted to resample the sex ratio of &lt;em&gt;Passer domesticus&lt;/em&gt; in Australia (AU) a year after our initial survey because of an especially hostile winter and we&amp;rsquo;d like to see whether this resulted in an alteration of the sex ratio.&lt;/p&gt;
&lt;p&gt;What is our sex ratio before the winter?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(Data_df$Sex[which(Data_df$Index == &amp;quot;AU&amp;quot;)])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Female   Male 
##     44     44
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Sexes_AU_Now_vec &amp;lt;- Data_df$Sex[which(Data_df$Index == &amp;quot;AU&amp;quot;)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The sex ratio is not skewed. So let&amp;rsquo;s hypothesise about what might happen to the sex ratio when a strong winter hits our population. The sex ratio could either (1) stay the same or (2) change. Although it would make sense to assume that the population would shrink, McNemar tests can&amp;rsquo;t account for that and so we assume that our population size will stay the same and only the sex ratio might change. Let&amp;rsquo;s create some new data for a changed sex ratio that is male biased:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Sexes &amp;lt;- c(&amp;quot;Male&amp;quot;, &amp;quot;Female&amp;quot;) # creating a vector of sexes to sample from
set.seed(42) # making it reproducible
Sexes_AU_Next_vec &amp;lt;- sample(Sexes, sum(Data_df$Index==&amp;quot;AU&amp;quot;), replace = TRUE, prob = c(0.8,0.2))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here&amp;rsquo;s the data we will be testing:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(Sexes_AU_Now_vec) 
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Sexes_AU_Now_vec
## Female   Male 
##     44     44
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(Sexes_AU_Next_vec)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Sexes_AU_Next_vec
## Female   Male 
##     21     67
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;running-the-test&#34;&gt;Running The Test&lt;/h3&gt;
&lt;p&gt;Now let&amp;rsquo;s go on to test the unbiased vs. the male-skewed sex ratio:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mcnemar_matrix_change &amp;lt;- matrix(rbind(table(Sexes_AU_Now_vec), table(Sexes_AU_Next_vec)), 2)
mcnemar.test(mcnemar_matrix_change)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	McNemar&#39;s Chi-squared test with continuity correction
## 
## data:  mcnemar_matrix_change
## McNemar&#39;s chi-squared = 7.4462, df = 1, p-value = 0.006357
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Obviously, with this data we would record a statistically significant change and &lt;strong&gt;reject the null hypothesis&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;making-sense-of-the-results&#34;&gt;Making Sense Of The Results&lt;/h3&gt;
&lt;p&gt;Unfortunately, McNemar only tells us &lt;em&gt;that&lt;/em&gt; there is a difference without any information about the &lt;em&gt;direction&lt;/em&gt; of the difference. For now, we will have to settle on a visualisation of the sexes to shed some light on the difference.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# preparing plotting
plot_df &amp;lt;- data.frame(Data = c(prop.table(mcnemar_matrix_change[1,]),
                               prop.table(mcnemar_matrix_change[2,])),
                      Identifiers = rep(c(&amp;quot;Female&amp;quot;, &amp;quot;Male&amp;quot;), 2),
                      Year = rep(c(&amp;quot;Now&amp;quot;, &amp;quot;Next Year&amp;quot;), each = 2))
# plotting
ggplot(plot_df, aes(x = Year, y = Data, fill = Identifiers)) + geom_bar(stat=&amp;quot;identity&amp;quot;) +
  ggtitle(label = &amp;quot;Abundances of the sexes among study organisms&amp;quot;) + theme_bw() +
  ylab(&amp;quot;Proportion&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;08---Nominal-Tests_files/figure-html/McNemar6-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The above plot is very crude and should only ever be used for data exploration and not for publishing purposes. Clearly, we can see the change in sex ratio towards a male-biased state (blue colour represents males).&lt;/p&gt;
&lt;h2 id=&#34;cochrans-q&#34;&gt;Cochran&amp;rsquo;s Q&lt;/h2&gt;
&lt;p&gt;Cochran&amp;rsquo;s Q is a non parametric test for finding differences in matched sets of three or more frequencies or proportions.&lt;/p&gt;
&lt;p&gt;As such, the Cochran&amp;rsquo;s Q Test is an extension of the McNemar test - the two tests are equal if Cochran&amp;rsquo;s Q is calculated for two groups.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;null hypothesis&lt;/strong&gt; for Cochran&amp;rsquo;s Q postulates an &lt;strong&gt;equal proportion of class assignents for all treatments&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;preparing-the-data-1&#34;&gt;Preparing The Data&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Are colours related to sex or predator parameters?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When exploring our data, we can clearly see a pattern concerning the colour polymorphism of house sparrows arise which is dependant on the value of Predator Presence.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;counts &amp;lt;- table(Data_df$Colour, Data_df$Predator.Presence)
# preparing plotting
plot_df &amp;lt;- data.frame(Data = c(prop.table(counts[,1]), prop.table(counts[,2])),
                      Identifiers = rep(c(&amp;quot;Black&amp;quot;, &amp;quot;Brown&amp;quot;, &amp;quot;Grey&amp;quot;), 2),
                      Predation = rep(c(&amp;quot;No&amp;quot;, &amp;quot;Yes&amp;quot;), each = 3))
# plotting
ggplot(plot_df, aes(x = Predation, y = Data, fill = Identifiers)) + geom_bar(stat=&amp;quot;identity&amp;quot;) + ggtitle(label = &amp;quot;Colour Variations of the common House Sparrow&amp;quot;) + theme_bw() +ylab(&amp;quot;Proportions&amp;quot;) + scale_fill_manual(values=c(&amp;quot;black&amp;quot;, &amp;quot;saddlebrown&amp;quot;, &amp;quot;grey&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;08---Nominal-Tests_files/figure-html/ChochranPrep-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This might lead us to believe that the presence of predators cause an evolutionary change of the plumage colour of &lt;em&gt;Passer domesticus&lt;/em&gt; (we will have a more in-depth look on this in later seminars) and we might even postulate that &amp;ldquo;Black&amp;rdquo; and &amp;ldquo;Grey&amp;rdquo; serve as camouflage.&lt;/p&gt;
&lt;p&gt;Cochran&amp;rsquo;s Q requires data to be delivered as binomial records. Therefore, we prepare colour as a binary variable of &amp;ldquo;Brown&amp;rdquo; and &amp;ldquo;Camouflage&amp;rdquo; (which we postulate to encompass &amp;ldquo;Grey&amp;rdquo; and &amp;ldquo;Black&amp;rdquo;). Since Colour is of type &lt;code&gt;factor&lt;/code&gt; within our data set, we need to take some precautions in changing the data records. Predator Presence and Sex don&amp;rsquo;t need any additional preparation.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Colour 
CochColour &amp;lt;- Data_df_base$Colour
# adding new level to factor list
levels(CochColour) &amp;lt;- c(levels(CochColour), &amp;quot;Camouflage&amp;quot;) 
# defining black and grey to be camouflage
CochColour[which(CochColour == &amp;quot;Grey&amp;quot;)] &amp;lt;- &amp;quot;Camouflage&amp;quot; 
CochColour[which(CochColour == &amp;quot;Black&amp;quot;)] &amp;lt;- &amp;quot;Camouflage&amp;quot; 
# dropping unnecessary factor levels
CochColour &amp;lt;- droplevels(CochColour) 

# Predator Presence
CochPredator.Presence &amp;lt;- factor(Data_df_base$Predator.Presence)

# Sex
CochSex &amp;lt;- factor(Data_df_base$Sex)

# Making vectors into a matrix
CochMatrix &amp;lt;- matrix(c(
  as.numeric(CochColour), 
  as.numeric(CochPredator.Presence), 
  as.numeric(CochSex)
  ), 
  ncol = 3)  - 1
colnames(CochMatrix) &amp;lt;- c(&amp;quot;Colour&amp;quot;, &amp;quot;Predator Presence&amp;quot;, &amp;quot;Sex&amp;quot;)
head(CochMatrix)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Colour Predator Presence Sex
## [1,]      0                 1   1
## [2,]      1                 1   1
## [3,]      1                 1   0
## [4,]      0                 1   0
## [5,]      1                 1   1
## [6,]      0                 1   0
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;runing-the-test&#34;&gt;Runing The Test&lt;/h3&gt;
&lt;p&gt;Now let&amp;rsquo;s run our test using the &lt;code&gt;cochrans.q()&lt;/code&gt; function that comes with the &lt;code&gt;nonpar&lt;/code&gt; package:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cochrans.q(CochMatrix)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Cochran&#39;s Q Test 
##  
##  H0: There is no difference in the effectiveness of treatments. 
##  HA: There is a difference in the effectiveness of treatments. 
##  
##  Q = 122.984939759036 
##  
##  Degrees of Freedom = 2 
##  
##  Significance Level = 0.05 
##  The p-value is  0 
##  There is enough evidence to conclude that the effectiveness of at least two treatments differ. 
## 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we can see, the output from this function is extremely user friendly. Additionally, as was to be expected the assignment probabilities for each class in each treatment are not equal thus forcing us to &lt;strong&gt;reject the null hypothesis&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;making-sense-of-the-results-1&#34;&gt;Making Sense Of The Results&lt;/h3&gt;
&lt;p&gt;Where are the differences coming from?&lt;/p&gt;
&lt;p&gt;As you may recall from just a few pages ago, using the binomial test, we can identify the assignment proportions for any binomial variable individually.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Firstly&lt;/em&gt;, let&amp;rsquo;s test the &lt;strong&gt;binary version&lt;/strong&gt; of the &lt;strong&gt;colour&lt;/strong&gt; variable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(CochColour)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## CochColour
##      Brown Camouflage 
##        298        769
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;binom.test(table(CochColour))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Exact binomial test
## 
## data:  table(CochColour)
## number of successes = 298, number of trials = 1067, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true probability of success is not equal to 0.5
## 95 percent confidence interval:
##  0.2525387 0.3072599
## sample estimates:
## probability of success 
##              0.2792877
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Based on this result, we &lt;strong&gt;reject the null hypothesis&lt;/strong&gt; of binary colour records being equally likely to occur.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Secondly&lt;/em&gt;, let&amp;rsquo;s test the &lt;strong&gt;predator presence&lt;/strong&gt; variable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(CochPredator.Presence)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## CochPredator.Presence
##  No Yes 
## 357 710
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;binom.test(table(CochPredator.Presence))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Exact binomial test
## 
## data:  table(CochPredator.Presence)
## number of successes = 357, number of trials = 1067, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true probability of success is not equal to 0.5
## 95 percent confidence interval:
##  0.3062918 0.3637898
## sample estimates:
## probability of success 
##              0.3345829
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Based on this result, we &lt;strong&gt;reject the null hypothesis&lt;/strong&gt; of predator presence records being equally likely to occur.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Lastly&lt;/em&gt;, recall the binomial test run on the &lt;strong&gt;sex&lt;/strong&gt; data records which exhibit an almost even 50/50 split.&lt;/p&gt;
&lt;p&gt;Whilst none of these test give us any idea about the overlap of similar assignments along these variable vectors, a 50/50 split (sex) can never link up comparably with a roughly 30/70 split (predator presence and binary colour). Therefore, we could hypothesize a linkage of predator presence and colour rather than sex and colour morphs.&lt;/p&gt;
&lt;h2 id=&#34;chi-squared&#34;&gt;Chi-Squared&lt;/h2&gt;
&lt;p&gt;The Chi-Squared (also known as $Chi^2$) Test can be regarded as a functional extension of the binomial test and is used to test the similarity of class assignment proportions for a categorical/nominal variable. Unlike the binomial test, however, this test is not constrained to binomial records alone.&lt;/p&gt;
&lt;p&gt;The null hypothesis states that: &lt;strong&gt;Every class assignment contained within a given variable is equally likely&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The Chi-Squared Test can be applied in a one or two sample situation. One sample represents one variable in this setting.&lt;/p&gt;
&lt;h3 id=&#34;one-sample-situation&#34;&gt;One Sample Situation&lt;/h3&gt;
&lt;h4 id=&#34;binary-colour&#34;&gt;Binary Colour&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s asses the proportions of one variable we have already looked at - the &lt;strong&gt;binary version&lt;/strong&gt; of the &lt;strong&gt;colour&lt;/strong&gt; variable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(CochColour)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## CochColour
##      Brown Camouflage 
##        298        769
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;chisq.test(table(CochColour))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Chi-squared test for given probabilities
## 
## data:  table(CochColour)
## X-squared = 207.91, df = 1, p-value &amp;lt; 2.2e-16
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Based on this result, we &lt;strong&gt;reject the null hypothesis&lt;/strong&gt; of binary colour records being equally likely to occur. Note how the Chi-Squared test returns the same p-value as the binomial test above (within the Cochran&amp;rsquo;s Q section).&lt;/p&gt;
&lt;h4 id=&#34;colour&#34;&gt;Colour&lt;/h4&gt;
&lt;p&gt;Now let&amp;rsquo;s run the same test on the non-binary &lt;strong&gt;colour&lt;/strong&gt; data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(Data_df$Colour)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Black Brown  Grey 
##   356   298   413
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;chisq.test(table(Data_df$Colour))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Chi-squared test for given probabilities
## 
## data:  table(Data_df$Colour)
## X-squared = 18.592, df = 2, p-value = 9.178e-05
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, we &lt;strong&gt;reject the null hypothesis&lt;/strong&gt; thus concluding differing class proportions for every possible class of &amp;ldquo;Colour&amp;rdquo;.&lt;/p&gt;
&lt;h3 id=&#34;two-sample-situation&#34;&gt;Two Sample Situation&lt;/h3&gt;
&lt;p&gt;The two sample Chi-Squared approach lets us identify whether class assignment proportions of one variable differ when they are considered in a dependency of another nominal variable.&lt;/p&gt;
&lt;h4 id=&#34;sexual-dimorphism-1&#34;&gt;Sexual Dimorphism&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Are colours of Passer domesticus related to their sexes?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Firstly&lt;/em&gt;, let&amp;rsquo;s see if males and females share the same likelihoods of being of a certain colour:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(Data_df$Colour, Data_df$Sex)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        
##         Female Male
##   Black    320   36
##   Brown    122  176
##   Grey      81  332
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;chisq.test(table(Data_df$Colour, Data_df$Sex))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Pearson&#39;s Chi-squared test
## 
## data:  table(Data_df$Colour, Data_df$Sex)
## X-squared = 388.63, df = 2, p-value &amp;lt; 2.2e-16
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Clearly, they don&amp;rsquo;t and we &lt;strong&gt;reject the null hypothesis&lt;/strong&gt;.&lt;/p&gt;
&lt;h4 id=&#34;predation-1&#34;&gt;Predation&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Are colours of Passer domesticus related to predator parameters?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Secondly&lt;/em&gt;, we test whether colour proportions change when considering predator presence. Although we partially considered this already in the Cochran&amp;rsquo;s Q section. This time, however, we use a non-binary version of the colour variable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(Data_df$Colour, Data_df$Predator.Presence)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        
##          No Yes
##   Black  64 292
##   Brown 211  87
##   Grey   82 331
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;chisq.test(table(Data_df$Colour, Data_df$Predator.Presence))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Pearson&#39;s Chi-squared test
## 
## data:  table(Data_df$Colour, Data_df$Predator.Presence)
## X-squared = 259.34, df = 2, p-value &amp;lt; 2.2e-16
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The statement holds. Predator presence seems likely to be a driver of the colour polymorphism in &lt;em&gt;Passer domesticus&lt;/em&gt; and we &lt;strong&gt;reject the null hypothesis&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;So what about a possible link of sparrow colour and predator type?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(Data_df$Colour, Data_df$Predator.Type)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        
##         Avian Non-Avian
##   Black   197        95
##   Brown    60        27
##   Grey    233        98
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;chisq.test(table(Data_df$Colour, Data_df$Predator.Type))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Pearson&#39;s Chi-squared test
## 
## data:  table(Data_df$Colour, Data_df$Predator.Type)
## X-squared = 0.62164, df = 2, p-value = 0.7328
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Nope, no link here. We have to &lt;strong&gt;accept the null hypothesis&lt;/strong&gt; and conclude that there may be no causal link of predator type and sparrow colour.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Are nesting sites of Passer domesticus related to predator parameters?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Third&lt;/em&gt;, let&amp;rsquo;s test whether nesting site assignments might differ based on predator presence:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(Data_df$Nesting.Site, Data_df$Predator.Presence)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        
##          No Yes
##   Shrub  87 205
##   Tree   94 137
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;chisq.test(table(Data_df$Nesting.Site, Data_df$Predator.Presence))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Pearson&#39;s Chi-squared test with Yates&#39; continuity correction
## 
## data:  table(Data_df$Nesting.Site, Data_df$Predator.Presence)
## X-squared = 6.2955, df = 1, p-value = 0.0121
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There seems to be a link here and we &lt;strong&gt;reject the null hypothesis&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;So what about a link of predator type and nesting site?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(Data_df$Nesting.Site, Data_df$Predator.Type)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        
##         Avian Non-Avian
##   Shrub   182        23
##   Tree     49        88
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;chisq.test(table(Data_df$Nesting.Site, Data_df$Predator.Type))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Pearson&#39;s Chi-squared test with Yates&#39; continuity correction
## 
## data:  table(Data_df$Nesting.Site, Data_df$Predator.Type)
## X-squared = 102.88, df = 1, p-value &amp;lt; 2.2e-16
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Apparently, there is a really strong one and we &lt;strong&gt;reject the null hypothesis&lt;/strong&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 08</title>
      <link>https://www.erikkusch.com/courses/rethinking/chapter-08/</link>
      <pubDate>Thu, 18 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/rethinking/chapter-08/</guid>
      <description>&lt;h1 id=&#34;conditional-manatees&#34;&gt;Conditional Manatees&lt;/h1&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/rethinking/9__19-02-2021_SUMMARY_-Interactions.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides Chapter 8&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to the exercises at the end of chapter 8 in 
&lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Satistical Rethinking 2&lt;/a&gt; by Richard McElreath. I have created these notes as a part of my ongoing involvement in the 
&lt;a href=&#34;https://www.erikkusch.com/project/aubayes/&#34;&gt;AU Bayes Study Group&lt;/a&gt;. Much of my inspiration for these solutions, where necessary, has been obtained from 
&lt;a href=&#34;https://jmgirard.com/statistical-rethinking-ch7/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jeffrey Girard&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;r-environment&#34;&gt;&lt;code&gt;R&lt;/code&gt; Environment&lt;/h2&gt;
&lt;p&gt;For today&amp;rsquo;s exercise, I load the following packages:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rethinking)
library(ggplot2)
library(viridis)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;easy-exercises&#34;&gt;Easy Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-e1&#34;&gt;Practice E1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; For each of the causal relationships below, name a hypothetical third variable that would lead to an interaction effect.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Bread dough rises because of yeast.&lt;/li&gt;
&lt;li&gt;Education leads to higher income.&lt;/li&gt;
&lt;li&gt;Gasoline makes a car go.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Temperature. Yeast is active only in a certain range of temperatures and varyingly so.&lt;/li&gt;
&lt;li&gt;Age. Time spent in a profession usually comes with raises and thus higher pay as one gets older. This is not the case in all jobs, of course.&lt;/li&gt;
&lt;li&gt;Engine efficiency will interact with the presence of gasoline to determine how much the car will go.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;practice-e2&#34;&gt;Practice E2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Which of the following explanations invokes an interaction?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Caramelizing onions requires cooking over low heat and making sure the onions do not dry out.&lt;/li&gt;
&lt;li&gt;A car will go faster when it has more cylinders or when it has a better fuel injector.&lt;/li&gt;
&lt;li&gt;Most people acquire their political beliefs from their parents, unless they get them instead from their friends.&lt;/li&gt;
&lt;li&gt;Intelligent animal species tend to be either highly social or have manipulative appendages (hands, tentacles, etc.).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Yes&lt;/em&gt;, there is an interaction here. Water and low heat interact to caramelize onions.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Yes&lt;/em&gt;, there is an interaction here. Number of cylinders and quality of fuel injector interact to determine the speed of the car.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;No&lt;/em&gt;, there is no interaction here. You either get your political belief from your parents or your friends. The two do not interact.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Yes&lt;/em&gt;, there is an interaction here. Degree of sociality and possession of manipulative appendages combine to determine intelligence level.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;practice-e3&#34;&gt;Practice E3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; For each of the explanations in E2, write a linear model that expresses the stated relationship.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\mu_i = \beta_T * T_i + \beta_W*W_i + \beta_{TW} * T_i W_i$&lt;/li&gt;
&lt;li&gt;$\mu_i = \beta_C * C_i + \beta_F * F_i + \beta_{CF} * C_i F_i$&lt;/li&gt;
&lt;li&gt;$\mu_i = \beta_P * P_i + \beta_F * F_i$&lt;/li&gt;
&lt;li&gt;$\mu_i = \beta_S * S_i + \beta_A * A_i + \beta_{SA} S_iA_i$&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;medium-exercises&#34;&gt;Medium Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-m1&#34;&gt;Practice M1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Recall the tulips example from the chapter. Suppose another set of treatments adjusted the temperature in the greenhouse over two levels: cold and hot. The data in the chapter were collected at the cold temperature. You find none of the plants grown under the hot temperature developed any blooms at all, regardless of the water and shade levels. Can you explain this result in terms of interactions between water, shade, and temperature?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; We now have a model with a three-way interaction which comes with three two-way interactions:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\mu_i = &amp;amp; \alpha + \beta_T * T_i + \beta_W * W_i + \beta_S * S_i + \newline 
&amp;amp; \beta_{TW} * T_iW_i + \beta_{TS} * T_iS_i + \beta_{WS} * W_iS_i + \newline
&amp;amp; \beta_{TWS} * T_iW_iS_i
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Within this model all parameters work out such that when $T_i = 2$ (hot condition) we obtain $\mu_i = 0$ (no blooms).&lt;/p&gt;
&lt;h3 id=&#34;practice-m2&#34;&gt;Practice M2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Can you invent a regression equation that would make the bloom size zero, whenever the temperature is hot?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Oops. I partially answered that in my previous exercise, but let&amp;rsquo;s go more in detail. Let&amp;rsquo;s first remember some values from the chapter: (1) water is recorded as 1-3 (dry to wet), (2) shade is coded as 1-3 (high to low), and (3) temperature is coded as 0/1 (cold/hot). So now we know that, irrespective of the values of water or shade, as long as $T = 1$, $mu_i$ has to be $0$ in this forumla:&lt;/p&gt;
&lt;p&gt;$$
\begin{split}
\mu_i = &amp;amp;\alpha + \beta_T * T_i + \beta_W * W_i + \beta_S * S_i + \newline
&amp;amp;\beta_{TW} * T_iW_i + \beta_{TS} * T_iS_i + \beta_{WS} * W_iS_i + \newline
&amp;amp;\beta_{TWS} * T_iW_iS_i
\end{split}
$$&lt;/p&gt;
&lt;p&gt;How do we get there? For now, let&amp;rsquo;s set water and shade to 1 and temperature to 1. Doing so will result in a rewriting of the formula above to:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\mu_{i|T=1,W=1,S=1} = \alpha + \beta_T + \beta_W + \beta_S + \beta_{TW} + \beta_{TS} + \beta_{WS} + \beta_{TWS}
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;So now we need to get this formula to always work out to 0, irrespective of values of $\alpha$, $\beta_s$, $\beta_W$, and $\beta_{WS}$. We can do so by having parameters which include the effect of temperature ($\beta_T$, $\beta_{TW}$, $\beta_{TS}$, and $\beta_{TWS}$) counteract the $\alpha$, $\beta_s$, $\beta_W$, and $\beta_{WS}$. This has us rewrite the above formula as:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\mu_{i|T=1,W=1,S=1} = (\alpha + \beta_T) + (\beta_W + \beta_{TW}) + (\beta_S + \beta_{TS}) + (\beta_{WS} + \beta_{TWS})
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;Now for the outcome to be $0$, the contents of the brackets need to be 0, so $\beta_T = -\alpha$, $\beta_{TW} = -\beta_W$, and so on. This morphs our equation into:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\mu_{i|T=1,W=1,S=1} = (\alpha - \alpha) + (\beta_W - \beta_W) + (\beta_S - \beta_S) + (\beta_{WS} - \beta_{WS})
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;So now, irrespective of $W$ or $S$, we will always obtain $\mu_i = 0$ when $T=1$. When $T=0$, all temperature effects drop out and we obtain the same formula as in the book chapter.&lt;/p&gt;
&lt;h3 id=&#34;practice-m3&#34;&gt;Practice M3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; In parts of North America, ravens depend upon wolves for their food. This is because ravens are carnivorous but cannot usually kill or open carcasses of prey. Wolves however can and do kill and tear open animals, and they tolerate ravens co-feeding at their kills. This species relationship is generally described as a âspecies interaction.â Can you invent a hypothetical set of data on raven population size in which this relationship would manifest as a statistical interaction? Do you think the biological interaction could be linear? Why or why not?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Here&amp;rsquo;s our regression:&lt;/p&gt;
&lt;p&gt;$Ravensâ¼Normal(Î¼,Ï)$&lt;/p&gt;
&lt;p&gt;$Î¼=Î±+Î²_pPrey+Î²_wWolves+Î²_{pw}Prey*Wolves$&lt;/p&gt;
&lt;p&gt;with $Ravens$, $Wolves$, and $Prey$ being the number of ravens, wolves, and prey animals respectively, in a given habitat.&lt;/p&gt;
&lt;p&gt;Now let&amp;rsquo;s make up some data with an in-built interaction effect:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;N &amp;lt;- 1e5 # simulation size
rPW &amp;lt;- 0.2 # correlation between prey and wolf
bP &amp;lt;- 0.05 # regression coefficient for prey
bW &amp;lt;- -0.3 # regression coefficient for wolf
bPW &amp;lt;- 0.2 # regression coefficient for prey-by-wolf interaction
# Simulate data
prey &amp;lt;- as.integer(rnorm(N, mean = 100, sd = 15)) # as.integer, so we have &amp;quot;whole&amp;quot; animals
wolves &amp;lt;- as.integer(rnorm(N, mean = 10 + rPW * prey, sd = 7))
ravens &amp;lt;- as.integer(rnorm(N, mean = 5 + bP * prey + bW * wolves + bPW * wolves * prey, sd = 9))
d &amp;lt;- data.frame(prey = prey, wolves = wolves, ravens = ravens)
# plot the data
par(mfrow = c(1, 2))
plot(ravens ~ prey, data = d, main = &amp;quot;Ravens like prey!&amp;quot;)
plot(ravens ~ wolves, data = d, main = &amp;quot;Ravens like wolves?&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-02-18-statistical-rethinking-chapter-08_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Immediately, we see in our data that, despite us having simulated the data in such a way that ravens do not flock around wolves, when not conditioning on prey, we would think that ravens do flock around wolves.&lt;/p&gt;
&lt;p&gt;Time for a model run:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m &amp;lt;- quap(
  alist(
    ravens ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bP * prey + bW * wolves + bPW * prey * wolves,
    a ~ dnorm(min(d$ravens), 10), # minimum of ravens for intercept prior
    bW ~ dnorm(0, 1),
    bP ~ dnorm(0, 1),
    bPW ~ dnorm(0, 1),
    sigma ~ dnorm(sd(d$ravens), 10) # sd of raven as an initial guess
  ),
  data = d
)
precis(m)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              mean           sd       5.5%       94.5%
## a      5.41309759 0.6926177989  4.3061606  6.52003460
## bW    -0.32805262 0.0233269747 -0.3653336 -0.29077161
## bP     0.04084714 0.0070894052  0.0295169  0.05217738
## bPW    0.20027501 0.0002308127  0.1999061  0.20064390
## sigma  8.97789166 0.0200777428  8.9458035  9.00997977
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And we successfully reconstructed our input interactions.&lt;/p&gt;
&lt;h2 id=&#34;hard-exercises&#34;&gt;Hard Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-h1&#34;&gt;Practice H1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Return to the &lt;code&gt;data(tulips)&lt;/code&gt; example in the chapter. Now include the bed variable as a predictor in the interaction model. Donât interact bed with the other predictors; just include it as a main effect. Note that bed is categorical. So to use it properly, you will need to either construct dummy variables or rather an index variable, as explained in Chapter 6.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Data
data(tulips)
d &amp;lt;- tulips
d$bed_id &amp;lt;- coerce_index(d$bed)
d$blooms_std &amp;lt;- d$blooms / max(d$blooms) # now on a scale from 0 to 1
d$shade_cent &amp;lt;- d$shade - mean(d$shade) # now on a scale from -1 to 1
d$water_cent &amp;lt;- d$water - mean(d$water) # now on a scale from -1 to 1
## Model
set.seed(20) # setting a seed because I sometimes run out of model iterations here
m.H1 &amp;lt;- quap(alist(
  blooms ~ dnorm(mu, sigma),
  mu &amp;lt;- a[bed_id] + bW * water_cent + bS * shade_cent + bWS * water_cent * shade_cent,
  a[bed_id] ~ dnorm(130, 100),
  bW ~ dnorm(0, 100),
  bS ~ dnorm(0, 100),
  bWS ~ dnorm(0, 100),
  sigma ~ dunif(0, 100)
),
data = d
)
precis(m.H1, depth = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            mean        sd      5.5%     94.5%
## a[1]   97.54986 12.951192  76.85135 118.24837
## a[2]  142.41547 12.950773 121.71763 163.11330
## a[3]  147.11128 12.950771 126.41344 167.80911
## bW     75.12289  9.197989  60.42272  89.82305
## bS    -41.23747  9.196690 -55.93555 -26.53938
## bWS   -52.23345 11.240444 -70.19785 -34.26905
## sigma  39.18206  5.333939  30.65740  47.70673
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-h2&#34;&gt;Practice H2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Use WAIC to compare the model from H1 to a model that omits bed. What do you infer from this comparison? Can you reconcile the WAIC results with the posterior distribution of the bed coefficients?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m.H2 &amp;lt;- quap(
  alist(
    blooms ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bW * water_cent + bS * shade_cent + bWS * water_cent * shade_cent,
    a ~ dnorm(130, 100),
    bW ~ dnorm(0, 100),
    bS ~ dnorm(0, 100),
    bWS ~ dnorm(0, 100),
    sigma ~ dunif(0, 100)
  ),
  data = d,
  start = list(a = mean(d$blooms), bW = 0, bS = 0, bWS = 0, sigma = sd(d$blooms))
)
precis(m.H2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            mean        sd      5.5%     94.5%
## a     129.00797  8.670771 115.15041 142.86554
## bW     74.95946 10.601997  58.01542  91.90350
## bS    -41.14054 10.600309 -58.08188 -24.19920
## bWS   -51.87265 12.948117 -72.56625 -31.17906
## sigma  45.22497  6.152982  35.39132  55.05863
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;compare(m.H1, m.H2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          WAIC        SE    dWAIC      dSE     pWAIC    weight
## m.H2 295.0441  9.873189 0.000000       NA  6.062662 0.6776652
## m.H1 296.5302 10.544146 1.486126 8.181914 10.771689 0.3223348
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model including the bed index variables (&lt;code&gt;m.H2&lt;/code&gt;) shows a slightly better WAIC than the model that contains the bed variable (&lt;code&gt;m.H1&lt;/code&gt;), and most of the Akaike weight. Judging from this (and the variation in the bed intercepts of model &lt;code&gt;m.H1&lt;/code&gt;), we can infer that there&amp;rsquo;s a lot of variability between the flower beds which model &lt;code&gt;m.H1&lt;/code&gt; addresses, but might overfit in doing so. Let me visualise this in a plot:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;post &amp;lt;- extract.samples(m.H1)
post.a &amp;lt;- post$a[, 1]
post.b &amp;lt;- post$a[, 2]
post.c &amp;lt;- post$a[, 3]
dens(post.a, col = &amp;quot;red&amp;quot;, xlim = c(50, 200), ylim = c(0, 0.035))
dens(post.b, col = &amp;quot;blue&amp;quot;, add = TRUE)
dens(post.c, col = &amp;quot;black&amp;quot;, add = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-02-18-statistical-rethinking-chapter-08_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;practice-h3&#34;&gt;Practice H3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Consider again the &lt;code&gt;data(rugged)&lt;/code&gt; data on economic development and terrain ruggedness, examined in this chapter. One of the African countries in that example, Seychelles, is far outside the cloud of other nations, being a rare country with both relatively high GDP and high ruggedness. Seychelles is also unusual, in that it is a group of islands far from the coast of mainland Africa, and its main economic activity is tourism.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(rugged)
d &amp;lt;- rugged
d &amp;lt;- rugged[complete.cases(rugged$rgdppc_2000), ]
d$log_gdp &amp;lt;- log(d$rgdppc_2000)
d$log_gdp_std &amp;lt;- d$log_gdp / mean(d$log_gdp)
d$rugged_std &amp;lt;- d$rugged / max(d$rugged)
d$cid &amp;lt;- ifelse(d$cont_africa == 1, 1, 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-a&#34;&gt;Part A&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Focus on model m8.5 from the chapter. Use WAIC point-wise penalties and PSIS Pareto k values to measure relative influence of each country. By these criteria, is Seychelles influencing the results? Are there other nations that are relatively influential? If so, can you explain why?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;br&gt;
Firstly, the model that the exercise is after is not model m8.5, but model m8.3. Let&amp;rsquo;s run that and look at the PSIS point-wise values:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m.H3a &amp;lt;- quap(alist(
  log_gdp_std ~ dnorm(mu, sigma),
  mu &amp;lt;- a[cid] + b[cid] * (rugged_std - 0.215),
  a[cid] ~ dnorm(1, 0.1),
  b[cid] ~ dnorm(0, 0.3),
  sigma ~ dexp(1)
),
data = d
)
precis(m.H3a, depth = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean          sd        5.5%       94.5%
## a[1]   0.8865690 0.015675759  0.86151610  0.91162189
## a[2]   1.0505745 0.009936639  1.03469382  1.06645515
## b[1]   0.1325281 0.074204674  0.01393472  0.25112152
## b[2]  -0.1425744 0.054749596 -0.23007484 -0.05507398
## sigma  0.1094945 0.005935353  0.10000867  0.11898035
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we look at point-wise WAIC values. I think there&amp;rsquo;s a pretty clear separation of high point-wise WAIC-values in the plot at around 1 so I draw that in and obtain the country names for these:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;WAIC &amp;lt;- WAIC(m.H3a, pointwise = TRUE)
plot(WAIC$WAIC)
abline(h = 1, col = &amp;quot;red&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-02-18-statistical-rethinking-chapter-08_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;as.character(d[WAIC$WAIC &amp;gt; 1, ]$country)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Austria&amp;quot;             &amp;quot;Bangladesh&amp;quot;          &amp;quot;Switzerland&amp;quot;         &amp;quot;Equatorial Guinea&amp;quot;   &amp;quot;Luxembourg&amp;quot;          &amp;quot;Republic of Moldova&amp;quot; &amp;quot;Seychelles&amp;quot;          &amp;quot;Uzbekistan&amp;quot;         
## [9] &amp;quot;Yemen&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we do the same with point-wise PSIS Pareto k values. Again, I believe there is a separation. This time at 0.35:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;PSIS &amp;lt;- PSIS(m.H3a, pointwise = TRUE)
plot(PSIS$k)
abline(h = .35, col = &amp;quot;red&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-02-18-statistical-rethinking-chapter-08_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;as.character(d[PSIS$k &amp;gt; .35, ]$country)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Switzerland&amp;quot; &amp;quot;Lesotho&amp;quot;     &amp;quot;Seychelles&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Honestly, I cannot make too much sense of the countries I obtained via the point-wise WAIC-values, but the point-wise PSIS Pareto k values make obvious sense here. All of these are extremely rugged and much richer than even their surrounding flat countries.&lt;/p&gt;
&lt;h4 id=&#34;part-b&#34;&gt;Part B&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Now use robust regression, as described in the previous chapter. Modify m8.5 to use a Student-t distribution with $Î½ = 2$. Does this change the results in a substantial way?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m.H3b &amp;lt;- quap(alist(
  log_gdp_std ~ dstudent(2, mu, sigma),
  mu &amp;lt;- a[cid] + b[cid] * (rugged_std - 0.215),
  a[cid] ~ dnorm(1, 0.1),
  b[cid] ~ dnorm(0, 0.3),
  sigma ~ dexp(1)
),
data = d
)
precis(m.H3b, depth = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              mean          sd         5.5%       94.5%
## a[1]   0.86265354 0.016153871  0.836836529  0.88847054
## a[2]   1.04573322 0.010971670  1.028198374  1.06326807
## b[1]   0.11279640 0.075195199 -0.007380055  0.23297285
## b[2]  -0.21362933 0.063538165 -0.315175587 -0.11208307
## sigma  0.08452953 0.006732778  0.073769255  0.09528981
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The parameter estimates changed slightly, but not crazily so. Let&amp;rsquo;s look at the point-wise importance again. This time, I see a split in WAIC values around 2:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;WAIC &amp;lt;- WAIC(m.H3b, pointwise = TRUE)
plot(WAIC$WAIC)
abline(h = 2, col = &amp;quot;red&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-02-18-statistical-rethinking-chapter-08_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;as.character(d[WAIC$WAIC &amp;gt; 2, ]$country)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Switzerland&amp;quot;       &amp;quot;Equatorial Guinea&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now for point-wise PSIS-values. This time, I see don&amp;rsquo;t really a separation so I&amp;rsquo;ll just pull out the highest ranking nations:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;PSIS &amp;lt;- PSIS(m.H3b, pointwise = TRUE)
plot(PSIS$k)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-02-18-statistical-rethinking-chapter-08_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;as.character(d$country[as.numeric(rownames(PSIS[order(PSIS$k), ])[1:5])])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Malawi&amp;quot;    &amp;quot;Ethiopia&amp;quot;  &amp;quot;Yemen&amp;quot;     &amp;quot;Singapore&amp;quot; &amp;quot;Burundi&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Interestingly, only Switzerland shows up in our subset of potentially highly influential nations. That being said, the outliers in the WAIC and point-wise PSIS values are much closer to the main cloud of data points than they were previously. I expect this to be the effect of the ropbust regression.&lt;/p&gt;
&lt;h3 id=&#34;practice-h4&#34;&gt;Practice H4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; The values in &lt;code&gt;data(nettle)&lt;/code&gt; are data on language diversity in 74 nations. The meaning of each column is given below:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;country&lt;/code&gt;: Name of the country&lt;/li&gt;
&lt;li&gt;&lt;code&gt;num.lang&lt;/code&gt;: Number of recognized languages spoken&lt;/li&gt;
&lt;li&gt;&lt;code&gt;area&lt;/code&gt;: Area in square kilometres&lt;/li&gt;
&lt;li&gt;&lt;code&gt;k.pop&lt;/code&gt;: Population, in thousands&lt;/li&gt;
&lt;li&gt;&lt;code&gt;num.stations&lt;/code&gt;: Number of weather stations that provided data for the next two columns&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mean.growing.season&lt;/code&gt;: Average length of growing season, in months&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sd.growing.season&lt;/code&gt;: Standard deviation of length of growing season, in months&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Use these data to evaluate the hypothesis that language diversity is partly a product of food security. The notion is that, in productive ecologies, people donât need large social networks to buffer them against risk of food shortfalls. This means ethnic groups can be smaller and more self-sufficient, leading to more languages per capita. In contrast, in a poor ecology, there is more subsistence risk, and so human societies have adapted by building larger networks of mutual obligation to provide food insurance. This in turn creates social forces that help prevent languages from diversifying. Specifically, you will try to model the number of languages per capita as the outcome variable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;d$lang.per.cap &amp;lt;- d$num.lang / d$k.pop
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use the logarithm of this new variable as your regression outcome. (A count model would be better here, but youâll learn those later, in Chapter 11.) This problem is open ended, allowing you to decide how you address the hypotheses and the uncertain advice the modelling provides. If you think you need to use WAIC any place, please do. If you think you need certain priors, argue for them. If you think you need to plot predictions in a certain way, please do. Just try to honestly evaluate the main effects of both &lt;code&gt;mean.growing.season&lt;/code&gt; and &lt;code&gt;sd.growing.season&lt;/code&gt;, as well as their two-way interaction, as outlined in parts (a), (b), and (c) below. If you are not sure which approach to use, try several.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(nettle)
d &amp;lt;- nettle
d$lang.per.cap &amp;lt;- d$num.lang / d$k.pop
d$log_lpc &amp;lt;- log(d$lang.per.cap)
d$log_area &amp;lt;- log(d$area)
d$log_area.c &amp;lt;- d$log_area - mean(d$log_area)
d$mgs.c &amp;lt;- d$mean.growing.season - mean(d$mean.growing.season)
d$sgs.c &amp;lt;- d$sd.growing.season - mean(d$sd.growing.season)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-a-1&#34;&gt;Part A&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Evaluate the hypothesis that language diversity, as measured by &lt;code&gt;log(lang.per.cap)&lt;/code&gt;, is positively associated with the average length of the growing season, &lt;code&gt;mean.growing.season&lt;/code&gt;. Consider &lt;code&gt;log(area)&lt;/code&gt; in your regression(s) as a covariate (not an interaction). Interpret your results.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Let&amp;rsquo;s start this off by building a model that attempts to identify the logarithmic language per capita as defined above (&lt;code&gt;log_lpc&lt;/code&gt;). I use this as my response variable to smooth out the effects of highly multilingual communities which would otherwise look like freakish outliers:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;### Model
m.H4a &amp;lt;- quap(
  alist(
    log_lpc ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bM * mean.growing.season + bA * log_area.c,
    a ~ dnorm(mean(d$log_lpc), sd(d$log_lpc)),
    bM ~ dnorm(0, 2),
    bA ~ dnorm(0, 2),
    sigma ~ dunif(0, 10)
  ),
  data = d
)
precis(m.H4a)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean         sd       5.5%        94.5%
## a     -6.3965435 0.40827385 -7.0490440 -5.744043063
## bM     0.1349813 0.05390024  0.0488383  0.221124289
## bA    -0.2091213 0.13661084 -0.4274518  0.009209185
## sigma  1.3894990 0.11424753  1.2069094  1.572088628
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;### Prediction plot
mean.growing.season.seq &amp;lt;- seq(from = min(d$mean.growing.season), to = max(d$mean.growing.season), length.out = 50)
mu &amp;lt;- link(m.H4a, data = data.frame(mean.growing.season = mean.growing.season.seq, log_area.c = 0), refresh = 0)
mu.mean &amp;lt;- apply(mu, 2, mean)
mu.PI &amp;lt;- apply(mu, 2, PI, prob = 0.97)
plot(log_lpc ~ mean.growing.season, data = d, col = rangi2)
lines(mean.growing.season.seq, mu.mean)
shade(mu.PI, mean.growing.season.seq)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-02-18-statistical-rethinking-chapter-08_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we can see, there&amp;rsquo;s a positive linear relationship between &lt;code&gt;mean.growing.season&lt;/code&gt; and number of languages spoken by a population while also conditioning on area of the nations in question. I highly doubt that there is any causality here, however and instead hypothesise that &lt;code&gt;mean.growing.season&lt;/code&gt; can be used as a proxy for &amp;ldquo;sunniness&amp;rdquo; of a nation which, from anecdotal evidence, is much more appealing to immigration from all over the world than freezing countries with short growing seasons. To follow the rationale of the task however, this is evidence of ample food supply allowing for cultural diversity and thus higher language count.&lt;/p&gt;
&lt;h4 id=&#34;part-b-1&#34;&gt;Part B&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Now evaluate the hypothesis that language diversity is negatively associated with the standard deviation of length of growing season, &lt;code&gt;sd.growing.season&lt;/code&gt;. This hypothesis follows from uncertainty in harvest favouring social insurance through larger social networks and therefore fewer languages. Again, consider &lt;code&gt;log(area)&lt;/code&gt; as a covariate (not an interaction). Interpret your results.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;### Model
m.H4b &amp;lt;- quap(
  alist(
    log_lpc ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bS * sd.growing.season + bA * log_area.c,
    a ~ dnorm(mean(d$log_lpc), sd(d$log_lpc)),
    bS ~ dnorm(0, 5),
    bA ~ dnorm(0, 5),
    sigma ~ dunif(0, 10)
  ),
  data = d
)
precis(m.H4b)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean        sd       5.5%        94.5%
## a     -5.1199557 0.3485729 -5.6770426 -4.562868804
## bS    -0.2005191 0.1824921 -0.4921768  0.091138529
## bA    -0.2433936 0.1552325 -0.4914852  0.004697949
## sigma  1.4384790 0.1182463  1.2494986  1.627459505
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;### Prediction plot
sd.growing.season.seq &amp;lt;- seq(from = min(d$sd.growing.season), to = max(d$sd.growing.season), length.out = 50)
mu &amp;lt;- link(m.H4b, data = data.frame(sd.growing.season = sd.growing.season.seq, log_area.c = 0), refresh = 0)
mu.mean &amp;lt;- apply(mu, 2, mean)
mu.PI &amp;lt;- apply(mu, 2, PI, prob = 0.97)
plot(log_lpc ~ sd.growing.season, data = d, col = rangi2)
lines(sd.growing.season.seq, mu.mean)
shade(mu.PI, sd.growing.season.seq)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-02-18-statistical-rethinking-chapter-08_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Again, I find myself agreeing with the hypothesis seeing how I have identified a negative relationship between &lt;code&gt;sd.growing.season&lt;/code&gt; and language count.&lt;/p&gt;
&lt;h4 id=&#34;part-c&#34;&gt;Part C&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Finally, evaluate the hypothesis that &lt;code&gt;mean.growing.season&lt;/code&gt; and &lt;code&gt;sd.growing.season&lt;/code&gt; interact to synergistically reduce language diversity. The idea is that, in nations with longer average growing seasons, high variance makes storage and redistribution even more important than it would be otherwise. That way, people can cooperate to preserve and protect windfalls to be used during the droughts. These forces in turn may lead to greater social integration and fewer languages.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Again, I build a model which conditions on centred logarithmic area of nations, while building an interaction between &lt;code&gt;mean.growing.season&lt;/code&gt; and &lt;code&gt;sd.growing.season&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m.H4c &amp;lt;- quap(
  alist(
    log_lpc ~ dnorm(mu, sigma),
    mu &amp;lt;- a +
      bM * mean.growing.season +
      bS * sd.growing.season +
      bMS * mean.growing.season * sd.growing.season +
      bA * log_area.c,
    a ~ dnorm(mean(d$log_lpc), sd(d$log_lpc)),
    bM ~ dnorm(0, 5),
    bS ~ dnorm(0, 5),
    bA ~ dnorm(0, 5),
    bMS ~ dnorm(0, 5),
    sigma ~ dunif(0, 10)
  ),
  data = d
)
precis(m.H4c)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               mean         sd       5.5%       94.5%
## a     -6.771794698 0.55161186 -7.6533770 -5.89021241
## bM     0.277261541 0.07288992  0.1607694  0.39375371
## bS     0.327529661 0.37052336 -0.2646382  0.91969756
## bA    -0.005517158 0.15891352 -0.2594917  0.24845734
## bMS   -0.098088334 0.04565210 -0.1710492 -0.02512746
## sigma  1.307674873 0.10763683  1.1356504  1.47969932
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We seem to be quite sure of the interaction effect &lt;code&gt;bMS&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To investiagte the interaction of the two predictors we are interested in, let&amp;rsquo;s produce a Tryptich each:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;par(mfrow = c(2, 3))
# Discretize variables into groups
d$mean.growing.season.group &amp;lt;- cut(
  d$mean.growing.season,
  breaks = quantile(d$mean.growing.season, probs = c(0, 1 / 3, 2 / 3, 1)),
  include.lowest = TRUE,
  dig.lab = 2
)
d$sd.growing.season.group &amp;lt;- cut(
  d$sd.growing.season,
  breaks = quantile(d$sd.growing.season, probs = c(0, 1 / 3, 2 / 3, 1)),
  include.lowest = TRUE,
  dig.lab = 2
)
# Plot first row as mean against SD
mean.growing.season.seq &amp;lt;- seq(from = min(d$mean.growing.season), to = max(d$mean.growing.season), length.out = 50)
for (group in levels(d$sd.growing.season.group)) {
  dt &amp;lt;- d[d$sd.growing.season.group == group, ]
  plot(log_lpc ~ mean.growing.season,
    data = dt, col = rangi2, xlim = c(min(d$mean.growing.season), max(d$mean.growing.season)), ylim = c(min(d$log_lpc), max(d$log_lpc)),
    main = paste(&amp;quot;SD GS = &amp;quot;, group), xlab = &amp;quot;Mean GS&amp;quot;
  )
  mu &amp;lt;- link(m.H4c,
    data = data.frame(mean.growing.season = mean.growing.season.seq, sd.growing.season = mean(dt$sd.growing.season), log_area.c = 0),
    refresh = 0
  )
  mu.mean &amp;lt;- apply(mu, 2, mean)
  mu.PI &amp;lt;- apply(mu, 2, PI, prob = 0.97)
  lines(mean.growing.season.seq, mu.mean)
  lines(mean.growing.season.seq, mu.PI[1, ], lty = 2)
  lines(mean.growing.season.seq, mu.PI[2, ], lty = 2)
}
# Plot second row as SD against mean
sd.growing.season.seq &amp;lt;- seq(from = min(d$sd.growing.season), to = max(d$sd.growing.season), length.out = 50)
for (group in levels(d$mean.growing.season.group)) {
  dt &amp;lt;- d[d$mean.growing.season.group == group, ]
  plot(log_lpc ~ sgs.c,
    data = dt, col = rangi2, xlim = c(min(d$sd.growing.season), max(d$sd.growing.season)), ylim = c(min(d$log_lpc), max(d$log_lpc)),
    main = paste(&amp;quot;Mean GS = &amp;quot;, group), xlab = &amp;quot;SD GS&amp;quot;
  )
  mu &amp;lt;- link(m.H4c,
    data = data.frame(sd.growing.season = sd.growing.season.seq, mean.growing.season = mean(dt$mean.growing.season), log_area.c = 0),
    refresh = 0
  )
  mu.mean &amp;lt;- apply(mu, 2, mean)
  mu.PI &amp;lt;- apply(mu, 2, PI, prob = 0.97)
  lines(sd.growing.season.seq, mu.mean)
  lines(sd.growing.season.seq, mu.PI[1, ], lty = 2)
  lines(sd.growing.season.seq, mu.PI[2, ], lty = 2)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-02-18-statistical-rethinking-chapter-08_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;These plots show that the association between mean length of growing season and language diversity is positive when SD length of growing season is low, but is basically zero when SD length of growing season is high. Similarly, the association between SD length of growing season and language diversity is basically zero when mean length of growing season is low, but is negative when mean length of growing season is high. This is consistent with the hypothesis presented in the question.&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.5 (2021-03-31)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19043)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.1252  LC_CTYPE=English_United Kingdom.1252    LC_MONETARY=English_United Kingdom.1252 LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.1252    
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] viridis_0.6.0        viridisLite_0.4.0    rethinking_2.13      rstan_2.21.2         ggplot2_3.3.6        StanHeaders_2.21.0-7
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.7         mvtnorm_1.1-1      lattice_0.20-41    prettyunits_1.1.1  ps_1.6.0           assertthat_0.2.1   digest_0.6.27      utf8_1.2.1         V8_3.4.1           R6_2.5.0          
## [11] backports_1.2.1    stats4_4.0.5       evaluate_0.14      coda_0.19-4        highr_0.9          blogdown_1.3       pillar_1.6.0       rlang_0.4.11       curl_4.3.2         callr_3.7.0       
## [21] jquerylib_0.1.4    R.utils_2.10.1     R.oo_1.24.0        rmarkdown_2.7      styler_1.4.1       stringr_1.4.0      loo_2.4.1          munsell_0.5.0      compiler_4.0.5     xfun_0.22         
## [31] pkgconfig_2.0.3    pkgbuild_1.2.0     shape_1.4.5        htmltools_0.5.1.1  tidyselect_1.1.0   tibble_3.1.1       gridExtra_2.3      bookdown_0.22      codetools_0.2-18   matrixStats_0.61.0
## [41] fansi_0.4.2        crayon_1.4.1       dplyr_1.0.5        withr_2.4.2        MASS_7.3-53.1      R.methodsS3_1.8.1  grid_4.0.5         jsonlite_1.7.2     gtable_0.3.0       lifecycle_1.0.0   
## [51] DBI_1.1.1          magrittr_2.0.1     scales_1.1.1       RcppParallel_5.1.2 cli_3.0.0          stringi_1.5.3      bslib_0.2.4        ellipsis_0.3.2     generics_0.1.0     vctrs_0.3.7       
## [61] rematch2_2.1.2     tools_4.0.5        R.cache_0.14.0     glue_1.4.2         purrr_0.3.4        processx_3.5.1     yaml_2.2.1         inline_0.3.17      colorspace_2.0-0   knitr_1.33        
## [71] sass_0.3.1
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Statistical Significance in Biology</title>
      <link>https://www.erikkusch.com/courses/excursions-into-biostatistics/8_statistical-significance-in-biology-conventions-abstractions-and-the-future/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/excursions-into-biostatistics/8_statistical-significance-in-biology-conventions-abstractions-and-the-future/</guid>
      <description>&lt;p&gt;I have prepared some &lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/Excursions-into-Biostatistics/Statistical-Significance-in-Biology---Conventions,-Abstractions,-and-the-Future.html&#34; target=&#34;_blank&#34;&gt; Lecture Slides &lt;/a&gt; for this session.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Correlation Tests</title>
      <link>https://www.erikkusch.com/courses/an-introduction-to-biostatistics/correlation-tests/</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/an-introduction-to-biostatistics/correlation-tests/</guid>
      <description>&lt;h2 id=&#34;theory&#34;&gt;Theory&lt;/h2&gt;
&lt;p&gt;Welcome to our third practical experience in R. Throughout the following notes, I will introduce you to a couple statistical correlation approaches that might be useful to you and are, to varying degrees, often used in biology. To do so, I will enlist the sparrow data set we handled in our first exercise.&lt;/p&gt;
&lt;p&gt;I have prepared some &lt;a href=&#34;https://www.erikkusch.com/courses/an-introduction-to-biostatistics/09---Correlation-Tests_Handout.html&#34; target=&#34;_blank&#34;&gt; Lecture Slides &lt;/a&gt; for this session.&lt;/p&gt;
&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;p&gt;Find the data for this exercise &lt;a href=&#34;https://www.erikkusch.com/courses/an-introduction-to-biostatistics/Data/1%20-%20Sparrow_Data_READY.rds&#34; target=&#34;_blank&#34;&gt; here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;preparing-our-procedure&#34;&gt;Preparing Our Procedure&lt;/h2&gt;
&lt;p&gt;To ensure others can reproduce our analysis we run the following three lines of code at the beginning of our &lt;code&gt;R&lt;/code&gt; coding file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rm(list=ls()) # clearing environment
Dir.Base &amp;lt;- getwd() # soft-coding our working directory
Dir.Data &amp;lt;- paste(Dir.Base, &amp;quot;Data&amp;quot;, sep=&amp;quot;/&amp;quot;) # soft-coding our data directory 
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;packages&#34;&gt;Packages&lt;/h3&gt;
&lt;p&gt;Using the following, user-defined function, we install/load all the necessary packages into our current &lt;code&gt;R&lt;/code&gt; session.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# function to load packages and install them if they haven&#39;t been installed yet
install.load.package &amp;lt;- function(x) {
  if (!require(x, character.only = TRUE))
    install.packages(x)
  require(x, character.only = TRUE)
}
package_vec &amp;lt;- c(&amp;quot;DescTools&amp;quot;, # Needed for Contingency Coefficient
                 &amp;quot;ggplot2&amp;quot; # needed for data visualisation
                 )
sapply(package_vec, install.load.package)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: DescTools
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggplot2
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## DescTools   ggplot2 
##      TRUE      TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;loading-data&#34;&gt;Loading Data&lt;/h3&gt;
&lt;p&gt;During our first exercise (Data Mining and Data Handling - Fixing The Sparrow Data Set) we saved our clean data set as an RDS file. To load this, we use the &lt;code&gt;readRDS()&lt;/code&gt; command that comes with base &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df_base &amp;lt;- readRDS(file = paste(Dir.Data, &amp;quot;/1 - Sparrow_Data_READY.rds&amp;quot;, sep=&amp;quot;&amp;quot;))
Data_df &amp;lt;- Data_df_base # duplicate and save initial data on a new object
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;nominal-scale---contingency-coefficient&#34;&gt;Nominal Scale - Contingency Coefficient&lt;/h2&gt;
&lt;p&gt;We can analyse correlations/dependencies of variables of the categorical kind using the contingency coefficient by calling the &lt;code&gt;ContCoef()&lt;/code&gt; function of base &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Keep in mind that the contingency coefficient is not &lt;em&gt;really&lt;/em&gt; a measure of correlation but merely of association of variables. A value of $c \approx 0$ indicates independent variables.&lt;/p&gt;
&lt;h3 id=&#34;predation&#34;&gt;Predation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Are colour morphs of Passer domesticus linked to predator presence and/or predator type?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This analysis builds on our findings within our previous exercise (Nominal Tests - Analysing The Sparrow Data Set). Remember that, using the two-sample situation Chi-Squared Test, we found no change in treatment effects (as far as colour polymorphism went) for predator type values but did so regarding the presence of predators. Let&amp;rsquo;s repeat this here:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(Data_df$Colour, Data_df$Predator.Presence)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        
##          No Yes
##   Black  64 292
##   Brown 211  87
##   Grey   82 331
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ContCoef(table(Data_df$Colour, Data_df$Predator.Presence))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.4421914
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(Data_df$Colour, Data_df$Predator.Type)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        
##         Avian Non-Avian
##   Black   197        95
##   Brown    60        27
##   Grey    233        98
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ContCoef(table(Data_df$Colour, Data_df$Predator.Type))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.02957682
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, we find the same results as when using the Chi-Squared statistic and conclude that colour morphs of the common house sparrow are likely to be driven by predator presence but not the type of predator that is present.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Are nesting sites of Passer domesticus linked to predator presence and/or predator type?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Again, following our two-sample situation Chi-Squared analysis from last exercise, we want to test whether nesting site and predator presence/predator type are linked. The Chi-Squared analyses identified a possible link of nesting site and predator type but nor predator presence.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(Data_df$Nesting.Site, Data_df$Predator.Presence)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        
##          No Yes
##   Shrub  87 205
##   Tree   94 137
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ContCoef(table(Data_df$Nesting.Site, Data_df$Predator.Presence))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1130328
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(Data_df$Nesting.Site, Data_df$Predator.Type)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        
##         Avian Non-Avian
##   Shrub   182        23
##   Tree     49        88
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ContCoef(table(Data_df$Nesting.Site, Data_df$Predator.Type))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.4851588
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Whilst there doesn&amp;rsquo;t seem to be any strong evidence linking nesting site and predator presence, predator type seems to be linked to what kind of nesting site &lt;em&gt;Passer domesticus&lt;/em&gt; prefers thus supporting our Chi-Squared results.&lt;/p&gt;
&lt;h3 id=&#34;sexual-dimorphism&#34;&gt;Sexual Dimorphism&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Are sex ratios of Passer domesticus related to climate types?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Recall that, in our last exercise, we found no discrepancy of proportions of the sexes among the entire data set using a binomial test. What we didn&amp;rsquo;t check yet was whether the sexes are distributed across the sites somewhat homogeneously or whether the sex ratios might be skewed according to climate types. Let&amp;rsquo;s do this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# prepare climate type testing data
Data_df &amp;lt;- Data_df_base
Index &amp;lt;- Data_df$Index
# select all data belonging to the stations at which all parameters except for climate type are held constant
Rows &amp;lt;- which(Index == &amp;quot;SI&amp;quot; | Index == &amp;quot;UK&amp;quot; | Index == &amp;quot;RE&amp;quot; | Index == &amp;quot;AU&amp;quot; )
Data_df &amp;lt;- Data_df[Rows,]
Data_df$Climate &amp;lt;- droplevels(factor(Data_df$Climate))

# analysis
table(Data_df$Sex, Data_df$Climate)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         
##          Coastal Continental
##   Female      91          76
##   Male        72          78
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ContCoef(table(Data_df$Sex, Data_df$Climate))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.06470702
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Quite obviously, they aren&amp;rsquo;t and, if there are any patterns in sex ratios to emerge, these are not likely to stem from climate types. Also keep in mind that we have a plethora of other variables at play whilst the information contained within the climate type variable is somewhat constrained and, in this case, bordering on uninformative (i.e. a coastal site in the Arctic might be more closely resembled by a continental mid-latitude site than by a tropical coastal site).&lt;/p&gt;
&lt;h2 id=&#34;ordinal-scale---kendalls-tau&#34;&gt;Ordinal Scale - Kendall&amp;rsquo;s Tau&lt;/h2&gt;
&lt;h3 id=&#34;climate-warmingextremes&#34;&gt;Climate Warming/Extremes&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Do heavier sparrows have heavier/less eggs?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A heavier weight of individual females alludes to a higher pool of resources being allocated by said individuals. There are multiple ways they might make use of it, one of them being investment in reproduction. To test how heavier females of &lt;em&gt;Passer domesticus&lt;/em&gt; utilise their resources in reproduction, we use a Kendall&amp;rsquo;s Tau approach to finding links between female weight and average egg weight per nest/number of eggs per nest.&lt;br&gt;
Obviously, both weight variables are metric in nature and so we could use other methods as well. On top of that, we first need to convert these into ranks before being able to run a Kendall&amp;rsquo;s Tau analysis as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# overwriting changes in Data_df with base data
Data_df &amp;lt;- Data_df_base
# Establishing Ranks of Egg Weight
RankedEggWeight &amp;lt;- rank(Data_df$Egg.Weight[which(Data_df$Sex == &amp;quot;Female&amp;quot;)],
                        ties.method = &amp;quot;average&amp;quot;)
# Establishing Ranks of Female Weight
RankedWeight_Female &amp;lt;- rank(Data_df$Weight[which(Data_df$Sex == &amp;quot;Female&amp;quot;)],
                            ties.method = &amp;quot;average&amp;quot;)
# Extracting Numbers of Eggs
RankedEggs &amp;lt;- Data_df$Number.of.Eggs[which(Data_df$Sex == &amp;quot;Female&amp;quot;)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Luckily enough, the number of eggs per nest already represent a ranked (ordinal) variable and so we can move straight on to running our analyses:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Test ranked female weight vs. ranked egg weight
cor.test(x = RankedWeight_Female, y = RankedEggWeight, 
         use = &amp;quot;pairwise.complete.obs&amp;quot;, method = &amp;quot;kendall&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Kendall&#39;s rank correlation tau
## 
## data:  RankedWeight_Female and RankedEggWeight
## z = 19.771, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true tau is not equal to 0
## sample estimates:
##       tau 
## 0.5804546
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is strong evidence to suggest that heavier females tend to lay heavier eggs (tau = 0.5804546 at p $\approx$ 0).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Test ranked female weight vs. number of eggs
cor.test(x = RankedWeight_Female, y = RankedEggs, 
         use = &amp;quot;pairwise.complete.obs&amp;quot;, method = &amp;quot;kendall&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Kendall&#39;s rank correlation tau
## 
## data:  RankedWeight_Female and RankedEggs
## z = -21.787, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true tau is not equal to 0
## sample estimates:
##        tau 
## -0.6880932
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Additionally, the heavier a female of &lt;em&gt;Passer domesticus&lt;/em&gt;, the less eggs she produces (tau = -0.6880932 at p $\approx$ 0).&lt;/p&gt;
&lt;p&gt;Now we can visualise the underlying patterns:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot_df &amp;lt;- data.frame(RankedWeight = RankedWeight_Female,
                      RankedEggWeight = RankedEggWeight,
                      RankedEggNumber = RankedEggs)
# plot ranked female weight vs. ranked egg weight
ggplot(data = plot_df, aes(x = RankedWeight, y = RankedEggWeight)) +
  geom_point() + theme_bw() + stat_smooth(method = &amp;quot;lm&amp;quot;) + 
  labs(title = &amp;quot;Ranked female weight of Passer domesticus vs. Ranked weigt of eggs&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &#39;y ~ x&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;09---Correlation-Tests_files/figure-html/Kendall.26b-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# plot ranked female weight vs. number of eggs
ggplot(data = plot_df, aes(x = RankedWeight, y = RankedEggNumber)) +
  geom_point() + theme_bw() + stat_smooth(method = &amp;quot;lm&amp;quot;) + 
  labs(title = &amp;quot;Ranked female weight of Passer domesticus vs. Ranked number of eggs&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &#39;y ~ x&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;09---Correlation-Tests_files/figure-html/Kendall.26b-2.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This highlights an obvious and intuitive trade-off in nature and has us &lt;strong&gt;reject the null hypotheses&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;metric-and-ordinal-scales&#34;&gt;Metric and Ordinal Scales&lt;/h2&gt;
&lt;p&gt;Metric scale correlation analyses call for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Spearman&lt;/em&gt; correlation test (non-parametric)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Pearson&lt;/em&gt; correlation test (parametric, requires data to be normal distributed)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;testing-for-normality&#34;&gt;Testing for Normality&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;DISCLAIMER:&lt;/strong&gt; I do not expect you to do it this way as of right now but wanted to give you reference material of how to automate this testing step.&lt;/p&gt;
&lt;p&gt;Since most of our following analyses are focussing on latitude effects (i.e. &amp;ldquo;climate warming&amp;rdquo;), we need to alter our base data. Before we can run the analyses, we need to eliminate the sites that we have set aside for testing climate extreme effects on (Siberia, United Kingdom, Reunion and Australia) from the data set. We are also not interested in all variables and so reduce the data set further.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df &amp;lt;- Data_df_base
Index &amp;lt;- Data_df$Index
Rows &amp;lt;- which(Index != &amp;quot;SI&amp;quot; &amp;amp; Index != &amp;quot;UK&amp;quot; &amp;amp; Index != &amp;quot;RE&amp;quot; &amp;amp; Index != &amp;quot;AU&amp;quot; )
Data_df &amp;lt;- Data_df[Rows,c(&amp;quot;Index&amp;quot;, &amp;quot;Latitude&amp;quot;, &amp;quot;Weight&amp;quot;, &amp;quot;Height&amp;quot;, &amp;quot;Wing.Chord&amp;quot;, &amp;quot;Nesting.Height&amp;quot;, 
                             &amp;quot;Number.of.Eggs&amp;quot;, &amp;quot;Egg.Weight&amp;quot;)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to know which test we can use with which variable, we need to first identify whether our data is normal distributed using the Shapiro-Test (Seminar 3) as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Normal_df &amp;lt;- data.frame(Normality = as.character(), stringsAsFactors = FALSE)
for(i in 2:length(colnames(Data_df))){
  Normal_df[1,i] &amp;lt;- colnames(Data_df)[i]
  Normal_df[2,i] &amp;lt;- round(shapiro.test(as.numeric(Data_df[,i]))$p.value, 2)
}
colnames(Normal_df) &amp;lt;- c()
rownames(Normal_df) &amp;lt;- c(&amp;quot;Variable&amp;quot;, &amp;quot;p&amp;quot;)
Normal_df &amp;lt;- Normal_df[,-1] # removing superfluous Index column
Normal_df
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                                                         
## Variable Latitude Weight Height Wing.Chord Nesting.Height Number.of.Eggs
## p               0      0      0          0              0              0
##                    
## Variable Egg.Weight
## p                 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unfortunately, none of these variables seem to be normal distributed (this was to be expected for some, to be fair) thus barring us from using Pearson correlation on the entire data set leaving us with the Spearman correlation method.&lt;/p&gt;
&lt;h3 id=&#34;spearman&#34;&gt;Spearman&lt;/h3&gt;
&lt;h4 id=&#34;climate-warmingextremes-1&#34;&gt;Climate Warming/Extremes&lt;/h4&gt;
&lt;h5 id=&#34;heightlength&#34;&gt;Height/Length&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;Do height/length records of Passer domesticus and latitude correlate?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Following Bergmann&amp;rsquo;s rule, we expect a positive correlation between sparrow height/length and absolute values of latitude:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cor.test(x = abs(Data_df$Latitude), y = Data_df$Height, 
         use = &amp;quot;pairwise.complete.obs&amp;quot;, method = &amp;quot;spearman&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in cor.test.default(x = abs(Data_df$Latitude), y = Data_df$Height, :
## Cannot compute exact p-value with ties
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Spearman&#39;s rank correlation rho
## 
## data:  abs(Data_df$Latitude) and Data_df$Height
## S = 128104735, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true rho is not equal to 0
## sample estimates:
##        rho 
## -0.8219373
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(data = Data_df, aes(x = abs(Latitude), y = Height)) +
  geom_point() + theme_bw() + stat_smooth(method = &amp;quot;lm&amp;quot;) + 
  labs(title = &amp;quot;Height of Passer domesticus vs. Latitude&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &#39;y ~ x&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;09---Correlation-Tests_files/figure-html/Spearman1-1.png&#34; width=&#34;1440&#34; /&gt;
Interestingly enough, our analysis yields a negative correlation which would disproof Bermann&amp;rsquo;s rule. This is a good example to show how important biological background knowledge is when doing biostatistics. Whilst a pure statistician might now believe to have just dis-proven a big rule of biology, it should be apparent to any biologist that Bergmann spoke of &amp;ldquo;bigger&amp;rdquo; organisms in colder climates (higher latitudes) and not of &amp;ldquo;taller&amp;rdquo; individuals. What our sparrows lack in height, they might make up for in circumference. This is an example where we would &lt;strong&gt;reject the null hypothesis&lt;/strong&gt; but shouldn&amp;rsquo;t &lt;strong&gt;accept the alternative hypothesis&lt;/strong&gt; based on biological understanding.&lt;/p&gt;
&lt;h5 id=&#34;weight&#34;&gt;Weight&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;Do weight records of Passer domesticus and latitude correlate?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Again, following Bergmann&amp;rsquo;s rule, we expect a positive correlation between sparrow weight and absolute values of latitude.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cor.test(x = abs(Data_df$Latitude), y = Data_df$Weight, 
         use = &amp;quot;pairwise.complete.obs&amp;quot;, method = &amp;quot;spearman&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in cor.test.default(x = abs(Data_df$Latitude), y = Data_df$Weight, :
## Cannot compute exact p-value with ties
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Spearman&#39;s rank correlation rho
## 
## data:  abs(Data_df$Latitude) and Data_df$Weight
## S = 9349037, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true rho is not equal to 0
## sample estimates:
##       rho 
## 0.8670357
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(data = Data_df, aes(x = abs(Latitude), y = Weight)) +
  geom_point() + theme_bw() + stat_smooth(method = &amp;quot;lm&amp;quot;) + 
  labs(title = &amp;quot;Weight of Passer domesticus vs. Latitude&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &#39;y ~ x&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;09---Correlation-Tests_files/figure-html/Spearman-1.png&#34; width=&#34;1440&#34; /&gt;
Bergmann was obviously right and we &lt;strong&gt;reject the null hypothesis&lt;/strong&gt;.&lt;/p&gt;
&lt;h5 id=&#34;wing-chord&#34;&gt;Wing Chord&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;Do wing chord/wing span records of Passer domesticus and latitude correlate?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We would expect sparrows in higher latitudes (e.g. colder climates) to have smaller wings as to radiate less body heat.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cor.test(x = abs(Data_df$Latitude), y = Data_df$Wing.Chord, 
         use = &amp;quot;pairwise.complete.obs&amp;quot;, method = &amp;quot;spearman&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in cor.test.default(x = abs(Data_df$Latitude), y = Data_df$Wing.Chord, :
## Cannot compute exact p-value with ties
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Spearman&#39;s rank correlation rho
## 
## data:  abs(Data_df$Latitude) and Data_df$Wing.Chord
## S = 134573929, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true rho is not equal to 0
## sample estimates:
##        rho 
## -0.9139437
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(data = Data_df, aes(x = abs(Latitude), y = Wing.Chord)) +
  geom_point() + theme_bw() + stat_smooth(method = &amp;quot;lm&amp;quot;) + 
  labs(title = &amp;quot;Wing chord of Passer domesticus vs. Latitude&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &#39;y ~ x&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;09---Correlation-Tests_files/figure-html/Spearman2-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And we were right! Sparrows have shorter wingspans in higher latitudes and we &lt;strong&gt;reject the null hypothesis&lt;/strong&gt;.&lt;/p&gt;
&lt;h5 id=&#34;number-of-eggs&#34;&gt;Number of Eggs&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;Do numbers of eggs per nest of Passer domesticus and latitude correlate?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Due to resource constraints in colder climates, we expect female &lt;em&gt;Passer domesticus&lt;/em&gt; individuals to invest in quality over quantity by prioritising caring your fledglings by educing the amount of eggs they produce.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cor.test(x = abs(Data_df$Latitude), y = Data_df$Number.of.Eggs, 
         use = &amp;quot;pairwise.complete.obs&amp;quot;, method = &amp;quot;spearman&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in cor.test.default(x = abs(Data_df$Latitude), y =
## Data_df$Number.of.Eggs, : Cannot compute exact p-value with ties
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Spearman&#39;s rank correlation rho
## 
## data:  abs(Data_df$Latitude) and Data_df$Number.of.Eggs
## S = 14501794, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true rho is not equal to 0
## sample estimates:
##      rho 
## -0.92853
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(data = Data_df, aes(x = abs(Latitude), y = Number.of.Eggs)) +
  geom_point() + theme_bw() + stat_smooth(method = &amp;quot;lm&amp;quot;) + 
  labs(title = &amp;quot;Number of eggs of Passer domesticus nests vs. Latitude&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &#39;y ~ x&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 394 rows containing non-finite values (stat_smooth).
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 394 rows containing missing values (geom_point).
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;09---Correlation-Tests_files/figure-html/Spearman3-1.png&#34; width=&#34;1440&#34; /&gt;
We were right. Female house sparrows produce less eggs per capita in higher latitudes and we &lt;strong&gt;reject the null hypothesis&lt;/strong&gt;.&lt;/p&gt;
&lt;h5 id=&#34;egg-weight&#34;&gt;Egg Weight&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;Does average weight of eggs per nest of Passer domesticus and latitude correlate?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Due to the reduced investment in egg numbers that we have just proven, we expect females of &lt;em&gt;Passer domesticus&lt;/em&gt; to allocate some of their saved resources into heavier eggs which may nurture unhatched offspring for longer and more effectively.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cor.test(x = abs(Data_df$Latitude), y = Data_df$Egg.Weight, 
         use = &amp;quot;pairwise.complete.obs&amp;quot;, method = &amp;quot;spearman&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in cor.test.default(x = abs(Data_df$Latitude), y = Data_df$Egg.Weight, :
## Cannot compute exact p-value with ties
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Spearman&#39;s rank correlation rho
## 
## data:  abs(Data_df$Latitude) and Data_df$Egg.Weight
## S = 1318221, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true rho is not equal to 0
## sample estimates:
##     rho 
## 0.82321
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(data = Data_df, aes(x = abs(Latitude), y = Egg.Weight)) +
  geom_point() + theme_bw() + stat_smooth(method = &amp;quot;lm&amp;quot;) + 
  labs(title = &amp;quot;Weight of Passer domesticus eggs vs. Latitude&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &#39;y ~ x&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 395 rows containing non-finite values (stat_smooth).
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 395 rows containing missing values (geom_point).
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;09---Correlation-Tests_files/figure-html/Spearman4-1.png&#34; width=&#34;1440&#34; /&gt;
Indeed, the higher the latitude, the heavier the average egg per nest of &lt;em&gt;Passer domesticus&lt;/em&gt; and we &lt;strong&gt;reject the null hypothesis&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;pearson&#34;&gt;Pearson&lt;/h3&gt;
&lt;p&gt;We already know that we can&amp;rsquo;t analyse the entire data set at once for these two variables since their data values are not normal distirbuted. How about site-wise variable value distributions, though?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Further reducing the data set
Data_df &amp;lt;- Data_df[,c(&amp;quot;Index&amp;quot;, &amp;quot;Weight&amp;quot;, &amp;quot;Height&amp;quot;)]
# establishing an empty data frame and an index vector that doesn&#39;t repeat
Normal_df &amp;lt;- data.frame(Normality = as.character(), stringsAsFactors = FALSE)
Indices &amp;lt;- as.character(unique(Data_df_base$Index[Rows]))
# site-wise shapiro test
for(i in 2:length(colnames(Data_df))){ # variables
  for(k in 1:length(Indices)){ # sites
    Normal_df[i,k] &amp;lt;- round(shapiro.test(as.numeric(
          Data_df[,i][which(Data_df_base$Index[Rows] == Indices[k])])
        )$p.value, 2)}} # site loop, variable loop
rownames(Normal_df) &amp;lt;- colnames(Data_df)
colnames(Normal_df) &amp;lt;- Indices
Normal_df[-1,] # remove superfluous index row
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          NU   MA   LO   BE   FG   SA   FI
## Weight 0.57 0.12 0.50 0.38 0.18 0.76 0.43
## Height 0.23 0.03 0.19 0.59 0.88 0.27 0.86
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;According to these results intra-site correlations of sparrow weight and height can be carried out! So, in order to show Pearson correlation, we run a simple, site-wise correlation analysis.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Do weight and height records of Passer domesticus correlate within each site?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To shed some light on our previous findings, we might want to see whether weight and height of sparrows correlate. Without running the analysis, we can conclude that they do because both correlate with latitude and are thus what we call &lt;strong&gt;collinear&lt;/strong&gt;. However, now we are running the analysis on a site level - does the correlation still exist?&lt;/p&gt;
&lt;p&gt;Take note that we are now using our entire data set again.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# overwriting altered Data_df
Data_df &amp;lt;- Data_df_base
# establishing an empty data frame and an index vector that doesn&#39;t repeat
Pearson_df &amp;lt;- data.frame(Pearson = as.character(), stringsAsFactors = FALSE)
Indices &amp;lt;- as.character(unique(Data_df$Index))
# site-internal correlation tests, weight and height
for(i in 1:length(unique(Data_df$Index))){
  Weights &amp;lt;- Data_df$Weight[which(Data_df$Index == Indices[i])]
  Heights &amp;lt;- Data_df$Height[which(Data_df$Index == Indices[i])]
  Pearson_df[1,i] &amp;lt;- round(cor.test(x = Weights, y = Heights, 
                                    use = &amp;quot;pairwise.complete.obs&amp;quot;)[[&amp;quot;estimate&amp;quot;]][[&amp;quot;cor&amp;quot;]], 2)
  Pearson_df[2,i] &amp;lt;- round(cor.test(x = Weights, y = Heights, 
                                    use = &amp;quot;pairwise.complete.obs&amp;quot;)$p.value, 2)}
colnames(Pearson_df) &amp;lt;- Indices
rownames(Pearson_df) &amp;lt;- c(&amp;quot;r&amp;quot;, &amp;quot;p&amp;quot;)
Pearson_df
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     SI   UK   AU   RE   NU   MA   LO   BE   FG   SA   FI
## r 0.76 0.83 0.77 0.75 0.84 0.84 0.79 0.82 0.79 0.76 0.81
## p    0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Apparently, it does. Heavier birds are taller!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Correlation Tests</title>
      <link>https://www.erikkusch.com/courses/biostat101/correlation-tests/</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/biostat101/correlation-tests/</guid>
      <description>&lt;h2 id=&#34;theory&#34;&gt;Theory&lt;/h2&gt;
&lt;p&gt;Welcome to our third practical experience in R. Throughout the following notes, I will introduce you to a couple statistical correlation approaches that might be useful to you and are, to varying degrees, often used in biology. To do so, I will enlist the sparrow data set we handled in our first exercise. I have prepared some slides for this session: &lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/biostat101/09---Correlation-Tests_Handout.html&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://www.erikkusch.com/courses/biostat101/09---BioStat101_featured.png&#34; width=&#34;900&#34; margin-top = &#34;0&#34;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;p&gt;Find the data for this exercise &lt;a href=&#34;https://github.com/ErikKusch/Homepage/raw/master/content/courses/biostat101/Data/1%20-%20Sparrow_Data_READY.rds&#34; target=&#34;_blank&#34;&gt; here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;preparing-our-procedure&#34;&gt;Preparing Our Procedure&lt;/h2&gt;
&lt;p&gt;To ensure others can reproduce our analysis we run the following three lines of code at the beginning of our &lt;code&gt;R&lt;/code&gt; coding file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rm(list=ls()) # clearing environment
Dir.Base &amp;lt;- getwd() # soft-coding our working directory
Dir.Data &amp;lt;- paste(Dir.Base, &amp;quot;Data&amp;quot;, sep=&amp;quot;/&amp;quot;) # soft-coding our data directory 
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;packages&#34;&gt;Packages&lt;/h3&gt;
&lt;p&gt;Using the following, user-defined function, we install/load all the necessary packages into our current &lt;code&gt;R&lt;/code&gt; session.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# function to load packages and install them if they haven&#39;t been installed yet
install.load.package &amp;lt;- function(x) {
  if (!require(x, character.only = TRUE))
    install.packages(x)
  require(x, character.only = TRUE)
}
package_vec &amp;lt;- c(&amp;quot;DescTools&amp;quot;, # Needed for Contingency Coefficient
                 &amp;quot;ggplot2&amp;quot; # needed for data visualisation
                 )
sapply(package_vec, install.load.package)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: DescTools
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggplot2
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## DescTools   ggplot2 
##      TRUE      TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;loading-data&#34;&gt;Loading Data&lt;/h3&gt;
&lt;p&gt;During our first exercise (Data Mining and Data Handling - Fixing The Sparrow Data Set) we saved our clean data set as an RDS file. To load this, we use the &lt;code&gt;readRDS()&lt;/code&gt; command that comes with base &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df_base &amp;lt;- readRDS(file = paste(Dir.Data, &amp;quot;/1 - Sparrow_Data_READY.rds&amp;quot;, sep=&amp;quot;&amp;quot;))
Data_df &amp;lt;- Data_df_base # duplicate and save initial data on a new object
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;nominal-scale---contingency-coefficient&#34;&gt;Nominal Scale - Contingency Coefficient&lt;/h2&gt;
&lt;p&gt;We can analyse correlations/dependencies of variables of the categorical kind using the contingency coefficient by calling the &lt;code&gt;ContCoef()&lt;/code&gt; function of base &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Keep in mind that the contingency coefficient is not &lt;em&gt;really&lt;/em&gt; a measure of correlation but merely of association of variables. A value of $c \approx 0$ indicates independent variables.&lt;/p&gt;
&lt;h3 id=&#34;predation&#34;&gt;Predation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Are colour morphs of Passer domesticus linked to predator presence and/or predator type?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This analysis builds on our findings within our previous exercise (Nominal Tests - Analysing The Sparrow Data Set). Remember that, using the two-sample situation Chi-Squared Test, we found no change in treatment effects (as far as colour polymorphism went) for predator type values but did so regarding the presence of predators. Let&amp;rsquo;s repeat this here:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(Data_df$Colour, Data_df$Predator.Presence)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        
##          No Yes
##   Black  64 292
##   Brown 211  87
##   Grey   82 331
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ContCoef(table(Data_df$Colour, Data_df$Predator.Presence))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.4421914
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(Data_df$Colour, Data_df$Predator.Type)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        
##         Avian Non-Avian
##   Black   197        95
##   Brown    60        27
##   Grey    233        98
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ContCoef(table(Data_df$Colour, Data_df$Predator.Type))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.02957682
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, we find the same results as when using the Chi-Squared statistic and conclude that colour morphs of the common house sparrow are likely to be driven by predator presence but not the type of predator that is present.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Are nesting sites of Passer domesticus linked to predator presence and/or predator type?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Again, following our two-sample situation Chi-Squared analysis from last exercise, we want to test whether nesting site and predator presence/predator type are linked. The Chi-Squared analyses identified a possible link of nesting site and predator type but nor predator presence.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(Data_df$Nesting.Site, Data_df$Predator.Presence)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        
##          No Yes
##   Shrub  87 205
##   Tree   94 137
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ContCoef(table(Data_df$Nesting.Site, Data_df$Predator.Presence))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1130328
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(Data_df$Nesting.Site, Data_df$Predator.Type)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        
##         Avian Non-Avian
##   Shrub   182        23
##   Tree     49        88
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ContCoef(table(Data_df$Nesting.Site, Data_df$Predator.Type))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.4851588
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Whilst there doesn&amp;rsquo;t seem to be any strong evidence linking nesting site and predator presence, predator type seems to be linked to what kind of nesting site &lt;em&gt;Passer domesticus&lt;/em&gt; prefers thus supporting our Chi-Squared results.&lt;/p&gt;
&lt;h3 id=&#34;sexual-dimorphism&#34;&gt;Sexual Dimorphism&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Are sex ratios of Passer domesticus related to climate types?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Recall that, in our last exercise, we found no discrepancy of proportions of the sexes among the entire data set using a binomial test. What we didn&amp;rsquo;t check yet was whether the sexes are distributed across the sites somewhat homogeneously or whether the sex ratios might be skewed according to climate types. Let&amp;rsquo;s do this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# prepare climate type testing data
Data_df &amp;lt;- Data_df_base
Index &amp;lt;- Data_df$Index
# select all data belonging to the stations at which all parameters except for climate type are held constant
Rows &amp;lt;- which(Index == &amp;quot;SI&amp;quot; | Index == &amp;quot;UK&amp;quot; | Index == &amp;quot;RE&amp;quot; | Index == &amp;quot;AU&amp;quot; )
Data_df &amp;lt;- Data_df[Rows,]
Data_df$Climate &amp;lt;- droplevels(factor(Data_df$Climate))

# analysis
table(Data_df$Sex, Data_df$Climate)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         
##          Coastal Continental
##   Female      91          76
##   Male        72          78
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ContCoef(table(Data_df$Sex, Data_df$Climate))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.06470702
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Quite obviously, they aren&amp;rsquo;t and, if there are any patterns in sex ratios to emerge, these are not likely to stem from climate types. Also keep in mind that we have a plethora of other variables at play whilst the information contained within the climate type variable is somewhat constrained and, in this case, bordering on uninformative (i.e. a coastal site in the Arctic might be more closely resembled by a continental mid-latitude site than by a tropical coastal site).&lt;/p&gt;
&lt;h2 id=&#34;ordinal-scale---kendalls-tau&#34;&gt;Ordinal Scale - Kendall&amp;rsquo;s Tau&lt;/h2&gt;
&lt;h3 id=&#34;climate-warmingextremes&#34;&gt;Climate Warming/Extremes&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Do heavier sparrows have heavier/less eggs?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A heavier weight of individual females alludes to a higher pool of resources being allocated by said individuals. There are multiple ways they might make use of it, one of them being investment in reproduction. To test how heavier females of &lt;em&gt;Passer domesticus&lt;/em&gt; utilise their resources in reproduction, we use a Kendall&amp;rsquo;s Tau approach to finding links between female weight and average egg weight per nest/number of eggs per nest.&lt;br&gt;
Obviously, both weight variables are metric in nature and so we could use other methods as well. On top of that, we first need to convert these into ranks before being able to run a Kendall&amp;rsquo;s Tau analysis as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# overwriting changes in Data_df with base data
Data_df &amp;lt;- Data_df_base
# Establishing Ranks of Egg Weight
RankedEggWeight &amp;lt;- rank(Data_df$Egg.Weight[which(Data_df$Sex == &amp;quot;Female&amp;quot;)],
                        ties.method = &amp;quot;average&amp;quot;)
# Establishing Ranks of Female Weight
RankedWeight_Female &amp;lt;- rank(Data_df$Weight[which(Data_df$Sex == &amp;quot;Female&amp;quot;)],
                            ties.method = &amp;quot;average&amp;quot;)
# Extracting Numbers of Eggs
RankedEggs &amp;lt;- Data_df$Number.of.Eggs[which(Data_df$Sex == &amp;quot;Female&amp;quot;)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Luckily enough, the number of eggs per nest already represent a ranked (ordinal) variable and so we can move straight on to running our analyses:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Test ranked female weight vs. ranked egg weight
cor.test(x = RankedWeight_Female, y = RankedEggWeight, 
         use = &amp;quot;pairwise.complete.obs&amp;quot;, method = &amp;quot;kendall&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Kendall&#39;s rank correlation tau
## 
## data:  RankedWeight_Female and RankedEggWeight
## z = 19.771, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true tau is not equal to 0
## sample estimates:
##       tau 
## 0.5804546
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is strong evidence to suggest that heavier females tend to lay heavier eggs (tau = 0.5804546 at p $\approx$ 0).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Test ranked female weight vs. number of eggs
cor.test(x = RankedWeight_Female, y = RankedEggs, 
         use = &amp;quot;pairwise.complete.obs&amp;quot;, method = &amp;quot;kendall&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Kendall&#39;s rank correlation tau
## 
## data:  RankedWeight_Female and RankedEggs
## z = -21.787, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true tau is not equal to 0
## sample estimates:
##        tau 
## -0.6880932
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Additionally, the heavier a female of &lt;em&gt;Passer domesticus&lt;/em&gt;, the less eggs she produces (tau = -0.6880932 at p $\approx$ 0).&lt;/p&gt;
&lt;p&gt;Now we can visualise the underlying patterns:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot_df &amp;lt;- data.frame(RankedWeight = RankedWeight_Female,
                      RankedEggWeight = RankedEggWeight,
                      RankedEggNumber = RankedEggs)
# plot ranked female weight vs. ranked egg weight
ggplot(data = plot_df, aes(x = RankedWeight, y = RankedEggWeight)) +
  geom_point() + theme_bw() + stat_smooth(method = &amp;quot;lm&amp;quot;) + 
  labs(title = &amp;quot;Ranked female weight of Passer domesticus vs. Ranked weigt of eggs&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula = &#39;y ~ x&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;09---Correlation-Tests_files/figure-html/Kendall.26b-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# plot ranked female weight vs. number of eggs
ggplot(data = plot_df, aes(x = RankedWeight, y = RankedEggNumber)) +
  geom_point() + theme_bw() + stat_smooth(method = &amp;quot;lm&amp;quot;) + 
  labs(title = &amp;quot;Ranked female weight of Passer domesticus vs. Ranked number of eggs&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula = &#39;y ~ x&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;09---Correlation-Tests_files/figure-html/Kendall.26b-2.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This highlights an obvious and intuitive trade-off in nature and has us &lt;strong&gt;reject the null hypotheses&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;metric-and-ordinal-scales&#34;&gt;Metric and Ordinal Scales&lt;/h2&gt;
&lt;p&gt;Metric scale correlation analyses call for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Spearman&lt;/em&gt; correlation test (non-parametric)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Pearson&lt;/em&gt; correlation test (parametric, requires data to be normal distributed)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;testing-for-normality&#34;&gt;Testing for Normality&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;DISCLAIMER:&lt;/strong&gt; I do not expect you to do it this way as of right now but wanted to give you reference material of how to automate this testing step.&lt;/p&gt;
&lt;p&gt;Since most of our following analyses are focussing on latitude effects (i.e. &amp;ldquo;climate warming&amp;rdquo;), we need to alter our base data. Before we can run the analyses, we need to eliminate the sites that we have set aside for testing climate extreme effects on (Siberia, United Kingdom, Reunion and Australia) from the data set. We are also not interested in all variables and so reduce the data set further.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df &amp;lt;- Data_df_base
Index &amp;lt;- Data_df$Index
Rows &amp;lt;- which(Index != &amp;quot;SI&amp;quot; &amp;amp; Index != &amp;quot;UK&amp;quot; &amp;amp; Index != &amp;quot;RE&amp;quot; &amp;amp; Index != &amp;quot;AU&amp;quot; )
Data_df &amp;lt;- Data_df[Rows,c(&amp;quot;Index&amp;quot;, &amp;quot;Latitude&amp;quot;, &amp;quot;Weight&amp;quot;, &amp;quot;Height&amp;quot;, &amp;quot;Wing.Chord&amp;quot;, &amp;quot;Nesting.Height&amp;quot;, 
                             &amp;quot;Number.of.Eggs&amp;quot;, &amp;quot;Egg.Weight&amp;quot;)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to know which test we can use with which variable, we need to first identify whether our data is normal distributed using the Shapiro-Test (Seminar 3) as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Normal_df &amp;lt;- data.frame(Normality = as.character(), stringsAsFactors = FALSE)
for(i in 2:length(colnames(Data_df))){
  Normal_df[1,i] &amp;lt;- colnames(Data_df)[i]
  Normal_df[2,i] &amp;lt;- round(shapiro.test(as.numeric(Data_df[,i]))$p.value, 2)
}
colnames(Normal_df) &amp;lt;- c()
rownames(Normal_df) &amp;lt;- c(&amp;quot;Variable&amp;quot;, &amp;quot;p&amp;quot;)
Normal_df &amp;lt;- Normal_df[,-1] # removing superfluous Index column
Normal_df
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                                                         
## Variable Latitude Weight Height Wing.Chord Nesting.Height Number.of.Eggs
## p               0      0      0          0              0              0
##                    
## Variable Egg.Weight
## p                 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unfortunately, none of these variables seem to be normal distributed (this was to be expected for some, to be fair) thus barring us from using Pearson correlation on the entire data set leaving us with the Spearman correlation method.&lt;/p&gt;
&lt;h3 id=&#34;spearman&#34;&gt;Spearman&lt;/h3&gt;
&lt;h4 id=&#34;climate-warmingextremes-1&#34;&gt;Climate Warming/Extremes&lt;/h4&gt;
&lt;h5 id=&#34;heightlength&#34;&gt;Height/Length&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;Do height/length records of Passer domesticus and latitude correlate?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Following Bergmann&amp;rsquo;s rule, we expect a positive correlation between sparrow height/length and absolute values of latitude:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cor.test(x = abs(Data_df$Latitude), y = Data_df$Height, 
         use = &amp;quot;pairwise.complete.obs&amp;quot;, method = &amp;quot;spearman&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in cor.test.default(x = abs(Data_df$Latitude), y = Data_df$Height, :
## Cannot compute exact p-value with ties
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Spearman&#39;s rank correlation rho
## 
## data:  abs(Data_df$Latitude) and Data_df$Height
## S = 128104735, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true rho is not equal to 0
## sample estimates:
##        rho 
## -0.8219373
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(data = Data_df, aes(x = abs(Latitude), y = Height)) +
  geom_point() + theme_bw() + stat_smooth(method = &amp;quot;lm&amp;quot;) + 
  labs(title = &amp;quot;Height of Passer domesticus vs. Latitude&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula = &#39;y ~ x&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;09---Correlation-Tests_files/figure-html/Spearman1-1.png&#34; width=&#34;1440&#34; /&gt;
Interestingly enough, our analysis yields a negative correlation which would disproof Bermann&amp;rsquo;s rule. This is a good example to show how important biological background knowledge is when doing biostatistics. Whilst a pure statistician might now believe to have just dis-proven a big rule of biology, it should be apparent to any biologist that Bergmann spoke of &amp;ldquo;bigger&amp;rdquo; organisms in colder climates (higher latitudes) and not of &amp;ldquo;taller&amp;rdquo; individuals. What our sparrows lack in height, they might make up for in circumference. This is an example where we would &lt;strong&gt;reject the null hypothesis&lt;/strong&gt; but shouldn&amp;rsquo;t &lt;strong&gt;accept the alternative hypothesis&lt;/strong&gt; based on biological understanding.&lt;/p&gt;
&lt;h5 id=&#34;weight&#34;&gt;Weight&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;Do weight records of Passer domesticus and latitude correlate?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Again, following Bergmann&amp;rsquo;s rule, we expect a positive correlation between sparrow weight and absolute values of latitude.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cor.test(x = abs(Data_df$Latitude), y = Data_df$Weight, 
         use = &amp;quot;pairwise.complete.obs&amp;quot;, method = &amp;quot;spearman&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in cor.test.default(x = abs(Data_df$Latitude), y = Data_df$Weight, :
## Cannot compute exact p-value with ties
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Spearman&#39;s rank correlation rho
## 
## data:  abs(Data_df$Latitude) and Data_df$Weight
## S = 9349037, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true rho is not equal to 0
## sample estimates:
##       rho 
## 0.8670357
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(data = Data_df, aes(x = abs(Latitude), y = Weight)) +
  geom_point() + theme_bw() + stat_smooth(method = &amp;quot;lm&amp;quot;) + 
  labs(title = &amp;quot;Weight of Passer domesticus vs. Latitude&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula = &#39;y ~ x&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;09---Correlation-Tests_files/figure-html/Spearman-1.png&#34; width=&#34;1440&#34; /&gt;
Bergmann was obviously right and we &lt;strong&gt;reject the null hypothesis&lt;/strong&gt;.&lt;/p&gt;
&lt;h5 id=&#34;wing-chord&#34;&gt;Wing Chord&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;Do wing chord/wing span records of Passer domesticus and latitude correlate?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We would expect sparrows in higher latitudes (e.g. colder climates) to have smaller wings as to radiate less body heat.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cor.test(x = abs(Data_df$Latitude), y = Data_df$Wing.Chord, 
         use = &amp;quot;pairwise.complete.obs&amp;quot;, method = &amp;quot;spearman&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in cor.test.default(x = abs(Data_df$Latitude), y = Data_df$Wing.Chord,
## : Cannot compute exact p-value with ties
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Spearman&#39;s rank correlation rho
## 
## data:  abs(Data_df$Latitude) and Data_df$Wing.Chord
## S = 134573929, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true rho is not equal to 0
## sample estimates:
##        rho 
## -0.9139437
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(data = Data_df, aes(x = abs(Latitude), y = Wing.Chord)) +
  geom_point() + theme_bw() + stat_smooth(method = &amp;quot;lm&amp;quot;) + 
  labs(title = &amp;quot;Wing chord of Passer domesticus vs. Latitude&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula = &#39;y ~ x&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;09---Correlation-Tests_files/figure-html/Spearman2-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And we were right! Sparrows have shorter wingspans in higher latitudes and we &lt;strong&gt;reject the null hypothesis&lt;/strong&gt;.&lt;/p&gt;
&lt;h5 id=&#34;number-of-eggs&#34;&gt;Number of Eggs&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;Do numbers of eggs per nest of Passer domesticus and latitude correlate?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Due to resource constraints in colder climates, we expect female &lt;em&gt;Passer domesticus&lt;/em&gt; individuals to invest in quality over quantity by prioritising caring your fledglings by educing the amount of eggs they produce.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cor.test(x = abs(Data_df$Latitude), y = Data_df$Number.of.Eggs, 
         use = &amp;quot;pairwise.complete.obs&amp;quot;, method = &amp;quot;spearman&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in cor.test.default(x = abs(Data_df$Latitude), y =
## Data_df$Number.of.Eggs, : Cannot compute exact p-value with ties
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Spearman&#39;s rank correlation rho
## 
## data:  abs(Data_df$Latitude) and Data_df$Number.of.Eggs
## S = 14501794, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true rho is not equal to 0
## sample estimates:
##      rho 
## -0.92853
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(data = Data_df, aes(x = abs(Latitude), y = Number.of.Eggs)) +
  geom_point() + theme_bw() + stat_smooth(method = &amp;quot;lm&amp;quot;) + 
  labs(title = &amp;quot;Number of eggs of Passer domesticus nests vs. Latitude&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula = &#39;y ~ x&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 394 rows containing non-finite values (`stat_smooth()`).
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 394 rows containing missing values (`geom_point()`).
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;09---Correlation-Tests_files/figure-html/Spearman3-1.png&#34; width=&#34;1440&#34; /&gt;
We were right. Female house sparrows produce less eggs per capita in higher latitudes and we &lt;strong&gt;reject the null hypothesis&lt;/strong&gt;.&lt;/p&gt;
&lt;h5 id=&#34;egg-weight&#34;&gt;Egg Weight&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;Does average weight of eggs per nest of Passer domesticus and latitude correlate?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Due to the reduced investment in egg numbers that we have just proven, we expect females of &lt;em&gt;Passer domesticus&lt;/em&gt; to allocate some of their saved resources into heavier eggs which may nurture unhatched offspring for longer and more effectively.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cor.test(x = abs(Data_df$Latitude), y = Data_df$Egg.Weight, 
         use = &amp;quot;pairwise.complete.obs&amp;quot;, method = &amp;quot;spearman&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in cor.test.default(x = abs(Data_df$Latitude), y = Data_df$Egg.Weight,
## : Cannot compute exact p-value with ties
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Spearman&#39;s rank correlation rho
## 
## data:  abs(Data_df$Latitude) and Data_df$Egg.Weight
## S = 1318221, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true rho is not equal to 0
## sample estimates:
##     rho 
## 0.82321
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(data = Data_df, aes(x = abs(Latitude), y = Egg.Weight)) +
  geom_point() + theme_bw() + stat_smooth(method = &amp;quot;lm&amp;quot;) + 
  labs(title = &amp;quot;Weight of Passer domesticus eggs vs. Latitude&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula = &#39;y ~ x&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 395 rows containing non-finite values (`stat_smooth()`).
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 395 rows containing missing values (`geom_point()`).
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;09---Correlation-Tests_files/figure-html/Spearman4-1.png&#34; width=&#34;1440&#34; /&gt;
Indeed, the higher the latitude, the heavier the average egg per nest of &lt;em&gt;Passer domesticus&lt;/em&gt; and we &lt;strong&gt;reject the null hypothesis&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;pearson&#34;&gt;Pearson&lt;/h3&gt;
&lt;p&gt;We already know that we can&amp;rsquo;t analyse the entire data set at once for these two variables since their data values are not normal distirbuted. How about site-wise variable value distributions, though?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Further reducing the data set
Data_df &amp;lt;- Data_df[,c(&amp;quot;Index&amp;quot;, &amp;quot;Weight&amp;quot;, &amp;quot;Height&amp;quot;)]
# establishing an empty data frame and an index vector that doesn&#39;t repeat
Normal_df &amp;lt;- data.frame(Normality = as.character(), stringsAsFactors = FALSE)
Indices &amp;lt;- as.character(unique(Data_df_base$Index[Rows]))
# site-wise shapiro test
for(i in 2:length(colnames(Data_df))){ # variables
  for(k in 1:length(Indices)){ # sites
    Normal_df[i,k] &amp;lt;- round(shapiro.test(as.numeric(
          Data_df[,i][which(Data_df_base$Index[Rows] == Indices[k])])
        )$p.value, 2)}} # site loop, variable loop
rownames(Normal_df) &amp;lt;- colnames(Data_df)
colnames(Normal_df) &amp;lt;- Indices
Normal_df[-1,] # remove superfluous index row
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          NU   MA   LO   BE   FG   SA   FI
## Weight 0.57 0.12 0.50 0.38 0.18 0.76 0.43
## Height 0.23 0.03 0.19 0.59 0.88 0.27 0.86
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;According to these results intra-site correlations of sparrow weight and height can be carried out! So, in order to show Pearson correlation, we run a simple, site-wise correlation analysis.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Do weight and height records of Passer domesticus correlate within each site?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To shed some light on our previous findings, we might want to see whether weight and height of sparrows correlate. Without running the analysis, we can conclude that they do because both correlate with latitude and are thus what we call &lt;strong&gt;collinear&lt;/strong&gt;. However, now we are running the analysis on a site level - does the correlation still exist?&lt;/p&gt;
&lt;p&gt;Take note that we are now using our entire data set again.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# overwriting altered Data_df
Data_df &amp;lt;- Data_df_base
# establishing an empty data frame and an index vector that doesn&#39;t repeat
Pearson_df &amp;lt;- data.frame(Pearson = as.character(), stringsAsFactors = FALSE)
Indices &amp;lt;- as.character(unique(Data_df$Index))
# site-internal correlation tests, weight and height
for(i in 1:length(unique(Data_df$Index))){
  Weights &amp;lt;- Data_df$Weight[which(Data_df$Index == Indices[i])]
  Heights &amp;lt;- Data_df$Height[which(Data_df$Index == Indices[i])]
  Pearson_df[1,i] &amp;lt;- round(cor.test(x = Weights, y = Heights, 
                                    use = &amp;quot;pairwise.complete.obs&amp;quot;)[[&amp;quot;estimate&amp;quot;]][[&amp;quot;cor&amp;quot;]], 2)
  Pearson_df[2,i] &amp;lt;- round(cor.test(x = Weights, y = Heights, 
                                    use = &amp;quot;pairwise.complete.obs&amp;quot;)$p.value, 2)}
colnames(Pearson_df) &amp;lt;- Indices
rownames(Pearson_df) &amp;lt;- c(&amp;quot;r&amp;quot;, &amp;quot;p&amp;quot;)
Pearson_df
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     SI   UK   AU   RE   NU   MA   LO   BE   FG   SA   FI
## r 0.76 0.83 0.77 0.75 0.84 0.84 0.79 0.82 0.79 0.76 0.81
## p    0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Apparently, it does. Heavier birds are taller!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Downloading &amp; Processing</title>
      <link>https://www.erikkusch.com/courses/krigr/download/</link>
      <pubDate>Thu, 26 May 2022 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/krigr/download/</guid>
      <description>&lt;p&gt;&lt;div class=&#34;alert alert-danger&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;&lt;code&gt;KrigR&lt;/code&gt; is currently undergoing development. As a result, this part of the workshop has become deprecated. Please refer to the &lt;a href=&#34;https://www.erikkusch.com/courses/krigr/setup/&#34;&gt;setup&lt;/a&gt; &lt;a href=&#34;https://www.erikkusch.com/courses/krigr/quickstart/&#34;&gt;quick guide&lt;/a&gt; portions of this material as these are up-to-date. &lt;/strong&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;div class=&#34;alert alert-danger&#34;&gt;
  &lt;div&gt;
    This part of the workshop is dependant on set-up and preparation done previously &lt;a href=&#34;https://www.erikkusch.com/courses/krigr/prep/&#34;&gt;here&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;First, we load &lt;code&gt;KrigR&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(KrigR)
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-info&#34;&gt;
  &lt;div&gt;
    Downloads and data processing with &lt;code&gt;KrigR&lt;/code&gt; are staged and executed with the &lt;code&gt;download_ERA()&lt;/code&gt;function.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;code&gt;download_ERA()&lt;/code&gt; is a very versatile function and I will show you it&amp;rsquo;s capabilities throughout this material.&lt;/p&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    We will start with a &lt;strong&gt;simple calls&lt;/strong&gt;  to &lt;code&gt;KrigR&lt;/code&gt; and subsequently make them &lt;strong&gt;more sophisticated&lt;/strong&gt; during this workshop.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;downloading-climate-data&#34;&gt;Downloading Climate Data&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s start with a very basic call to &lt;code&gt;download_ERA()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For this part of the workshop, we download air temperature for my birth month (January 1995) using the extent of our 
&lt;a href=&#34;https://www.erikkusch.com/courses/krigr/prep/#our-workshop-target-region&#34;&gt;target region&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;See the code chunk below for explanations on each function argument. If you want to know about the defaults for any argument in &lt;code&gt;download_ERA()&lt;/code&gt; simply run &lt;code&gt;?download_ERA()&lt;/code&gt;. Doing so should make it obvious why we specify the function as we do below.&lt;/p&gt;
&lt;div class=&#34;alert alert-&#34;&gt;
  &lt;div&gt;
    Notice that the downloading of ERA-family reanalysis data may take a short while to start as the download request gets queued with the CDS of the ECMWF before it is executed.
  &lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for file if download takes too long:&lt;/summary&gt;
    Download 
&lt;a href=&#34;https://github.com/ErikKusch/Homepage/raw/master/content/courses/krigr/Data/FirstDL.nc&#34;&gt;FirstDL.nc&lt;/a&gt; and place it into your data directory.
&lt;/details&gt; 
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;FirstDL &amp;lt;- download_ERA(
    Variable = &amp;quot;2m_temperature&amp;quot;, # the variable we want to obtain data for
    DataSet = &amp;quot;era5-land&amp;quot;, # the data set we want to obtain data from
    DateStart = &amp;quot;1995-01-01&amp;quot;, # the starting date of our time-window
    DateStop = &amp;quot;1995-01-31&amp;quot;, # the final date of our time-window
    Extent = Extent_ext, # the spatial preference we are after
    Dir = Dir.Data, # where to store the downloaded data
    FileName = &amp;quot;FirstDL&amp;quot;, # a name for our downloaded file
    API_User = API_User, # your API User Number
    API_Key = API_Key # your API User Key
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## download_ERA() is starting. Depending on your specifications, this can take a significant time.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## User 39340 for cds service added successfully in keychain
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Staging 1 download(s).
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 0001_FirstDL.nc download queried
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Requesting data to the cds service with username 39340
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## - staging data transfer at url endpoint or request id:
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   e48f036d-0979-4db4-bc4e-9d08be01c9d6
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## - timeout set to 10.0 hours
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## - polling server for a data transfer
\ polling server for a data transfer
| polling server for a data transfer
/ polling server for a data transfer
- polling server for a data transfer
\ polling server for a data transfer
| polling server for a data transfer
/ polling server for a data transfer
- polling server for a data transfer
\ polling server for a data transfer
| polling server for a data transfer
/ polling server for a data transfer
- polling server for a data transfer
\ polling server for a data transfer
| polling server for a data transfer
/ polling server for a data transfer
- polling server for a data transfer
\ polling server for a data transfer
| polling server for a data transfer
/ polling server for a data transfer
- polling server for a data transfer
\ polling server for a data transfer
| polling server for a data transfer
/ polling server for a data transfer
- polling server for a data transfer
## Downloading file
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
  |                                                                                      
  |                                                                                |   0%
  |                                                                                      
  |================================================================================| 100%
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## - moved temporary file to -&amp;gt; /Users/erikkus/Documents/HomePage/content/courses/krigr/Data/0001_FirstDL.nc
## - Delete data from queue for url endpoint or request id:
##   https://cds.climate.copernicus.eu/api/v2/tasks/e48f036d-0979-4db4-bc4e-9d08be01c9d6
## 
## Checking for known data issues.
## Loading downloaded data for masking and aggregation.
## Aggregating to temporal resolution of choice
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see the &lt;code&gt;download_ERA()&lt;/code&gt; function updates you on what it is currently working on at each major step. I implemented this to make sure people don&amp;rsquo;t get too anxious staring at an empty console in &lt;code&gt;R&lt;/code&gt;. If this feature is not appealing to you, you can turn this progress tracking off by setting &lt;code&gt;verbose = FALSE&lt;/code&gt; in the function call to &lt;code&gt;download_ERA()&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    For the rest of this workshop, I suppress messages from &lt;code&gt;download_ERA()&lt;/code&gt; via other means so that when you execute, you get progress tracking.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;I will make exceptions to this rule when there are special things I want to demonstrate.&lt;/p&gt;
&lt;p&gt;Now, let&amp;rsquo;s look at the raster that was produced:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;FirstDL
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## class      : RasterStack 
## dimensions : 34, 54, 1836, 1  (nrow, ncol, ncell, nlayers)
## resolution : 0.09999999, 0.09999998  (x, y)
## extent     : 9.72, 15.12, 49.74, 53.14  (xmin, xmax, ymin, ymax)
## crs        : +proj=longlat +datum=WGS84 +no_defs 
## names      : X1995.01.01
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One layer (i.e., one month) worth of data. That seems to have worked. If you are keen-eyed, you will notice that the extent on this object does not align with the extent we supplied with &lt;code&gt;Extent_ext&lt;/code&gt;. The reason? To download the data, we need to snap to the nearest full cell in the data set from which we query our downloads. &lt;code&gt;KrigR&lt;/code&gt; always ends up widening the extent to ensure all the data you desire will be downloaded.&lt;/p&gt;
&lt;p&gt;Finally, let&amp;rsquo;s visualise our downloaded data with one of our 
&lt;a href=&#34;https://www.erikkusch.com/courses/krigr/prep/#visualising-our-study-setting&#34;&gt;user-defined plotting functions&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Plot_Raw(FirstDL, Dates = &amp;quot;01-1995&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-downloads_files/figure-html/FirstDLVis-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That is all there is to downloading ERA5(-Land) data with &lt;code&gt;KrigR&lt;/code&gt;. You can already see how, even at the relatively course resolution of ERA5-Land, the mountain ridges along the German-Czech border are showing up. This will become a lot clearer of a pattern once we 
&lt;a href=&#34;https://www.erikkusch.com/courses/krigr/kriging/&#34;&gt;downscale our data&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;alert alert-info&#34;&gt;
  &lt;div&gt;
    &lt;code&gt;download_ERA()&lt;/code&gt; provides you with a lot more functionality than &lt;em&gt;just&lt;/em&gt; access to the ERA5(-Land) data sets.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;With &lt;code&gt;download_ERA()&lt;/code&gt;, you can also carry out processing of the downloaded data. Data processing with &lt;code&gt;download_ERA()&lt;/code&gt; includes:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Spatial Limitation&lt;/em&gt; to cut down on the data that is stored on your end.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Temporal Aggregation&lt;/em&gt; to establish data at the temporal resolution you desire.&lt;/li&gt;
&lt;/ol&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;spatial-limitation&#34;&gt;Spatial Limitation&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s start with spatial limitation. As discussed 
&lt;a href=&#34;https://www.erikkusch.com/courses/krigr/prep/#spatial-preferences-in-krigr&#34;&gt;previously&lt;/a&gt;, &lt;code&gt;download_ERA()&lt;/code&gt; can handle a variety of inputs describing spatial preferences.&lt;/p&gt;
&lt;div class=&#34;alert alert-&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;code&gt;KrigR&lt;/code&gt; is capable of learning about your spatial preferences in three ways:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;As an &lt;code&gt;extent&lt;/code&gt; input (a rectangular box).&lt;/li&gt;
&lt;li&gt;As a &lt;code&gt;SpatialPolygons&lt;/code&gt; input (a polygon or set of polygons).&lt;/li&gt;
&lt;li&gt;As a set of locations stored in a &lt;code&gt;data.frame&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These spatial preferences are registered in &lt;code&gt;KrigR&lt;/code&gt; functions using the &lt;code&gt;Extent&lt;/code&gt; argument.&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;You might now ask yourself: How does &lt;code&gt;KrigR&lt;/code&gt; achieve spatial limitation of the data? Couldn&amp;rsquo;t we just simply download only the data we are interested in?&lt;/p&gt;
&lt;p&gt;The ECMWF CDS gives us tremendous capability of retrieving only the data we want. However, the CDS only recognises rectangular boxes (i.e., &lt;code&gt;extent&lt;/code&gt;s) for spatial limitation. Consequently, we always have to download data corresponding to a rectangular box in space. When informing &lt;code&gt;KrigR&lt;/code&gt; of your spatial preferences using a &lt;code&gt;data.frame&lt;/code&gt; or &lt;code&gt;SpatialPolygons&lt;/code&gt;, &lt;code&gt;download_ERA()&lt;/code&gt; automatically (1) identifies the smallest &lt;code&gt;extent&lt;/code&gt; required by your input, (2) downloads data corresponding to this &lt;code&gt;extent&lt;/code&gt;, and (3) masks our any data not queried by you.&lt;/p&gt;
&lt;div class=&#34;alert alert-info&#34;&gt;
  &lt;div&gt;
    Using &lt;code&gt;KrigR&lt;/code&gt;&amp;rsquo;s spatial limitation features ensures faster computation and smaller file sizes (depending on file type).
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;In the following, I demonstrate how to use the &lt;code&gt;Extent&lt;/code&gt; argument in &lt;code&gt;download_ERA()&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;shape-spatialpolygons&#34;&gt;Shape (&lt;code&gt;SpatialPolygons&lt;/code&gt;)&lt;/h3&gt;
&lt;p&gt;Let me show you how &lt;code&gt;SpatialPolygons&lt;/code&gt; show up in our data with &lt;code&gt;download_ERA()&lt;/code&gt;. Remember that these &lt;code&gt;SpatialPolygons&lt;/code&gt; originate 
&lt;a href=&#34;https://www.erikkusch.com/courses/krigr/prep/#shape-of-interest-spatialpolygons&#34;&gt;here&lt;/a&gt;. First, we query our download as follows:&lt;/p&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for file if download takes too long:&lt;/summary&gt;
    Download 
&lt;a href=&#34;https://github.com/ErikKusch/Homepage/raw/master/content/courses/krigr/Data/SpatialPolygons_DL.nc&#34;&gt;SpatialPolygons_DL.nc&lt;/a&gt; and place it into your data directory.
&lt;/details&gt; 
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;SpatialPolygons_DL &amp;lt;- download_ERA(
  Variable = &amp;quot;2m_temperature&amp;quot;,
  DataSet = &amp;quot;era5-land&amp;quot;,
  DateStart = &amp;quot;1995-01-01&amp;quot;,
  DateStop = &amp;quot;1995-01-31&amp;quot;,
  Extent = Shape_shp, # we simply switch the Extent Argument
  Dir = Dir.Data,
  FileName = &amp;quot;SpatialPolygons_DL&amp;quot;,
  API_User = API_User,
  API_Key = API_Key
)  
Plot_Raw(SpatialPolygons_DL, Dates = &amp;quot;01-1995&amp;quot;, Shp = Shape_shp)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-downloads_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You will find that the data retained with the spatial limitation in &lt;code&gt;download_ERA()&lt;/code&gt; contains all raster cells of which even a fraction falls within the bounds of the &lt;code&gt;SpatialPolygons&lt;/code&gt; you supplied. This is different from standard &lt;code&gt;raster&lt;/code&gt; masking through which only cells whose centroids fall within the &lt;code&gt;SpatialPolygons&lt;/code&gt; are retained.&lt;/p&gt;
&lt;div class=&#34;alert alert-info&#34;&gt;
  &lt;div&gt;
    &lt;code&gt;raster&lt;/code&gt; masking in &lt;code&gt;KrigR&lt;/code&gt; always ensures that the entire area of your spatial preferences are retained.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;points-dataframe&#34;&gt;Points (&lt;code&gt;data.frame&lt;/code&gt;)&lt;/h3&gt;
&lt;p&gt;Now we move on to point-locations. Often times, we are researching very specific sets of coordinates, rather than entire regions. &lt;code&gt;download_ERA()&lt;/code&gt; is capable of limiting data to only small areas (of a size of your choosing) around your point-locations. For our purposes here, we make use of a set of mountain-top coordinates throughout our study region. Remember that these coordinates (stored in a &lt;code&gt;data.frame&lt;/code&gt;) originate 
&lt;a href=&#34;https://www.erikkusch.com/courses/krigr/prep/#points-of-interest-dataframe&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This time around, we need to tell &lt;code&gt;download_ERA()&lt;/code&gt; about not just the &lt;code&gt;Extent&lt;/code&gt;, but also specify how much of a buffer (&lt;code&gt;Buffer&lt;/code&gt; in $Â°$) to retain data for around each individual (&lt;code&gt;ID&lt;/code&gt;) location.&lt;/p&gt;
&lt;div class=&#34;alert alert-&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;The &lt;code&gt;data.frame&lt;/code&gt; input to the &lt;code&gt;Extent&lt;/code&gt; must contain a column called &lt;code&gt;Lat&lt;/code&gt; and a column called &lt;code&gt;Lon&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;In addition, one must also specify:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A &lt;code&gt;Buffer&lt;/code&gt; in $Â°$ to be drawn around each location.&lt;/li&gt;
&lt;li&gt;The name of the &lt;code&gt;ID&lt;/code&gt; column in your &lt;code&gt;data.frame&lt;/code&gt; which indexes each individual location.&lt;/li&gt;
&lt;/ol&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-&#34;&gt;
  &lt;div&gt;
    Our &lt;a href=&#34;https://www.erikkusch.com/courses/krigr/outlook/&#34;&gt;development goals&lt;/a&gt; include support for a broader range of point-location specifications.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Let&amp;rsquo;s stage such a download:&lt;/p&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for file if download takes too long:&lt;/summary&gt;
    Download 
&lt;a href=&#34;https://github.com/ErikKusch/Homepage/raw/master/content/courses/krigr/Data/points_DL.nc&#34;&gt;points_DL.nc&lt;/a&gt; and place it into your data directory.
&lt;/details&gt; 
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;points_DL &amp;lt;- download_ERA(
  Variable = &amp;quot;2m_temperature&amp;quot;,
  DataSet = &amp;quot;era5-land&amp;quot;,
  DateStart = &amp;quot;1995-01-01&amp;quot;,
  DateStop = &amp;quot;1995-01-31&amp;quot;,
  Extent = Mountains_df, # our data.frame with Lat and Lon columns
  Buffer = 0.5, # a half-degree buffer
  ID = &amp;quot;Mountain&amp;quot;, # the ID column in our data.frame
  Dir = Dir.Data,
  FileName = &amp;quot;points_DL&amp;quot;,
  API_User = API_User,
  API_Key = API_Key
)
Plot_Raw(points_DL, Dates = &amp;quot;01-1995&amp;quot;) + 
  geom_point(aes(x = Lon, y = Lat), data = Mountains_df, 
             colour = &amp;quot;green&amp;quot;, size = 10, pch = 14)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-downloads_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Above you can see how the mountain tops we are interested in lie exactly at the centre of the retained data. As we will see later, such spatial limitation greatly reduces computation cost of statistical downscaling procedures.&lt;/p&gt;
&lt;h2 id=&#34;temporal-aggregation&#34;&gt;Temporal Aggregation&lt;/h2&gt;
&lt;p&gt;So far, we have downloaded a single layer of data (i.e., one monthly average layer) from the CDS. However, ERA5(-Land) products come at &lt;strong&gt;hourly temporal resolutions&lt;/strong&gt; from which we can generate climate data at almost any temporal resolution we may require. This is what &lt;strong&gt;temporal aggregation&lt;/strong&gt; in &lt;code&gt;download_ERA()&lt;/code&gt; is for.&lt;/p&gt;
&lt;div class=&#34;alert alert-info&#34;&gt;
  &lt;div&gt;
    With &lt;strong&gt;temporal aggregation&lt;/strong&gt; in &lt;code&gt;download_ERA()&lt;/code&gt; you can achieve almost any temporal resolution and aggregate metric you may desire.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;Temporal aggregation with &lt;code&gt;download_ERA()&lt;/code&gt; uses the arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;TResolution&lt;/code&gt; and &lt;code&gt;TStep&lt;/code&gt; to achieve desired temporal resolutions&lt;/li&gt;
&lt;li&gt;&lt;code&gt;FUN&lt;/code&gt; to calculate desired aggregate metrics&lt;/li&gt;
&lt;/ul&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;temporal-resolution-tresolution-and-tstep&#34;&gt;Temporal Resolution (&lt;code&gt;TResolution&lt;/code&gt; and &lt;code&gt;TStep&lt;/code&gt;)&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s start by querying data at non-CDS temporal resolutions.&lt;/p&gt;
&lt;div class=&#34;alert alert-&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;The &lt;code&gt;download_ERA()&lt;/code&gt; function in the &lt;code&gt;KrigR&lt;/code&gt; package accepts the following arguments which you can use to control the temporal resolution of your climate data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;TResolution&lt;/code&gt; controls the time-line that &lt;code&gt;TStep&lt;/code&gt; indexes. You can specify anything from the following:  &lt;code&gt;&#39;hour&#39;&lt;/code&gt;, &lt;code&gt;&#39;day&#39;&lt;/code&gt;, &lt;code&gt;&#39;month&#39;&lt;/code&gt;, or &lt;code&gt;&#39;year&#39;&lt;/code&gt;. The default is &lt;code&gt;&#39;month&#39;&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;TStep&lt;/code&gt; controls how many time-steps to aggregate into one layer of data each. Aggregation is done via taking the mean per cell in each raster comprising time steps that go into the final, aggregated time-step. The default is &lt;code&gt;1&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;For now, let&amp;rsquo;s download hourly data from the CDS (this achieved by specifying a &lt;code&gt;TResolution&lt;/code&gt; of &lt;code&gt;&amp;quot;hour&amp;quot;&lt;/code&gt; or &lt;code&gt;&amp;quot;day&amp;quot;&lt;/code&gt;) and aggregate these to 1-day intervals. To make the result easier to visualise, we focus only on the first four days of January 1995:&lt;/p&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for file if download takes too long:&lt;/summary&gt;
    Download 
&lt;a href=&#34;https://github.com/ErikKusch/Homepage/raw/master/content/courses/krigr/Data/TimeSeries.nc&#34;&gt;TimeSeries.nc&lt;/a&gt; and place it into your data directory.
&lt;/details&gt; 
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;TimeSeries &amp;lt;- download_ERA(
  Variable = &amp;quot;2m_temperature&amp;quot;,
  DataSet = &amp;quot;era5-land&amp;quot;,
  DateStart = &amp;quot;1995-01-01&amp;quot;,
  DateStop = &amp;quot;1995-01-04&amp;quot;,
  TResolution = &amp;quot;day&amp;quot;, # aggregate to days
  TStep = 1, # aggregate to 1 day each
  Extent = Shape_shp,
  Dir = Dir.Data,
  FileName = &amp;quot;TimeSeries&amp;quot;,
  API_User = API_User,
  API_Key = API_Key
)
Plot_Raw(TimeSeries, Dates = c(&amp;quot;01-1995&amp;quot;, &amp;quot;02-1995&amp;quot;, 
                               &amp;quot;03-1995&amp;quot;, &amp;quot;04-1995&amp;quot;),
         Shp = Shape_shp)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-downloads_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;1440&#34; /&gt;
Looks like a cold front rolled over my home area at the beginning of 1995.&lt;/p&gt;
&lt;div class=&#34;alert alert-info&#34;&gt;
  &lt;div&gt;
    &lt;code&gt;KrigR&lt;/code&gt; automatically identifies which data set to download from given your temporal aggregation specification.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;As soon as &lt;code&gt;TResolution&lt;/code&gt; is set to &lt;code&gt;&#39;month&#39;&lt;/code&gt; or &lt;code&gt;&#39;year&#39;&lt;/code&gt;, the package automatically downloads monthly mean data from the CDS. We do this to make the temporal aggregation calculation more light-weight on your computing units and to make downloads less heavy.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s run through a few examples to make clear how desired temporal resolution of data can be achieved using the &lt;code&gt;KrigR&lt;/code&gt; package:&lt;/p&gt;
&lt;table&gt;
  &lt;tr&gt;
    &lt;th&gt;What We Want&lt;/th&gt;
    &lt;th&gt;TResolution &lt;/th&gt;
    &lt;th&gt;TStep&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Hourly intervals&lt;/td&gt;
    &lt;td&gt;hour&lt;/td&gt;
    &lt;td&gt;1&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;6-hour intervals&lt;/td&gt;
    &lt;td&gt;hour&lt;/td&gt;
    &lt;td&gt;6&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Half-day intervals&lt;/td&gt;
    &lt;td&gt;hour&lt;/td&gt;
    &lt;td&gt;12&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Daily intervals &lt;/td&gt;
    &lt;td&gt;day&lt;/td&gt;
    &lt;td&gt;1&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;3-day intervals&lt;/td&gt;
    &lt;td&gt;day&lt;/td&gt;
    &lt;td&gt;3&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Weekly intervals&lt;/td&gt;
    &lt;td&gt;day&lt;/td&gt;
    &lt;td&gt;7&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Monthly aggregates&lt;/td&gt;
    &lt;td&gt;month&lt;/td&gt;
    &lt;td&gt;1&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;4-month intervals&lt;/td&gt;
    &lt;td&gt;month&lt;/td&gt;
    &lt;td&gt;4&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Annual intervals&lt;/td&gt;
    &lt;td&gt;year&lt;/td&gt;
    &lt;td&gt;1&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;10-year intervals&lt;/td&gt;
    &lt;td&gt;year&lt;/td&gt;
    &lt;td&gt;10&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt; 
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    Specifying &lt;code&gt;TResolution&lt;/code&gt; of &lt;code&gt;&#39;month&#39;&lt;/code&gt; will result in the download of full month aggregates for every month included in your time series.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;For example, &lt;code&gt;DateStart = &amp;quot;2000-01-20&amp;quot;&lt;/code&gt;, &lt;code&gt;DateStop = &amp;quot;2000-02-20&amp;quot;&lt;/code&gt; with &lt;code&gt;TResolution = &#39;month&#39;&lt;/code&gt;, and &lt;code&gt;TStep = 1&lt;/code&gt; &lt;strong&gt;does not&lt;/strong&gt; result in the mean aggregate for the month between the 20/01/200 and the 20/02/2000, but &lt;strong&gt;does result&lt;/strong&gt; in the monthly aggregates for January and February 2000. If you desire the former, you would need to specify &lt;code&gt;DateStart = &amp;quot;2000-01-20&amp;quot;&lt;/code&gt;, &lt;code&gt;DateStop = &amp;quot;2000-02-20&amp;quot;&lt;/code&gt; with &lt;code&gt;TResolution = &#39;day&#39;&lt;/code&gt;, and &lt;code&gt;TStep = 32&lt;/code&gt; (the number of days between the two dates).&lt;/p&gt;
&lt;h3 id=&#34;aggregate-metrics-fun&#34;&gt;Aggregate Metrics (&lt;code&gt;FUN&lt;/code&gt;)&lt;/h3&gt;
&lt;p&gt;Aggregate metrics can be particularly useful for certain study settings when climate variability or exposure to extreme events are sought after.&lt;/p&gt;
&lt;div class=&#34;alert alert-&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;The &lt;code&gt;FUN&lt;/code&gt; argument in &lt;code&gt;download_ERA()&lt;/code&gt; controls which values to calculate for the temporal aggregates, e.g.: &lt;code&gt;&#39;min&#39;&lt;/code&gt;, &lt;code&gt;&#39;max&#39;&lt;/code&gt;, or &lt;code&gt;&#39;mean&#39;&lt;/code&gt; (default).&lt;/p&gt;
&lt;p&gt;Any function which returns a single value when fed a vector of values is supported.&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Let&amp;rsquo;s say we are interested in the variability of temperature across our study region in daily intervals. Again, we shorten our time-series to just four days:&lt;/p&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for file if download takes too long:&lt;/summary&gt;
    Download 
&lt;a href=&#34;https://github.com/ErikKusch/Homepage/raw/master/content/courses/krigr/Data/TimeSeriesSD.nc&#34;&gt;TimeSeriesSD.nc&lt;/a&gt; and place it into your data directory.
&lt;/details&gt; 
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;TimeSeriesSD &amp;lt;- download_ERA(
  Variable = &amp;quot;2m_temperature&amp;quot;,
  DataSet = &amp;quot;era5-land&amp;quot;,
  DateStart = &amp;quot;1995-01-01&amp;quot;,
  DateStop = &amp;quot;1995-01-04&amp;quot;,
  TResolution = &amp;quot;day&amp;quot;,
  TStep = 1,
  FUN = sd, # query standard deviation
  Extent = Shape_shp,
  Dir = Dir.Data,
  FileName = &amp;quot;TimeSeriesSD&amp;quot;,
  API_User = API_User,
  API_Key = API_Key
)
Plot_Raw(TimeSeriesSD, Dates = c(&amp;quot;01-1995&amp;quot;, &amp;quot;02-1995&amp;quot;, 
                                 &amp;quot;03-1995&amp;quot;, &amp;quot;04-1995&amp;quot;),
         Shp = Shape_shp)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-downloads_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;1440&#34; /&gt;
Seems like the temperatures fluctuated most on the third and fourth of January, but the area of temperature fluctuations changed location between those two days.&lt;/p&gt;
&lt;div class=&#34;alert alert-info&#34;&gt;
  &lt;div&gt;
    You should now be able to query data for any location you study and achieve temporal resolutions and aggregate metrics which your study requires.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;dynamical-data-uncertainty&#34;&gt;Dynamical Data Uncertainty&lt;/h2&gt;
&lt;p&gt;With climate reanalyses, you also gain access to uncertainty flags of the data stored in the reanalysis product. For the ERA5-family of products, this uncertainty can be obtained by assessing the standard deviation of the 10 ensemble members which make up the underlying ERA5 model exercise.&lt;/p&gt;
&lt;p&gt;With &lt;code&gt;download_ERA()&lt;/code&gt; you can obtain this information as follows:&lt;/p&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for file if download takes too long:&lt;/summary&gt;
    Download 
&lt;a href=&#34;https://github.com/ErikKusch/Homepage/raw/master/content/courses/krigr/Data/SpatialPolygons_DL.nc&#34;&gt;SpatialPolygonsEns_DL.nc&lt;/a&gt; and place it into your data directory.
&lt;/details&gt; 
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;SpatialPolygonsEns_DL &amp;lt;- download_ERA(
    Variable = &amp;quot;2m_temperature&amp;quot;,
    DataSet = &amp;quot;era5&amp;quot;,
    Type = &amp;quot;ensemble_members&amp;quot;,
    DateStart = &amp;quot;1995-01-01&amp;quot;,
    DateStop = &amp;quot;1995-01-02&amp;quot;,
    TResolution = &amp;quot;day&amp;quot;,
    TStep = 1,
    FUN = sd,
    Extent = Shape_shp,
    Dir = Dir.Data,
    FileName = &amp;quot;SpatialPolygonsEns_DL&amp;quot;,
    API_User = API_User,
    API_Key = API_Key
  )
Plot_Raw(SpatialPolygonsEns, Dates = c(&amp;quot;01-1995&amp;quot;, &amp;quot;02-1995&amp;quot;),
         Shp = Shape_shp, COL = rev(viridis(100)))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-downloads_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As you can see here, there is substantial disagreement between the ensemble members of daily average temperatures across our study region. This uncertainty among ensemble members is greatest at high temporal resolution and becomes negligible at coarse temporal resolution. We document this phenomenon in 
&lt;a href=&#34;https://iopscience.iop.org/article/10.1088/1748-9326/ac39bf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this publication (Figure 1)&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;final-downloads-for-workshop-progress&#34;&gt;Final Downloads for Workshop Progress&lt;/h2&gt;
&lt;p&gt;Now that we know how to use spatial limitation and temporal aggregation with &lt;code&gt;download_ERA()&lt;/code&gt; it is time to generate the data products we will use for the rest of this workshop material.&lt;/p&gt;
&lt;h3 id=&#34;climate-data&#34;&gt;Climate Data&lt;/h3&gt;
&lt;div class=&#34;alert alert-info&#34;&gt;
  &lt;div&gt;
    To streamline this workshop material, I will focus on just three short-time series of data with different spatial limitations. I visualise them all side-by-side further down.
  &lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;&lt;summary&gt; Click here for download calls &lt;/summary&gt;
&lt;h4 id=&#34;extent-data&#34;&gt;&lt;code&gt;extent&lt;/code&gt; Data&lt;/h4&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for file if download takes too long:&lt;/summary&gt;
    Download 
&lt;a href=&#34;https://github.com/ErikKusch/Homepage/raw/master/content/courses/krigr/Data/ExtentRaw.nc&#34;&gt;ExtentRaw.nc&lt;/a&gt; and place it into your data directory.
&lt;/details&gt; 
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Extent_Raw &amp;lt;- download_ERA(
  Variable = &amp;quot;2m_temperature&amp;quot;,
  DataSet = &amp;quot;era5-land&amp;quot;,
  DateStart = &amp;quot;1995-01-01&amp;quot;,
  DateStop = &amp;quot;1995-01-04&amp;quot;,
  TResolution = &amp;quot;day&amp;quot;,
  TStep = 1,
  Extent = Extent_ext,
  Dir = Dir.Data,
  FileName = &amp;quot;ExtentRaw&amp;quot;,
  API_User = API_User,
  API_Key = API_Key
)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;spatialpolygons-data&#34;&gt;&lt;code&gt;SpatialPolygons&lt;/code&gt; Data&lt;/h4&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for file if download takes too long:&lt;/summary&gt;
    Download 
&lt;a href=&#34;https://github.com/ErikKusch/Homepage/raw/master/content/courses/krigr/Data/SpatialPolygonsRaw.nc&#34;&gt;SpatialPolygonsRaw.nc&lt;/a&gt; and place it into your data directory.
&lt;/details&gt; 
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;SpatialPolygonsRaw &amp;lt;- download_ERA(
  Variable = &amp;quot;2m_temperature&amp;quot;,
  DataSet = &amp;quot;era5-land&amp;quot;,
  DateStart = &amp;quot;1995-01-01&amp;quot;,
  DateStop = &amp;quot;1995-01-04&amp;quot;,
  TResolution = &amp;quot;day&amp;quot;,
  TStep = 1,
  Extent = Shape_shp,
  Dir = Dir.Data,
  FileName = &amp;quot;SpatialPolygonsRaw&amp;quot;,
  API_User = API_User,
  API_Key = API_Key
)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;pointdataframe-data&#34;&gt;Point(&lt;code&gt;data.frame&lt;/code&gt;) Data&lt;/h4&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for file if download takes too long:&lt;/summary&gt;
    Download 
&lt;a href=&#34;https://github.com/ErikKusch/Homepage/raw/master/content/courses/krigr/Data/PointsRaw.nc&#34;&gt;PointsRaw.nc&lt;/a&gt; and place it into your data directory.
&lt;/details&gt; 
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Points_Raw &amp;lt;- download_ERA(
  Variable = &amp;quot;2m_temperature&amp;quot;,
  DataSet = &amp;quot;era5-land&amp;quot;,
  DateStart = &amp;quot;1995-01-01&amp;quot;,
  DateStop = &amp;quot;1995-01-4&amp;quot;,
  TResolution = &amp;quot;day&amp;quot;,
  TStep = 1,
  Extent = Mountains_df,
  Buffer = 0.5,
  ID = &amp;quot;Mountain&amp;quot;,
  Dir = Dir.Data,
  FileName = &amp;quot;PointsRaw&amp;quot;,
  API_User = API_User,
  API_Key = API_Key
)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;Now let&amp;rsquo;s visualise these data for a better understanding of what they contain:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Extent_gg &amp;lt;- Plot_Raw(Extent_Raw[[1]], Dates = &amp;quot;Extent&amp;quot;)
SP_gg &amp;lt;- Plot_Raw(SpatialPolygonsRaw[[1]], Dates = &amp;quot;SpatialPolygons&amp;quot;)
Points_gg &amp;lt;- Plot_Raw(Points_Raw[[1]], Dates = &amp;quot;SpatialPolygons&amp;quot;)
plot_grid(Extent_gg, SP_gg, Points_gg, ncol = 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-downloads_files/figure-html/dataviz-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;dynamical-data-uncertainty-1&#34;&gt;Dynamical Data Uncertainty&lt;/h3&gt;
&lt;div class=&#34;alert alert-info&#34;&gt;
  &lt;div&gt;
    For an aggregate understanding of data uncertainty, we also obtain dynamical uncertainty for our target region and time frame. For simplicity, we do so only for the &lt;code&gt;SpatialPolygons&lt;/code&gt; specification.
  &lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;&lt;summary&gt; Click here for download call &lt;/summary&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for file if download takes too long:&lt;/summary&gt;
    Download 
&lt;a href=&#34;https://github.com/ErikKusch/Homepage/raw/master/content/courses/krigr/Data/SpatialPolygonsEns.nc&#34;&gt;SpatialPolygonsEns.nc&lt;/a&gt; and place it into your data directory.
&lt;/details&gt; 
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;SpatialPolygonsEns &amp;lt;- download_ERA(
    Variable = &amp;quot;2m_temperature&amp;quot;,
    DataSet = &amp;quot;era5&amp;quot;,
    Type = &amp;quot;ensemble_members&amp;quot;,
    DateStart = &amp;quot;1995-01-01&amp;quot;,
    DateStop = &amp;quot;1995-01-04&amp;quot;,
    TResolution = &amp;quot;day&amp;quot;,
    TStep = 1,
    FUN = sd,
    Extent = Shape_shp,
    Dir = Dir.Data,
    FileName = &amp;quot;SpatialPolygonsEns&amp;quot;,
    API_User = API_User,
    API_Key = API_Key
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt; 
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Plot_Raw(SpatialPolygonsEns, Dates = c(&amp;quot;01-1995&amp;quot;, &amp;quot;02-1995&amp;quot;, 
                                          &amp;quot;03-1995&amp;quot;, &amp;quot;04-1995&amp;quot;),
         Shp = Shape_shp, COL = rev(viridis(100)))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-downloads_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We will see how these uncertainties stack up against other sources of uncertainty when we arrive at 
&lt;a href=&#34;https://www.erikkusch.com/courses/krigr/kriging/#aggregate-uncertainty&#34;&gt;aggregate uncertainty&lt;/a&gt; of our final product.&lt;/p&gt;
&lt;h2 id=&#34;considerations-for-download_era&#34;&gt;Considerations for &lt;code&gt;download_ERA()&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;download_ERA()&lt;/code&gt; is a complex function with many things happening under the hood. To make sure you have the best experience with this interface to the ERA5(-Land) products through &lt;code&gt;R&lt;/code&gt;, I have compiled a few bits of &lt;em&gt;good-to-know&lt;/em&gt; information about the workings of &lt;code&gt;download_ERA()&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;effeciency&#34;&gt;Effeciency&lt;/h3&gt;
&lt;p&gt;Download speeds with &lt;code&gt;download_ERA()&lt;/code&gt; are largely tied to CDS queue time, but there are some things worth considering when querying downloads of time-series data.&lt;/p&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    The &lt;code&gt;download_ERA()&lt;/code&gt; function automatically breaks down download requests into monthly intervals thus circumventing the danger of running into making a download request that is too big for the CDS.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;For example, &lt;code&gt;DateStart = &amp;quot;2000-01-20&amp;quot;&lt;/code&gt;, &lt;code&gt;DateStop = &amp;quot;2000-02-20&amp;quot;&lt;/code&gt; with &lt;code&gt;TResolution = &#39;day&#39;&lt;/code&gt;, and &lt;code&gt;TStep = 8&lt;/code&gt; will lead to two download requests to the CDS: (1) hourly data in the range 20/01/2000 00:00 to 31/01/2000 23:00, and (2) hourly data in the range 01/02/2000 00:00 to 20/02/2000 23:00. These data sets are subsequently fused in &lt;code&gt;R&lt;/code&gt;, aggregated to daily aggregates, and finally, aggregated to four big aggregates.&lt;br&gt;
This gives you a lot of flexibility, but always keep in mind that third-party data sets might not account for leap-years so make sure the dates of third-party data (should you chose to use some) lines up with the ones as specified by your calls to the functions of the &lt;code&gt;KrigR&lt;/code&gt; package.&lt;/p&gt;
&lt;h4 id=&#34;singulardl&#34;&gt;&lt;code&gt;SingularDL&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;ECMWF CDS downloads come with a hard limit of 100,000 layers worth of data. This corresponds to more than 1 month worth of data. As a matter of fact, even ar hourly time-scales, you could theoretically download ~11 years worth of data without hitting this limit. In this particular case, &lt;code&gt;download_ERA()&lt;/code&gt; stages, by default, 132 individual downloads (1 per month) when the CDS would be just fine accepting the download request for all the data in one download call.&lt;/p&gt;
&lt;p&gt;Is there any way to bypass the monthly downloads in &lt;code&gt;download_ERA()&lt;/code&gt;? Yes, there is. With the &lt;code&gt;SingularDL&lt;/code&gt; argument.&lt;/p&gt;
&lt;div class=&#34;alert alert-info&#34;&gt;
  &lt;div&gt;
    Setting &lt;code&gt;SingularDL = TRUE&lt;/code&gt; in &lt;code&gt;download_ERA()&lt;/code&gt; bypasses the automatic month-wise download staging. A pre-staging check breaks the operation if you query more than the CDS hard limit on data.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-&#34;&gt;
  &lt;div&gt;
    Our &lt;a href=&#34;https://www.erikkusch.com/courses/krigr/outlook/&#34;&gt;development goals&lt;/a&gt; include changing month-wise default downloads to downloads of 100,000 layers at a time.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h4 id=&#34;cores&#34;&gt;&lt;code&gt;Cores&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;Continuing on from the previous point, let&amp;rsquo;s consider you want to obtain more than 100,000 layers worth of data for your analysis and thus can&amp;rsquo;t make use of the &lt;code&gt;SingularDL&lt;/code&gt; argument. 
By default &lt;code&gt;download_ERA()&lt;/code&gt; stages downloads sequentially. Most modern PCs come with multiple cores each of which could theoretically stage it&amp;rsquo;s own download in parallel. Couldn&amp;rsquo;t we make use of this for more efficient download staging? Yes, we can with the &lt;code&gt;Cores&lt;/code&gt; argument.&lt;/p&gt;
&lt;div class=&#34;alert alert-info&#34;&gt;
  &lt;div&gt;
    Using the &lt;code&gt;Cores&lt;/code&gt; argument in &lt;code&gt;download_ERA()&lt;/code&gt; you can specify how many downloads to stage in parallel rather than sequentially.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h4 id=&#34;disk-space&#34;&gt;Disk Space&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;KrigR&lt;/code&gt; uses NETCDF (.nc) files as they represent the standard in climate science. NETCDF file size is not connected to data content in the raster but number of cells. Other formats, such as GeoTiff (.tif) do however scale in file size with non-NA cell number in the saved rasters.&lt;/p&gt;
&lt;div class=&#34;alert alert-&#34;&gt;
  &lt;div&gt;
    Our &lt;a href=&#34;https://www.erikkusch.com/courses/krigr/outlook/&#34;&gt;development goals&lt;/a&gt; include giving the user control over the file type as which &lt;code&gt;KrigR&lt;/code&gt;-derived products are saved.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;For example, the file size of the above &lt;code&gt;FirstDL&lt;/code&gt; raster is 7kb while the &lt;code&gt;SpatialPolygons&lt;/code&gt; and &lt;code&gt;data.frame&lt;/code&gt; driven data is saved as GeoTiffs of 4kb and 3kb, respectively.&lt;/p&gt;
&lt;div class=&#34;alert alert-&#34;&gt;
  &lt;div&gt;
    If you need to optimise storage space, particularly when using &lt;a href=&#34;https://www.erikkusch.com/courses/krigr/download/#spatial-limitation&#34;&gt;spatial limitation&lt;/a&gt; with &lt;code&gt;KrigR&lt;/code&gt;, I can thus recommend re-saving &lt;code&gt;KrigR&lt;/code&gt; outputs as GeoTiffs.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;cummulative-variables-precipfix&#34;&gt;Cummulative Variables (&lt;code&gt;PrecipFix&lt;/code&gt;)&lt;/h3&gt;
&lt;div class=&#34;alert alert-danger&#34;&gt;
  &lt;div&gt;
    Some variables in the ERA5(-Land) data sets are stored as cumulative records for pre-set time-windows, but temporal aggregation in &lt;code&gt;download-ERA()&lt;/code&gt; cannot handle such data.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Consequently, cumulative records need to be transformed into single-time-step records with respect to their base temporal resolution and cumulative aggregation interval like so:&lt;/p&gt;
&lt;img src=&#34;https://www.erikkusch.com/courses/krigr/PrecipFix.jpg&#34; width=&#34;900&#34;/&gt;
&lt;div class=&#34;alert alert-&#34;&gt;
  &lt;div&gt;
    To make cumulatively stored variables compatible with temporal aggregation in &lt;code&gt;download_ERA()&lt;/code&gt; simple toggle &lt;code&gt;PrecipFix = TRUE&lt;/code&gt; in the function call.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-info&#34;&gt;
  &lt;div&gt;
    To identify which variables are stored cumulatively, we recommend searching for variables listed as &amp;ldquo;This variable is accumulated from the beginning of the forecast time to the end of the forecast step.&amp;rdquo; on the data set documentation page (e.g., &lt;a href=&#34;https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-land?tab=overview&#34;&gt;ERA5-Land&lt;/a&gt;).
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-&#34;&gt;
  &lt;div&gt;
    Our &lt;a href=&#34;https://www.erikkusch.com/courses/krigr/outlook/&#34;&gt;development goals&lt;/a&gt; include an error check for specification of &lt;code&gt;PrecipFix = TRUE&lt;/code&gt; on non-cumulatively stored variables.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;stability&#34;&gt;Stability&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;download_ERA()&lt;/code&gt; requires a stable connection to the ECWMF CDS. Sometimes, however, a connection may drop or the CDS queue is so long that our downloads just fail. To mitigate the annoyance caused by these issues, I have implemented to extra arguments to the &lt;code&gt;download_ERA()&lt;/code&gt; function call:&lt;/p&gt;
&lt;h4 id=&#34;timeout&#34;&gt;&lt;code&gt;TimeOut&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;TimeOut&lt;/code&gt; is a numeric argument which specifies how many seconds to wait for the CDS to return the queried data. The default equates to 10 hours.&lt;/p&gt;
&lt;h4 id=&#34;trydown&#34;&gt;&lt;code&gt;TryDown&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;TryDown&lt;/code&gt; is a numeric argument which specifies how often to retry a download before giving up and moving on or stopping the execution of &lt;code&gt;download_ERA()&lt;/code&gt;. The default is 10.&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.2.3 (2023-03-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Big Sur ... 10.16
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] mapview_2.11.0          rnaturalearthdata_0.1.0 rnaturalearth_0.3.2    
##  [4] gimms_1.2.1             ggmap_3.0.2             cowplot_1.1.1          
##  [7] viridis_0.6.2           viridisLite_0.4.1       ggplot2_3.4.1          
## [10] tidyr_1.3.0             KrigR_0.1.2             terra_1.7-21           
## [13] httr_1.4.5              stars_0.6-0             abind_1.4-5            
## [16] fasterize_1.0.4         sf_1.0-12               lubridate_1.9.2        
## [19] automap_1.1-9           doSNOW_1.0.20           snow_0.4-4             
## [22] doParallel_1.0.17       iterators_1.0.14        foreach_1.5.2          
## [25] rgdal_1.6-5             raster_3.6-20           sp_1.6-0               
## [28] stringr_1.5.0           keyring_1.3.1           ecmwfr_1.5.0           
## [31] ncdf4_1.21             
## 
## loaded via a namespace (and not attached):
##  [1] leafem_0.2.0             colorspace_2.1-0         class_7.3-21            
##  [4] leaflet_2.1.2            satellite_1.0.4          base64enc_0.1-3         
##  [7] rstudioapi_0.14          proxy_0.4-27             farver_2.1.1            
## [10] fansi_1.0.4              codetools_0.2-19         cachem_1.0.7            
## [13] knitr_1.42               jsonlite_1.8.4           png_0.1-8               
## [16] Kendall_2.2.1            compiler_4.2.3           assertthat_0.2.1        
## [19] fastmap_1.1.1            cli_3.6.0                htmltools_0.5.4         
## [22] tools_4.2.3              gtable_0.3.1             glue_1.6.2              
## [25] dplyr_1.1.0              Rcpp_1.0.10              jquerylib_0.1.4         
## [28] vctrs_0.6.1              blogdown_1.16            crosstalk_1.2.0         
## [31] lwgeom_0.2-11            xfun_0.37                timechange_0.2.0        
## [34] lifecycle_1.0.3          rnaturalearthhires_0.2.1 zoo_1.8-11              
## [37] scales_1.2.1             gstat_2.1-0              yaml_2.3.7              
## [40] curl_5.0.0               memoise_2.0.1            gridExtra_2.3           
## [43] sass_0.4.5               reshape_0.8.9            stringi_1.7.12          
## [46] highr_0.10               e1071_1.7-13             boot_1.3-28.1           
## [49] intervals_0.15.3         RgoogleMaps_1.4.5.3      rlang_1.1.0             
## [52] pkgconfig_2.0.3          bitops_1.0-7             evaluate_0.20           
## [55] lattice_0.20-45          purrr_1.0.1              htmlwidgets_1.6.1       
## [58] labeling_0.4.2           tidyselect_1.2.0         plyr_1.8.8              
## [61] magrittr_2.0.3           bookdown_0.33            R6_2.5.1                
## [64] generics_0.1.3           DBI_1.1.3                pillar_1.8.1            
## [67] withr_2.5.0              units_0.8-1              xts_0.13.0              
## [70] tibble_3.2.1             spacetime_1.2-8          KernSmooth_2.23-20      
## [73] utf8_1.2.3               rmarkdown_2.20           jpeg_0.1-10             
## [76] grid_4.2.3               zyp_0.11-1               FNN_1.1.3.2             
## [79] digest_0.6.31            classInt_0.4-9           webshot_0.5.4           
## [82] stats4_4.2.3             munsell_0.5.0            bslib_0.4.2
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 09</title>
      <link>https://www.erikkusch.com/courses/rethinking/chapter-09/</link>
      <pubDate>Thu, 25 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/rethinking/chapter-09/</guid>
      <description>&lt;h1 id=&#34;markov-chain-monte-carlo&#34;&gt;Markov Chain Monte Carlo&lt;/h1&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/rethinking/10__26-02-2021_SUMMARY_-MCMC.pptx.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides Chapter 9&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to the exercises at the end of chapter 9 in 
&lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Satistical Rethinking 2&lt;/a&gt; by Richard McElreath. I have created these notes as a part of my ongoing involvement in the 
&lt;a href=&#34;https://www.erikkusch.com/project/aubayes/&#34;&gt;AU Bayes Study Group&lt;/a&gt;. Much of my inspiration for these solutions, where necessary, has been obtained from 
&lt;a href=&#34;https://github.com/jffist/statistical-rethinking-solutions/blob/master/ch08_hw.R&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Taras Svirskyi&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/cavaunpeu/statistical-rethinking/blob/master/chapter-8/homework.R&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;William Wolf&lt;/a&gt;, and 
&lt;a href=&#34;https://www.samples-of-thoughts.com/projects/statistical-rethinking/chapter_8/chp8-ex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Corrie Bartelheimer&lt;/a&gt; as well as the solutions provided to instructors by Richard McElreath himself.&lt;/p&gt;
&lt;h2 id=&#34;r-environment&#34;&gt;&lt;code&gt;R&lt;/code&gt; Environment&lt;/h2&gt;
&lt;p&gt;For today&amp;rsquo;s exercise, I load the following packages:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rethinking)
library(rstan)
library(ggplot2)
library(tidybayes)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;easy-exercises&#34;&gt;Easy Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-e1&#34;&gt;Practice E1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Which of the following is a requirement of the simple Metropolis algorithm?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The parameters must be discrete.&lt;/li&gt;
&lt;li&gt;The likelihood function must be Gaussian.&lt;/li&gt;
&lt;li&gt;The proposal distribution must be symmetric.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Not a requirement. Metropolis can accommodate continuous and discrete parameters.&lt;/li&gt;
&lt;li&gt;Not a requirement. Distribution could be any symmetric distribution. Not just Gaussian.&lt;/li&gt;
&lt;li&gt;This is a requirement.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;practice-e2&#34;&gt;Practice E2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Gibbs sampling is more efficient than the Metropolis algorithm. How does it achieve this extra efficiency? Are there any limitations to the Gibbs sampling strategy?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Gibbs uses adaptive proposals when considering which location in the posterior to sample next. This makes it more efficient because less proposed steps are rejected.&lt;/p&gt;
&lt;h3 id=&#34;practice-e3&#34;&gt;Practice E3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Which sort of parameters can Hamiltonian Monte Carlo not handle? Can you explain why?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Discrete parameters. HMC depends on gradients which to explore using a physics simulation. Discrete parameters would not allow for the construction of any gradients.&lt;/p&gt;
&lt;h3 id=&#34;practice-e4&#34;&gt;Practice E4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Explain the difference between the effective number of samples, &lt;code&gt;n_eff&lt;/code&gt; as calculated by Stan, and the actual number of samples.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  Effective sample number (&lt;code&gt;n_eff&lt;/code&gt;) identifies the number of &amp;lsquo;ideal&amp;rsquo; (i.e. uncorrelated) samples. Since MCMC algorithms explore the posterior as a chain of samples, each sample is usually correlated with the previous one to some extent. Conclusively, &lt;code&gt;n_eff&lt;/code&gt; identifies the number of samples used for estimating the posterior mean/distribution whereas actual number of samples is simply the number of data points we have.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;n_eff&lt;/code&gt; is usually smaller than the actual number of samples (unless we have anti-correlated MCMC samples).&lt;/p&gt;
&lt;h3 id=&#34;practice-e5&#34;&gt;Practice E5&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Which value should &lt;code&gt;Rhat&lt;/code&gt; approach, when a chain is sampling the posterior distribution correctly?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; $\hat{R}$ or &lt;code&gt;Rhat&lt;/code&gt;, in &lt;code&gt;R&lt;/code&gt;, reflects variance within a chain versus variance between chains. If these are the same, $\hat{R}$ will be $1.0$ - i.e.: it does not matter from which chain we would infere parameters and predictions. Values higher than 1.0 can indicate problems in the model. Values much higher than 1 indicate serious issues.&lt;/p&gt;
&lt;h3 id=&#34;practice-e6&#34;&gt;Practice E6&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Sketch a good trace plot for a Markov chain, one that is effectively sampling from the posterior distribution. What is good about its shape? Then sketch a trace plot for a malfunctioning Markov chain. What about its shape indicates malfunction?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Good trace plot&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;y &amp;lt;- rnorm(1e4, mean = 1, sd = 2)
m.E6Good &amp;lt;- ulam(
  alist(
    y ~ dnorm(mu, sigma),
    mu &amp;lt;- alpha,
    alpha ~ dnorm(0, 10),
    sigma ~ dcauchy(0, 1)
  ),
  data = list(y = y),
  cores = 2,
  chains = 2,
  start = list(
    alpha = 0,
    sigma = 1
  )
)
traceplot(m.E6Good)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1000
## [1] 1
## [1] 1000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-02-25-statistical-rethinking-chapter-09_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;1440&#34; /&gt;
These trace plots show that the chains quickly find the region with highest posterior probability and stay there.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Bad trace plot&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;y &amp;lt;- rnorm(1e4, mean = 1, sd = 2)
m.E6Bad &amp;lt;- ulam(
  alist(
    y ~ dnorm(mu, sigma),
    mu &amp;lt;- a1 + a2,
    a1 ~ dnorm(0, 10),
    a2 ~ dnorm(0, 10),
    sigma ~ dcauchy(0, 1)
  ),
  data = list(y = y),
  chains = 2,
  cores = 2,
  start = list(
    a1 = 0,
    a2 = 0,
    sigma = 1
  ),
)
traceplot(m.E6Bad)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1000
## [1] 1
## [1] 1000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-02-25-statistical-rethinking-chapter-09_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is a problem of &lt;em&gt;unidentifiable parameters&lt;/em&gt; as &lt;code&gt;a1&lt;/code&gt; and &lt;code&gt;a2&lt;/code&gt; can cancel each other out to arrive at the correct &lt;code&gt;mu&lt;/code&gt; and so we see non-stationary behaviour in the trace plots of &lt;code&gt;a1&lt;/code&gt; and &lt;code&gt;a2&lt;/code&gt; while the trace plot for &lt;code&gt;sigma&lt;/code&gt; is doing alright.&lt;/p&gt;
&lt;h2 id=&#34;medium-exercises&#34;&gt;Medium Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-m1&#34;&gt;Practice M1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Re-estimate the terrain ruggedness model from the chapter, but now using a uniform prior and an exponential prior for the standard deviation, &lt;code&gt;sigma&lt;/code&gt;. The uniform prior should be &lt;code&gt;dunif(0,10)&lt;/code&gt; and the exponential should be &lt;code&gt;dexp(1)&lt;/code&gt;. Do the different priors have any detectable influence on the posterior distribution?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; The ruggedness model in question is &lt;code&gt;m8.3&lt;/code&gt; in the book (or &lt;code&gt;m9.1&lt;/code&gt; in &lt;code&gt;ulam()&lt;/code&gt; specification). First, I prepare the data like I did 
&lt;a href=&#34;post/2021-02-18-statistical-rethinking-chapter-08/index.Rmd&#34;&gt;previously&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(rugged)
d &amp;lt;- rugged
d$log_gdp &amp;lt;- log(d$rgdppc_2000)
d &amp;lt;- d[complete.cases(d$rgdppc_2000), ]
d$log_gdp_std &amp;lt;- d$log_gdp / mean(d$log_gdp)
d$rugged_std &amp;lt;- d$rugged / max(d$rugged)
d$cid &amp;lt;- ifelse(d$cont_africa == 1, 1, 2)
dd.trim &amp;lt;- list(
  log_gdp_std = d$log_gdp_std,
  rugged_std = d$rugged_std,
  cid = as.integer(d$cid)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s fit that model with the different priors:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Exponential prior for sigma
m.M1Exp &amp;lt;- ulam(
  alist(
    log_gdp_std ~ dnorm(mu, sigma),
    mu &amp;lt;- a[cid] + b[cid] * (rugged_std - 0.215),
    a[cid] ~ dnorm(1, 0.1),
    b[cid] ~ dnorm(0, 0.3),
    sigma ~ dexp(1)
  ),
  data = dd.trim,
  chains = 4,
  cores = 4,
)
## Uniform prior for sigma
m.M1Uni &amp;lt;- ulam(
  alist(
    log_gdp_std ~ dnorm(mu, sigma),
    mu &amp;lt;- a[cid] + b[cid] * (rugged_std - 0.215),
    a[cid] ~ dnorm(1, 0.1),
    b[cid] ~ dnorm(0, 0.3),
    sigma ~ dnorm(0, 10)
  ),
  data = dd.trim,
  chains = 4,
  cores = 4,
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now on to inspect the model. Let&amp;rsquo;s start with the parameter estimates in comparison&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;coeftab(m.M1Exp, m.M1Uni)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       m.M1Exp m.M1Uni
## a[1]     0.89    0.89
## a[2]     1.05    1.05
## b[1]     0.13    0.13
## b[2]    -0.14   -0.14
## sigma    0.11    0.11
## nobs      170     170
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These are strikingly the same. What about the individual model outputs in more detail?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m.M1Exp, depth = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean          sd        5.5%       94.5%    n_eff     Rhat4
## a[1]   0.8870817 0.015625699  0.86196179  0.91173540 2453.919 0.9995577
## a[2]   1.0507770 0.009968219  1.03527611  1.06640703 2834.441 0.9988734
## b[1]   0.1344067 0.074307822  0.01486287  0.25218389 2786.188 0.9993677
## b[2]  -0.1413442 0.054855132 -0.22964887 -0.05187494 2324.832 0.9983652
## sigma  0.1117154 0.006171670  0.10228974  0.12208002 2725.266 0.9988256
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m.M1Uni, depth = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean          sd        5.5%       94.5%    n_eff     Rhat4
## a[1]   0.8865936 0.015580736  0.86128553  0.91082334 2489.074 0.9989360
## a[2]   1.0501777 0.010010613  1.03404118  1.06614541 2152.883 1.0007549
## b[1]   0.1312147 0.074609926  0.01239339  0.24998421 2244.528 0.9993558
## b[2]  -0.1420136 0.054996077 -0.22957192 -0.05372842 2023.621 0.9987402
## sigma  0.1115782 0.006224722  0.10188964  0.12166315 3600.101 0.9990594
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, these are very similar aside from the effective number of samples (&lt;code&gt;n_eff&lt;/code&gt;) which is much higher for all parameter estimates in the model with the exponential prior on &lt;code&gt;sigma&lt;/code&gt; (&lt;code&gt;m.M1Exp&lt;/code&gt;) except for &lt;code&gt;sigma&lt;/code&gt; itself, which boasts a higher &lt;code&gt;n_eff&lt;/code&gt; in the uniform-prior model (&lt;code&gt;m.M1Uni&lt;/code&gt;). As such, we conclude that while the different priors have an impact on &lt;code&gt;n_eff&lt;/code&gt;, they do not change the posterior distributions. Let me visualise this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Plot_df &amp;lt;- data.frame(
  Posteriors = c(
    extract.samples(m.M1Exp, n = 1e4)$sigma,
    extract.samples(m.M1Uni, n = 1e4)$sigma
  ),
  Name = rep(c(&amp;quot;Exp&amp;quot;, &amp;quot;Uni&amp;quot;), each = 1e4),
  Model = rep(c(&amp;quot;m.M1Exp&amp;quot;, &amp;quot;m.M1Uni&amp;quot;), each = 1e4)
)

ggplot(Plot_df, aes(y = Model, x = Posteriors)) +
  stat_halfeye() +
  labs(x = &amp;quot;Parameter Estimate&amp;quot;, y = &amp;quot;Model&amp;quot;) +
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-02-25-statistical-rethinking-chapter-09_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;1440&#34; /&gt;
That really does look the same to me.&lt;/p&gt;
&lt;h3 id=&#34;practice-m2&#34;&gt;Practice M2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; The Cauchy and exponential priors from the terrain ruggedness model are very weak. They can be made more informative by reducing their scale. Compare the &lt;code&gt;dcauchy&lt;/code&gt; and &lt;code&gt;dexp&lt;/code&gt; priors for progressively smaller values of the scaling parameter. As these priors become stronger, how does each influence the posterior distribution?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  I write a &lt;code&gt;for&lt;/code&gt; loop here to minimise code needs:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;RepTimes &amp;lt;- 4 # how many steps I want to try
ScalingFactor &amp;lt;- 10 # by what factor to make priors stronger
# empty lists to store models in
Explist &amp;lt;- as.list(rep(NA, RepTimes))
Caulist &amp;lt;- as.list(rep(NA, RepTimes))
# Loop over all models
for (Mod_Iter in 0:(RepTimes - 1)) {
  dd.trim$ScalingFactor &amp;lt;- ScalingFactor
  dd.trim$Mod_Iter &amp;lt;- Mod_Iter
  ## Exponential prior for sigma
  m.M2Exp &amp;lt;- ulam(
    alist(
      log_gdp_std ~ dnorm(mu, sigma),
      mu &amp;lt;- a[cid] + b[cid] * (rugged_std - 0.215),
      a[cid] ~ dnorm(1, 0.1),
      b[cid] ~ dnorm(0, 0.3),
      sigma ~ dexp(1 * ScalingFactor^Mod_Iter)
    ),
    data = dd.trim,
    chains = 4,
    cores = 4,
  )
  Explist[[Mod_Iter + 1]] &amp;lt;- m.M2Exp
  ## Cauchy prior for sigma
  m.M2Cau &amp;lt;- ulam(
    alist(
      log_gdp_std ~ dnorm(mu, sigma),
      mu &amp;lt;- a[cid] + b[cid] * (rugged_std - 0.215),
      a[cid] ~ dnorm(1, 0.1),
      b[cid] ~ dnorm(0, 0.3),
      sigma ~ dcauchy(0, 1 / ScalingFactor^Mod_Iter)
    ),
    data = dd.trim,
    chains = 4,
    cores = 4,
  )
  Caulist[[Mod_Iter + 1]] &amp;lt;- m.M2Cau
}
coeftab(Explist[[1]], Explist[[2]], Explist[[3]], Explist[[4]])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Explist[[1]] Explist[[2]] Explist[[3]] Explist[[4]]
## a[1]     0.89         0.89         0.89         0.89     
## a[2]     1.05         1.05         1.05         1.05     
## b[1]     0.13         0.13         0.13         0.13     
## b[2]    -0.14        -0.14        -0.14        -0.15     
## sigma    0.11         0.11         0.11         0.09     
## nobs      170          170          170          170
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;coeftab(Caulist[[1]], Caulist[[2]], Caulist[[3]], Caulist[[4]])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Caulist[[1]] Caulist[[2]] Caulist[[3]] Caulist[[4]]
## a[1]     0.89         0.89         0.89         0.89     
## a[2]     1.05         1.05         1.05         1.05     
## b[1]     0.14         0.13         0.13         0.13     
## b[2]    -0.14        -0.14        -0.14        -0.14     
## sigma    0.11         0.11         0.11         0.11     
## nobs      170          170          170          170
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The more restrictive exponential priors decrease the estimate for sigma. On the other hand, the more restrictive cauchy priors have no effect, it seems.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s explore why this is by looking at the priors themselves:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;par(mfrow = c(1, 2))
curve(dexp(x, 1),
  from = 0, to = 5, ylab = &amp;quot;Density&amp;quot;, xlab = &amp;quot;sigma&amp;quot;,
  col = &amp;quot;royalblue4&amp;quot;
)
curve(dexp(x, 10), from = 0, to = 5, add = T)
curve(dexp(x, 100), from = 0, to = 5, add = T, col = col.desat(&amp;quot;red&amp;quot;))
curve(dexp(x, 1000), from = 0, to = 5, add = T, col = col.desat(&amp;quot;green&amp;quot;))
mtext(&amp;quot;Exponential Prior&amp;quot;)
legend(&amp;quot;topright&amp;quot;,
  col = c(&amp;quot;royalblue4&amp;quot;, &amp;quot;black&amp;quot;, col.desat(&amp;quot;red&amp;quot;), col.desat(&amp;quot;green&amp;quot;)),
  lty = c(1, 1, 1), legend = c(&amp;quot;Exp(1)&amp;quot;, &amp;quot;Exp(10)&amp;quot;, &amp;quot;Exp(100)&amp;quot;, &amp;quot;Exp(1000)&amp;quot;), bty = &amp;quot;n&amp;quot;
)

curve(2 * dcauchy(x, 0, 1),
  from = 0, to = 5, ylab = &amp;quot;Density&amp;quot;, xlab = &amp;quot;sigma&amp;quot;,
  col = &amp;quot;royalblue4&amp;quot;
)
curve(2 * dcauchy(x, 0, 0.1), from = 0, to = 5, add = T, col = &amp;quot;black&amp;quot;)
curve(2 * dcauchy(x, 0, 0.01), from = 0, to = 5, add = T, col = col.desat(&amp;quot;red&amp;quot;))
curve(2 * dcauchy(x, 0, 0.001), from = 0, to = 5, add = T, col = col.desat(&amp;quot;green&amp;quot;))
mtext(&amp;quot;Cauchy Prior&amp;quot;)
legend(&amp;quot;topright&amp;quot;,
  col = c(&amp;quot;royalblue4&amp;quot;, &amp;quot;black&amp;quot;, col.desat(&amp;quot;red&amp;quot;), col.desat(&amp;quot;green&amp;quot;)),
  lty = c(1, 1, 1), legend = c(&amp;quot;Cauchy(0, 1)&amp;quot;, &amp;quot;Cauchy(0, 0.1)&amp;quot;, &amp;quot;Cauchy(0, 0.01)&amp;quot;, &amp;quot;Cauchy(0, 0.001)&amp;quot;), bty = &amp;quot;n&amp;quot;
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-02-25-statistical-rethinking-chapter-09_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The cauchy distributions show thicker tails while the exponential distributions quickly concentrate. Hence why a concentrated Cauchy prior allow more flexibility that a concentrated exponential prior.&lt;/p&gt;
&lt;h3 id=&#34;practice-m3&#34;&gt;Practice M3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Re-estimate one of the Stan models from the chapter, but at different numbers of &lt;code&gt;warmup&lt;/code&gt; iterations. Be sure to use the same number of sampling iterations in each case. Compare the &lt;code&gt;n_eff&lt;/code&gt; values.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; The ruggedness model was fine so far so I continue with that one. Here, I build this model with a fixed run length and fixed starting values for each run with changing warmup values:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;start &amp;lt;- list(a = c(1, 1), b = c(0, 0), sigma = 1) # use fixed start values for comparability of runs
m.M3 &amp;lt;- ulam(
  alist(
    log_gdp_std ~ dnorm(mu, sigma),
    mu &amp;lt;- a[cid] + b[cid] * (rugged_std - 0.215),
    a[cid] ~ dnorm(1, 0.1),
    b[cid] ~ dnorm(0, 0.3),
    sigma ~ dexp(1)
  ),
  data = dd.trim,
  start = start,
  chains = 2, cores = 2,
  iter = 100
)
warm_list &amp;lt;- c(5, 10, 100, 500, 1000) # define warmup values to run through
n_eff &amp;lt;- matrix(NA, nrow = length(warm_list), ncol = 5) # first make matrix to hold n_eff results
for (i in 1:length(warm_list)) { # loop over warm_list and collect n_eff
  w &amp;lt;- warm_list[i]
  m_temp &amp;lt;- ulam(m.M3, chains = 2, cores = 2, iter = 1000 + w, warmup = w, start = start)
  n_eff[i, ] &amp;lt;- precis(m_temp, 2)$n_eff
}
colnames(n_eff) &amp;lt;- rownames(precis(m_temp, 2))
rownames(n_eff) &amp;lt;- warm_list
n_eff # columns show parameters, rows show n_eff
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             a[1]        a[2]        b[1]        b[2]       sigma
## 5       2.314186    1.587251    2.713325    1.270369    1.776862
## 10   2243.084776 2157.086156  737.957589 1010.214712  953.010860
## 100  1725.334719 2294.576251  878.481785 1177.016946 1122.495229
## 500  2999.738299 3282.963810 2292.173710 2737.037252 2200.949134
## 1000 2485.029304 3406.341675 2372.274092 2772.175825 2607.552453
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we can see, past just 10 warmup samples, &lt;code&gt;n_eff&lt;/code&gt; does not change much (in terms of how useful our samples are). In this case, we could be quite happy with a warmup of 10.&lt;/p&gt;
&lt;h2 id=&#34;hard-exercises&#34;&gt;Hard Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-h1&#34;&gt;Practice H1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Run the model below and then inspect the posterior distribution and explain what it is accomplishing.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mp &amp;lt;- map2stan(
  alist(
    a ~ dnorm(0, 1),
    b ~ dcauchy(0, 1)
  ),
  data = list(y = 1),
  start = list(a = 0, b = 0),
  iter = 1e4,
  chains = 2, cores = 2,
  warmup = 100,
  WAIC = FALSE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Compare the samples for the parameters &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;. Can you explain the different trace plots, using what you know about the Cauchy distribution?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; First of all, let&amp;rsquo;s inspect the posterior:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(mp)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            mean         sd      5.5%    94.5%     n_eff    Rhat4
## a  0.0003388167  0.9988213 -1.601441 1.590561 12762.761 1.000120
## b -0.1918181852 13.8995715 -5.379742 5.346423  3892.011 1.000517
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Oof. Those uncertainties don&amp;rsquo;t look good at all! So what does the model even do? It simply just samples &lt;code&gt;a&lt;/code&gt; from a normal distribution with mean 0 and standard deviation 1. &lt;code&gt;b&lt;/code&gt; is sampled from a cauchy distribution. Let&amp;rsquo;s look at the traceplot for this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(mp, n_cols = 1, col = &amp;quot;royalblue4&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-02-25-statistical-rethinking-chapter-09_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we can see, there are quite some outliers in the sampling of the cauchy distribution (&lt;code&gt;b&lt;/code&gt;). Why is that? Because the cauchy distribution has very heavy tails thus making it more likely to jump to a value that is far out there in terms of posterior probability. Note that this also decreases &lt;code&gt;n_eff&lt;/code&gt;. &lt;code&gt;lp&lt;/code&gt; in the above is the log-posterior.&lt;/p&gt;
&lt;p&gt;Now let&amp;rsquo;s see how the samples we drew measure up against the underlying functions of &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;, respectively:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;post &amp;lt;- extract.samples(mp)
par(mfrow = c(1, 2))
dens(post$a)
curve(dnorm(x, 0, 1), from = -4, to = 4, add = T, lty = 2)
legend(&amp;quot;topright&amp;quot;, lty = c(1, 2), legend = c(&amp;quot;Sample&amp;quot;, &amp;quot;Exact density&amp;quot;), bty = &amp;quot;n&amp;quot;)
mtext(&amp;quot;Normal&amp;quot;)
dens(post$b, col = &amp;quot;royalblue4&amp;quot;, xlim = c(-10, 10))
curve(dcauchy(x, 0, 1),
  from = -10, to = 10, add = T, lty = 2,
  col = &amp;quot;royalblue4&amp;quot;
)
mtext(&amp;quot;Cauchy&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-02-25-statistical-rethinking-chapter-09_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the normal distribution has been reconstructed well. The cauchy distributions hasn&amp;rsquo;t.&lt;/p&gt;
&lt;h3 id=&#34;practice-h2&#34;&gt;Practice H2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Recall the divorce rate example from Chapter 5. Repeat that analysis, using &lt;code&gt;ulam()&lt;/code&gt; this time, fitting models &lt;code&gt;m5.1&lt;/code&gt;, &lt;code&gt;m5.2&lt;/code&gt;, and &lt;code&gt;m5.3&lt;/code&gt;. Use compare to compare the models on the basis of WAIC or PSIS. Explain the results.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; First, I need to load the data and prepare it for &lt;code&gt;ulam()&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(WaffleDivorce)
d &amp;lt;- WaffleDivorce
d$D &amp;lt;- standardize(d$Divorce)
d$M &amp;lt;- standardize(d$Marriage)
d$A &amp;lt;- standardize(d$MedianAgeMarriage)
d_trim &amp;lt;- list(D = d$D, M = d$M, A = d$A)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I fit the models with &lt;code&gt;ulam()&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m5.1_stan &amp;lt;- ulam(
  alist(
    D ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bA * A,
    a ~ dnorm(0, 0.2),
    bA ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d_trim,
  chains = 4, cores = 4,
  log_lik = TRUE # this is needed to get the terms for calculating PSIS or WAIC
)
m5.2_stan &amp;lt;- ulam(
  alist(
    D ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bM * M,
    a ~ dnorm(0, 0.2),
    bM ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d_trim,
  chains = 4, cores = 4,
  log_lik = TRUE # this is needed to get the terms for calculating PSIS or WAIC
)
m5.3_stan &amp;lt;- ulam(
  alist(
    D ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bA * A + bM * M,
    a ~ dnorm(0, 0.2),
    bA ~ dnorm(0, 0.5),
    bM ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = d_trim,
  chains = 4, cores = 4,
  log_lik = TRUE # this is needed to get the terms for calculating PSIS or WAIC
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we compare the models:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;compare(m5.1_stan, m5.2_stan, m5.3_stan, func = PSIS)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               PSIS        SE     dPSIS       dSE    pPSIS       weight
## m5.1_stan 125.7210 12.708327  0.000000        NA 3.630705 0.7253039155
## m5.3_stan 127.6690 12.852350  1.947996 0.6705316 4.773054 0.2738533387
## m5.2_stan 139.2364  9.936093 13.515361 9.1363047 2.923975 0.0008427459
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;compare(m5.1_stan, m5.2_stan, m5.3_stan, func = WAIC)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               WAIC        SE     dWAIC       dSE    pWAIC       weight
## m5.1_stan 125.7778 12.641919  0.000000        NA 3.659072 0.6960655494
## m5.3_stan 127.4407 12.591741  1.662916 0.6770545 4.658881 0.3030766321
## m5.2_stan 139.1754  9.813604 13.397613 9.2109285 2.893468 0.0008578185
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;WAIC tells a similar story as PSIS, but the model only containing age (&lt;code&gt;m5.1_stan&lt;/code&gt;) wins. The model with both predictors (&lt;code&gt;m5.3_stan&lt;/code&gt;) does almost as well. However, their respective PSIS and WAIC values are nearly identical. Furthermore, both models get assigned all of the WAIC weight. Let&amp;rsquo;s call these equal in performance and investigate why:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m5.3_stan)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                mean         sd       5.5%      94.5%    n_eff     Rhat4
## a     -0.0001904293 0.10140928 -0.1591984  0.1619373 1877.251 1.0002239
## bA    -0.6023698429 0.16025804 -0.8510854 -0.3467602 1085.019 1.0007578
## bM    -0.0550634908 0.16034205 -0.3109204  0.2015101 1187.780 0.9998155
## sigma  0.8275838910 0.08826874  0.7028130  0.9779113 1474.265 1.0028212
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While &lt;code&gt;m5.3_stan&lt;/code&gt; contains the marriage predictor, it is very unsure of it&amp;rsquo;s influence. In practical terms, this means that &lt;code&gt;m5.1_stan&lt;/code&gt; and &lt;code&gt;m5.3_stan&lt;/code&gt; make basically the same predictions&lt;/p&gt;
&lt;h3 id=&#34;practice-h3&#34;&gt;Practice H3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Sometimes changing a prior for one parameter has unanticipated effects on other parameters. This is because when a parameter is highly correlated with another parameter in the posterior, the prior influences both parameters. Hereâs an example to work and think through.&lt;br&gt;
Go back to the leg length example in Chapter 5. Here is the code again, which simulates height and leg lengths for 100 imagined individuals:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;N &amp;lt;- 100 # number of individuals
height &amp;lt;- rnorm(N, 10, 2) # sim total height of each
leg_prop &amp;lt;- runif(N, 0.4, 0.5) # leg as proportion of height
leg_left &amp;lt;- leg_prop * height + rnorm(N, 0, 0.02) # sim left leg as proportion + error
leg_right &amp;lt;- leg_prop * height + rnorm(N, 0, 0.02) # sim right leg as proportion + error
d &amp;lt;- data.frame(height, leg_left, leg_right) # combine into data frame
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And below is the model you fit before, resulting in a highly correlated posterior for the two beta parameters. This time, fit the model using &lt;code&gt;ulam()&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m5.8s &amp;lt;- ulam(
  alist(
    height ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bl * leg_left + br * leg_right,
    a ~ dnorm(10, 100),
    bl ~ dnorm(2, 10),
    br ~ dnorm(2, 10),
    sigma ~ dexp(1)
  ),
  data = d,
  chains = 4,
  cores = 4,
  start = list(
    a = 10,
    bl = 0,
    br = 0.1,
    sigma = 1
  )
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Compare the posterior distribution produced by the code above to the posterior distribution produced when you change the prior for &lt;code&gt;br&lt;/code&gt; so that it is strictly positive:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m5.8s2 &amp;lt;- ulam(
  alist(
    height ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bl * leg_left + br * leg_right,
    a ~ dnorm(10, 100),
    bl ~ dnorm(2, 10),
    br ~ dnorm(2, 10),
    sigma ~ dexp(1)
  ),
  data = d,
  chains = 4,
  cores = 4,
  constraints = list(br = &amp;quot;lower=0&amp;quot;),
  start = list(
    a = 10,
    bl = 0,
    br = 0.1,
    sigma = 1
  )
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note the constraints list. What this does is constrain the prior distribution of &lt;code&gt;br&lt;/code&gt; so that it has positive probability only above zero. In other words, that prior ensures that the posterior distribution for &lt;code&gt;br&lt;/code&gt; will have no probability mass below zero.&lt;br&gt;
Compare the two posterior distributions for &lt;code&gt;m5.8s&lt;/code&gt; and &lt;code&gt;m5.8s2&lt;/code&gt;. What has changed in the posterior distribution of both beta parameters? Can you explain the change induced by the change in prior?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; It&amp;rsquo;s probably easiest to just look at the posterior distributions of the beta prameters through the &lt;code&gt;pairs()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;pairs(m5.8s, main = &amp;quot;Model 1&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-02-25-statistical-rethinking-chapter-09_files/figure-html/unnamed-chunk-24-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;pairs(m5.8s2, main = &amp;quot;Model 2&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-02-25-statistical-rethinking-chapter-09_files/figure-html/unnamed-chunk-24-2.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the beta distributions have shifted drastically between the different models. Interestingly, &lt;code&gt;bl&lt;/code&gt; and &lt;code&gt;br&lt;/code&gt; were perfectly symmetric in &lt;code&gt;m5.8s&lt;/code&gt;, but are skewed in &lt;code&gt;m5.8s2&lt;/code&gt;. Given how the height of a person is approximated in both models (&lt;code&gt;a + bl*leg_left + br*leg_right&lt;/code&gt;), the distributions of leg lengths are necessarily negatively correlated (you can be of the same height with a short right leg and long left leg, long left leg and short right leg, or two medium-length legs). Thus, by setting &lt;code&gt;br&lt;/code&gt; to be strictly positive in &lt;code&gt;m5.8s2&lt;/code&gt; and made it skewed, we have forced &lt;code&gt;bl&lt;/code&gt; to be equally skewed in a mirror image of &lt;code&gt;br&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;practice-h4&#34;&gt;Practice H4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; For the two models fit in the previous problem, use WAIC or PSIS to compare the effective numbers of parameters for each model. You will need to use &lt;code&gt;log_lik=TRUE&lt;/code&gt; to instruct &lt;code&gt;ulam()&lt;/code&gt; to compute the terms that both WAIC and PSIS need. Which model has more effective parameters? Why?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Let&amp;rsquo;s run the models:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m.H4_1 &amp;lt;- ulam(
  alist(
    height ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bl * leg_left + br * leg_right,
    a ~ dnorm(10, 100),
    bl ~ dnorm(2, 10),
    br ~ dnorm(2, 10),
    sigma ~ dexp(1)
  ),
  data = d,
  chains = 4,
  cores = 4,
  start = list(
    a = 10,
    bl = 0,
    br = 0.1,
    sigma = 1
  ),
  log_lik = TRUE
)
m.H4_2 &amp;lt;- ulam(
  alist(
    height ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bl * leg_left + br * leg_right,
    a ~ dnorm(10, 100),
    bl ~ dnorm(2, 10),
    br ~ dnorm(2, 10),
    sigma ~ dexp(1)
  ),
  data = d,
  chains = 4,
  cores = 4,
  constraints = list(br = &amp;quot;lower=0&amp;quot;),
  start = list(
    a = 10,
    bl = 0,
    br = 0.1,
    sigma = 1
  ),
  log_lik = TRUE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we compare them with WAIC:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;compare(m.H4_1, m.H4_2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            WAIC       SE     dWAIC      dSE    pWAIC    weight
## m.H4_1 182.1474 10.21060 0.0000000       NA 2.961292 0.6063273
## m.H4_2 183.0112  9.88398 0.8638001 2.349502 2.382919 0.3936727
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The models are pretty much tied. The model with truncated priors (&lt;code&gt;m.H4_2&lt;/code&gt;) is less flexible as indicated by &lt;code&gt;pWAIC&lt;/code&gt;. This is because the prior is more informative and the variance in the posterior distribution is smaller as a result.&lt;/p&gt;
&lt;h3 id=&#34;practice-h5&#34;&gt;Practice H5&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Modify the Metropolis algorithm code from the chapter to handle the case that the island populations have a different distribution than the island labels. This means the islandâs number will not be the same as its population.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; First of all, we need our 10 islands with population sizes of 1-10, but in random order:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;pop_size &amp;lt;- sample(1:10)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can use the code from the chapter almost unaltered safe for one exception - we need to use indexing to translate island location into population size:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;num_weeks &amp;lt;- 1e5
positions &amp;lt;- rep(NA, num_weeks)
current &amp;lt;- 10
for (i in 1:num_weeks) {
  positions[i] &amp;lt;- current # record current position
  proposal &amp;lt;- current + sample(c(-1, 1), size = 1) # flip coin to generate proposal
  # now make sure he loops around the archipelago
  if (proposal &amp;lt; 1) proposal &amp;lt;- 10
  if (proposal &amp;gt; 10) proposal &amp;lt;- 1
  prob_move &amp;lt;- pop_size[proposal] / pop_size[current] # move?
  current &amp;lt;- ifelse(runif(1) &amp;lt; prob_move, proposal, current)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To see if this works, we can plot population size against frequency of visit by the king:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;f &amp;lt;- table(positions) # compute frequencies
plot(as.vector(f), pop_size,
  type = &amp;quot;n&amp;quot;, # plot frequencies against relative population sizes
  xlab = &amp;quot;frequency&amp;quot;, ylab = &amp;quot;population size&amp;quot;
) # empty plot
text(x = f, y = pop_size, labels = names(f)) # add names of islands / their positions
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-02-25-statistical-rethinking-chapter-09_files/figure-html/unnamed-chunk-29-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;practice-h6&#34;&gt;Practice H6&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Modify the Metropolis algorithm code from the chapter to write your own simple MCMC estimator for globe tossing data and model from Chapter 2.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; We want to fit the following model:&lt;/p&gt;
&lt;p&gt;$$wâ¼Binom(Î¸,n)$$
$$Î¸â¼Unif(0,1)$$
Our Metropolis algorithm looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(42)
# the globe tossing data
w &amp;lt;- 6
n &amp;lt;- 9
# prior on p
p_prior &amp;lt;- function(p) dunif(p, min = 0, max = 1)
# initializing MCMC
iter &amp;lt;- 1e4
p_sample &amp;lt;- rep(0, iter)
p_current &amp;lt;- 0.5 # start value
for (i in 1:iter) {
  p_sample[i] &amp;lt;- p_current # # record current p
  p_proposal &amp;lt;- runif(1, min = 0, max = 1) # generate proposal
  # compute likelihood for current and proposal
  lkhd_current &amp;lt;- dbinom(w, n, p_current)
  lkhd_proposal &amp;lt;- dbinom(w, n, p_proposal)
  prob_proposal &amp;lt;- lkhd_proposal * p_prior(p_proposal)
  prob_current &amp;lt;- lkhd_current * p_prior(p_current)
  prob_accept &amp;lt;- prob_proposal / prob_current
  p_current &amp;lt;- ifelse(runif(1) &amp;lt; prob_accept, p_proposal, p_current)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s visualise what happened here:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(p_sample, type = &amp;quot;l&amp;quot;, col = &amp;quot;royalblue4&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-02-25-statistical-rethinking-chapter-09_files/figure-html/unnamed-chunk-31-1.png&#34; width=&#34;1440&#34; /&gt;
Finally, let&amp;rsquo;s plot the posterior distribution:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dens(p_sample, col = &amp;quot;royalblue4&amp;quot;, adj = 1)
curve(dbeta(x, w + 1, n - w + 1), from = 0, to = 1, add = T, lty = 2)
abline(v = median(p_sample))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-02-25-statistical-rethinking-chapter-09_files/figure-html/unnamed-chunk-32-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.5 (2021-03-31)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19043)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.1252  LC_CTYPE=English_United Kingdom.1252    LC_MONETARY=English_United Kingdom.1252 LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.1252    
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] tidybayes_2.3.1      rethinking_2.13      rstan_2.21.2         ggplot2_3.3.6        StanHeaders_2.21.0-7
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.7           mvtnorm_1.1-1        lattice_0.20-41      tidyr_1.1.3          prettyunits_1.1.1    ps_1.6.0             assertthat_0.2.1     digest_0.6.27        utf8_1.2.1          
## [10] V8_3.4.1             plyr_1.8.6           R6_2.5.0             backports_1.2.1      stats4_4.0.5         evaluate_0.14        coda_0.19-4          highr_0.9            blogdown_1.3        
## [19] pillar_1.6.0         rlang_0.4.11         curl_4.3.2           callr_3.7.0          jquerylib_0.1.4      R.utils_2.10.1       R.oo_1.24.0          rmarkdown_2.7        styler_1.4.1        
## [28] labeling_0.4.2       stringr_1.4.0        loo_2.4.1            munsell_0.5.0        compiler_4.0.5       xfun_0.22            pkgconfig_2.0.3      pkgbuild_1.2.0       shape_1.4.5         
## [37] htmltools_0.5.1.1    tidyselect_1.1.0     tibble_3.1.1         gridExtra_2.3        bookdown_0.22        arrayhelpers_1.1-0   codetools_0.2-18     matrixStats_0.61.0   fansi_0.4.2         
## [46] crayon_1.4.1         dplyr_1.0.5          withr_2.4.2          MASS_7.3-53.1        R.methodsS3_1.8.1    distributional_0.2.2 ggdist_2.4.0         grid_4.0.5           jsonlite_1.7.2      
## [55] gtable_0.3.0         lifecycle_1.0.0      DBI_1.1.1            magrittr_2.0.1       scales_1.1.1         KernSmooth_2.23-18   RcppParallel_5.1.2   cli_3.0.0            stringi_1.5.3       
## [64] farver_2.1.0         bslib_0.2.4          ellipsis_0.3.2       generics_0.1.0       vctrs_0.3.7          rematch2_2.1.2       forcats_0.5.1        tools_4.0.5          svUnit_1.0.6        
## [73] R.cache_0.14.0       glue_1.4.2           purrr_0.3.4          processx_3.5.1       yaml_2.2.1           inline_0.3.17        colorspace_2.0-0     knitr_1.33           sass_0.3.1
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>RMarkdown - Manuscript Workflow Revisited</title>
      <link>https://www.erikkusch.com/courses/excursions-into-biostatistics/9_rmarkdown-manuscript-workflow-revisited/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/excursions-into-biostatistics/9_rmarkdown-manuscript-workflow-revisited/</guid>
      <description>&lt;p&gt;I have prepared some &lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/Excursions-into-Biostatistics/Rmarkdown---Manuscript-Workflow-Revisited.html&#34; target=&#34;_blank&#34;&gt; Lecture Slides &lt;/a&gt; for this session.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ordinal &amp; Metric Tests (Two-Sample Situations)</title>
      <link>https://www.erikkusch.com/courses/an-introduction-to-biostatistics/ordinal-metric-tests-two-sample-situations/</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/an-introduction-to-biostatistics/ordinal-metric-tests-two-sample-situations/</guid>
      <description>&lt;h2 id=&#34;theory&#34;&gt;Theory&lt;/h2&gt;
&lt;p&gt;Welcome to our fourth practical experience in R. Throughout the following notes, I will introduce you to a couple statistical approaches for metric or ordinal data when wanting to compare two samples/populations that might be useful to you and are, to varying degrees, often used in biology. To do so, I will enlist the sparrow data set we handled in our first exercise.&lt;/p&gt;
&lt;p&gt;I have prepared some &lt;a href=&#34;https://www.erikkusch.com/courses/an-introduction-to-biostatistics/10---Ordinal-and-Metric-Test--Two-Sample-_Handout.html&#34; target=&#34;_blank&#34;&gt; Lecture Slides &lt;/a&gt; for this session.&lt;/p&gt;
&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;p&gt;Find the data for this exercise &lt;a href=&#34;https://www.erikkusch.com/courses/an-introduction-to-biostatistics/Data/2b%20-%20Sparrow_ResettledSIUK_READY.rds&#34; target=&#34;_blank&#34;&gt; here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;preparing-our-procedure&#34;&gt;Preparing Our Procedure&lt;/h2&gt;
&lt;p&gt;To ensure others can reproduce our analysis we run the following three lines of code at the beginning of our &lt;code&gt;R&lt;/code&gt; coding file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rm(list=ls()) # clearing environment
Dir.Base &amp;lt;- getwd() # soft-coding our working directory
Dir.Data &amp;lt;- paste(Dir.Base, &amp;quot;Data&amp;quot;, sep=&amp;quot;/&amp;quot;) # soft-coding our data directory 
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;packages&#34;&gt;Packages&lt;/h3&gt;
&lt;p&gt;Using the following, user-defined function, we install/load all the necessary packages into our current &lt;code&gt;R&lt;/code&gt; session.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# function to load packages and install them if they haven&#39;t been installed yet
install.load.package &amp;lt;- function(x) {
  if (!require(x, character.only = TRUE))
    install.packages(x)
  require(x, character.only = TRUE)
}
package_vec &amp;lt;- c()
sapply(package_vec, install.load.package)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## list()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, we don&amp;rsquo;t need any packages for our analyses in this practical.&lt;/p&gt;
&lt;h3 id=&#34;loading-data&#34;&gt;Loading Data&lt;/h3&gt;
&lt;p&gt;During our first exercise (Data Mining and Data Handling - Fixing The Sparrow Data Set) we saved our clean data set as an RDS file. To load this, we use the &lt;code&gt;readRDS()&lt;/code&gt; command that comes with base &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df_base &amp;lt;- readRDS(file = paste(Dir.Data, &amp;quot;/1 - Sparrow_Data_READY.rds&amp;quot;, sep=&amp;quot;&amp;quot;))
Data_df &amp;lt;- Data_df_base # duplicate and save initial data on a new object
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;mann-whitney-u-test&#34;&gt;Mann-Whitney U Test&lt;/h2&gt;
&lt;p&gt;We can analyse the significance of two population/sample medians of metric variables which are independent of one another using the &lt;code&gt;wilcox.test()&lt;/code&gt; function in base &lt;code&gt;R&lt;/code&gt; whilst specifying &lt;code&gt;paired = FALSE&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;climate-warmingextremes&#34;&gt;Climate Warming/Extremes&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Does morphology of Passer domesticus depend on climate?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Logically, we&amp;rsquo;d expect morphological aspects of &lt;em&gt;Passer domesticus&lt;/em&gt; to change given different frequencies and severities of climate extremes. Don&amp;rsquo;t forget, however, that our statistical procedures are usually built on the null hypothesis of no differences or correlations being present and so is the Mann-Whitney U Test.&lt;/p&gt;
&lt;p&gt;Our data set recorded three aspects of sparrow morphology and three climate levels (). Remember, however, that we set aside four stations (Siberia, United Kingdom, Reunion and Australia) to test climate effects on which are strictly limited to continental and coastal climate types. We need to exclude all other sites records from our data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# prepare climate type testing data
Data_df &amp;lt;- Data_df_base
Index &amp;lt;- Data_df$Index
Rows &amp;lt;- which(Index == &amp;quot;SI&amp;quot; | Index == &amp;quot;UK&amp;quot; | Index == &amp;quot;RE&amp;quot; | Index == &amp;quot;AU&amp;quot; )
Data_df &amp;lt;- Data_df[Rows,]
Data_df$Climate &amp;lt;- droplevels(factor(Data_df$Climate))
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;sparrow-weight&#34;&gt;Sparrow Weight&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s start with the weight of &lt;em&gt;Passer domesticus&lt;/em&gt; individuals as grouped by the climate type present at the site weights have been recorded at:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Analysis
with(Data_df, 
     wilcox.test(x = Weight[Climate == &amp;quot;Continental&amp;quot;], 
                 y = Weight[Climate != &amp;quot;Continental&amp;quot;], 
                 paired = FALSE)
     )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Wilcoxon rank sum test with continuity correction
## 
## data:  Weight[Climate == &amp;quot;Continental&amp;quot;] and Weight[Climate != &amp;quot;Continental&amp;quot;]
## W = 22104, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true location shift is not equal to 0
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Direction of effect
median(Data_df$Weight[which(Data_df$Climate == &amp;quot;Continental&amp;quot;)], na.rm = TRUE) &amp;gt; 
  median(Data_df$Weight[which(Data_df$Climate != &amp;quot;Continental&amp;quot;)], na.rm = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Quite obviously, the weight of sparrows seems to be dependent on the type of climate the individuals are suspected to (p = $1.7179744\times 10^{-32}$) and &lt;strong&gt;reject the null hypothesis&lt;/strong&gt;. Weights of sparrows in continental climates are, on average, heavier than respective weights of their peers in coastal climates.&lt;/p&gt;
&lt;h4 id=&#34;sparrow-height&#34;&gt;Sparrow Height&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Analysis
with(Data_df, 
     wilcox.test(x = Height[Climate == &amp;quot;Continental&amp;quot;], 
                 y = Height[Climate != &amp;quot;Continental&amp;quot;], 
                 paired = FALSE)
     )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Wilcoxon rank sum test with continuity correction
## 
## data:  Height[Climate == &amp;quot;Continental&amp;quot;] and Height[Climate != &amp;quot;Continental&amp;quot;]
## W = 11498, p-value = 0.1971
## alternative hypothesis: true location shift is not equal to 0
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Direction of effect
median(Data_df$Height[which(Data_df$Climate == &amp;quot;Continental&amp;quot;)], na.rm = TRUE) &amp;gt; 
  median(Data_df$Height[which(Data_df$Climate != &amp;quot;Continental&amp;quot;)], na.rm = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We conclude that the height of sparrows seems to not be dependent on the type of climate the individuals are suspected to (p = $0.1970959$) and &lt;strong&gt;accept the null hypothesis&lt;/strong&gt;. Sparrows in continental climates are, on average, smaller than their peers in coastal climates but not to a statistically significant degree.&lt;/p&gt;
&lt;h4 id=&#34;sparrow-wing-chord&#34;&gt;Sparrow Wing Chord&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Analysis
with(Data_df, 
     wilcox.test(x = Wing.Chord[Climate == &amp;quot;Continental&amp;quot;], 
                 y = Wing.Chord[Climate != &amp;quot;Continental&amp;quot;], 
                 paired = FALSE)
     )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Wilcoxon rank sum test with continuity correction
## 
## data:  Wing.Chord[Climate == &amp;quot;Continental&amp;quot;] and Wing.Chord[Climate != &amp;quot;Continental&amp;quot;]
## W = 10505, p-value = 0.01213
## alternative hypothesis: true location shift is not equal to 0
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Direction of effect
median(Data_df$Wing.Chord[which(Data_df$Climate == &amp;quot;Continental&amp;quot;)], na.rm = TRUE) &amp;gt; 
  median(Data_df$Wing.Chord[which(Data_df$Climate != &amp;quot;Continental&amp;quot;)], na.rm = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Apparently, the wing chord of sparrows seems to be dependent on the type of climate the individuals are suspected to (p = $0.0121264$) and &lt;strong&gt;reject the null hypothesis&lt;/strong&gt;. Sparrows in continental climates have, generally speaking, shorter wings than their peers in coastal climates.&lt;/p&gt;
&lt;h3 id=&#34;predation&#34;&gt;Predation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Does nesting height of nest sites of Passer domesticus depend on predator characteristics?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In our second practical (Nominal Tests), we used a Chi-Squared approach in a two-sample situation to identify whether predator presence and type had any influence over the nesting sites that individuals of &lt;em&gt;Passer domesticus&lt;/em&gt; preferred. Our findings showed that they did and so we should expect similar results here when using &lt;code&gt;Nesting Site&lt;/code&gt; as our response variable instead of &lt;code&gt;Nesting Height&lt;/code&gt; as these two are highly related to each other.&lt;/p&gt;
&lt;p&gt;Additionally, to save some space in these notes, I am not showing how to identify the direction of the effect via code any more for now. We may wish to use the entirety of our data set again for this purpose:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df &amp;lt;- Data_df_base
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;predator-presence&#34;&gt;Predator Presence&lt;/h4&gt;
&lt;p&gt;First, we start with a possible link to predator presence and nesting height chosen by common house sparrows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;with(Data_df, 
     wilcox.test(x = Nesting.Height[Predator.Presence == &amp;quot;Yes&amp;quot;], 
                 y = Nesting.Height[Predator.Presence != &amp;quot;Yes&amp;quot;], 
                 paired = FALSE)
     )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Wilcoxon rank sum test with continuity correction
## 
## data:  Nesting.Height[Predator.Presence == &amp;quot;Yes&amp;quot;] and Nesting.Height[Predator.Presence != &amp;quot;Yes&amp;quot;]
## W = 25420, p-value = 0.0007678
## alternative hypothesis: true location shift is not equal to 0
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The nesting height of sparrows depends on whether a predator is present or not (p = $7.6777684\times 10^{-4}$) thus &lt;strong&gt;rejecting the null hypothesis&lt;/strong&gt;. Sparrows tend to go for nesting sites in more elevated positions when no predator is present.&lt;/p&gt;
&lt;h4 id=&#34;predator-type&#34;&gt;Predator Type&lt;/h4&gt;
&lt;p&gt;Again, we might want to check whether the position of a given nest might be related to what kind of predator is present:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;with(Data_df, 
     wilcox.test(x = Nesting.Height[Predator.Type == &amp;quot;Avian&amp;quot;], 
                 y = Nesting.Height[Predator.Type != &amp;quot;Avian&amp;quot;], 
                 paired = FALSE)
     )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Wilcoxon rank sum test with continuity correction
## 
## data:  Nesting.Height[Predator.Type == &amp;quot;Avian&amp;quot;] and Nesting.Height[Predator.Type != &amp;quot;Avian&amp;quot;]
## W = 5228, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true location shift is not equal to 0
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We conclude that the nesting height of sparrows depends on what kind of predator is present (p = $7.4380407\times 10^{-19}$) conclusively &lt;strong&gt;rejecting the null hypothesis&lt;/strong&gt;. Therefore, we are confident in stating that sparrows tend to go for nesting sites in more elevated positions when non-avian predators are present.&lt;/p&gt;
&lt;h3 id=&#34;competition&#34;&gt;Competition&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Do home ranges of Passer domesticus depend on climate?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We might expect different behaviour of &lt;em&gt;Passer domesticus&lt;/em&gt; given different climate types. Since &lt;code&gt;Home Range&lt;/code&gt; is on an ordinal scale () we can run a Mann-Whitney U Test on these whilst taking &lt;code&gt;Climate&lt;/code&gt; into account as our predictor variable.&lt;/p&gt;
&lt;p&gt;Take note that we need to limit our analysis to our climate type testing sites again as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# prepare climate type testing data
Data_df &amp;lt;- Data_df_base
Index &amp;lt;- Data_df$Index
Rows &amp;lt;- which(Index == &amp;quot;SI&amp;quot; | Index == &amp;quot;UK&amp;quot; | Index == &amp;quot;RE&amp;quot; | Index == &amp;quot;AU&amp;quot; )
Data_df &amp;lt;- Data_df[Rows,]
Data_df$Climate &amp;lt;- droplevels(factor(Data_df$Climate))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To be able to use the &lt;code&gt;wilcox.test()&lt;/code&gt; function on &lt;code&gt;Home Range&lt;/code&gt;, we need to transform its elements into a numeric type. Luckily, this is as easy as using the &lt;code&gt;as.numeric()&lt;/code&gt; function on the data since it will assign every factor level a number corresponding to its position in &lt;code&gt;levels()&lt;/code&gt; as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;levels(factor(Data_df$Home.Range))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Large&amp;quot;  &amp;quot;Medium&amp;quot; &amp;quot;Small&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;HR_vec &amp;lt;- as.numeric(factor((Data_df$Home.Range)))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, the levels of &lt;code&gt;Home.Range&lt;/code&gt; are ordered alphabetically. The &lt;code&gt;as.numeric()&lt;/code&gt; command will thus transform every record of &lt;code&gt;&amp;quot;Large&amp;quot;&lt;/code&gt; into &lt;code&gt;1&lt;/code&gt;, every record of &lt;code&gt;&amp;quot;Medium&amp;quot;&lt;/code&gt; into &lt;code&gt;2&lt;/code&gt; and every record of &lt;code&gt;&amp;quot;Small&amp;quot;&lt;/code&gt; into &lt;code&gt;3&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We are ready to run the analysis:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Analysis
with(Data_df, 
     wilcox.test(x = HR_vec[Climate == &amp;quot;Continental&amp;quot;], 
                 y = HR_vec[Climate != &amp;quot;Continental&amp;quot;], 
                 paired = FALSE)
     )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Wilcoxon rank sum test with continuity correction
## 
## data:  HR_vec[Climate == &amp;quot;Continental&amp;quot;] and HR_vec[Climate != &amp;quot;Continental&amp;quot;]
## W = 11897, p-value = 0.3666
## alternative hypothesis: true location shift is not equal to 0
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Direction of effect
median(HR_vec[which(Data_df$Climate == &amp;quot;Continental&amp;quot;)], na.rm = TRUE) &amp;gt; 
  median(HR_vec[which(Data_df$Climate != &amp;quot;Continental&amp;quot;)], na.rm = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;According to the output of our analysis, the home ranges of &lt;em&gt;Passer domesticus&lt;/em&gt; do not depend on the climate characteristics of their respective habitats (p = $0.2766088$). Thus, we &lt;strong&gt;accept the null hypothesis&lt;/strong&gt;.&lt;br&gt;
As you can see, the median of numeric home ranges is smaller in continental climates (just not statistically significant). Remember that small numeric ranges mean large actual ranges in this set-up and so we can conclude that  climates force common house sparrows to adapt to bigger home ranges.&lt;/p&gt;
&lt;h3 id=&#34;sexual-dimorphism&#34;&gt;Sexual Dimorphism&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Does morphology of Passer domesticus depend on sex?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If we assume a sexual dimorphism to have manifested itself in &lt;em&gt;Passer domesticus&lt;/em&gt; over evolutionary time, we&amp;rsquo;d expect different morphological features of males and females. In our second practical (Nominal Tests) we already assessed this using a Chi-Squared approach in a two-sample situation on colour morphs of the common house sparrows. At the time, we concluded that colouring is not equal for the sexes. So what about characteristics of our sparrows we can put into a meaningful order?&lt;/p&gt;
&lt;p&gt;We may wish to use the entirety of our data set again for this purpose:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df &amp;lt;- Data_df_base
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;sparrow-weight-1&#34;&gt;Sparrow Weight&lt;/h4&gt;
&lt;p&gt;We start by assessing the median weight of sparrows again, as driven by their sexes:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Analysis
with(Data_df, 
     wilcox.test(x = Weight[Sex == &amp;quot;Male&amp;quot;], 
                 y = Weight[Sex != &amp;quot;Male&amp;quot;], 
                 paired = FALSE)
     )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Wilcoxon rank sum test with continuity correction
## 
## data:  Weight[Sex == &amp;quot;Male&amp;quot;] and Weight[Sex != &amp;quot;Male&amp;quot;]
## W = 187772, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true location shift is not equal to 0
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Direction of effect
median(Data_df$Weight[which(Data_df$Sex == &amp;quot;Male&amp;quot;)], na.rm = TRUE) &amp;gt; 
  median(Data_df$Weight[which(Data_df$Sex != &amp;quot;Male&amp;quot;)], na.rm = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We already identified &lt;code&gt;Climate&lt;/code&gt; to be a major driver of median sparrow weight within this practical. Now, we need to add &lt;code&gt;Sex&lt;/code&gt; to the list of drivers of sparrow weight (p = $8.2580833\times 10^{-20}$) and &lt;strong&gt;reject the null hypothesis&lt;/strong&gt; that sparrow weights do not differ according to Sex.  Males tend to be heavier than females.&lt;/p&gt;
&lt;h4 id=&#34;sparrow-height-1&#34;&gt;Sparrow Height&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s move on and see if the height/length of our observed sparrows are dependent on their sexes:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;with(Data_df, 
     wilcox.test(x = Height[Sex == &amp;quot;Male&amp;quot;], 
                 y = Height[Sex != &amp;quot;Male&amp;quot;], 
                 paired = FALSE)
     )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Wilcoxon rank sum test with continuity correction
## 
## data:  Height[Sex == &amp;quot;Male&amp;quot;] and Height[Sex != &amp;quot;Male&amp;quot;]
## W = 141956, p-value = 0.9525
## alternative hypothesis: true location shift is not equal to 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We already identified &lt;code&gt;Climate&lt;/code&gt; to be a major driver of median sparrow height within this practical. Sparrow &lt;code&gt;Sex&lt;/code&gt; does not seem to be an informative characteristic when trying to understand sparrow heights (p = $0.9524599$). So we &lt;strong&gt;accept the null hypothesis&lt;/strong&gt; and don&amp;rsquo;t identify any direction of effects since there is no effect.&lt;/p&gt;
&lt;h4 id=&#34;sparrow-wing-chord-1&#34;&gt;Sparrow Wing Chord&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;with(Data_df, 
     wilcox.test(x = Wing.Chord[Sex == &amp;quot;Male&amp;quot;], 
                 y = Wing.Chord[Sex != &amp;quot;Male&amp;quot;], 
                 paired = FALSE)
     )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Wilcoxon rank sum test with continuity correction
## 
## data:  Wing.Chord[Sex == &amp;quot;Male&amp;quot;] and Wing.Chord[Sex != &amp;quot;Male&amp;quot;]
## W = 141637, p-value = 0.9021
## alternative hypothesis: true location shift is not equal to 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;According to our previous analysis within this practical, &lt;code&gt;Climate&lt;/code&gt; has been determined to be a major driver wing chords of common house sparrows. With our current analysis in mind, we can conclude that the &lt;code&gt;Sex&lt;/code&gt; of any given &lt;em&gt;Passer domesticus&lt;/em&gt; individual does not influence the wing chord of said individual (p = $0.9020933$). Therefore we &lt;strong&gt;accept the null hypothesis&lt;/strong&gt; and don&amp;rsquo;t identify any direction of effects since there is no effect.&lt;/p&gt;
&lt;h2 id=&#34;wilcoxon-signed-rank-test&#34;&gt;Wilcoxon Signed Rank Test&lt;/h2&gt;
&lt;p&gt;We can analyse the significance of two population/sample medians of metric variables which are dependent of one another using the &lt;code&gt;wilcox.test()&lt;/code&gt; function in base &lt;code&gt;R&lt;/code&gt; whilst specifying &lt;code&gt;paired = TRUE&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;preparing-data&#34;&gt;Preparing Data&lt;/h3&gt;
&lt;p&gt;Obviously, none of our data records are paired as such. Whilst one may want to make the argument that many characteristics of individuals that group together might be dependant on the expressions of themselves found throughout said group, we will not concentrate on this possibility within these practicals.&lt;/p&gt;
&lt;p&gt;Conclusively, we need an &lt;strong&gt;additional data set with truly paired records&lt;/strong&gt; of sparrows. Within our study set-up, think of a &lt;strong&gt;resettling experiment&lt;/strong&gt;, were you take &lt;em&gt;Passer domesticus&lt;/em&gt; individuals from one site, transfer them to another and check back with them after some time has passed to see whether some of their characteristics have changed in their expression.&lt;br&gt;
To this end, presume we have taken the entire &lt;em&gt;Passer domesticus&lt;/em&gt; population found at our &lt;strong&gt;Siberian&lt;/strong&gt; research station and moved them to the &lt;strong&gt;United Kingdom&lt;/strong&gt;. Whilst this keeps the latitude stable, the sparrows &lt;em&gt;now experience a coastal climate instead of a continental one&lt;/em&gt;. After some time (let&amp;rsquo;s say: a year), we have come back and recorded all the characteristics for the same individuals again.&lt;/p&gt;
&lt;p&gt;You will find the corresponding &lt;em&gt;new data&lt;/em&gt; in &lt;code&gt;2b - Sparrow_ResettledSIUK_READY.rds&lt;/code&gt;. Take note that this set only contains records for the transferred individuals in the &lt;strong&gt;same order&lt;/strong&gt; as in the old data set.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df_Resettled &amp;lt;- readRDS(file = paste(Dir.Data, &amp;quot;/2b - Sparrow_ResettledSIUK_READY.rds&amp;quot;, sep=&amp;quot;&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With these data records, we can now re-evaluate how the characteristics of sparrows can change when subjected to different conditions than previously thus shedding light on their &lt;strong&gt;plasticity&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;climate-warmingextremes-1&#34;&gt;Climate Warming/Extremes&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Does morphology of Passer domesticus depend on climate?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We have already concluded that the overall morphological aspects of populations of &lt;em&gt;Passer domesticus&lt;/em&gt; are shaped by climate, but what happens if we take birds from one climate and resettle them to another climate?&lt;/p&gt;
&lt;h4 id=&#34;sparrow-weight-2&#34;&gt;Sparrow Weight&lt;/h4&gt;
&lt;p&gt;First, let&amp;rsquo;s see how the average weight of our individual sparrows changed a year after they were relocated from Siberia to the UK:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;with(Data_df[which(Data_df$Index == &amp;quot;SI&amp;quot;),], 
     wilcox.test(x = Weight, y = Data_df_Resettled$Weight, paired = TRUE))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Wilcoxon signed rank test with continuity correction
## 
## data:  Weight and Data_df_Resettled$Weight
## V = 2044, p-value = 2.073e-09
## alternative hypothesis: true location shift is not equal to 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Apparently, the weight of the individual sparrows have significantly changed following their relocation (p = $2.0725016\times 10^{-9}$) and we &lt;strong&gt;reject the null hypothesis&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Earlier, we identified sparrows to be heavier in continental climates when compared to coastal climates - does this sentiment hold true with relocated birds?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Direction of effect
median(Data_df$Weight[which(Data_df$Index == &amp;quot;SI&amp;quot;)], na.rm = TRUE) &amp;gt; 
  median(Data_df_Resettled$Weight, na.rm = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Yes, it does. The resettled birds have reduced their median weight (probably not a conscious decision on behalf of the sparrows).&lt;/p&gt;
&lt;h4 id=&#34;sparrow-height-2&#34;&gt;Sparrow Height&lt;/h4&gt;
&lt;p&gt;Secondly, have our relocated sparrows become taller or shorter?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;with(Data_df[which(Data_df$Index == &amp;quot;SI&amp;quot;),], 
     wilcox.test(x = Height, y = Data_df_Resettled$Height, paired = TRUE))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in wilcox.test.default(x = Height, y = Data_df_Resettled$Height, :
## cannot compute exact p-value with zeroes
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Wilcoxon signed rank test with continuity correction
## 
## data:  Height and Data_df_Resettled$Height
## V = 0, p-value = NA
## alternative hypothesis: true location shift is not equal to 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Interestingly enough, we do not receive either meaningful W (&lt;code&gt;V&lt;/code&gt;) statistic nor an informative p-value (&lt;code&gt;NA&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;This could only be due to one reason:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;unique(Data_df$Height[which(Data_df$Index == &amp;quot;SI&amp;quot;)] == Data_df_Resettled$Height)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our sparrows have not become any shorter or taller! In fact, no height/length record has changed for any of the sparrows we relocated. This may usually be indicative of a data handling error but, in this case, makes a lot of sense when considering how difficult it may be for mature individuals to change in size.&lt;/p&gt;
&lt;h4 id=&#34;sparrow-wing-chord-2&#34;&gt;Sparrow Wing Chord&lt;/h4&gt;
&lt;p&gt;Third, let&amp;rsquo;s check whether wing chords have changed across the board. We can expect them to behave just like height records did:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;with(Data_df[which(Data_df$Index == &amp;quot;SI&amp;quot;),], 
     wilcox.test(x = Wing.Chord, y = Data_df_Resettled$Wing.Chord, paired = TRUE))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in wilcox.test.default(x = Wing.Chord, y =
## Data_df_Resettled$Wing.Chord, : cannot compute exact p-value with zeroes
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Wilcoxon signed rank test with continuity correction
## 
## data:  Wing.Chord and Data_df_Resettled$Wing.Chord
## V = 0, p-value = NA
## alternative hypothesis: true location shift is not equal to 0
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;unique(Data_df$Wing.Chord[which(Data_df$Index == &amp;quot;SI&amp;quot;)] == Data_df_Resettled$Wing.Chord)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, none of the wing chord records have changed.&lt;/p&gt;
&lt;h3 id=&#34;predation-1&#34;&gt;Predation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Does nesting height of nest sites of Passer domesticus depend on predator characteristics?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We have already identified predator characteristics at our sites to be influential in the overall nesting site and height of &lt;em&gt;Passer domesticus&lt;/em&gt;. Does this trend hold true when considering a relocation experiment?&lt;/p&gt;
&lt;p&gt;Firstly, we will test whether nesting heights have changed after the relocation. Before we do so, we should first check whether we&amp;rsquo;d expect a change based on whether &lt;strong&gt;predator presence&lt;/strong&gt; is different between Siberia and the UK:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;PP_Sib &amp;lt;- unique(Data_df$Predator.Presence[which(Data_df$Index == &amp;quot;SI&amp;quot;)])
PP_UK &amp;lt;- unique(Data_df_Resettled$Predator.Presence)

PP_Sib == PP_UK
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Apparently, predators are present at both of these sites and so we would not expect a significant change in nesting height. Let&amp;rsquo;s check this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;with(Data_df[which(Data_df$Index == &amp;quot;SI&amp;quot;),], 
     wilcox.test(x = Nesting.Height, 
                 y = Data_df_Resettled$Nesting.Height, 
                 paired = TRUE))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Wilcoxon signed rank exact test
## 
## data:  Nesting.Height and Data_df_Resettled$Nesting.Height
## V = 65, p-value = 7.404e-05
## alternative hypothesis: true location shift is not equal to 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There actually is an effect after the resettling! Therefore, we have to &lt;strong&gt;reject the null hypothesis&lt;/strong&gt; (p = $7.4040145\times 10^{-5}$)! How can this be?&lt;/p&gt;
&lt;p&gt;Maybe it has to do with the kind of predator at each site:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;unique(Data_df$Predator.Type[which(Data_df$Index == &amp;quot;SI&amp;quot;)])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] Avian
## Levels: Avian Non-Avian
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;unique(Data_df_Resettled$Predator.Type)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] Non-Avian
## Levels: Avian Non-Avian
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, Siberian sparrows are subject to avian predation whilst the sparrow populations that we monitored in the UK are experiencing non-avian predator presence. A &lt;strong&gt;causal link between nesting height and predator type&lt;/strong&gt; seems to be logical!&lt;/p&gt;
&lt;p&gt;Which direction is the effect headed? Earlier within this practical, we hypothesized that avian predation forces lower nesting heights in &lt;em&gt;Passer domesticus&lt;/em&gt; - does this hold true?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;NH_Sib &amp;lt;- mean(Data_df$Nesting.Height[which(Data_df$Index == &amp;quot;SI&amp;quot;)], na.rm = TRUE) 
NH_UK &amp;lt;- mean(Data_df_Resettled$Nesting.Height, na.rm = TRUE)

NH_Sib &amp;lt; NH_UK
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Yes, it does!&lt;/p&gt;
&lt;h3 id=&#34;competition-1&#34;&gt;Competition&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Do home ranges of Passer domesticus depend on climate?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Earlier in this practical, we have shown that home ranges of flocks of &lt;em&gt;Passer domesticus&lt;/em&gt; are affected by the climate conditions they are experiencing. Let&amp;rsquo;s see if our relocated sparrows have altered their behaviour:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;with(Data_df[which(Data_df$Index == &amp;quot;SI&amp;quot;),], 
     wilcox.test(x = as.numeric(factor(Home.Range)), 
                 y = as.numeric(factor(Data_df_Resettled$Home.Range)), 
                 paired = TRUE))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Wilcoxon signed rank test with continuity correction
## 
## data:  as.numeric(factor(Home.Range)) and as.numeric(factor(Data_df_Resettled$Home.Range))
## V = 348.5, p-value = 0.002891
## alternative hypothesis: true location shift is not equal to 0
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.002890788
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, they haven&amp;rsquo;t! Given the p-value of 7.4040145\times 10^{-5}, we &lt;strong&gt;accept the null hypothesis&lt;/strong&gt; and conclude that home ranges of our flocks of sparrows have not changed significantly after the relocation to the UK.&lt;/p&gt;
&lt;p&gt;Due to our earlier analysis, we would expect smaller home ranges of sparrows in the UK when compared to their previous home ranges in Siberia. Before testing this, remember that, when converted to numeric records, low values indicate larger home ranges:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;HR_Sib &amp;lt;- as.numeric(Data_df$Home.Range[which(Data_df$Index == &amp;quot;SI&amp;quot;)])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: NAs introduced by coercion
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;HR_UK &amp;lt;- as.numeric(Data_df_Resettled$Home.Range)

median(HR_Sib, na.rm = TRUE) &amp;lt; median(HR_UK, na.rm = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] NA
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We were right! The assignment of home ranges did shift to accommodate smaller home ranges in the coastal climate of the UK it is just not intense enough for statistical significance - this will be further evaluated in our next seminar.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ordinal &amp; Metric Tests (Two-Sample Situations)</title>
      <link>https://www.erikkusch.com/courses/biostat101/ordinal-metric-tests-two-sample-situations/</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/biostat101/ordinal-metric-tests-two-sample-situations/</guid>
      <description>&lt;h2 id=&#34;theory&#34;&gt;Theory&lt;/h2&gt;
&lt;p&gt;Welcome to our fourth practical experience in R. Throughout the following notes, I will introduce you to a couple statistical approaches for metric or ordinal data when wanting to compare two samples/populations that might be useful to you and are, to varying degrees, often used in biology. To do so, I will enlist the sparrow data set we handled in our first exercise. I have prepared some slides for this session: &lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/biostat101/10---Ordinal-and-Metric-Test--Two-Sample-_Handout.html&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://www.erikkusch.com/courses/biostat101/10---BioStat101_featured.png&#34; width=&#34;900&#34; margin-top = &#34;0&#34;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;p&gt;Find the data for this exercise &lt;a href=&#34;https://github.com/ErikKusch/Homepage/raw/master/content/courses/biostat101/Data/2b%20-%20Sparrow_ResettledSIUK_READY.rds&#34; target=&#34;_blank&#34;&gt; here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;preparing-our-procedure&#34;&gt;Preparing Our Procedure&lt;/h2&gt;
&lt;p&gt;To ensure others can reproduce our analysis we run the following three lines of code at the beginning of our &lt;code&gt;R&lt;/code&gt; coding file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rm(list=ls()) # clearing environment
Dir.Base &amp;lt;- getwd() # soft-coding our working directory
Dir.Data &amp;lt;- paste(Dir.Base, &amp;quot;Data&amp;quot;, sep=&amp;quot;/&amp;quot;) # soft-coding our data directory 
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;packages&#34;&gt;Packages&lt;/h3&gt;
&lt;p&gt;Using the following, user-defined function, we install/load all the necessary packages into our current &lt;code&gt;R&lt;/code&gt; session.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# function to load packages and install them if they haven&#39;t been installed yet
install.load.package &amp;lt;- function(x) {
  if (!require(x, character.only = TRUE))
    install.packages(x)
  require(x, character.only = TRUE)
}
package_vec &amp;lt;- c()
sapply(package_vec, install.load.package)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## list()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, we don&amp;rsquo;t need any packages for our analyses in this practical.&lt;/p&gt;
&lt;h3 id=&#34;loading-data&#34;&gt;Loading Data&lt;/h3&gt;
&lt;p&gt;During our first exercise (Data Mining and Data Handling - Fixing The Sparrow Data Set) we saved our clean data set as an RDS file. To load this, we use the &lt;code&gt;readRDS()&lt;/code&gt; command that comes with base &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df_base &amp;lt;- readRDS(file = paste(Dir.Data, &amp;quot;/1 - Sparrow_Data_READY.rds&amp;quot;, sep=&amp;quot;&amp;quot;))
Data_df &amp;lt;- Data_df_base # duplicate and save initial data on a new object
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;mann-whitney-u-test&#34;&gt;Mann-Whitney U Test&lt;/h2&gt;
&lt;p&gt;We can analyse the significance of two population/sample medians of metric variables which are independent of one another using the &lt;code&gt;wilcox.test()&lt;/code&gt; function in base &lt;code&gt;R&lt;/code&gt; whilst specifying &lt;code&gt;paired = FALSE&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;climate-warmingextremes&#34;&gt;Climate Warming/Extremes&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Does morphology of Passer domesticus depend on climate?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Logically, we&amp;rsquo;d expect morphological aspects of &lt;em&gt;Passer domesticus&lt;/em&gt; to change given different frequencies and severities of climate extremes. Don&amp;rsquo;t forget, however, that our statistical procedures are usually built on the null hypothesis of no differences or correlations being present and so is the Mann-Whitney U Test.&lt;/p&gt;
&lt;p&gt;Our data set recorded three aspects of sparrow morphology and three climate levels (). Remember, however, that we set aside four stations (Siberia, United Kingdom, Reunion and Australia) to test climate effects on which are strictly limited to continental and coastal climate types. We need to exclude all other sites records from our data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# prepare climate type testing data
Data_df &amp;lt;- Data_df_base
Index &amp;lt;- Data_df$Index
Rows &amp;lt;- which(Index == &amp;quot;SI&amp;quot; | Index == &amp;quot;UK&amp;quot; | Index == &amp;quot;RE&amp;quot; | Index == &amp;quot;AU&amp;quot; )
Data_df &amp;lt;- Data_df[Rows,]
Data_df$Climate &amp;lt;- droplevels(factor(Data_df$Climate))
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;sparrow-weight&#34;&gt;Sparrow Weight&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s start with the weight of &lt;em&gt;Passer domesticus&lt;/em&gt; individuals as grouped by the climate type present at the site weights have been recorded at:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Analysis
with(Data_df, 
     wilcox.test(x = Weight[Climate == &amp;quot;Continental&amp;quot;], 
                 y = Weight[Climate != &amp;quot;Continental&amp;quot;], 
                 paired = FALSE)
     )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Wilcoxon rank sum test with continuity correction
## 
## data:  Weight[Climate == &amp;quot;Continental&amp;quot;] and Weight[Climate != &amp;quot;Continental&amp;quot;]
## W = 22104, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true location shift is not equal to 0
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Direction of effect
median(Data_df$Weight[which(Data_df$Climate == &amp;quot;Continental&amp;quot;)], na.rm = TRUE) &amp;gt; 
  median(Data_df$Weight[which(Data_df$Climate != &amp;quot;Continental&amp;quot;)], na.rm = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Quite obviously, the weight of sparrows seems to be dependent on the type of climate the individuals are suspected to (p = $1.7179744\times 10^{-32}$) and &lt;strong&gt;reject the null hypothesis&lt;/strong&gt;. Weights of sparrows in continental climates are, on average, heavier than respective weights of their peers in coastal climates.&lt;/p&gt;
&lt;h4 id=&#34;sparrow-height&#34;&gt;Sparrow Height&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Analysis
with(Data_df, 
     wilcox.test(x = Height[Climate == &amp;quot;Continental&amp;quot;], 
                 y = Height[Climate != &amp;quot;Continental&amp;quot;], 
                 paired = FALSE)
     )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Wilcoxon rank sum test with continuity correction
## 
## data:  Height[Climate == &amp;quot;Continental&amp;quot;] and Height[Climate != &amp;quot;Continental&amp;quot;]
## W = 11498, p-value = 0.1971
## alternative hypothesis: true location shift is not equal to 0
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Direction of effect
median(Data_df$Height[which(Data_df$Climate == &amp;quot;Continental&amp;quot;)], na.rm = TRUE) &amp;gt; 
  median(Data_df$Height[which(Data_df$Climate != &amp;quot;Continental&amp;quot;)], na.rm = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We conclude that the height of sparrows seems to not be dependent on the type of climate the individuals are suspected to (p = $0.1970959$) and &lt;strong&gt;accept the null hypothesis&lt;/strong&gt;. Sparrows in continental climates are, on average, smaller than their peers in coastal climates but not to a statistically significant degree.&lt;/p&gt;
&lt;h4 id=&#34;sparrow-wing-chord&#34;&gt;Sparrow Wing Chord&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Analysis
with(Data_df, 
     wilcox.test(x = Wing.Chord[Climate == &amp;quot;Continental&amp;quot;], 
                 y = Wing.Chord[Climate != &amp;quot;Continental&amp;quot;], 
                 paired = FALSE)
     )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Wilcoxon rank sum test with continuity correction
## 
## data:  Wing.Chord[Climate == &amp;quot;Continental&amp;quot;] and Wing.Chord[Climate != &amp;quot;Continental&amp;quot;]
## W = 10505, p-value = 0.01213
## alternative hypothesis: true location shift is not equal to 0
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Direction of effect
median(Data_df$Wing.Chord[which(Data_df$Climate == &amp;quot;Continental&amp;quot;)], na.rm = TRUE) &amp;gt; 
  median(Data_df$Wing.Chord[which(Data_df$Climate != &amp;quot;Continental&amp;quot;)], na.rm = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Apparently, the wing chord of sparrows seems to be dependent on the type of climate the individuals are suspected to (p = $0.0121264$) and &lt;strong&gt;reject the null hypothesis&lt;/strong&gt;. Sparrows in continental climates have, generally speaking, shorter wings than their peers in coastal climates.&lt;/p&gt;
&lt;h3 id=&#34;predation&#34;&gt;Predation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Does nesting height of nest sites of Passer domesticus depend on predator characteristics?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In our second practical (Nominal Tests), we used a Chi-Squared approach in a two-sample situation to identify whether predator presence and type had any influence over the nesting sites that individuals of &lt;em&gt;Passer domesticus&lt;/em&gt; preferred. Our findings showed that they did and so we should expect similar results here when using &lt;code&gt;Nesting Site&lt;/code&gt; as our response variable instead of &lt;code&gt;Nesting Height&lt;/code&gt; as these two are highly related to each other.&lt;/p&gt;
&lt;p&gt;Additionally, to save some space in these notes, I am not showing how to identify the direction of the effect via code any more for now. We may wish to use the entirety of our data set again for this purpose:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df &amp;lt;- Data_df_base
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;predator-presence&#34;&gt;Predator Presence&lt;/h4&gt;
&lt;p&gt;First, we start with a possible link to predator presence and nesting height chosen by common house sparrows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;with(Data_df, 
     wilcox.test(x = Nesting.Height[Predator.Presence == &amp;quot;Yes&amp;quot;], 
                 y = Nesting.Height[Predator.Presence != &amp;quot;Yes&amp;quot;], 
                 paired = FALSE)
     )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Wilcoxon rank sum test with continuity correction
## 
## data:  Nesting.Height[Predator.Presence == &amp;quot;Yes&amp;quot;] and Nesting.Height[Predator.Presence != &amp;quot;Yes&amp;quot;]
## W = 25420, p-value = 0.0007678
## alternative hypothesis: true location shift is not equal to 0
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The nesting height of sparrows depends on whether a predator is present or not (p = $7.6777684\times 10^{-4}$) thus &lt;strong&gt;rejecting the null hypothesis&lt;/strong&gt;. Sparrows tend to go for nesting sites in more elevated positions when no predator is present.&lt;/p&gt;
&lt;h4 id=&#34;predator-type&#34;&gt;Predator Type&lt;/h4&gt;
&lt;p&gt;Again, we might want to check whether the position of a given nest might be related to what kind of predator is present:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;with(Data_df, 
     wilcox.test(x = Nesting.Height[Predator.Type == &amp;quot;Avian&amp;quot;], 
                 y = Nesting.Height[Predator.Type != &amp;quot;Avian&amp;quot;], 
                 paired = FALSE)
     )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Wilcoxon rank sum test with continuity correction
## 
## data:  Nesting.Height[Predator.Type == &amp;quot;Avian&amp;quot;] and Nesting.Height[Predator.Type != &amp;quot;Avian&amp;quot;]
## W = 5228, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true location shift is not equal to 0
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We conclude that the nesting height of sparrows depends on what kind of predator is present (p = $7.4380407\times 10^{-19}$) conclusively &lt;strong&gt;rejecting the null hypothesis&lt;/strong&gt;. Therefore, we are confident in stating that sparrows tend to go for nesting sites in more elevated positions when non-avian predators are present.&lt;/p&gt;
&lt;h3 id=&#34;competition&#34;&gt;Competition&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Do home ranges of Passer domesticus depend on climate?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We might expect different behaviour of &lt;em&gt;Passer domesticus&lt;/em&gt; given different climate types. Since &lt;code&gt;Home Range&lt;/code&gt; is on an ordinal scale () we can run a Mann-Whitney U Test on these whilst taking &lt;code&gt;Climate&lt;/code&gt; into account as our predictor variable.&lt;/p&gt;
&lt;p&gt;Take note that we need to limit our analysis to our climate type testing sites again as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# prepare climate type testing data
Data_df &amp;lt;- Data_df_base
Index &amp;lt;- Data_df$Index
Rows &amp;lt;- which(Index == &amp;quot;SI&amp;quot; | Index == &amp;quot;UK&amp;quot; | Index == &amp;quot;RE&amp;quot; | Index == &amp;quot;AU&amp;quot; )
Data_df &amp;lt;- Data_df[Rows,]
Data_df$Climate &amp;lt;- droplevels(factor(Data_df$Climate))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To be able to use the &lt;code&gt;wilcox.test()&lt;/code&gt; function on &lt;code&gt;Home Range&lt;/code&gt;, we need to transform its elements into a numeric type. Luckily, this is as easy as using the &lt;code&gt;as.numeric()&lt;/code&gt; function on the data since it will assign every factor level a number corresponding to its position in &lt;code&gt;levels()&lt;/code&gt; as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;levels(factor(Data_df$Home.Range))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Large&amp;quot;  &amp;quot;Medium&amp;quot; &amp;quot;Small&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;HR_vec &amp;lt;- as.numeric(factor((Data_df$Home.Range)))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, the levels of &lt;code&gt;Home.Range&lt;/code&gt; are ordered alphabetically. The &lt;code&gt;as.numeric()&lt;/code&gt; command will thus transform every record of &lt;code&gt;&amp;quot;Large&amp;quot;&lt;/code&gt; into &lt;code&gt;1&lt;/code&gt;, every record of &lt;code&gt;&amp;quot;Medium&amp;quot;&lt;/code&gt; into &lt;code&gt;2&lt;/code&gt; and every record of &lt;code&gt;&amp;quot;Small&amp;quot;&lt;/code&gt; into &lt;code&gt;3&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We are ready to run the analysis:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Analysis
with(Data_df, 
     wilcox.test(x = HR_vec[Climate == &amp;quot;Continental&amp;quot;], 
                 y = HR_vec[Climate != &amp;quot;Continental&amp;quot;], 
                 paired = FALSE)
     )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Wilcoxon rank sum test with continuity correction
## 
## data:  HR_vec[Climate == &amp;quot;Continental&amp;quot;] and HR_vec[Climate != &amp;quot;Continental&amp;quot;]
## W = 11897, p-value = 0.3666
## alternative hypothesis: true location shift is not equal to 0
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Direction of effect
median(HR_vec[which(Data_df$Climate == &amp;quot;Continental&amp;quot;)], na.rm = TRUE) &amp;gt; 
  median(HR_vec[which(Data_df$Climate != &amp;quot;Continental&amp;quot;)], na.rm = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;According to the output of our analysis, the home ranges of &lt;em&gt;Passer domesticus&lt;/em&gt; do not depend on the climate characteristics of their respective habitats (p = $0.2766088$). Thus, we &lt;strong&gt;accept the null hypothesis&lt;/strong&gt;.&lt;br&gt;
As you can see, the median of numeric home ranges is smaller in continental climates (just not statistically significant). Remember that small numeric ranges mean large actual ranges in this set-up and so we can conclude that  climates force common house sparrows to adapt to bigger home ranges.&lt;/p&gt;
&lt;h3 id=&#34;sexual-dimorphism&#34;&gt;Sexual Dimorphism&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Does morphology of Passer domesticus depend on sex?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If we assume a sexual dimorphism to have manifested itself in &lt;em&gt;Passer domesticus&lt;/em&gt; over evolutionary time, we&amp;rsquo;d expect different morphological features of males and females. In our second practical (Nominal Tests) we already assessed this using a Chi-Squared approach in a two-sample situation on colour morphs of the common house sparrows. At the time, we concluded that colouring is not equal for the sexes. So what about characteristics of our sparrows we can put into a meaningful order?&lt;/p&gt;
&lt;p&gt;We may wish to use the entirety of our data set again for this purpose:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df &amp;lt;- Data_df_base
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;sparrow-weight-1&#34;&gt;Sparrow Weight&lt;/h4&gt;
&lt;p&gt;We start by assessing the median weight of sparrows again, as driven by their sexes:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Analysis
with(Data_df, 
     wilcox.test(x = Weight[Sex == &amp;quot;Male&amp;quot;], 
                 y = Weight[Sex != &amp;quot;Male&amp;quot;], 
                 paired = FALSE)
     )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Wilcoxon rank sum test with continuity correction
## 
## data:  Weight[Sex == &amp;quot;Male&amp;quot;] and Weight[Sex != &amp;quot;Male&amp;quot;]
## W = 187772, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true location shift is not equal to 0
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Direction of effect
median(Data_df$Weight[which(Data_df$Sex == &amp;quot;Male&amp;quot;)], na.rm = TRUE) &amp;gt; 
  median(Data_df$Weight[which(Data_df$Sex != &amp;quot;Male&amp;quot;)], na.rm = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We already identified &lt;code&gt;Climate&lt;/code&gt; to be a major driver of median sparrow weight within this practical. Now, we need to add &lt;code&gt;Sex&lt;/code&gt; to the list of drivers of sparrow weight (p = $8.2580833\times 10^{-20}$) and &lt;strong&gt;reject the null hypothesis&lt;/strong&gt; that sparrow weights do not differ according to Sex.  Males tend to be heavier than females.&lt;/p&gt;
&lt;h4 id=&#34;sparrow-height-1&#34;&gt;Sparrow Height&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s move on and see if the height/length of our observed sparrows are dependent on their sexes:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;with(Data_df, 
     wilcox.test(x = Height[Sex == &amp;quot;Male&amp;quot;], 
                 y = Height[Sex != &amp;quot;Male&amp;quot;], 
                 paired = FALSE)
     )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Wilcoxon rank sum test with continuity correction
## 
## data:  Height[Sex == &amp;quot;Male&amp;quot;] and Height[Sex != &amp;quot;Male&amp;quot;]
## W = 141956, p-value = 0.9525
## alternative hypothesis: true location shift is not equal to 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We already identified &lt;code&gt;Climate&lt;/code&gt; to be a major driver of median sparrow height within this practical. Sparrow &lt;code&gt;Sex&lt;/code&gt; does not seem to be an informative characteristic when trying to understand sparrow heights (p = $0.9524599$). So we &lt;strong&gt;accept the null hypothesis&lt;/strong&gt; and don&amp;rsquo;t identify any direction of effects since there is no effect.&lt;/p&gt;
&lt;h4 id=&#34;sparrow-wing-chord-1&#34;&gt;Sparrow Wing Chord&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;with(Data_df, 
     wilcox.test(x = Wing.Chord[Sex == &amp;quot;Male&amp;quot;], 
                 y = Wing.Chord[Sex != &amp;quot;Male&amp;quot;], 
                 paired = FALSE)
     )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Wilcoxon rank sum test with continuity correction
## 
## data:  Wing.Chord[Sex == &amp;quot;Male&amp;quot;] and Wing.Chord[Sex != &amp;quot;Male&amp;quot;]
## W = 141636, p-value = 0.9021
## alternative hypothesis: true location shift is not equal to 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;According to our previous analysis within this practical, &lt;code&gt;Climate&lt;/code&gt; has been determined to be a major driver wing chords of common house sparrows. With our current analysis in mind, we can conclude that the &lt;code&gt;Sex&lt;/code&gt; of any given &lt;em&gt;Passer domesticus&lt;/em&gt; individual does not influence the wing chord of said individual (p = $0.9020933$). Therefore we &lt;strong&gt;accept the null hypothesis&lt;/strong&gt; and don&amp;rsquo;t identify any direction of effects since there is no effect.&lt;/p&gt;
&lt;h2 id=&#34;wilcoxon-signed-rank-test&#34;&gt;Wilcoxon Signed Rank Test&lt;/h2&gt;
&lt;p&gt;We can analyse the significance of two population/sample medians of metric variables which are dependent of one another using the &lt;code&gt;wilcox.test()&lt;/code&gt; function in base &lt;code&gt;R&lt;/code&gt; whilst specifying &lt;code&gt;paired = TRUE&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;preparing-data&#34;&gt;Preparing Data&lt;/h3&gt;
&lt;p&gt;Obviously, none of our data records are paired as such. Whilst one may want to make the argument that many characteristics of individuals that group together might be dependant on the expressions of themselves found throughout said group, we will not concentrate on this possibility within these practicals.&lt;/p&gt;
&lt;p&gt;Conclusively, we need an &lt;strong&gt;additional data set with truly paired records&lt;/strong&gt; of sparrows. Within our study set-up, think of a &lt;strong&gt;resettling experiment&lt;/strong&gt;, were you take &lt;em&gt;Passer domesticus&lt;/em&gt; individuals from one site, transfer them to another and check back with them after some time has passed to see whether some of their characteristics have changed in their expression.&lt;br&gt;
To this end, presume we have taken the entire &lt;em&gt;Passer domesticus&lt;/em&gt; population found at our &lt;strong&gt;Siberian&lt;/strong&gt; research station and moved them to the &lt;strong&gt;United Kingdom&lt;/strong&gt;. Whilst this keeps the latitude stable, the sparrows &lt;em&gt;now experience a coastal climate instead of a continental one&lt;/em&gt;. After some time (let&amp;rsquo;s say: a year), we have come back and recorded all the characteristics for the same individuals again.&lt;/p&gt;
&lt;p&gt;You will find the corresponding &lt;em&gt;new data&lt;/em&gt; in &lt;code&gt;2b - Sparrow_ResettledSIUK_READY.rds&lt;/code&gt;. Take note that this set only contains records for the transferred individuals in the &lt;strong&gt;same order&lt;/strong&gt; as in the old data set.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df_Resettled &amp;lt;- readRDS(file = paste(Dir.Data, &amp;quot;/2b - Sparrow_ResettledSIUK_READY.rds&amp;quot;, sep=&amp;quot;&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With these data records, we can now re-evaluate how the characteristics of sparrows can change when subjected to different conditions than previously thus shedding light on their &lt;strong&gt;plasticity&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;climate-warmingextremes-1&#34;&gt;Climate Warming/Extremes&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Does morphology of Passer domesticus depend on climate?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We have already concluded that the overall morphological aspects of populations of &lt;em&gt;Passer domesticus&lt;/em&gt; are shaped by climate, but what happens if we take birds from one climate and resettle them to another climate?&lt;/p&gt;
&lt;h4 id=&#34;sparrow-weight-2&#34;&gt;Sparrow Weight&lt;/h4&gt;
&lt;p&gt;First, let&amp;rsquo;s see how the average weight of our individual sparrows changed a year after they were relocated from Siberia to the UK:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;with(Data_df[which(Data_df$Index == &amp;quot;SI&amp;quot;),], 
     wilcox.test(x = Weight, y = Data_df_Resettled$Weight, paired = TRUE))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Wilcoxon signed rank test with continuity correction
## 
## data:  Weight and Data_df_Resettled$Weight
## V = 2044, p-value = 2.073e-09
## alternative hypothesis: true location shift is not equal to 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Apparently, the weight of the individual sparrows have significantly changed following their relocation (p = $2.0725016\times 10^{-9}$) and we &lt;strong&gt;reject the null hypothesis&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Earlier, we identified sparrows to be heavier in continental climates when compared to coastal climates - does this sentiment hold true with relocated birds?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Direction of effect
median(Data_df$Weight[which(Data_df$Index == &amp;quot;SI&amp;quot;)], na.rm = TRUE) &amp;gt; 
  median(Data_df_Resettled$Weight, na.rm = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Yes, it does. The resettled birds have reduced their median weight (probably not a conscious decision on behalf of the sparrows).&lt;/p&gt;
&lt;h4 id=&#34;sparrow-height-2&#34;&gt;Sparrow Height&lt;/h4&gt;
&lt;p&gt;Secondly, have our relocated sparrows become taller or shorter?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;with(Data_df[which(Data_df$Index == &amp;quot;SI&amp;quot;),], 
     wilcox.test(x = Height, y = Data_df_Resettled$Height, paired = TRUE))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in wilcox.test.default(x = Height, y = Data_df_Resettled$Height, :
## cannot compute exact p-value with zeroes
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Wilcoxon signed rank test with continuity correction
## 
## data:  Height and Data_df_Resettled$Height
## V = 0, p-value = NA
## alternative hypothesis: true location shift is not equal to 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Interestingly enough, we do not receive either meaningful W (&lt;code&gt;V&lt;/code&gt;) statistic nor an informative p-value (&lt;code&gt;NA&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;This could only be due to one reason:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;unique(Data_df$Height[which(Data_df$Index == &amp;quot;SI&amp;quot;)] == Data_df_Resettled$Height)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our sparrows have not become any shorter or taller! In fact, no height/length record has changed for any of the sparrows we relocated. This may usually be indicative of a data handling error but, in this case, makes a lot of sense when considering how difficult it may be for mature individuals to change in size.&lt;/p&gt;
&lt;h4 id=&#34;sparrow-wing-chord-2&#34;&gt;Sparrow Wing Chord&lt;/h4&gt;
&lt;p&gt;Third, let&amp;rsquo;s check whether wing chords have changed across the board. We can expect them to behave just like height records did:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;with(Data_df[which(Data_df$Index == &amp;quot;SI&amp;quot;),], 
     wilcox.test(x = Wing.Chord, y = Data_df_Resettled$Wing.Chord, paired = TRUE))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in wilcox.test.default(x = Wing.Chord, y =
## Data_df_Resettled$Wing.Chord, : cannot compute exact p-value with zeroes
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Wilcoxon signed rank test with continuity correction
## 
## data:  Wing.Chord and Data_df_Resettled$Wing.Chord
## V = 0, p-value = NA
## alternative hypothesis: true location shift is not equal to 0
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;unique(Data_df$Wing.Chord[which(Data_df$Index == &amp;quot;SI&amp;quot;)] == Data_df_Resettled$Wing.Chord)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, none of the wing chord records have changed.&lt;/p&gt;
&lt;h3 id=&#34;predation-1&#34;&gt;Predation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Does nesting height of nest sites of Passer domesticus depend on predator characteristics?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We have already identified predator characteristics at our sites to be influential in the overall nesting site and height of &lt;em&gt;Passer domesticus&lt;/em&gt;. Does this trend hold true when considering a relocation experiment?&lt;/p&gt;
&lt;p&gt;Firstly, we will test whether nesting heights have changed after the relocation. Before we do so, we should first check whether we&amp;rsquo;d expect a change based on whether &lt;strong&gt;predator presence&lt;/strong&gt; is different between Siberia and the UK:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;PP_Sib &amp;lt;- unique(Data_df$Predator.Presence[which(Data_df$Index == &amp;quot;SI&amp;quot;)])
PP_UK &amp;lt;- unique(Data_df_Resettled$Predator.Presence)

PP_Sib == PP_UK
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Apparently, predators are present at both of these sites and so we would not expect a significant change in nesting height. Let&amp;rsquo;s check this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;with(Data_df[which(Data_df$Index == &amp;quot;SI&amp;quot;),], 
     wilcox.test(x = Nesting.Height, 
                 y = Data_df_Resettled$Nesting.Height, 
                 paired = TRUE))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Wilcoxon signed rank exact test
## 
## data:  Nesting.Height and Data_df_Resettled$Nesting.Height
## V = 65, p-value = 7.404e-05
## alternative hypothesis: true location shift is not equal to 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There actually is an effect after the resettling! Therefore, we have to &lt;strong&gt;reject the null hypothesis&lt;/strong&gt; (p = $7.4040145\times 10^{-5}$)! How can this be?&lt;/p&gt;
&lt;p&gt;Maybe it has to do with the kind of predator at each site:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;unique(Data_df$Predator.Type[which(Data_df$Index == &amp;quot;SI&amp;quot;)])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] Avian
## Levels: Avian Non-Avian
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;unique(Data_df_Resettled$Predator.Type)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] Non-Avian
## Levels: Avian Non-Avian
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, Siberian sparrows are subject to avian predation whilst the sparrow populations that we monitored in the UK are experiencing non-avian predator presence. A &lt;strong&gt;causal link between nesting height and predator type&lt;/strong&gt; seems to be logical!&lt;/p&gt;
&lt;p&gt;Which direction is the effect headed? Earlier within this practical, we hypothesized that avian predation forces lower nesting heights in &lt;em&gt;Passer domesticus&lt;/em&gt; - does this hold true?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;NH_Sib &amp;lt;- mean(Data_df$Nesting.Height[which(Data_df$Index == &amp;quot;SI&amp;quot;)], na.rm = TRUE) 
NH_UK &amp;lt;- mean(Data_df_Resettled$Nesting.Height, na.rm = TRUE)

NH_Sib &amp;lt; NH_UK
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Yes, it does!&lt;/p&gt;
&lt;h3 id=&#34;competition-1&#34;&gt;Competition&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Do home ranges of Passer domesticus depend on climate?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Earlier in this practical, we have shown that home ranges of flocks of &lt;em&gt;Passer domesticus&lt;/em&gt; are affected by the climate conditions they are experiencing. Let&amp;rsquo;s see if our relocated sparrows have altered their behaviour:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;with(Data_df[which(Data_df$Index == &amp;quot;SI&amp;quot;),], 
     wilcox.test(x = as.numeric(factor(Home.Range)), 
                 y = as.numeric(factor(Data_df_Resettled$Home.Range)), 
                 paired = TRUE))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Wilcoxon signed rank test with continuity correction
## 
## data:  as.numeric(factor(Home.Range)) and as.numeric(factor(Data_df_Resettled$Home.Range))
## V = 348.5, p-value = 0.002891
## alternative hypothesis: true location shift is not equal to 0
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.002890788
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, they haven&amp;rsquo;t! Given the p-value of 7.4040145\times 10^{-5}, we &lt;strong&gt;accept the null hypothesis&lt;/strong&gt; and conclude that home ranges of our flocks of sparrows have not changed significantly after the relocation to the UK.&lt;/p&gt;
&lt;p&gt;Due to our earlier analysis, we would expect smaller home ranges of sparrows in the UK when compared to their previous home ranges in Siberia. Before testing this, remember that, when converted to numeric records, low values indicate larger home ranges:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;HR_Sib &amp;lt;- as.numeric(Data_df$Home.Range[which(Data_df$Index == &amp;quot;SI&amp;quot;)])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: NAs introduced by coercion
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;HR_UK &amp;lt;- as.numeric(Data_df_Resettled$Home.Range)

median(HR_Sib, na.rm = TRUE) &amp;lt; median(HR_UK, na.rm = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] NA
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We were right! The assignment of home ranges did shift to accommodate smaller home ranges in the coastal climate of the UK it is just not intense enough for statistical significance - this will be further evaluated in our next seminar.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 10 &amp; 11</title>
      <link>https://www.erikkusch.com/courses/rethinking/chapter-11/</link>
      <pubDate>Thu, 18 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/rethinking/chapter-11/</guid>
      <description>&lt;h1 id=&#34;god-spiked-the-integers&#34;&gt;God Spiked The Integers&lt;/h1&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/rethinking/11__05-03-2021_SUMMARY_-Generalised-Linear-Models.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides Chapter 10&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/rethinking/13__19-03-2021_SUMMARY_-Discrete-Outcomes.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides Chapter 11&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to the exercises at the end of chapter 11 in 
&lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Satistical Rethinking 2&lt;/a&gt; by Richard McElreath. For anyone reading through these in order and wondering why I skipped chapter 10: chapter 10 did not contain any exercises (to my dismay, as you can imagine). I have created these notes as a part of my ongoing involvement in the 
&lt;a href=&#34;https://www.erikkusch.com/project/aubayes/&#34;&gt;AU Bayes Study Group&lt;/a&gt;. Much of my inspiration for these solutions, where necessary, has been obtained from 
&lt;a href=&#34;https://github.com/jffist/statistical-rethinking-solutions/blob/master/ch10_hw.R&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Taras Svirskyi&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/cavaunpeu/statistical-rethinking/blob/master/chapter-10/homework.R&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;William Wolf&lt;/a&gt;, and 
&lt;a href=&#34;https://www.samples-of-thoughts.com/projects/statistical-rethinking/chapter_10/chp10-ex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Corrie Bartelheimer&lt;/a&gt; as well as the solutions provided to instructors by Richard McElreath himself.&lt;/p&gt;
&lt;h2 id=&#34;r-environment&#34;&gt;&lt;code&gt;R&lt;/code&gt; Environment&lt;/h2&gt;
&lt;p&gt;For today&amp;rsquo;s exercise, I load the following packages:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rethinking)
library(rstan)
library(ggplot2)
library(tidybayes)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;easy-exercises&#34;&gt;Easy Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-e1&#34;&gt;Practice E1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  If an event has probability 0.35, what are the log-odds of this event?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  When $p = 0.35$ then the log-odds are $log\frac{0.35}{1-0.35}$, or in &lt;code&gt;R&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;log(0.35 / (1 - 0.35))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.6190392
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-e2&#34;&gt;Practice E2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  If an event has log-odds 3.2, what is the probability of this event?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; To transform log-odds into probability space, we want to use the &lt;code&gt;inv_logit()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;inv_logit(3.2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9608343
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-e3&#34;&gt;Practice E3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Suppose that a coefficient in a logistic regression has value 1.7. What does this imply about the proportional change in odds of the outcome?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;exp(1.7)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5.473947
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each one-unit increase in the predictor linked to this coefficient results in a multiplication of the odds of the event occurring by 5.47.&lt;/p&gt;
&lt;p&gt;The linear model behind the logistic regression simply represents the log-odds of an event happening. The odds of the events happening can thus be shown as $exp(odds)$. Comparing how the odds change when increasing the predictor variable by one unit comes down to solving this equation then:&lt;/p&gt;
&lt;p&gt;$$exp(Î± + Î²x)Z = exp(Î± + Î²(x + 1))$$
Solving this for $z$ results in:&lt;/p&gt;
&lt;p&gt;$$z = exp(\beta)$$&lt;/p&gt;
&lt;p&gt;which is how we derived the answer to this question.&lt;/p&gt;
&lt;h3 id=&#34;practice-e4&#34;&gt;Practice E4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Why do Poisson regressions sometimes require the use of an offset? Provide an example.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; When study regimes aren&amp;rsquo;t rigidly standardised, we may end up with count data collected over different time/distance intervals. Comparing these data without accounting for the difference in the underlying sampling frequency will inevitably lead to horribly inadequate predictions of our model(s).&lt;/p&gt;
&lt;p&gt;As an example, think of how many ants leave a hive in a certain interval. If we recorded numbers of ants leaving to forage on a minute-by-minute basis, we would obtain much smaller counts than if our sampling regime dictated hourly observation periods. Any poisson model we want to run between differing sampling regimes has to account for the heterogeneity in the observation period lengths. We do so as follows:&lt;/p&gt;
&lt;p&gt;$$Ants_iâ¼Poisson(Î»)$$
$$log(Î»)=log(period_i)+Î±+Î²Hive_i$$&lt;/p&gt;
&lt;h2 id=&#34;medium-exercises&#34;&gt;Medium Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-m1&#34;&gt;Practice M1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  As explained in the chapter, binomial data can be organized in aggregated and disaggregated forms, without any impact on inference. But the likelihood of the data does change when the data are converted between the two formats. Can you explain why?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Think back to the 
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/rethinking/statistical-rethinking-chapter-02/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Card Drawing Example&lt;/a&gt; from chapter 2. We know a certain outcome. Let&amp;rsquo;s assume two black face, and one white face card are drawn.&lt;/p&gt;
&lt;p&gt;In the aggregated form of the data, we obtain the probability of our observation as $3p(1-p)$ (a binomial distribution with $3$ trials and a rate of black face cards of $p = \frac{2}{3}$). This tells us how many ways there are to get two black-face cards out of three pulls of cards. The order is irrelevant.&lt;/p&gt;
&lt;p&gt;With disaggregated data, we do not cope with any order, but simply predict the result of each draw of a card by itself and finally multiply our predictions together to form a joint probability according to $p(1-p)$.&lt;/p&gt;
&lt;p&gt;In conclusion, aggregated data is modelled with an extra constant to handle permutations. This does not change our inference, but merely changes the likelihood and log-likelihood.&lt;/p&gt;
&lt;h3 id=&#34;practice-m2&#34;&gt;Practice M2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  If a coefficient in a Poisson regression has value 1.7, what does this imply about the change in the outcome?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; A basic Poisson regression is expressed as such:
$$log(Î») = Î± + Î²x$$
$$Î» = exp(Î± + Î²x)$$&lt;/p&gt;
&lt;p&gt;In this specific case $\beta = 1.7$. So what happens to $\lambda$ when $x$ increases by $1$? To solve this, we write a formula for the change in $\lambda$:&lt;/p&gt;
&lt;p&gt;$$ÎÎ» = exp(Î± + Î²(x + 1)) â exp(Î± + Î²x)$$
$$ÎÎ» = exp(Î± + Î²x)(exp(Î²) â 1)$$&lt;/p&gt;
&lt;p&gt;Unfortunately, this is about as far as we can take solving this formula. The change in $\lambda$ depends on all contents of the model. But about the ratio of $\lambda$ following a one-unit increase in $x$ compared to $\lambda$ a t base-level? We can compute this ratio as:&lt;/p&gt;
&lt;p&gt;$$\frac{Î»_{x+1}}{Î»x} = \frac{exp(Î± + Î²(x + 1))}{exp(Î± + Î²x)} = exp(Î²)$$&lt;/p&gt;
&lt;p&gt;This is reminiscent of the proportional change in odds for logistic regressions. Conclusively, a coefficient of $\beta = 1.7$ in a Poisson model results in a proportional change in the expected count of exp(1.7) =  5.47 when the corresponding predictor variable increases by one unit.&lt;/p&gt;
&lt;h3 id=&#34;practice-m3&#34;&gt;Practice M3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Explain why the logit link is appropriate for a binomial generalized linear model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  With a binomial generalised linear model, we are interested in an outcome space between 0 and 1. With the outcome space denoting probabilities of an event transpiring. Our underlying linear model has no qualms about estimating parameter values outside of this interval. The logit link maps such probability space into $â$ (linear model space).&lt;/p&gt;
&lt;h3 id=&#34;practice-m4&#34;&gt;Practice M4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Explain why the log link is appropriate for a Poisson generalized linear model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Poisson generalised linear models are producing strictly non-negative outputs (negative counts are impossible). As such, the underlying linear model space needs to be matched to the outcome space which is strictly non-negative. The log function maps positive value onto $â$ and thus the function links count values (positive values) to a linear model.&lt;/p&gt;
&lt;h3 id=&#34;practice-m5&#34;&gt;Practice M5&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  What would it imply to use a logit link for the mean of a Poisson generalized linear model? Can you think of a real research problem for which this would make sense?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Using a logit link in a Poisson model implies that the mean of the Poisson distribution lies between 0 and 1:&lt;/p&gt;
&lt;p&gt;$$y_i â¼ Poisson(Î¼_i)$$
$$logit(Î¼_i) = Î± + Î²x_i$$
This would imply that there is at most one event per time interval. This might be the case for very rare or extremely cyclical events such as counting the number of El NiÃ±o events every four years or so.&lt;/p&gt;
&lt;h3 id=&#34;practice-m6&#34;&gt;Practice M6&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  State the constraints for which the binomial and Poisson distributions have maximum entropy. Are the constraints different at all for binomial and Poisson? Why or why not?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  For binomial and Poisson distributions to have maximum entropy, we need to meet the following assumptions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Discrete, binary outcomes&lt;/li&gt;
&lt;li&gt;Constant probability of event occurring across al trials (this is the same as a constant expected value)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Both distributions have the same constraints as Poisson is a simplified form of the binomial.&lt;/p&gt;
&lt;h2 id=&#34;hard-exercises&#34;&gt;Hard Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-h1&#34;&gt;Practice H1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Use &lt;code&gt;quap&lt;/code&gt; to construct a quadratic approximate posterior distribution for the chimpanzee model that includes a unique intercept for each actor, &lt;code&gt;m11.4&lt;/code&gt; (page 338). Compare the quadratic approximation to the posterior distribution produced instead from MCMC. Can you explain both the differences and the similarities between the approximate and the MCMC distributions?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Here are the models according to the book:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(chimpanzees)
d &amp;lt;- chimpanzees
d$treatment &amp;lt;- 1 + d$prosoc_left + 2 * d$condition
dat_list &amp;lt;- list(
  pulled_left = d$pulled_left,
  actor = d$actor,
  treatment = as.integer(d$treatment)
)
## MCMC model
m11.4 &amp;lt;- ulam(
  alist(
    pulled_left ~ dbinom(1, p),
    logit(p) &amp;lt;- a[actor] + b[treatment],
    a[actor] ~ dnorm(0, 1.5),
    b[treatment] ~ dnorm(0, 0.5)
  ),
  data = dat_list, chains = 4, log_lik = TRUE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &#39;80e2b6267e3dc4ff0c2916d0cf0879e8&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0.001 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 10 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 1: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 1: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 1: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 1: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 1.57 seconds (Warm-up)
## Chain 1:                1.629 seconds (Sampling)
## Chain 1:                3.199 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;80e2b6267e3dc4ff0c2916d0cf0879e8&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 0 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 2: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 2: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 2: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 2: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 2: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 2: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 2: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 2: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 2: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 2: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 2: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 1.667 seconds (Warm-up)
## Chain 2:                1.211 seconds (Sampling)
## Chain 2:                2.878 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &#39;80e2b6267e3dc4ff0c2916d0cf0879e8&#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 0 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 3: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 3: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 3: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 3: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 3: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 3: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 3: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 3: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 3: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 3: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 3: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 1.639 seconds (Warm-up)
## Chain 3:                2.718 seconds (Sampling)
## Chain 3:                4.357 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &#39;80e2b6267e3dc4ff0c2916d0cf0879e8&#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 0 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 4: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 4: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 4: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 4: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 4: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 4: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 4: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 4: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 4: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 4: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 4: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 2.031 seconds (Warm-up)
## Chain 4:                2.195 seconds (Sampling)
## Chain 4:                4.226 seconds (Total)
## Chain 4:
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m11.4, depth = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean        sd        5.5%       94.5%    n_eff    Rhat4
## a[1] -0.45075443 0.3331248 -0.98953991  0.06075382 698.8809 1.005361
## a[2]  3.91891754 0.8083257  2.74211339  5.24785152 901.5623 0.998913
## a[3] -0.75442240 0.3312761 -1.28541454 -0.24524706 699.9722 1.003839
## a[4] -0.76567679 0.3369675 -1.30127846 -0.23894487 857.8075 1.003696
## a[5] -0.44359742 0.3267862 -0.97201306  0.07594653 674.2582 1.005466
## a[6]  0.46443104 0.3317285 -0.05869988  0.99613901 810.7556 1.003976
## a[7]  1.96593450 0.4242551  1.31622570  2.65044970 899.8839 1.001162
## b[1] -0.03793442 0.2858597 -0.49368201  0.42539507 686.8362 1.005492
## b[2]  0.48704573 0.2866880  0.03296044  0.95020885 703.0115 1.004415
## b[3] -0.38805106 0.2815222 -0.82364579  0.06867557 639.3679 1.004886
## b[4]  0.37075102 0.2834843 -0.08070002  0.83024441 623.7355 1.006236
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Quap Model
m11.4quap &amp;lt;- quap(
  alist(
    pulled_left ~ dbinom(1, p),
    logit(p) &amp;lt;- a[actor] + b[treatment],
    a[actor] ~ dnorm(0, 1.5),
    b[treatment] ~ dnorm(0, 0.5)
  ),
  data = dat_list
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(coeftab(m11.4, m11.4quap),
  labels = paste(rep(rownames(coeftab(m11.4, m11.4quap)@coefs), each = 2),
    rep(c(&amp;quot;MCMC&amp;quot;, &amp;quot;quap&amp;quot;), nrow(coeftab(m11.4, m11.4quap)@coefs) * 2),
    sep = &amp;quot;-&amp;quot;
  )
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-18-statistical-rethinking-chapter-11_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looking at these parameter estimates, it is apparent that quadratic approximation is doing a good job in this case. The only noticeable difference lies with &lt;code&gt;a[2]&lt;/code&gt; which shows a higher estimate with the &lt;code&gt;ulam&lt;/code&gt; model. Let&amp;rsquo;s look at the densities of the estimates of this parameter:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;post &amp;lt;- extract.samples(m11.4)
postq &amp;lt;- extract.samples(m11.4quap)
dens(post$a[, 2], lwd = 2)
dens(postq$a[, 2], add = TRUE, lwd = 2, col = rangi2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-18-statistical-rethinking-chapter-11_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;ulam&lt;/code&gt; model (in black) placed more probability mass in the upper end of the tail which ends up pushing the mean of this posterior distribution further to the right when compared to that of the quadratic approximation model. This is because the quadratic approximation assumes the posterior distribution to be Gaussian thus producing a symmetric distribution with less probability mass in the upper tail.&lt;/p&gt;
&lt;h3 id=&#34;practice-h2&#34;&gt;Practice H2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Use &lt;code&gt;WAIC&lt;/code&gt; to compare the chimpanzee model that includes a unique intercept for each actor, &lt;code&gt;m11.4&lt;/code&gt; (page 338), to the simpler models fit in the same section.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  The models in question are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Intercept only&lt;/em&gt; model:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m11.1 &amp;lt;- quap(
  alist(
    pulled_left ~ dbinom(1, p),
    logit(p) &amp;lt;- a,
    a ~ dnorm(0, 10)
  ),
  data = d
)
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;em&gt;Intercept and Treatment&lt;/em&gt; model:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m11.3 &amp;lt;- quap(
  alist(
    pulled_left ~ dbinom(1, p),
    logit(p) &amp;lt;- a + b[treatment],
    a ~ dnorm(0, 1.5),
    b[treatment] ~ dnorm(0, 0.5)
  ),
  data = d
)
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;em&gt;Individual Intercept and Treatment&lt;/em&gt; model:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m11.4 &amp;lt;- ulam(
  alist(
    pulled_left ~ dbinom(1, p),
    logit(p) &amp;lt;- a[actor] + b[treatment],
    a[actor] ~ dnorm(0, 1.5),
    b[treatment] ~ dnorm(0, 0.5)
  ),
  data = dat_list, chains = 4, log_lik = TRUE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &#39;80e2b6267e3dc4ff0c2916d0cf0879e8&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 1: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 1: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 1: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 1: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.978 seconds (Warm-up)
## Chain 1:                0.775 seconds (Sampling)
## Chain 1:                1.753 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;80e2b6267e3dc4ff0c2916d0cf0879e8&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 0 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 2: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 2: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 2: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 2: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 2: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 2: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 2: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 2: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 2: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 2: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 2: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.865 seconds (Warm-up)
## Chain 2:                0.834 seconds (Sampling)
## Chain 2:                1.699 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &#39;80e2b6267e3dc4ff0c2916d0cf0879e8&#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 0 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 3: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 3: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 3: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 3: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 3: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 3: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 3: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 3: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 3: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 3: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 3: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.857 seconds (Warm-up)
## Chain 3:                0.612 seconds (Sampling)
## Chain 3:                1.469 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &#39;80e2b6267e3dc4ff0c2916d0cf0879e8&#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 0 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 4: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 4: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 4: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 4: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 4: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 4: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 4: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 4: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 4: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 4: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 4: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.919 seconds (Warm-up)
## Chain 4:                0.937 seconds (Sampling)
## Chain 4:                1.856 seconds (Total)
## Chain 4:
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To compare these, we can run:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(comp &amp;lt;- compare(m11.1, m11.3, m11.4))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           WAIC        SE    dWAIC      dSE    pWAIC       weight
## m11.4 532.4794 18.927161   0.0000       NA 8.572123 1.000000e+00
## m11.3 682.4152  8.973761 149.9358 18.37892 3.553310 2.765987e-33
## m11.1 687.9540  6.994012 155.4746 18.91781 1.004840 1.734267e-34
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(comp)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-18-statistical-rethinking-chapter-11_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This shows clearly that the model accounting for individual intercepts as well as treatment effects (&lt;code&gt;m11.4&lt;/code&gt;) outperforms the simpler models.&lt;/p&gt;
&lt;h3 id=&#34;practice-h3&#34;&gt;Practice H3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; The data contained in &lt;code&gt;library(MASS);data(eagles)&lt;/code&gt; are records of salmon pirating attempts by Bald Eagles in Washington State. See &lt;code&gt;?eagles&lt;/code&gt; for details. While one eagle feeds, sometimes another will swoop in and try to steal the salmon from it. Call the feeding eagle the âvictimâ and the thief the âpirate.â Use the available data to build a binomial GLM of successful pirating attempts.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(MASS)
data(eagles)
d &amp;lt;- eagles
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;part-a&#34;&gt;Part A&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Consider the following model:&lt;/p&gt;
&lt;p&gt;$$y_i â¼ Binomial(n_i, p_i)$$
$$log\frac{p_i}{1 â p_i} = Î± + Î²_PP_i + Î²_VV_i + Î²_AA_i $$
$$Î± â¼ Normal(0, 1.5)$$
$$Î²_P â¼ Normal(0, 0.5)$$ 
$$Î²_V â¼ Normal(0, 0.5)$$ 
$$Î²_A â¼ Normal(0, 0.5)$$
where $y$ is the number of successful attempts, $n$ is the total number of attempts, $P$ is a dummy variable indicating whether or not the pirate had large body size, $V$ is a dummy variable indicating whether or not the victim had large body size, and finally $A$ is a dummy variable indicating whether or not the pirate was an adult.&lt;/p&gt;
&lt;p&gt;Fit the model above to the eagles data, using both &lt;code&gt;quap&lt;/code&gt; and &lt;code&gt;ulam&lt;/code&gt;. Is the quadratic approximation okay?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; First, we have to make our dummy variables:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;d$pirateL &amp;lt;- ifelse(d$P == &amp;quot;L&amp;quot;, 1, 0)
d$victimL &amp;lt;- ifelse(d$V == &amp;quot;L&amp;quot;, 1, 0)
d$pirateA &amp;lt;- ifelse(d$A == &amp;quot;A&amp;quot;, 1, 0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Fitting the models is now trivial:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# define model list specification
f &amp;lt;- alist(
  y ~ dbinom(n, p),
  logit(p) &amp;lt;- a + bP * pirateL + bV * victimL + bA * pirateA,
  a ~ dnorm(0, 1.5),
  bP ~ dnorm(0, .5),
  bV ~ dnorm(0, .5),
  bA ~ dnorm(0, .5)
)
## quap model
mH3quap &amp;lt;- quap(f, data = d)
## ulam model
mH3ulam &amp;lt;- ulam(f, data = d, chains = 4, log_lik = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &#39;4eaf24dd51e5e9fce10e2cc7d32e0b01&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 1: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 1: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 1: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 1: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.07 seconds (Warm-up)
## Chain 1:                0.066 seconds (Sampling)
## Chain 1:                0.136 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;4eaf24dd51e5e9fce10e2cc7d32e0b01&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 0 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 2: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 2: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 2: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 2: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 2: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 2: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 2: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 2: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 2: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 2: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 2: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.08 seconds (Warm-up)
## Chain 2:                0.109 seconds (Sampling)
## Chain 2:                0.189 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &#39;4eaf24dd51e5e9fce10e2cc7d32e0b01&#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 0 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 3: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 3: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 3: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 3: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 3: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 3: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 3: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 3: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 3: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 3: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 3: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.114 seconds (Warm-up)
## Chain 3:                0.084 seconds (Sampling)
## Chain 3:                0.198 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &#39;4eaf24dd51e5e9fce10e2cc7d32e0b01&#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 0 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 4: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 4: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 4: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 4: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 4: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 4: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 4: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 4: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 4: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 4: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 4: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.101 seconds (Warm-up)
## Chain 4:                0.098 seconds (Sampling)
## Chain 4:                0.199 seconds (Total)
## Chain 4:
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, we visualise the parameter estimates&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(coeftab(mH3quap, mH3ulam),
  labels = paste(rep(rownames(coeftab(mH3quap, mH3ulam)@coefs), each = 2),
    rep(c(&amp;quot;MCMC&amp;quot;, &amp;quot;quap&amp;quot;), nrow(coeftab(mH3quap, mH3ulam)@coefs) * 2),
    sep = &amp;quot;-&amp;quot;
  )
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-18-statistical-rethinking-chapter-11_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;These are pretty similar looking to me.&lt;/p&gt;
&lt;h4 id=&#34;part-b&#34;&gt;Part B&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Now interpret the estimates. If the quadratic approximation turned out okay, then itâs okay
to use the &lt;code&gt;quap&lt;/code&gt; estimates. Otherwise stick to &lt;code&gt;ulam&lt;/code&gt; estimates. Then plot the posterior predictions. Compute and display both (1) the predicted &lt;strong&gt;probability&lt;/strong&gt; of success and its 89% interval for each row ($i$) in the data, as well as (2) the predicted success &lt;strong&gt;count&lt;/strong&gt; and its 89% interval. What different information does each type of posterior prediction provide?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Personally, I don&amp;rsquo;t think there&amp;rsquo;s much difference between the model estimates. Here, I am sticking to the &lt;code&gt;ulam&lt;/code&gt; model, because I feel like it. No other reason.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start by getting a baseline understanding of how often a non-adult, small-bodied pirate is able to fetch a salmon from a small-bodied victim(all dummy variables are at value 0) - this is our intercept &lt;code&gt;a&lt;/code&gt;. These are log-odds:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;post &amp;lt;- extract.samples(mH3ulam)
mean(logistic(post$a))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5695376
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We expect about 0.57% of all of our immature, small pirates to be successful when pirating on small victims.&lt;/p&gt;
&lt;p&gt;Now that we are armed with our baseline, we are ready to look at how our slope parameters affect what&amp;rsquo;s happening in our model.&lt;/p&gt;
&lt;p&gt;First, we start with the effect of pirate-body-size (&lt;code&gt;bP&lt;/code&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mean(logistic(post$a + post$bP))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8678798
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Damn. Large-bodied pirates win almost all of the time! We could repeat this for all slope parameters, but I find it prudent to move on to our actual task:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Probability&lt;/strong&gt; of success:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;d$psuccess &amp;lt;- d$y / d$n # successes divided by attempts
p &amp;lt;- link(mH3ulam) # success probability with inverse link
## Mean and Interval Calculation
p.mean &amp;lt;- apply(p, 2, mean)
p.PI &amp;lt;- apply(p, 2, PI)
# plot raw proportions success for each case
plot(d$psuccess,
  col = rangi2,
  ylab = &amp;quot;successful proportion&amp;quot;, xlab = &amp;quot;case&amp;quot;, xaxt = &amp;quot;n&amp;quot;,
  xlim = c(0.75, 8.25), pch = 16
)
# label cases on horizontal axis
axis(1,
  at = 1:8,
  labels = c(&amp;quot;LLA&amp;quot;, &amp;quot;LSA&amp;quot;, &amp;quot;LLI&amp;quot;, &amp;quot;LSI&amp;quot;, &amp;quot;SLA&amp;quot;, &amp;quot;SSA&amp;quot;, &amp;quot;SLI&amp;quot;, &amp;quot;SSI&amp;quot;) # same order as in data frame d
)
# display posterior predicted proportions successful
points(1:8, p.mean)
for (i in 1:8) lines(c(i, i), p.PI[, i])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-18-statistical-rethinking-chapter-11_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;Counts&lt;/strong&gt; of successes:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;y &amp;lt;- sim(mH3ulam) # simulate posterior for counts of successes
## Mean and Interval Calculation
y.mean &amp;lt;- apply(y, 2, mean)
y.PI &amp;lt;- apply(y, 2, PI)
# plot raw counts success for each case
plot(d$y,
  col = rangi2,
  ylab = &amp;quot;successful attempts&amp;quot;, xlab = &amp;quot;case&amp;quot;, xaxt = &amp;quot;n&amp;quot;,
  xlim = c(0.75, 8.25), pch = 16
)
# label cases on horizontal axis
axis(1,
  at = 1:8,
  labels = c(&amp;quot;LAL&amp;quot;, &amp;quot;LAS&amp;quot;, &amp;quot;LIL&amp;quot;, &amp;quot;LIS&amp;quot;, &amp;quot;SAL&amp;quot;, &amp;quot;SAS&amp;quot;, &amp;quot;SIL&amp;quot;, &amp;quot;SIS&amp;quot;)
)
# display posterior predicted successes
points(1:8, y.mean)
for (i in 1:8) lines(c(i, i), y.PI[, i])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-18-statistical-rethinking-chapter-11_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In conclusion, the probability plot makes the different settings of predictor variables more comparable because the number of piracy attempts are ignored in setting the y-axis. The count plot, however, shows the additional uncertainty stemming from the underlying sample size.&lt;/p&gt;
&lt;h4 id=&#34;part-c&#34;&gt;Part C&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Now try to improve the model. Consider an interaction between the pirateâs size and age(immature or adult). Compare this model to the previous one, using &lt;code&gt;WAIC&lt;/code&gt;. Interpret.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  Let&amp;rsquo;s fit a model with &lt;code&gt;ulam&lt;/code&gt; containing the interaction effect we were asked for:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mH3c &amp;lt;- ulam(
  alist(
    y ~ dbinom(n, p),
    logit(p) &amp;lt;- a + bP * pirateL + bV * victimL + bA * pirateA + bPA * pirateL * pirateA,
    a ~ dnorm(0, 1.5),
    bP ~ dnorm(0, .5),
    bV ~ dnorm(0, .5),
    bA ~ dnorm(0, .5),
    bPA ~ dnorm(0, .5)
  ),
  data = d, chains = 4, log_lik = TRUE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &#39;3f6607198507ea4881438baca721629d&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 1: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 1: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 1: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 1: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.162 seconds (Warm-up)
## Chain 1:                0.118 seconds (Sampling)
## Chain 1:                0.28 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;3f6607198507ea4881438baca721629d&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 0 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 2: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 2: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 2: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 2: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 2: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 2: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 2: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 2: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 2: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 2: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 2: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.142 seconds (Warm-up)
## Chain 2:                0.124 seconds (Sampling)
## Chain 2:                0.266 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &#39;3f6607198507ea4881438baca721629d&#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 0 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 3: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 3: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 3: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 3: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 3: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 3: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 3: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 3: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 3: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 3: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 3: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.141 seconds (Warm-up)
## Chain 3:                0.119 seconds (Sampling)
## Chain 3:                0.26 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &#39;3f6607198507ea4881438baca721629d&#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 0 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 4: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 4: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 4: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 4: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 4: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 4: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 4: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 4: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 4: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 4: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 4: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.107 seconds (Warm-up)
## Chain 4:                0.12 seconds (Sampling)
## Chain 4:                0.227 seconds (Total)
## Chain 4:
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;compare(mH3ulam, mH3c)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             WAIC       SE    dWAIC      dSE    pWAIC    weight
## mH3ulam 59.09875 11.34469 0.000000       NA 8.303613 0.6932173
## mH3c    60.72916 11.90457 1.630407 1.467142 9.165510 0.3067827
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is quite obviously a tie. So what about the model estimates?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(coeftab(mH3ulam, mH3c),
  labels = paste(rep(rownames(coeftab(mH3ulam, mH3c)@coefs), each = 2),
    rep(c(&amp;quot;Base&amp;quot;, &amp;quot;Interac&amp;quot;), nrow(coeftab(mH3ulam, mH3c)@coefs) * 2),
    sep = &amp;quot;-&amp;quot;
  )
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-18-statistical-rethinking-chapter-11_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;1440&#34; /&gt;
Jup, there&amp;rsquo;s not really much of a difference here. For the interaction model: the log-odds of successful piracy is just weakly bigger when the pirating individual is large and an adult. That is counter-intuitive, isn&amp;rsquo;t it? It is worth pointing out that the individual parameters for these conditions show the expected effects and the identified negative effect of their interaction may be down to the sparsity of the underlying data and we are also highly uncertain of it&amp;rsquo;s sign to begin with.&lt;/p&gt;
&lt;h3 id=&#34;practice-h4&#34;&gt;Practice H4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; The data contained in &lt;code&gt;data(salamanders)&lt;/code&gt; are counts of salamanders (&lt;em&gt;Plethodon elongatus&lt;/em&gt;) from 47 different 49$m^2$ plots in northern California. The column &lt;code&gt;SALAMAN&lt;/code&gt; is the count in each plot, and the columns &lt;code&gt;PCTCOVER&lt;/code&gt; and &lt;code&gt;FORESTAGE&lt;/code&gt; are percent of ground cover and age of trees in the plot, respectively. You will model &lt;code&gt;SALAMAN&lt;/code&gt; as a Poisson variable.&lt;/p&gt;
&lt;h4 id=&#34;part-a-1&#34;&gt;Part A&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Model the relationship between density and percent cover, using a log-link (same as the ex-
ample in the book and lecture). Use weakly informative priors of your choosing. Check the quadratic approximation again, by comparing &lt;code&gt;quap&lt;/code&gt; to &lt;code&gt;ulam&lt;/code&gt;. Then plot the expected counts and their 89% interval against percent cover. In which ways does the model do a good job? In which ways does it do a bad job?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  First, we load the data and standardise the predictors to get around their inconvenient scales which do not overlap well with each other:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(salamanders)
d &amp;lt;- salamanders
d$C &amp;lt;- standardize(d$PCTCOVER)
d$A &amp;lt;- standardize(d$FORESTAGE)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now it is time to write our Poisson model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;f &amp;lt;- alist(
  SALAMAN ~ dpois(lambda),
  log(lambda) &amp;lt;- a + bC * C,
  a ~ dnorm(0, 1),
  bC ~ dnorm(0, 1)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That was easy enough, but do those priors make sense? Let&amp;rsquo;s simulate:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;N &amp;lt;- 50 # 50 samples from prior
a &amp;lt;- rnorm(N, 0, 1)
bC &amp;lt;- rnorm(N, 0, 1)
C_seq &amp;lt;- seq(from = -2, to = 2, length.out = 30)
plot(NULL,
  xlim = c(-2, 2), ylim = c(0, 20),
  xlab = &amp;quot;cover(stanardized)&amp;quot;, ylab = &amp;quot;salamanders&amp;quot;
)
for (i in 1:N) {
  lines(C_seq, exp(a[i] + bC[i] * C_seq), col = grau(), lwd = 1.5)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-18-statistical-rethinking-chapter-11_files/figure-html/unnamed-chunk-24-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;While not terrible (the prior allows your some explosive trends, but mostly sticks to a reasonable count of individuals), we may want to consider making the prior a bit more informative:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bC &amp;lt;- rnorm(N, 0, 0.5)
plot(NULL,
  xlim = c(-2, 2), ylim = c(0, 20),
  xlab = &amp;quot;cover(stanardized)&amp;quot;, ylab = &amp;quot;salamanders&amp;quot;
)
for (i in 1:N) {
  lines(C_seq, exp(a[i] + bC[i] * C_seq), col = grau(), lwd = 1.5)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-18-statistical-rethinking-chapter-11_files/figure-html/unnamed-chunk-25-1.png&#34; width=&#34;1440&#34; /&gt;
Yup - I am happy with that.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s update the model specification and run it:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;f &amp;lt;- alist(
  SALAMAN ~ dpois(lambda),
  log(lambda) &amp;lt;- a + bC * C,
  a ~ dnorm(0, 1),
  bC ~ dnorm(0, 0.5)
)
mH4a &amp;lt;- ulam(f, data = d, chains = 4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &#39;ce27f50b1ba56f91eaeb68bb1bf4432c&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 1: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 1: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 1: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 1: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.045 seconds (Warm-up)
## Chain 1:                0.046 seconds (Sampling)
## Chain 1:                0.091 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;ce27f50b1ba56f91eaeb68bb1bf4432c&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 0 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 2: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 2: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 2: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 2: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 2: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 2: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 2: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 2: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 2: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 2: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 2: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.054 seconds (Warm-up)
## Chain 2:                0.062 seconds (Sampling)
## Chain 2:                0.116 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &#39;ce27f50b1ba56f91eaeb68bb1bf4432c&#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 0 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 3: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 3: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 3: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 3: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 3: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 3: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 3: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 3: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 3: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 3: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 3: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.052 seconds (Warm-up)
## Chain 3:                0.051 seconds (Sampling)
## Chain 3:                0.103 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &#39;ce27f50b1ba56f91eaeb68bb1bf4432c&#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 0 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 4: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 4: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 4: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 4: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 4: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 4: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 4: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 4: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 4: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 4: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 4: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.065 seconds (Warm-up)
## Chain 4:                0.058 seconds (Sampling)
## Chain 4:                0.123 seconds (Total)
## Chain 4:
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mH4aquap &amp;lt;- quap(f, data = d)
plot(coeftab(mH4a, mH4aquap),
  labels = paste(rep(rownames(coeftab(mH4a, mH4aquap)@coefs), each = 2),
    rep(c(&amp;quot;MCMC&amp;quot;, &amp;quot;quap&amp;quot;), nrow(coeftab(mH4a, mH4aquap)@coefs) * 2),
    sep = &amp;quot;-&amp;quot;
  )
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-18-statistical-rethinking-chapter-11_files/figure-html/unnamed-chunk-26-1.png&#34; width=&#34;1440&#34; /&gt;
Again, both models are doing fine and we continue to our plotting of expected counts and their interval with the &lt;code&gt;ulam&lt;/code&gt; model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(d$C, d$SALAMAN,
  col = rangi2, lwd = 2,
  xlab = &amp;quot;cover(standardized)&amp;quot;, ylab = &amp;quot;salamanders observed&amp;quot;
)
C_seq &amp;lt;- seq(from = -2, to = 2, length.out = 30)
l &amp;lt;- link(mH4a, data = list(C = C_seq))
lines(C_seq, colMeans(l))
shade(apply(l, 2, PI), C_seq)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-18-statistical-rethinking-chapter-11_files/figure-html/unnamed-chunk-27-1.png&#34; width=&#34;1440&#34; /&gt;
Well that model doesn&amp;rsquo;t fit all that nicely and the data seems over-dispersed to me.&lt;/p&gt;
&lt;h4 id=&#34;part-b-1&#34;&gt;Part B&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Can you improve the model by using the other predictor, &lt;code&gt;FORESTAGE&lt;/code&gt;? Try any models you think useful. Can you explain why &lt;code&gt;FORESTAGE&lt;/code&gt; helps or does not help with prediction?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Forest cover might be confounded by forest age. The older a forest, the bigger its coverage? A model to investigate this could look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;f2 &amp;lt;- alist(
  SALAMAN ~ dpois(lambda),
  log(lambda) &amp;lt;- a + bC * C + bA * A,
  a ~ dnorm(0, 1),
  c(bC, bA) ~ dnorm(0, 0.5)
)
mH4b &amp;lt;- ulam(f2, data = d, chains = 4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &#39;4850e2c86bda45f77f837aaee26a4da5&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 1: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 1: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 1: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 1: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.066 seconds (Warm-up)
## Chain 1:                0.067 seconds (Sampling)
## Chain 1:                0.133 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;4850e2c86bda45f77f837aaee26a4da5&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 0 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 2: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 2: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 2: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 2: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 2: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 2: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 2: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 2: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 2: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 2: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 2: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.068 seconds (Warm-up)
## Chain 2:                0.079 seconds (Sampling)
## Chain 2:                0.147 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &#39;4850e2c86bda45f77f837aaee26a4da5&#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 0 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 3: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 3: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 3: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 3: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 3: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 3: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 3: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 3: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 3: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 3: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 3: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.064 seconds (Warm-up)
## Chain 3:                0.062 seconds (Sampling)
## Chain 3:                0.126 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &#39;4850e2c86bda45f77f837aaee26a4da5&#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 0 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 4: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 4: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 4: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 4: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 4: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 4: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 4: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 4: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 4: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 4: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 4: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.076 seconds (Warm-up)
## Chain 4:                0.058 seconds (Sampling)
## Chain 4:                0.134 seconds (Total)
## Chain 4:
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(mH4b)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          mean         sd       5.5%     94.5%     n_eff    Rhat4
## a  0.48361398 0.13609701  0.2618982 0.6896444  873.3384 1.003115
## bA 0.01904618 0.09647959 -0.1361230 0.1679860 1102.3547 1.002465
## bC 1.04260846 0.17335950  0.7795899 1.3262340  919.5342 1.000832
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Fascinating! The estimate for &lt;code&gt;bA&lt;/code&gt; is nearly $0$ with a lot of certainty (i.e. a small interval) behind it. While conditioning on percent cover, forest age does not influence salamander count. This looks like a post-treatment effect to me.&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.5 (2021-03-31)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19043)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.1252  LC_CTYPE=English_United Kingdom.1252    LC_MONETARY=English_United Kingdom.1252 LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.1252    
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] MASS_7.3-53.1        tidybayes_2.3.1      rethinking_2.13      rstan_2.21.2         ggplot2_3.3.6        StanHeaders_2.21.0-7
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.7           mvtnorm_1.1-1        lattice_0.20-41      tidyr_1.1.3          prettyunits_1.1.1    ps_1.6.0             assertthat_0.2.1     digest_0.6.27        utf8_1.2.1          
## [10] V8_3.4.1             plyr_1.8.6           R6_2.5.0             backports_1.2.1      stats4_4.0.5         evaluate_0.14        coda_0.19-4          highr_0.9            blogdown_1.3        
## [19] pillar_1.6.0         rlang_0.4.11         curl_4.3.2           callr_3.7.0          jquerylib_0.1.4      R.utils_2.10.1       R.oo_1.24.0          rmarkdown_2.7        styler_1.4.1        
## [28] stringr_1.4.0        loo_2.4.1            munsell_0.5.0        compiler_4.0.5       xfun_0.22            pkgconfig_2.0.3      pkgbuild_1.2.0       shape_1.4.5          htmltools_0.5.1.1   
## [37] tidyselect_1.1.0     tibble_3.1.1         gridExtra_2.3        bookdown_0.22        arrayhelpers_1.1-0   codetools_0.2-18     matrixStats_0.61.0   fansi_0.4.2          crayon_1.4.1        
## [46] dplyr_1.0.5          withr_2.4.2          R.methodsS3_1.8.1    distributional_0.2.2 ggdist_2.4.0         grid_4.0.5           jsonlite_1.7.2       gtable_0.3.0         lifecycle_1.0.0     
## [55] DBI_1.1.1            magrittr_2.0.1       scales_1.1.1         RcppParallel_5.1.2   cli_3.0.0            stringi_1.5.3        farver_2.1.0         bslib_0.2.4          ellipsis_0.3.2      
## [64] generics_0.1.0       vctrs_0.3.7          rematch2_2.1.2       forcats_0.5.1        tools_4.0.5          svUnit_1.0.6         R.cache_0.14.0       glue_1.4.2           purrr_0.3.4         
## [73] processx_3.5.1       yaml_2.2.1           inline_0.3.17        colorspace_2.0-0     knitr_1.33           sass_0.3.1
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Troubleshooting R - Isolating Issues and Asking Questions</title>
      <link>https://www.erikkusch.com/courses/excursions-into-biostatistics/10_troubleshooting_r-isolating-issues-and-asking-questions/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/excursions-into-biostatistics/10_troubleshooting_r-isolating-issues-and-asking-questions/</guid>
      <description>&lt;p&gt;I have prepared some &lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/Excursions-into-Biostatistics/Troubleshooting---Isolating-Issues-and-Asking-Questions.html&#34; target=&#34;_blank&#34;&gt; Lecture Slides &lt;/a&gt; for this session.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ordinal &amp; Metric Tests (More-Than-Two-Sample Situations)</title>
      <link>https://www.erikkusch.com/courses/an-introduction-to-biostatistics/ordinal-metric-tests-more-than-two-sample-situations/</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/an-introduction-to-biostatistics/ordinal-metric-tests-more-than-two-sample-situations/</guid>
      <description>&lt;h2 id=&#34;theory&#34;&gt;Theory&lt;/h2&gt;
&lt;p&gt;Welcome to our fifth practical experience in R. Throughout the following notes, I will introduce you to a couple statistical approaches for metric or ordinal data when wanting to compare more than two samples/populations that might be useful to you and are, to varying degrees, often used in biology. To do so, I will enlist the sparrow data set we handled in our first exercise.&lt;/p&gt;
&lt;p&gt;I have prepared some &lt;a href=&#34;https://www.erikkusch.com/courses/an-introduction-to-biostatistics/11---Ordinal-and-Metric-Test--More-Than-Two-Sample-_Handout.html&#34; target=&#34;_blank&#34;&gt; Lecture Slides &lt;/a&gt; for this session.&lt;/p&gt;
&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;p&gt;Find the data for this exercise &lt;a href=&#34;https://www.erikkusch.com/courses/an-introduction-to-biostatistics/Data/1%20-%20Sparrow_Data_READY.rds&#34; target=&#34;_blank&#34;&gt; here&lt;/a&gt;,  &lt;a href=&#34;https://www.erikkusch.com/courses/an-introduction-to-biostatistics/Data/2a%20-%20Sparrow_ResettledSIMA_READY.rds&#34; target=&#34;_blank&#34;&gt; here&lt;/a&gt;, and &lt;a href=&#34;https://www.erikkusch.com/courses/an-introduction-to-biostatistics/Data/2b%20-%20Sparrow_ResettledSIUK_READY.rds&#34; target=&#34;_blank&#34;&gt; here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;preparing-our-procedure&#34;&gt;Preparing Our Procedure&lt;/h2&gt;
&lt;p&gt;To ensure others can reproduce our analysis we run the following three lines of code at the beginning of our &lt;code&gt;R&lt;/code&gt; coding file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rm(list=ls()) # clearing environment
Dir.Base &amp;lt;- getwd() # soft-coding our working directory
Dir.Data &amp;lt;- paste(Dir.Base, &amp;quot;Data&amp;quot;, sep=&amp;quot;/&amp;quot;) # soft-coding our data directory 
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;packages&#34;&gt;Packages&lt;/h3&gt;
&lt;p&gt;Using the following, user-defined function, we install/load all the necessary packages into our current &lt;code&gt;R&lt;/code&gt; session.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# function to load packages and install them if they haven&#39;t been installed yet
install.load.package &amp;lt;- function(x) {
  if (!require(x, character.only = TRUE))
    install.packages(x)
  require(x, character.only = TRUE)
}
package_vec &amp;lt;- c()
sapply(package_vec, install.load.package)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## list()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, we don&amp;rsquo;t need any packages for our analyses in this practical. Take note that I am not using &lt;code&gt;ggplot2&lt;/code&gt; for data visualisation today. Personally, I find it cumbersome for &amp;ldquo;behind-the-scenes&amp;rdquo; boxplots (which is what I&amp;rsquo;ll use a lot today) and so I am presenting you with the base &lt;code&gt;R&lt;/code&gt; alternative.&lt;/p&gt;
&lt;h3 id=&#34;loading-data&#34;&gt;Loading Data&lt;/h3&gt;
&lt;p&gt;During our first exercise (Data Mining and Data Handling - Fixing The Sparrow Data Set) we saved our clean data set as an RDS file. To load this, we use the &lt;code&gt;readRDS()&lt;/code&gt; command that comes with base &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df_base &amp;lt;- readRDS(file = paste(Dir.Data, &amp;quot;/1 - Sparrow_Data_READY.rds&amp;quot;, sep=&amp;quot;&amp;quot;))
Data_df &amp;lt;- Data_df_base # duplicate and save initial data on a new object
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;kruskal-wallis-test&#34;&gt;Kruskal-Wallis Test&lt;/h2&gt;
&lt;h3 id=&#34;climate-warmingextremes&#34;&gt;Climate Warming/Extremes&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Does morphology of Passer domesticus depend on climate?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Using the Mann-Witney U Test in our last practical, we concluded that climate (when recorded as &amp;ldquo;Continental&amp;rdquo; and &amp;ldquo;Non-Continental&amp;rdquo;) is an important driver of &lt;em&gt;Passer domesticus&lt;/em&gt; morphology. Now we will see whether this holds true when considering non-continental climates as coastal and semi-coastal ones.&lt;/p&gt;
&lt;p&gt;Take note that we need to limit our analysis to our climate type testing sites again as follows (we include Manitoba this time as it is at the same latitude as the UK and Siberia and holds a semi-coastal climate type):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# prepare climate type testing data
Data_df &amp;lt;- Data_df_base
Index &amp;lt;- Data_df$Index
Rows &amp;lt;- which(Index == &amp;quot;SI&amp;quot; | Index == &amp;quot;UK&amp;quot; | Index == &amp;quot;RE&amp;quot; | Index == &amp;quot;AU&amp;quot; | Index == &amp;quot;MA&amp;quot;)
Data_df &amp;lt;- Data_df[Rows,]
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;weight&#34;&gt;Weight&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s start with weight records of common house sparrows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;WeightCont &amp;lt;- with(Data_df, Weight[which(Climate == &amp;quot;Continental&amp;quot;)])
WeightSemi &amp;lt;- with(Data_df, Weight[which(Climate == &amp;quot;Semi-Coastal&amp;quot;)])
WeightCoast &amp;lt;- with(Data_df, Weight[which(Climate == &amp;quot;Coastal&amp;quot;)])
Weights_vec &amp;lt;- c(WeightCont, WeightSemi, WeightCoast)

Climates &amp;lt;- c(
  rep(&amp;quot;Continental&amp;quot;, length(WeightCont)),
  rep(&amp;quot;Semi-Coastal&amp;quot;, length(WeightSemi)),
  rep(&amp;quot;Coastal&amp;quot;, length(WeightCoast))
)
Climates &amp;lt;- as.factor(Climates)

kruskal.test(Weights_vec, Climates)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Kruskal-Wallis rank sum test
## 
## data:  Weights_vec and Climates
## Kruskal-Wallis chi-squared = 150.98, df = 2, p-value &amp;lt; 2.2e-16
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We conclude that the three-level climate variable is an important source of information to understand what drives weight records of &lt;em&gt;Passer domesticus&lt;/em&gt; and thus &lt;strong&gt;reject the null hypothesis&lt;/strong&gt; (p = $1.6418184\times 10^{-33}$).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;boxplot(Weights_vec ~ Climates)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;11---Ordinal-and-Metric-Test--More-Than-Two-Sample-_files/figure-html/KruskalSite1b-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looking at the boxplot, we can understand the distribution of weight records as grouped by climate types and identify weight records to be biggest for continental climates followed by semi-coastal ones with similar spreads. Coastal climates, on the other hand, show a remarkable spread of weight records of &lt;em&gt;Passer domesticus&lt;/em&gt; with a markedly lower median than the two previous categories.&lt;/p&gt;
&lt;h4 id=&#34;height&#34;&gt;Height&lt;/h4&gt;
&lt;p&gt;Secondly, let&amp;rsquo;s repeat the above Kruskal-Wallis Test for the height/length records of our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;HeightCont &amp;lt;- with(Data_df, Height[which(Climate == &amp;quot;Continental&amp;quot;)])
HeightSemi &amp;lt;- with(Data_df, Height[which(Climate == &amp;quot;Semi-Coastal&amp;quot;)])
HeightCoast &amp;lt;- with(Data_df, Height[which(Climate == &amp;quot;Coastal&amp;quot;)])
Heights_vec &amp;lt;- c(HeightCont, HeightSemi, HeightCoast)

Climates &amp;lt;- c(
  rep(&amp;quot;Continental&amp;quot;, length(HeightCont)),
  rep(&amp;quot;Semi-Coastal&amp;quot;, length(HeightSemi)),
  rep(&amp;quot;Coastal&amp;quot;, length(HeightCoast))
)
Climates &amp;lt;- as.factor(Climates)

kruskal.test(Heights_vec, Climates)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Kruskal-Wallis rank sum test
## 
## data:  Heights_vec and Climates
## Kruskal-Wallis chi-squared = 15.635, df = 2, p-value = 0.0004027
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;boxplot(Heights_vec ~ Climates)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;11---Ordinal-and-Metric-Test--More-Than-Two-Sample-_files/figure-html/KruskalSite2a-1.png&#34; width=&#34;1440&#34; /&gt;
We conclude that the three-level climate variable is an important source of information to understand what drives height records of &lt;em&gt;Passer domesticus&lt;/em&gt; and thus &lt;strong&gt;reject the null hypothesis&lt;/strong&gt; (p = $4.0267296\times 10^{-4}$).&lt;/p&gt;
&lt;p&gt;Looking at the boxplot, we can understand the distribution of height records as grouped by climate types and identify height records to be smallest for continental climates followed by semi-coastal ones with similar spreads. Coastal climates, on the other hand, show a remarkable spread of height records of &lt;em&gt;Passer domesticus&lt;/em&gt; with a markedly higher median than the two previous categories.&lt;/p&gt;
&lt;h4 id=&#34;wing-chord&#34;&gt;Wing Chord&lt;/h4&gt;
&lt;p&gt;Third, we will test whether climate is a good predictor for wing chord of common house sparrows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Wing.ChordCont &amp;lt;- with(Data_df, Wing.Chord[which(Climate == &amp;quot;Continental&amp;quot;)])
Wing.ChordSemi &amp;lt;- with(Data_df, Wing.Chord[which(Climate == &amp;quot;Semi-Coastal&amp;quot;)])
Wing.ChordCoast &amp;lt;- with(Data_df, Wing.Chord[which(Climate == &amp;quot;Coastal&amp;quot;)])
Wing.Chords_vec &amp;lt;- c(Wing.ChordCont, Wing.ChordSemi, Wing.ChordCoast)

Climates &amp;lt;- c(
  rep(&amp;quot;Continental&amp;quot;, length(Wing.ChordCont)),
  rep(&amp;quot;Semi-Coastal&amp;quot;, length(Wing.ChordSemi)),
  rep(&amp;quot;Coastal&amp;quot;, length(Wing.ChordCoast))
)
Climates &amp;lt;- as.factor(Climates)

kruskal.test(Wing.Chords_vec, Climates)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Kruskal-Wallis rank sum test
## 
## data:  Wing.Chords_vec and Climates
## Kruskal-Wallis chi-squared = 41.539, df = 2, p-value = 9.548e-10
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;boxplot(Wing.Chords_vec ~ Climates)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;11---Ordinal-and-Metric-Test--More-Than-Two-Sample-_files/figure-html/KruskalSite3a-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We conclude that the three-level climate variable is an important source of information to understand what drives wing chord records of &lt;em&gt;Passer domesticus&lt;/em&gt; and thus &lt;strong&gt;reject the null hypothesis&lt;/strong&gt; (p = $9.5482279\times 10^{-10}$).&lt;/p&gt;
&lt;p&gt;Looking at the boxplot, we can understand the distribution of wing chord records as grouped by climate types and identify wing chord records to be smallest for continental climates followed by semi-coastal ones with similar spreads. Coastal climates, on the other hand, show a remarkable spread of wing chord records of &lt;em&gt;Passer domesticus&lt;/em&gt; with a markedly higher median than the two previous categories.&lt;/p&gt;
&lt;h4 id=&#34;automating-the-analysis&#34;&gt;Automating the Analysis&lt;/h4&gt;
&lt;p&gt;As we have seen, running seperate tests for every research question may be a bit cumbersome and so we may want to automate the analysis by establishing our own user-defined function as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;AutomatedKruskal &amp;lt;- function(Variables, Groups, Plotting){
# establish data frame to save results to
Export &amp;lt;- data.frame(
  Variables = Variables,
  Grouped_by = rep(Groups, length(Variables)),
  Chi_Squared = rep(NA, length(Variables)),
  DF = rep(NA, length(Variables)),
  p_value = rep(NA, length(Variables))
)

for(i in 1:length(Variables)){
# extract data and groups from data frame
YData &amp;lt;- Data_df[,which(colnames(Data_df)==Variables[i])]
XData &amp;lt;- Data_df[,which(colnames(Data_df)==Groups)]

# establish a list holding our groups for our data
Data &amp;lt;- list()
Grouping &amp;lt;- list()
for(k in 1:length(unique(XData))){
  Data[[k]] &amp;lt;- YData[which(XData == unique(XData)[k])]
  Grouping[[k]] &amp;lt;- rep(unique(XData)[k], length = length(Data[[k]]))
} # end of k-loop

Data &amp;lt;- unlist(Data)
Grouping &amp;lt;- unlist(Grouping)

# fill data frame
Export[i, 3] &amp;lt;- kruskal.test(Data, Grouping)[[&amp;quot;statistic&amp;quot;]][[&amp;quot;Kruskal-Wallis chi-squared&amp;quot;]]
Export[i, 4] &amp;lt;- kruskal.test(Data, Grouping)[[&amp;quot;parameter&amp;quot;]]
Export[i, 5] &amp;lt;- kruskal.test(Data, Grouping)[[&amp;quot;p.value&amp;quot;]]

# optional plotting
if(Plotting == TRUE){
plot(Data ~ factor(Grouping), ylab = Variables[i])
}

} # end of i loop

# return data frame to R outside of function
return(Export)
} # end of function
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function is &lt;em&gt;named&lt;/em&gt; &lt;code&gt;AutomatedKruskal()&lt;/code&gt; and takes three &lt;em&gt;arguments&lt;/em&gt;: (1) &lt;code&gt;Variables&lt;/code&gt; - a vector of character typed identifiers for the variables we want to have tested, (2) &lt;code&gt;Groups&lt;/code&gt; - a character string identifying the grouping variable, (3) &lt;code&gt;Plotting&lt;/code&gt; - a logical statement (&lt;code&gt;TRUE&lt;/code&gt; or &lt;code&gt;FALSE&lt;/code&gt;) whether boxplots shall be produced.&lt;/p&gt;
&lt;p&gt;The function then proceeds to establish an empty data frame which it will store the results of our Kurskal-Wallis Tests in. Afterwards, it cycles through all variables contained within the &lt;code&gt;Variables&lt;/code&gt; statement, extracts the relevant data, grouping it according to the specified grouping variable (&lt;code&gt;Groups&lt;/code&gt;), runs the test, fills the data frame and plots the data if &lt;code&gt;Plotting&lt;/code&gt; has been set to &lt;code&gt;TRUE&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s re-run our earlier test on sparrow morphology as influenced by climate using this function by &lt;em&gt;calling&lt;/em&gt; it:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;par(mfrow = c(3,1)) # adjust plotting panes
AutomatedKruskal(Variables = c(&amp;quot;Weight&amp;quot;, &amp;quot;Height&amp;quot;, &amp;quot;Wing.Chord&amp;quot;), 
                 Groups = &amp;quot;Climate&amp;quot;, 
                 Plotting = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;11---Ordinal-and-Metric-Test--More-Than-Two-Sample-_files/figure-html/KruskalSite7b-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##    Variables Grouped_by Chi_Squared DF      p_value
## 1     Weight    Climate   150.97901  2 1.641818e-33
## 2     Height    Climate    15.63477  2 4.026730e-04
## 3 Wing.Chord    Climate    41.53899  2 9.548228e-10
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we can see from the results above, our function works flawlessly and we can use it going ahead.&lt;/p&gt;
&lt;p&gt;Furthermore, we can confirm some of the results of our Mann-Whitney U Test from last seminar.&lt;/p&gt;
&lt;h3 id=&#34;predation&#34;&gt;Predation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Does nesting height depend on predator characteristics?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Again, using the Mann-WHitney U Test in our last exercise, we identified both predator presence as well as predator type to be important predictors for nesting height of &lt;em&gt;Passer domesticus&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;We may wish to use the entirety of our data set again for this purpose:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df &amp;lt;- Data_df_base
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the Kruskal-Wallis Test, we can combine these two predictors by turning every record of predator type that is recorded as &lt;code&gt;NA&lt;/code&gt; into &amp;ldquo;None&amp;rdquo; which will then serve as an identifier for the absence of any predators effectively making the predator presence variable redundant:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# changing levels in predator type
levels(Data_df$Predator.Type) &amp;lt;- c(levels(Data_df$Predator.Type), &amp;quot;None&amp;quot;)
Data_df$Predator.Type[which(is.na(Data_df$Predator.Type))] &amp;lt;- &amp;quot;None&amp;quot;

# running analysis
AutomatedKruskal(Variables = &amp;quot;Nesting.Height&amp;quot;, Groups = &amp;quot;Predator.Type&amp;quot;, Plotting = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;11---Ordinal-and-Metric-Test--More-Than-Two-Sample-_files/figure-html/KruskalPredationa-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##        Variables    Grouped_by Chi_Squared DF      p_value
## 1 Nesting.Height Predator.Type    88.81797  2 5.169206e-20
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using our &lt;code&gt;Automated Kruskal()&lt;/code&gt; function, we can conclude that the aggregation of predator presence to predator type records serve as an excellent predictor for sparrow nesting height and &lt;strong&gt;reject the null hypothesis&lt;/strong&gt; (p = $5.169206\times 10^{-20}$).&lt;/p&gt;
&lt;p&gt;Therefore, we can argue that avian predation forces sparrows into low nesting sites, non-avian predation leads to more elevated nesting sites in &lt;em&gt;Passer domesticus&lt;/em&gt; and absence of predators seems to not force nesting height in any direction or restricting its spread.&lt;/p&gt;
&lt;h3 id=&#34;competition&#34;&gt;Competition&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Does home range depend on climate?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Having used the Mann-Whitney U Test to identify possible climate-driven changes in home ranges of &lt;em&gt;Passer domesticus&lt;/em&gt; in our last seminar, we concluded that climate types largely affect home ranges of the common house sparrow.&lt;/p&gt;
&lt;p&gt;Take note that we need to limit our analysis to our climate type testing sites again as follows (we include Manitoba this time as it is at the same latitude as the UK and Siberia and holds a semi-coastal climate type):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# prepare climate type testing data
Data_df &amp;lt;- Data_df_base
Index &amp;lt;- Data_df$Index
Rows &amp;lt;- which(Index == &amp;quot;SI&amp;quot; | Index == &amp;quot;UK&amp;quot; | Index == &amp;quot;RE&amp;quot; | Index == &amp;quot;AU&amp;quot; | Index == &amp;quot;MA&amp;quot;)
Data_df &amp;lt;- Data_df[Rows,]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s test this for our three-level climate variable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df$Home.Range &amp;lt;- as.numeric(factor(Data_df$Home.Range))
AutomatedKruskal(Variables = &amp;quot;Home.Range&amp;quot;, Groups = &amp;quot;Climate&amp;quot;, Plotting = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;11---Ordinal-and-Metric-Test--More-Than-Two-Sample-_files/figure-html/KruskalCompetitiona-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##    Variables Grouped_by Chi_Squared DF    p_value
## 1 Home.Range    Climate    6.243918  2 0.04407075
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using our &lt;code&gt;Automated Kruskal()&lt;/code&gt; function, we can conclude that the three-loevel climate variable serves as an excellent predictor for sparrow home range and &lt;strong&gt;reject the null hypothesis&lt;/strong&gt; (p = $0.0440707$) thus being at odds with our Mann-Whitney U results (that were only based on two climate types).&lt;/p&gt;
&lt;p&gt;Remember that small numeric ranges mean large actual ranges in this set-up and so we can conclude that  climates force common house sparrows to adapt to bigger home ranges.&lt;/p&gt;
&lt;h2 id=&#34;friedman-test&#34;&gt;Friedman Test&lt;/h2&gt;
&lt;p&gt;We can analyse the significance of more than two population/sample medians of metric variables which are dependent of one another using the &lt;code&gt;friedman.test()&lt;/code&gt; function in base &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;preparing-data&#34;&gt;Preparing Data&lt;/h3&gt;
&lt;p&gt;Obviously, none of our data records are paired as such. Whilst one may want to make the argument that many characteristics of individuals that group together might be dependant on the expressions of themselves found throughout said group, we will not concentrate on this possibility within these practicals.&lt;/p&gt;
&lt;p&gt;Conclusively, we need &lt;strong&gt;additional data sets with truly paired records&lt;/strong&gt; of sparrows. Within our study set-up, think of a &lt;strong&gt;resettling experiment&lt;/strong&gt;, were you take &lt;em&gt;Passer domesticus&lt;/em&gt; individuals from one site, transfer them to another and check back with them after some time has passed to see whether some of their characteristics have changed in their expression.&lt;br&gt;
To this end, presume we have taken the entire &lt;em&gt;Passer domesticus&lt;/em&gt; population found at our &lt;strong&gt;Siberian&lt;/strong&gt; research station and moved them to &lt;strong&gt;Manitoba&lt;/strong&gt;. After a given time at their new location, we are again moving the population from Manitoba to the &lt;strong&gt;United Kingdom&lt;/strong&gt;. Whilst this keeps the latitude stable, the sparrows &lt;em&gt;now experience a semi-coastal climate followed by a coastal one instead of a continental one&lt;/em&gt;. After some time (let&amp;rsquo;s say: a year), we have come back and recorded all the characteristics for the same individuals again. Within our data, none of the original individuals have gone missing or died throghout our study period. This is usually not the case in nature and such records would need to be deleted from the data set.&lt;/p&gt;
&lt;p&gt;You will find the corresponding &lt;em&gt;new data&lt;/em&gt; in &lt;code&gt;2a - Sparrow_ResettledSIMA_READY.rds&lt;/code&gt; (Siberia to Manitoba) and &lt;code&gt;2b - Sparrow_ResettledSIUK_READY.rds&lt;/code&gt; (former SIberian population from manitoba to the UK). Take note that these sets only contain records for the transferred individuals in the &lt;strong&gt;same order&lt;/strong&gt; as in the old data set.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df_SIMA &amp;lt;- readRDS(file = paste(Dir.Data, &amp;quot;/2a - Sparrow_ResettledSIMA_READY.rds&amp;quot;, sep=&amp;quot;&amp;quot;))
Data_df_SIUK &amp;lt;- readRDS(file = paste(Dir.Data, &amp;quot;/2b - Sparrow_ResettledSIUK_READY.rds&amp;quot;, sep=&amp;quot;&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With these data records, we can now re-evaluate how the characteristics of sparrows can change when subjected to different conditions than previously thus shedding light on their &lt;strong&gt;plasticity&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;As such, this program is very reminiscent of the resettling program in our last exercise when using Wilcoxon Signed Rank Test to account for plasticity of our sparrow individuals. This new program includes the additional step of transferring sparrows via Manitoba first. Why have we chosen this order of resettlements?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Our stations SI, MA and UK are all on roughly the same latitude.&lt;/li&gt;
&lt;li&gt;Moving the sparrows from SI to UK via MA results in them experiencing a gradient from continental to semi-coastal to coastal climate.&lt;/li&gt;
&lt;li&gt;Whilst Siberia is populated by avian predators, no predators are present at Manitoba and our sparrows are subject to non-avian predation in the UK.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;All of this serves to &lt;strong&gt;maximise variation&lt;/strong&gt; that we want to research whilst &lt;strong&gt;minising constraining factors&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;climate-warmingextremes-1&#34;&gt;Climate Warming/Extremes&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Does sparrow morphology change depend on climate?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Thinking back to out Wilcoxon Signed Rank test, we can already argue that weight records of sparrows should change according to climate whilst height and wing chord records should remain unaltered for every individual.&lt;/p&gt;
&lt;p&gt;We may wish to use the entirety of our data set again for this purpose:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df &amp;lt;- Data_df_base
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since this involves testing three seperate criterions of sparrow morphology, we again establish a user-defined function. This one is called &lt;code&gt;AutomatedFried()&lt;/code&gt; and has dropped the &lt;code&gt;Groups&lt;/code&gt; argument that was present in &lt;code&gt;AutomatedKruskal()&lt;/code&gt; since the grouping will always be our three stations:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;AutomatedFried &amp;lt;- function(Variables, Plotting){
# establish data frame to save results to
Export &amp;lt;- data.frame(
  Variables = Variables,
  Grouped_by = rep(&amp;quot;Resettling&amp;quot;, length(Variables)),
  Chi_Squared = rep(NA, length(Variables)),
  DF = rep(NA, length(Variables)),
  p_value = rep(NA, length(Variables))
)

for(i in 1:length(Variables)){
# extract data and groups from data frame
YDataSI &amp;lt;- Data_df[,which(colnames(Data_df)==Variables[i])]
YDataMA &amp;lt;- Data_df_SIMA[,which(colnames(Data_df)==Variables[i])]
YDataUK &amp;lt;- Data_df_SIUK[,which(colnames(Data_df)==Variables[i])]

Data &amp;lt;- matrix(c(YDataSI[which(Data_df$Index == &amp;quot;SI&amp;quot;)],
          YDataMA, YDataUK), nrow = dim(Data_df_SIMA)[1],
       byrow = FALSE, dimnames = list(1:dim(Data_df_SIMA)[1], 
                                      c(&amp;quot;SI&amp;quot;, &amp;quot;MA&amp;quot;, &amp;quot;UK&amp;quot;))
  )

# fill data frame
Export[i, 3] &amp;lt;- friedman.test(Data)[[&amp;quot;statistic&amp;quot;]][[&amp;quot;Friedman chi-squared&amp;quot;]]
Export[i, 4] &amp;lt;- friedman.test(Data)[[&amp;quot;parameter&amp;quot;]]
Export[i, 5] &amp;lt;- friedman.test(Data)[[&amp;quot;p.value&amp;quot;]]

# optional plotting
if(Plotting == TRUE){
  
  # prepare plotting data
  PlotData &amp;lt;- as.vector(Data)
  Grouping &amp;lt;- as.factor(
    rep(c(&amp;quot;SI&amp;quot;, &amp;quot;MA&amp;quot;, &amp;quot;UK&amp;quot;), each = dim(Data_df_SIMA)[1])
    )
  # plotting
  plot(PlotData ~ Grouping, ylab = Variables[i])
}
} # end of i loop

# return data frame to R outside of function
return(Export)
} # end of function
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As such, the above function operates a lot like the earlier user-defined counterpart for the Kruskal-Wallis Test. It returns the important test characteristics and allows for plots. Internally, however, it is built on a matrix rather than vectors.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s get to testing our prediction:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;par(mfrow = c(3,1)) # adjust plotting panes
AutomatedFried(Variables = c(&amp;quot;Weight&amp;quot;, &amp;quot;Height&amp;quot;, &amp;quot;Wing.Chord&amp;quot;), Plotting = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;11---Ordinal-and-Metric-Test--More-Than-Two-Sample-_files/figure-html/FriedClimateb-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##    Variables Grouped_by Chi_Squared DF      p_value
## 1     Weight Resettling    97.84848  2 5.655506e-22
## 2     Height Resettling         NaN  2          NaN
## 3 Wing.Chord Resettling         NaN  2          NaN
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, whilst climate is a good predictor for the weight of resettled sparrows (weight in continental climates is higher than in semi-coastal or coastal ones), height and wing chord records couldn&amp;rsquo;t be properly tested on using the &lt;code&gt;friedman.test()&lt;/code&gt; function since they have remained unalterd. Thetrefore, we &lt;strong&gt;reject the null hypothesis&lt;/strong&gt; for weight records of &lt;em&gt;Passer domesticus&lt;/em&gt; and &lt;strong&gt;accept the null hypothesis&lt;/strong&gt; for height and wing chord records.&lt;/p&gt;
&lt;h3 id=&#34;predation-1&#34;&gt;Predation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Does nesting height depend on predator characteristics?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;According to the results of our last practical, we would assume &lt;em&gt;Passer doemsticus&lt;/em&gt; to adhere to local conditions when chosing a nesting site and corresponding nesting height depending on predator presence:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;AutomatedFried(Variables = &amp;quot;Nesting.Height&amp;quot;, Plotting = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;11---Ordinal-and-Metric-Test--More-Than-Two-Sample-_files/figure-html/FriedPreda-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##        Variables Grouped_by Chi_Squared DF     p_value
## 1 Nesting.Height Resettling      10.864  2 0.004374338
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Just like we expected, nesting height of resettled sparrows depends hugely on predator presence at the sties they have been moved to (p = $7.3991389\times 10^{-19}$) and we &lt;strong&gt;reject the null hypothesis&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;competition-1&#34;&gt;Competition&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Does home range depend on climate?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As we&amp;rsquo;ve seen in our last seminar, a statistically signficant change in home ranges did not occur when resettling Siberian sparrows directly to the UK. How about when we resettle them via Manitoba?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df_SIMA$Home.Range &amp;lt;- as.numeric(Data_df_SIMA$Home.Range)
Data_df_SIUK$Home.Range &amp;lt;- as.numeric(Data_df_SIUK$Home.Range)

AutomatedFried(Variables = &amp;quot;Home.Range&amp;quot;, Plotting = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in xy.coords(x, y, xlabel, ylabel, log): NAs introduced by coercion
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;11---Ordinal-and-Metric-Test--More-Than-Two-Sample-_files/figure-html/FriedCompetitiona-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##    Variables Grouped_by Chi_Squared DF      p_value
## 1 Home.Range Resettling         132  2 2.170522e-29
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With our three-step resettling program we do record a statistically significant change in home ranges of our sparrow flocks and &lt;strong&gt;reject the null hypothesis&lt;/strong&gt; (p = $2.170522\times 10^{-29}$).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ordinal &amp; Metric Tests (More-Than-Two-Sample Situations)</title>
      <link>https://www.erikkusch.com/courses/biostat101/ordinal-metric-tests-more-than-two-sample-situations/</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/biostat101/ordinal-metric-tests-more-than-two-sample-situations/</guid>
      <description>&lt;h2 id=&#34;theory&#34;&gt;Theory&lt;/h2&gt;
&lt;p&gt;Welcome to our fifth practical experience in R. Throughout the following notes, I will introduce you to a couple statistical approaches for metric or ordinal data when wanting to compare more than two samples/populations that might be useful to you and are, to varying degrees, often used in biology. To do so, I will enlist the sparrow data set we handled in our first exercise. I have prepared some slides for this session: &lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/biostat101/11---Ordinal-and-Metric-Test--More-Than-Two-Sample-_Handout.html&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://www.erikkusch.com/courses/biostat101/11---BioStat101_featured.png&#34; width=&#34;900&#34; margin-top = &#34;0&#34;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;p&gt;Find the data for this exercise &lt;a href=&#34;https://github.com/ErikKusch/Homepage/raw/master/content/courses/biostat101/Data/1%20-%20Sparrow_Data_READY.rds&#34; target=&#34;_blank&#34;&gt; here&lt;/a&gt;,  &lt;a href=&#34;https://github.com/ErikKusch/Homepage/raw/master/content/courses/biostat101/Data/2a%20-%20Sparrow_ResettledSIMA_READY.rds&#34; target=&#34;_blank&#34;&gt; here&lt;/a&gt;, and &lt;a href=&#34;https://github.com/ErikKusch/Homepage/raw/master/content/courses/biostat101/Data/2b%20-%20Sparrow_ResettledSIUK_READY.rds&#34; target=&#34;_blank&#34;&gt; here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;preparing-our-procedure&#34;&gt;Preparing Our Procedure&lt;/h2&gt;
&lt;p&gt;To ensure others can reproduce our analysis we run the following three lines of code at the beginning of our &lt;code&gt;R&lt;/code&gt; coding file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rm(list=ls()) # clearing environment
Dir.Base &amp;lt;- getwd() # soft-coding our working directory
Dir.Data &amp;lt;- paste(Dir.Base, &amp;quot;Data&amp;quot;, sep=&amp;quot;/&amp;quot;) # soft-coding our data directory 
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;packages&#34;&gt;Packages&lt;/h3&gt;
&lt;p&gt;Using the following, user-defined function, we install/load all the necessary packages into our current &lt;code&gt;R&lt;/code&gt; session.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# function to load packages and install them if they haven&#39;t been installed yet
install.load.package &amp;lt;- function(x) {
  if (!require(x, character.only = TRUE))
    install.packages(x)
  require(x, character.only = TRUE)
}
package_vec &amp;lt;- c()
sapply(package_vec, install.load.package)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## list()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, we don&amp;rsquo;t need any packages for our analyses in this practical. Take note that I am not using &lt;code&gt;ggplot2&lt;/code&gt; for data visualisation today. Personally, I find it cumbersome for &amp;ldquo;behind-the-scenes&amp;rdquo; boxplots (which is what I&amp;rsquo;ll use a lot today) and so I am presenting you with the base &lt;code&gt;R&lt;/code&gt; alternative.&lt;/p&gt;
&lt;h3 id=&#34;loading-data&#34;&gt;Loading Data&lt;/h3&gt;
&lt;p&gt;During our first exercise (Data Mining and Data Handling - Fixing The Sparrow Data Set) we saved our clean data set as an RDS file. To load this, we use the &lt;code&gt;readRDS()&lt;/code&gt; command that comes with base &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df_base &amp;lt;- readRDS(file = paste(Dir.Data, &amp;quot;/1 - Sparrow_Data_READY.rds&amp;quot;, sep=&amp;quot;&amp;quot;))
Data_df &amp;lt;- Data_df_base # duplicate and save initial data on a new object
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;kruskal-wallis-test&#34;&gt;Kruskal-Wallis Test&lt;/h2&gt;
&lt;h3 id=&#34;climate-warmingextremes&#34;&gt;Climate Warming/Extremes&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Does morphology of Passer domesticus depend on climate?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Using the Mann-Witney U Test in our last practical, we concluded that climate (when recorded as &amp;ldquo;Continental&amp;rdquo; and &amp;ldquo;Non-Continental&amp;rdquo;) is an important driver of &lt;em&gt;Passer domesticus&lt;/em&gt; morphology. Now we will see whether this holds true when considering non-continental climates as coastal and semi-coastal ones.&lt;/p&gt;
&lt;p&gt;Take note that we need to limit our analysis to our climate type testing sites again as follows (we include Manitoba this time as it is at the same latitude as the UK and Siberia and holds a semi-coastal climate type):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# prepare climate type testing data
Data_df &amp;lt;- Data_df_base
Index &amp;lt;- Data_df$Index
Rows &amp;lt;- which(Index == &amp;quot;SI&amp;quot; | Index == &amp;quot;UK&amp;quot; | Index == &amp;quot;RE&amp;quot; | Index == &amp;quot;AU&amp;quot; | Index == &amp;quot;MA&amp;quot;)
Data_df &amp;lt;- Data_df[Rows,]
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;weight&#34;&gt;Weight&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s start with weight records of common house sparrows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;WeightCont &amp;lt;- with(Data_df, Weight[which(Climate == &amp;quot;Continental&amp;quot;)])
WeightSemi &amp;lt;- with(Data_df, Weight[which(Climate == &amp;quot;Semi-Coastal&amp;quot;)])
WeightCoast &amp;lt;- with(Data_df, Weight[which(Climate == &amp;quot;Coastal&amp;quot;)])
Weights_vec &amp;lt;- c(WeightCont, WeightSemi, WeightCoast)

Climates &amp;lt;- c(
  rep(&amp;quot;Continental&amp;quot;, length(WeightCont)),
  rep(&amp;quot;Semi-Coastal&amp;quot;, length(WeightSemi)),
  rep(&amp;quot;Coastal&amp;quot;, length(WeightCoast))
)
Climates &amp;lt;- as.factor(Climates)

kruskal.test(Weights_vec, Climates)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Kruskal-Wallis rank sum test
## 
## data:  Weights_vec and Climates
## Kruskal-Wallis chi-squared = 150.98, df = 2, p-value &amp;lt; 2.2e-16
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We conclude that the three-level climate variable is an important source of information to understand what drives weight records of &lt;em&gt;Passer domesticus&lt;/em&gt; and thus &lt;strong&gt;reject the null hypothesis&lt;/strong&gt; (p = $1.6418184\times 10^{-33}$).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;boxplot(Weights_vec ~ Climates)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;11---Ordinal-and-Metric-Test--More-Than-Two-Sample-_files/figure-html/KruskalSite1b-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looking at the boxplot, we can understand the distribution of weight records as grouped by climate types and identify weight records to be biggest for continental climates followed by semi-coastal ones with similar spreads. Coastal climates, on the other hand, show a remarkable spread of weight records of &lt;em&gt;Passer domesticus&lt;/em&gt; with a markedly lower median than the two previous categories.&lt;/p&gt;
&lt;h4 id=&#34;height&#34;&gt;Height&lt;/h4&gt;
&lt;p&gt;Secondly, let&amp;rsquo;s repeat the above Kruskal-Wallis Test for the height/length records of our &lt;em&gt;Passer domesticus&lt;/em&gt; individuals:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;HeightCont &amp;lt;- with(Data_df, Height[which(Climate == &amp;quot;Continental&amp;quot;)])
HeightSemi &amp;lt;- with(Data_df, Height[which(Climate == &amp;quot;Semi-Coastal&amp;quot;)])
HeightCoast &amp;lt;- with(Data_df, Height[which(Climate == &amp;quot;Coastal&amp;quot;)])
Heights_vec &amp;lt;- c(HeightCont, HeightSemi, HeightCoast)

Climates &amp;lt;- c(
  rep(&amp;quot;Continental&amp;quot;, length(HeightCont)),
  rep(&amp;quot;Semi-Coastal&amp;quot;, length(HeightSemi)),
  rep(&amp;quot;Coastal&amp;quot;, length(HeightCoast))
)
Climates &amp;lt;- as.factor(Climates)

kruskal.test(Heights_vec, Climates)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Kruskal-Wallis rank sum test
## 
## data:  Heights_vec and Climates
## Kruskal-Wallis chi-squared = 15.635, df = 2, p-value = 0.0004027
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;boxplot(Heights_vec ~ Climates)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;11---Ordinal-and-Metric-Test--More-Than-Two-Sample-_files/figure-html/KruskalSite2a-1.png&#34; width=&#34;1440&#34; /&gt;
We conclude that the three-level climate variable is an important source of information to understand what drives height records of &lt;em&gt;Passer domesticus&lt;/em&gt; and thus &lt;strong&gt;reject the null hypothesis&lt;/strong&gt; (p = $4.0267296\times 10^{-4}$).&lt;/p&gt;
&lt;p&gt;Looking at the boxplot, we can understand the distribution of height records as grouped by climate types and identify height records to be smallest for continental climates followed by semi-coastal ones with similar spreads. Coastal climates, on the other hand, show a remarkable spread of height records of &lt;em&gt;Passer domesticus&lt;/em&gt; with a markedly higher median than the two previous categories.&lt;/p&gt;
&lt;h4 id=&#34;wing-chord&#34;&gt;Wing Chord&lt;/h4&gt;
&lt;p&gt;Third, we will test whether climate is a good predictor for wing chord of common house sparrows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Wing.ChordCont &amp;lt;- with(Data_df, Wing.Chord[which(Climate == &amp;quot;Continental&amp;quot;)])
Wing.ChordSemi &amp;lt;- with(Data_df, Wing.Chord[which(Climate == &amp;quot;Semi-Coastal&amp;quot;)])
Wing.ChordCoast &amp;lt;- with(Data_df, Wing.Chord[which(Climate == &amp;quot;Coastal&amp;quot;)])
Wing.Chords_vec &amp;lt;- c(Wing.ChordCont, Wing.ChordSemi, Wing.ChordCoast)

Climates &amp;lt;- c(
  rep(&amp;quot;Continental&amp;quot;, length(Wing.ChordCont)),
  rep(&amp;quot;Semi-Coastal&amp;quot;, length(Wing.ChordSemi)),
  rep(&amp;quot;Coastal&amp;quot;, length(Wing.ChordCoast))
)
Climates &amp;lt;- as.factor(Climates)

kruskal.test(Wing.Chords_vec, Climates)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Kruskal-Wallis rank sum test
## 
## data:  Wing.Chords_vec and Climates
## Kruskal-Wallis chi-squared = 41.539, df = 2, p-value = 9.548e-10
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;boxplot(Wing.Chords_vec ~ Climates)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;11---Ordinal-and-Metric-Test--More-Than-Two-Sample-_files/figure-html/KruskalSite3a-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We conclude that the three-level climate variable is an important source of information to understand what drives wing chord records of &lt;em&gt;Passer domesticus&lt;/em&gt; and thus &lt;strong&gt;reject the null hypothesis&lt;/strong&gt; (p = $9.5482279\times 10^{-10}$).&lt;/p&gt;
&lt;p&gt;Looking at the boxplot, we can understand the distribution of wing chord records as grouped by climate types and identify wing chord records to be smallest for continental climates followed by semi-coastal ones with similar spreads. Coastal climates, on the other hand, show a remarkable spread of wing chord records of &lt;em&gt;Passer domesticus&lt;/em&gt; with a markedly higher median than the two previous categories.&lt;/p&gt;
&lt;h4 id=&#34;automating-the-analysis&#34;&gt;Automating the Analysis&lt;/h4&gt;
&lt;p&gt;As we have seen, running seperate tests for every research question may be a bit cumbersome and so we may want to automate the analysis by establishing our own user-defined function as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;AutomatedKruskal &amp;lt;- function(Variables, Groups, Plotting){
# establish data frame to save results to
Export &amp;lt;- data.frame(
  Variables = Variables,
  Grouped_by = rep(Groups, length(Variables)),
  Chi_Squared = rep(NA, length(Variables)),
  DF = rep(NA, length(Variables)),
  p_value = rep(NA, length(Variables))
)

for(i in 1:length(Variables)){
# extract data and groups from data frame
YData &amp;lt;- Data_df[,which(colnames(Data_df)==Variables[i])]
XData &amp;lt;- Data_df[,which(colnames(Data_df)==Groups)]

# establish a list holding our groups for our data
Data &amp;lt;- list()
Grouping &amp;lt;- list()
for(k in 1:length(unique(XData))){
  Data[[k]] &amp;lt;- YData[which(XData == unique(XData)[k])]
  Grouping[[k]] &amp;lt;- rep(unique(XData)[k], length = length(Data[[k]]))
} # end of k-loop

Data &amp;lt;- unlist(Data)
Grouping &amp;lt;- unlist(Grouping)

# fill data frame
Export[i, 3] &amp;lt;- kruskal.test(Data, Grouping)[[&amp;quot;statistic&amp;quot;]][[&amp;quot;Kruskal-Wallis chi-squared&amp;quot;]]
Export[i, 4] &amp;lt;- kruskal.test(Data, Grouping)[[&amp;quot;parameter&amp;quot;]]
Export[i, 5] &amp;lt;- kruskal.test(Data, Grouping)[[&amp;quot;p.value&amp;quot;]]

# optional plotting
if(Plotting == TRUE){
plot(Data ~ factor(Grouping), ylab = Variables[i])
}

} # end of i loop

# return data frame to R outside of function
return(Export)
} # end of function
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function is &lt;em&gt;named&lt;/em&gt; &lt;code&gt;AutomatedKruskal()&lt;/code&gt; and takes three &lt;em&gt;arguments&lt;/em&gt;: (1) &lt;code&gt;Variables&lt;/code&gt; - a vector of character typed identifiers for the variables we want to have tested, (2) &lt;code&gt;Groups&lt;/code&gt; - a character string identifying the grouping variable, (3) &lt;code&gt;Plotting&lt;/code&gt; - a logical statement (&lt;code&gt;TRUE&lt;/code&gt; or &lt;code&gt;FALSE&lt;/code&gt;) whether boxplots shall be produced.&lt;/p&gt;
&lt;p&gt;The function then proceeds to establish an empty data frame which it will store the results of our Kurskal-Wallis Tests in. Afterwards, it cycles through all variables contained within the &lt;code&gt;Variables&lt;/code&gt; statement, extracts the relevant data, grouping it according to the specified grouping variable (&lt;code&gt;Groups&lt;/code&gt;), runs the test, fills the data frame and plots the data if &lt;code&gt;Plotting&lt;/code&gt; has been set to &lt;code&gt;TRUE&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s re-run our earlier test on sparrow morphology as influenced by climate using this function by &lt;em&gt;calling&lt;/em&gt; it:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;par(mfrow = c(3,1)) # adjust plotting panes
AutomatedKruskal(Variables = c(&amp;quot;Weight&amp;quot;, &amp;quot;Height&amp;quot;, &amp;quot;Wing.Chord&amp;quot;), 
                 Groups = &amp;quot;Climate&amp;quot;, 
                 Plotting = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;11---Ordinal-and-Metric-Test--More-Than-Two-Sample-_files/figure-html/KruskalSite7b-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##    Variables Grouped_by Chi_Squared DF      p_value
## 1     Weight    Climate   150.97901  2 1.641818e-33
## 2     Height    Climate    15.63477  2 4.026730e-04
## 3 Wing.Chord    Climate    41.53899  2 9.548228e-10
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we can see from the results above, our function works flawlessly and we can use it going ahead.&lt;/p&gt;
&lt;p&gt;Furthermore, we can confirm some of the results of our Mann-Whitney U Test from last seminar.&lt;/p&gt;
&lt;h3 id=&#34;predation&#34;&gt;Predation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Does nesting height depend on predator characteristics?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Again, using the Mann-WHitney U Test in our last exercise, we identified both predator presence as well as predator type to be important predictors for nesting height of &lt;em&gt;Passer domesticus&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;We may wish to use the entirety of our data set again for this purpose:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df &amp;lt;- Data_df_base
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the Kruskal-Wallis Test, we can combine these two predictors by turning every record of predator type that is recorded as &lt;code&gt;NA&lt;/code&gt; into &amp;ldquo;None&amp;rdquo; which will then serve as an identifier for the absence of any predators effectively making the predator presence variable redundant:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# changing levels in predator type
levels(Data_df$Predator.Type) &amp;lt;- c(levels(Data_df$Predator.Type), &amp;quot;None&amp;quot;)
Data_df$Predator.Type[which(is.na(Data_df$Predator.Type))] &amp;lt;- &amp;quot;None&amp;quot;

# running analysis
AutomatedKruskal(Variables = &amp;quot;Nesting.Height&amp;quot;, Groups = &amp;quot;Predator.Type&amp;quot;, Plotting = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;11---Ordinal-and-Metric-Test--More-Than-Two-Sample-_files/figure-html/KruskalPredationa-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##        Variables    Grouped_by Chi_Squared DF      p_value
## 1 Nesting.Height Predator.Type    88.81797  2 5.169206e-20
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using our &lt;code&gt;Automated Kruskal()&lt;/code&gt; function, we can conclude that the aggregation of predator presence to predator type records serve as an excellent predictor for sparrow nesting height and &lt;strong&gt;reject the null hypothesis&lt;/strong&gt; (p = $5.169206\times 10^{-20}$).&lt;/p&gt;
&lt;p&gt;Therefore, we can argue that avian predation forces sparrows into low nesting sites, non-avian predation leads to more elevated nesting sites in &lt;em&gt;Passer domesticus&lt;/em&gt; and absence of predators seems to not force nesting height in any direction or restricting its spread.&lt;/p&gt;
&lt;h3 id=&#34;competition&#34;&gt;Competition&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Does home range depend on climate?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Having used the Mann-Whitney U Test to identify possible climate-driven changes in home ranges of &lt;em&gt;Passer domesticus&lt;/em&gt; in our last seminar, we concluded that climate types largely affect home ranges of the common house sparrow.&lt;/p&gt;
&lt;p&gt;Take note that we need to limit our analysis to our climate type testing sites again as follows (we include Manitoba this time as it is at the same latitude as the UK and Siberia and holds a semi-coastal climate type):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# prepare climate type testing data
Data_df &amp;lt;- Data_df_base
Index &amp;lt;- Data_df$Index
Rows &amp;lt;- which(Index == &amp;quot;SI&amp;quot; | Index == &amp;quot;UK&amp;quot; | Index == &amp;quot;RE&amp;quot; | Index == &amp;quot;AU&amp;quot; | Index == &amp;quot;MA&amp;quot;)
Data_df &amp;lt;- Data_df[Rows,]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s test this for our three-level climate variable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df$Home.Range &amp;lt;- as.numeric(factor(Data_df$Home.Range))
AutomatedKruskal(Variables = &amp;quot;Home.Range&amp;quot;, Groups = &amp;quot;Climate&amp;quot;, Plotting = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;11---Ordinal-and-Metric-Test--More-Than-Two-Sample-_files/figure-html/KruskalCompetitiona-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##    Variables Grouped_by Chi_Squared DF    p_value
## 1 Home.Range    Climate    6.243918  2 0.04407075
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using our &lt;code&gt;Automated Kruskal()&lt;/code&gt; function, we can conclude that the three-loevel climate variable serves as an excellent predictor for sparrow home range and &lt;strong&gt;reject the null hypothesis&lt;/strong&gt; (p = $0.0440707$) thus being at odds with our Mann-Whitney U results (that were only based on two climate types).&lt;/p&gt;
&lt;p&gt;Remember that small numeric ranges mean large actual ranges in this set-up and so we can conclude that  climates force common house sparrows to adapt to bigger home ranges.&lt;/p&gt;
&lt;h2 id=&#34;friedman-test&#34;&gt;Friedman Test&lt;/h2&gt;
&lt;p&gt;We can analyse the significance of more than two population/sample medians of metric variables which are dependent of one another using the &lt;code&gt;friedman.test()&lt;/code&gt; function in base &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;preparing-data&#34;&gt;Preparing Data&lt;/h3&gt;
&lt;p&gt;Obviously, none of our data records are paired as such. Whilst one may want to make the argument that many characteristics of individuals that group together might be dependant on the expressions of themselves found throughout said group, we will not concentrate on this possibility within these practicals.&lt;/p&gt;
&lt;p&gt;Conclusively, we need &lt;strong&gt;additional data sets with truly paired records&lt;/strong&gt; of sparrows. Within our study set-up, think of a &lt;strong&gt;resettling experiment&lt;/strong&gt;, were you take &lt;em&gt;Passer domesticus&lt;/em&gt; individuals from one site, transfer them to another and check back with them after some time has passed to see whether some of their characteristics have changed in their expression.&lt;br&gt;
To this end, presume we have taken the entire &lt;em&gt;Passer domesticus&lt;/em&gt; population found at our &lt;strong&gt;Siberian&lt;/strong&gt; research station and moved them to &lt;strong&gt;Manitoba&lt;/strong&gt;. After a given time at their new location, we are again moving the population from Manitoba to the &lt;strong&gt;United Kingdom&lt;/strong&gt;. Whilst this keeps the latitude stable, the sparrows &lt;em&gt;now experience a semi-coastal climate followed by a coastal one instead of a continental one&lt;/em&gt;. After some time (let&amp;rsquo;s say: a year), we have come back and recorded all the characteristics for the same individuals again. Within our data, none of the original individuals have gone missing or died throghout our study period. This is usually not the case in nature and such records would need to be deleted from the data set.&lt;/p&gt;
&lt;p&gt;You will find the corresponding &lt;em&gt;new data&lt;/em&gt; in &lt;code&gt;2a - Sparrow_ResettledSIMA_READY.rds&lt;/code&gt; (Siberia to Manitoba) and &lt;code&gt;2b - Sparrow_ResettledSIUK_READY.rds&lt;/code&gt; (former SIberian population from manitoba to the UK). Take note that these sets only contain records for the transferred individuals in the &lt;strong&gt;same order&lt;/strong&gt; as in the old data set.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df_SIMA &amp;lt;- readRDS(file = paste(Dir.Data, &amp;quot;/2a - Sparrow_ResettledSIMA_READY.rds&amp;quot;, sep=&amp;quot;&amp;quot;))
Data_df_SIUK &amp;lt;- readRDS(file = paste(Dir.Data, &amp;quot;/2b - Sparrow_ResettledSIUK_READY.rds&amp;quot;, sep=&amp;quot;&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With these data records, we can now re-evaluate how the characteristics of sparrows can change when subjected to different conditions than previously thus shedding light on their &lt;strong&gt;plasticity&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;As such, this program is very reminiscent of the resettling program in our last exercise when using Wilcoxon Signed Rank Test to account for plasticity of our sparrow individuals. This new program includes the additional step of transferring sparrows via Manitoba first. Why have we chosen this order of resettlements?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Our stations SI, MA and UK are all on roughly the same latitude.&lt;/li&gt;
&lt;li&gt;Moving the sparrows from SI to UK via MA results in them experiencing a gradient from continental to semi-coastal to coastal climate.&lt;/li&gt;
&lt;li&gt;Whilst Siberia is populated by avian predators, no predators are present at Manitoba and our sparrows are subject to non-avian predation in the UK.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;All of this serves to &lt;strong&gt;maximise variation&lt;/strong&gt; that we want to research whilst &lt;strong&gt;minising constraining factors&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;climate-warmingextremes-1&#34;&gt;Climate Warming/Extremes&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Does sparrow morphology change depend on climate?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Thinking back to out Wilcoxon Signed Rank test, we can already argue that weight records of sparrows should change according to climate whilst height and wing chord records should remain unaltered for every individual.&lt;/p&gt;
&lt;p&gt;We may wish to use the entirety of our data set again for this purpose:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df &amp;lt;- Data_df_base
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since this involves testing three seperate criterions of sparrow morphology, we again establish a user-defined function. This one is called &lt;code&gt;AutomatedFried()&lt;/code&gt; and has dropped the &lt;code&gt;Groups&lt;/code&gt; argument that was present in &lt;code&gt;AutomatedKruskal()&lt;/code&gt; since the grouping will always be our three stations:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;AutomatedFried &amp;lt;- function(Variables, Plotting){
# establish data frame to save results to
Export &amp;lt;- data.frame(
  Variables = Variables,
  Grouped_by = rep(&amp;quot;Resettling&amp;quot;, length(Variables)),
  Chi_Squared = rep(NA, length(Variables)),
  DF = rep(NA, length(Variables)),
  p_value = rep(NA, length(Variables))
)

for(i in 1:length(Variables)){
# extract data and groups from data frame
YDataSI &amp;lt;- Data_df[,which(colnames(Data_df)==Variables[i])]
YDataMA &amp;lt;- Data_df_SIMA[,which(colnames(Data_df)==Variables[i])]
YDataUK &amp;lt;- Data_df_SIUK[,which(colnames(Data_df)==Variables[i])]

Data &amp;lt;- matrix(c(YDataSI[which(Data_df$Index == &amp;quot;SI&amp;quot;)],
          YDataMA, YDataUK), nrow = dim(Data_df_SIMA)[1],
       byrow = FALSE, dimnames = list(1:dim(Data_df_SIMA)[1], 
                                      c(&amp;quot;SI&amp;quot;, &amp;quot;MA&amp;quot;, &amp;quot;UK&amp;quot;))
  )

# fill data frame
Export[i, 3] &amp;lt;- friedman.test(Data)[[&amp;quot;statistic&amp;quot;]][[&amp;quot;Friedman chi-squared&amp;quot;]]
Export[i, 4] &amp;lt;- friedman.test(Data)[[&amp;quot;parameter&amp;quot;]]
Export[i, 5] &amp;lt;- friedman.test(Data)[[&amp;quot;p.value&amp;quot;]]

# optional plotting
if(Plotting == TRUE){
  
  # prepare plotting data
  PlotData &amp;lt;- as.vector(Data)
  Grouping &amp;lt;- as.factor(
    rep(c(&amp;quot;SI&amp;quot;, &amp;quot;MA&amp;quot;, &amp;quot;UK&amp;quot;), each = dim(Data_df_SIMA)[1])
    )
  # plotting
  plot(PlotData ~ Grouping, ylab = Variables[i])
}
} # end of i loop

# return data frame to R outside of function
return(Export)
} # end of function
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As such, the above function operates a lot like the earlier user-defined counterpart for the Kruskal-Wallis Test. It returns the important test characteristics and allows for plots. Internally, however, it is built on a matrix rather than vectors.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s get to testing our prediction:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;par(mfrow = c(3,1)) # adjust plotting panes
AutomatedFried(Variables = c(&amp;quot;Weight&amp;quot;, &amp;quot;Height&amp;quot;, &amp;quot;Wing.Chord&amp;quot;), Plotting = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;11---Ordinal-and-Metric-Test--More-Than-Two-Sample-_files/figure-html/FriedClimateb-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##    Variables Grouped_by Chi_Squared DF      p_value
## 1     Weight Resettling    97.84848  2 5.655506e-22
## 2     Height Resettling         NaN  2          NaN
## 3 Wing.Chord Resettling         NaN  2          NaN
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, whilst climate is a good predictor for the weight of resettled sparrows (weight in continental climates is higher than in semi-coastal or coastal ones), height and wing chord records couldn&amp;rsquo;t be properly tested on using the &lt;code&gt;friedman.test()&lt;/code&gt; function since they have remained unalterd. Thetrefore, we &lt;strong&gt;reject the null hypothesis&lt;/strong&gt; for weight records of &lt;em&gt;Passer domesticus&lt;/em&gt; and &lt;strong&gt;accept the null hypothesis&lt;/strong&gt; for height and wing chord records.&lt;/p&gt;
&lt;h3 id=&#34;predation-1&#34;&gt;Predation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Does nesting height depend on predator characteristics?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;According to the results of our last practical, we would assume &lt;em&gt;Passer doemsticus&lt;/em&gt; to adhere to local conditions when chosing a nesting site and corresponding nesting height depending on predator presence:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;AutomatedFried(Variables = &amp;quot;Nesting.Height&amp;quot;, Plotting = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;11---Ordinal-and-Metric-Test--More-Than-Two-Sample-_files/figure-html/FriedPreda-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##        Variables Grouped_by Chi_Squared DF     p_value
## 1 Nesting.Height Resettling      10.864  2 0.004374338
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Just like we expected, nesting height of resettled sparrows depends hugely on predator presence at the sties they have been moved to (p = $7.3991389\times 10^{-19}$) and we &lt;strong&gt;reject the null hypothesis&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;competition-1&#34;&gt;Competition&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Does home range depend on climate?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As we&amp;rsquo;ve seen in our last seminar, a statistically signficant change in home ranges did not occur when resettling Siberian sparrows directly to the UK. How about when we resettle them via Manitoba?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df_SIMA$Home.Range &amp;lt;- as.numeric(Data_df_SIMA$Home.Range)
Data_df_SIUK$Home.Range &amp;lt;- as.numeric(Data_df_SIUK$Home.Range)

AutomatedFried(Variables = &amp;quot;Home.Range&amp;quot;, Plotting = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in xy.coords(x, y, xlabel, ylabel, log): NAs introduced by coercion
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;11---Ordinal-and-Metric-Test--More-Than-Two-Sample-_files/figure-html/FriedCompetitiona-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##    Variables Grouped_by Chi_Squared DF      p_value
## 1 Home.Range Resettling         132  2 2.170522e-29
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With our three-step resettling program we do record a statistically significant change in home ranges of our sparrow flocks and &lt;strong&gt;reject the null hypothesis&lt;/strong&gt; (p = $2.170522\times 10^{-29}$).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 12</title>
      <link>https://www.erikkusch.com/courses/rethinking/chapter-12/</link>
      <pubDate>Thu, 25 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/rethinking/chapter-12/</guid>
      <description>&lt;h1 id=&#34;monsters--mixtures&#34;&gt;Monsters &amp;amp; Mixtures&lt;/h1&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/rethinking/14__26-03-2021_SUMMARY_-Hybrid-_-Ordered-Models.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides Chapter 12&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to the exercises at the end of chapter 12 in 
&lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Satistical Rethinking 2&lt;/a&gt; by Richard McElreath. I have created these notes as a part of my ongoing involvement in the 
&lt;a href=&#34;https://www.erikkusch.com/project/aubayes/&#34;&gt;AU Bayes Study Group&lt;/a&gt;. Much of my inspiration for these solutions, where necessary, has been obtained from&lt;/p&gt;
&lt;!-- [Taras Svirskyi](https://github.com/jffist/statistical-rethinking-solutions/blob/master/ch10_hw.R), [William Wolf](https://github.com/cavaunpeu/statistical-rethinking/blob/master/chapter-10/homework.R), and [Corrie Bartelheimer](https://www.samples-of-thoughts.com/projects/statistical-rethinking/chapter_10/chp10-ex/) as well as  --&gt;
&lt;p&gt;the solutions provided to instructors by Richard McElreath himself.&lt;/p&gt;
&lt;h2 id=&#34;r-environment&#34;&gt;&lt;code&gt;R&lt;/code&gt; Environment&lt;/h2&gt;
&lt;p&gt;For today&amp;rsquo;s exercise, I load the following packages:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rethinking)
library(rstan)
library(ggplot2)
library(tidybayes)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;easy-exercises&#34;&gt;Easy Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-e1&#34;&gt;Practice E1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  What is the difference between an ordered categorical variable and an unordered one? Define and then give an example of each.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;Ordered&lt;/em&gt; categorical variables are those which are expressed on ordinal scales. Not very helpful, right? Well, these are variables which establish a pre-defined number of distinct outcomes. These may be expressed as numbers, but don&amp;rsquo;t have to be. What&amp;rsquo;s special about these variables is that their values (i.e. categories) can be ordered meaningfully from one extreme to the another without implying equal distances between the values. As an example, think of sparrow (&lt;em&gt;Passer domesticus&lt;/em&gt;) weight. We may have measured these in grams (as a continuous variable), but now want to model simply whether our sparrows are light-, medium-, or heavy-weights. This is an ordered categorical variable because we now how they can be ordered from one extreme to another, but we don&amp;rsquo;t assume that a sparrow has to become heavier by the same margin to classify as medium-weight instead of light-weight than to classify as heavy-weight instead of medium-weight.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Unordered&lt;/em&gt; categorical variables are much like their ordered counterpart, but come with an important distinction: we cannot order these in any meaningful way. Again, think of the common house sparrow (&lt;em&gt;Passer domesticus&lt;/em&gt;) and their colouration patterns. We may want to record them as overall black, brown, or grey. These are categories, but we certainly cannot order these from one extreme to another.&lt;/p&gt;
&lt;h3 id=&#34;practice-e2&#34;&gt;Practice E2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  What kind of link function does an ordered logistic regression employ? How does it differ from an ordinary logit link?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;br&gt;
Cumulative logit - $OrdLogit( = \phi, K)$. This link function defines a number of $K-1$ cumulative, proportional probabilities ($\phi$) of each outcome category. The $K-1$ $\phi_i$ sum up to 1 so that the $\phi_K = 1$ and can subsequently be dropped from the model. The link thus states that the linear model defines the log-cumulative odds of an event.&lt;/p&gt;
&lt;h3 id=&#34;practice-e3&#34;&gt;Practice E3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  When count data are zero-inflated, using a model that ignores zero-inflation will tend to induce which kind of inferential error?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;br&gt;
Under-prediction of the true rate of events. Zero-inflation means that counts of zero arise through more than one process at least one of which is not accounted for in our model. Subsequently our estimate of the true rate will be pushed closer to 0 than it truly is.&lt;/p&gt;
&lt;h3 id=&#34;practice-e4&#34;&gt;Practice E4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Over-dispersion is common in count data. Give an example of a natural process that might produce over-dispersed counts. Can you also give an example of a process that might produce under- dispersed counts?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;br&gt;
Over-dispersion often comes about as a result of heterogeneity in rates across different sampling units/systems. As an example, think of lizard counts in different patches of a dryland area over a given period of time. The resulting count data will likely be over-dispersed since the rate at which lizards are observed will vary strongly across the different study sites.&lt;/p&gt;
&lt;p&gt;Under-dispersion, on the other hand, shows less variation in the rates than would be expected. This is often the case when autocrrelation plays a role. For example, if we track our lizard abundances at each patch through time in half-day intervals, we are likely to end up with highly autocorrelated and under-dispersed counts/rates for each study site.&lt;/p&gt;
&lt;h2 id=&#34;medium-exercises&#34;&gt;Medium Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-m1&#34;&gt;Practice M1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  At a certain university, employees are annually rated from 1 to 4 on their productivity, with 1 being least productive and 4 most productive. In a certain department at this certain university in a certain year, the numbers of employees receiving each rating were (from 1 to 4): 12, 36, 7, 41. Compute the log cumulative odds of each rating.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;n &amp;lt;- c(12, 36, 7, 41) # assignment
q &amp;lt;- n / sum(n) # proportions
p &amp;lt;- cumsum(q) # cumulative proportions
o &amp;lt;- p / (1 - p) # cumulative odds
log(o) # log-cumulative odds
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -1.9459101  0.0000000  0.2937611        Inf
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-m2&#34;&gt;Practice M2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Make a version of Figure 12.5 for the employee ratings data given just above.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# plot raw proportions
plot(1:4, p,
  xlab = &amp;quot;rating&amp;quot;, ylab = &amp;quot;cumulative proportion&amp;quot;,
  xlim = c(0.7, 4.3), ylim = c(0, 1), xaxt = &amp;quot;n&amp;quot;, cex = 3
)
axis(1, at = 1:4, labels = 1:4)
# plot gray cumulative probability lines
for (x in 1:4) lines(c(x, x), c(0, p[x]), col = &amp;quot;gray&amp;quot;, lwd = 4)
# plot blue discrete probability segments
for (x in 1:4) lines(c(x, x) + 0.1, c(p[x] - q[x], p[x]), col = &amp;quot;slateblue&amp;quot;, lwd = 4)
# add number labels
text(1:4 + 0.2, p - q / 2, labels = 1:4, col = &amp;quot;slateblue&amp;quot;, cex = 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-25-statistical-rethinking-chapter-12_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;practice-m3&#34;&gt;Practice M3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Can you modify the derivation of the zero-inflated Poisson distribution (ZIPoisson) from the chapter to construct a zero-inflated binomial distribution?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;br&gt;
Let&amp;rsquo;s remind ourselves of the ZI-Poisson distribution from the chapter:&lt;/p&gt;
&lt;p&gt;$$ ZIPoisson(p, \lambda) $$&lt;/p&gt;
&lt;p&gt;with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;p = probability of no count generating process occurring&lt;/li&gt;
&lt;li&gt;Î» = rate at which counts are produced when process occurs&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is an extension of the Poisson distribution which is in itself a special version of the Binomial distribution with many trials and a low success rate. The zero-inflated Poisson may also be expressed as ($F$ and $S$ indicate binomial process has succeeded or failed in prohibiting the poisson process from happening, respectively):&lt;/p&gt;
&lt;p&gt;$$Pr(0|p_0, Î») = Pr(F|p_0) + Pr(S|p_0)*Pr(0|Î») = p_0 + (1 â p_0) * exp(âÎ»)$$
For zero-observations.&lt;/p&gt;
&lt;p&gt;$$Pr(y|y&amp;gt;0,p_0,\lambda) = Pr(S|p_0)(0) + Pr(F|p_0)*Pr(y|\lambda) = (1-p_0)\frac{\lambda^yexp(-\lambda)}{y!}$$
For non zero-observations.&lt;/p&gt;
&lt;p&gt;So how do we now get to a binomial specification here? By changing the Poisson likelihood ($exp(âÎ»)$) to a Binomial likelihood ($(1 â q)^n$ with $q$ denoting the probability of success):&lt;/p&gt;
&lt;p&gt;$$Pr(0|p_0, q, n) = p_0 + (1 â p_0)(1 â q)^n$$
For zero-observations.&lt;/p&gt;
&lt;p&gt;$$Pr(y|p_0, q, n) = (1 â p_0) Binom(y, n, q) = (1 â p_0) \frac{n!}{y!(n â y)!}*q^y(1 â q)^{nây}$$
For non zero-observations.&lt;/p&gt;
&lt;h2 id=&#34;hard-exercises&#34;&gt;Hard Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-h1&#34;&gt;Practice H1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  In 2014, a paper was published that was entitled âFemale hurricanes are deadlier than male hurricanes.â As the title suggests, the paper claimed that hurricanes with female names have caused greater loss of life, and the explanation given is that people unconsciously rate female hurricanes as less dangerous and so are less likely to evacuate. Statisticians severely criticized the paper after publication. Here, youâll explore the complete data
used in the paper and consider the hypothesis that hurricanes with female names are deadlier. Load the data with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rethinking)
data(Hurricanes)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Acquaint yourself with the columns by inspecting the help &lt;code&gt;?Hurricanes&lt;/code&gt;. In this problem, youâll focus on predicting deaths using &lt;code&gt;femininity&lt;/code&gt; of each hurricaneâs name.&lt;/p&gt;
&lt;p&gt;Fit and interpret the simplest possible model, a Poisson model of deaths using &lt;code&gt;femininity&lt;/code&gt; as a predictor. You can use &lt;code&gt;quap&lt;/code&gt; or &lt;code&gt;ulam&lt;/code&gt;. Compare the model to an intercept-only Poisson model of deaths. How strong is the association between femininity of name and deaths? Which storms does the model fit (&lt;code&gt;retrodict&lt;/code&gt;) well? Which storms does it fit poorly?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; First, let&amp;rsquo;s prepare the data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;d &amp;lt;- Hurricanes # load data on object called d
d$fem_std &amp;lt;- (d$femininity - mean(d$femininity)) / sd(d$femininity) # standardised femininity
dat &amp;lt;- list(D = d$deaths, F = d$fem_std)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have standardised data for the feminity of our hurricane names which makes priors easier to formulate, we can specify our initial model idea:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# model formula
f &amp;lt;- alist(
  D ~ dpois(lambda), # poisson outcome distribution
  log(lambda) &amp;lt;- a + bF * F, # log-link for lambda with linear model
  # priors in log-space, 0 corresponds to outcome of 1
  a ~ dnorm(1, 1),
  bF ~ dnorm(0, 1)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But are these priors any good? Let&amp;rsquo;s simulate them why don&amp;rsquo;t we:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;N &amp;lt;- 1e3
a &amp;lt;- rnorm(N, 1, 1)
bF &amp;lt;- rnorm(N, 0, 1)
F_seq &amp;lt;- seq(from = -2, to = 2, length.out = 30) # sequence from -2 to 2 because femininity data is standardised
plot(NULL,
  xlim = c(-2, 2), ylim = c(0, 500),
  xlab = &amp;quot;name femininity (std)&amp;quot;, ylab = &amp;quot;deaths&amp;quot;
)
for (i in 1:N) {
  lines(F_seq,
    exp(a[i] + bF[i] * F_seq), # inverse link to get outcome scale
    col = grau(), lwd = 1.5
  )
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-25-statistical-rethinking-chapter-12_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;d think that&amp;rsquo;s pretty alright. We allow for both positive and negative trends between death toll and femininity of hurricane name, but don&amp;rsquo;t have a lot of explosive trends in our priors. These strong trends are quite unintuitive. Our vast majority of trends however are very ambiguous and so I proceed with these priors and run the model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mH1 &amp;lt;- ulam(f, data = dat, chains = 4, cores = 4, log_lik = TRUE)
precis(mH1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         mean         sd      5.5%     94.5%    n_eff     Rhat4
## a  2.9994821 0.02344260 2.9612043 3.0356100 1145.726 0.9982237
## bF 0.2386957 0.02488145 0.1994521 0.2784917 1371.036 0.9996256
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So according to this, there is a positive relationship between hurricane name femininity and death toll. Which hurricanes do we actually retrodict well, though? Let&amp;rsquo;s plot, this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# plot raw data
plot(dat$F, dat$D,
  pch = 16, lwd = 2,
  col = rangi2, xlab = &amp;quot;femininity (std)&amp;quot;, ylab = &amp;quot;deaths&amp;quot;
)
# compute model-based trend
pred_dat &amp;lt;- list(F = seq(from = -2, to = 2, length.out = 1e2))
lambda &amp;lt;- link(mH1, data = pred_dat) # predict deaths
lambda.mu &amp;lt;- apply(lambda, 2, mean) # get mean prediction
lambda.PI &amp;lt;- apply(lambda, 2, PI) # get prediction interval
# superimpose trend
lines(pred_dat$F, lambda.mu)
shade(lambda.PI, pred_dat$F)
# compute sampling distribution
deaths_sim &amp;lt;- sim(mH1, data = pred_dat) # simulate posterior observations
deaths_sim.PI &amp;lt;- apply(deaths_sim, 2, PI) # get simulation interval
# superimpose sampling interval as dashed lines
lines(pred_dat$F, deaths_sim.PI[1, ], lty = 2)
lines(pred_dat$F, deaths_sim.PI[2, ], lty = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-25-statistical-rethinking-chapter-12_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Ok. There is quite a bit to unpack here. First of all, our model does not retrodict many of the hurricanes well even though it is quite certain of its predictions (grey shaded area which is hardly visible). Quite obviously, this model misses many of the hurricane death tolls to the right hand side of the above plot. This is a clear sign of over-dispersion which our model failed to account for. The weak, positive trend we are seeing here seems to be informed largely by these highly influential data points. We can assess whether and how influential some data points are with the Paraeto-K values (anything above 1 indicates an influential data point) following:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(as.data.frame(PSISk(mH1)), aes(x = PSISk(mH1))) +
  stat_halfeye() +
  theme_bw() +
  labs(title = &amp;quot;Paraeto-K values&amp;quot;, subtitle = &amp;quot;Values &amp;gt; 1 indicate highly influential data&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-25-statistical-rethinking-chapter-12_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Boy! Some hurricanes really do drive our model to a big extent!&lt;/p&gt;
&lt;h3 id=&#34;practice-h2&#34;&gt;Practice H2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Counts are nearly always over-dispersed relative to Poisson. So fit a gamma-Poisson (aka negative-binomial) model to predict deaths using &lt;code&gt;femininity&lt;/code&gt;. Show that the over-dispersed model no longer shows as precise a positive association between femininity and deaths, with an 89% interval that overlaps zero. Can you explain why the association diminished in strength?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; To start this off, I load the library and data again, so much of the exercise and my solutions can stand by itself:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rethinking)
data(Hurricanes)
d &amp;lt;- Hurricanes # load data on object called d
d$fem_std &amp;lt;- (d$femininity - mean(d$femininity)) / sd(d$femininity) # standardised femininity
dat &amp;lt;- list(D = d$deaths, F = d$fem_std)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, with the data prepared, we fit our model - the same model as before just with a different outcome distribution:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mH2 &amp;lt;- ulam(
  alist(
    D ~ dgampois(lambda, scale),
    log(lambda) &amp;lt;- a + bF * F,
    a ~ dnorm(1, 1),
    bF ~ dnorm(0, 1),
    scale ~ dexp(1) # strictly positive hence why exponential prior
  ),
  data = dat, chains = 4, cores = 4, log_lik = TRUE
)
precis(mH2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            mean         sd        5.5%     94.5%    n_eff     Rhat4
## a     2.9780290 0.15272205  2.73722408 3.2334449 1908.835 0.9989114
## bF    0.2107239 0.15442227 -0.04075382 0.4566408 1737.246 0.9993676
## scale 0.4532751 0.06226293  0.35889278 0.5579919 1897.214 1.0003374
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Cool. Our previously identified positive relationship between standardised femininity of hurricane name and death toll is still there albeit slightly diminished in magnitude. However, the credible interval around it has widened considerably and overlaps zero now.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s compare the estimates of our models side by side:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(coeftab(mH1, mH2))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-25-statistical-rethinking-chapter-12_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;These shows quite clearly how our new model is much more uncertain of the parameters.&lt;/p&gt;
&lt;p&gt;So what about the predictions of this new model? I plot them the exact same way as previously:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# plot raw data
plot(dat$F, dat$D,
  pch = 16, lwd = 2,
  col = rangi2, xlab = &amp;quot;femininity (std)&amp;quot;, ylab = &amp;quot;deaths&amp;quot;
)
# compute model-based trend
pred_dat &amp;lt;- list(F = seq(from = -2, to = 2, length.out = 1e2))
lambda &amp;lt;- link(mH2, data = pred_dat)
lambda.mu &amp;lt;- apply(lambda, 2, mean)
lambda.PI &amp;lt;- apply(lambda, 2, PI)
# superimpose trend
lines(pred_dat$F, lambda.mu)
shade(lambda.PI, pred_dat$F)
# compute sampling distribution
deaths_sim &amp;lt;- sim(mH2, data = pred_dat)
deaths_sim.PI &amp;lt;- apply(deaths_sim, 2, PI)
# superimpose sampling interval as dashed lines
lines(pred_dat$F, deaths_sim.PI[1, ], lty = 2)
lines(pred_dat$F, deaths_sim.PI[2, ], lty = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-25-statistical-rethinking-chapter-12_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;1440&#34; /&gt;
What&amp;rsquo;s there left to say other than: &amp;ldquo;Look at that increased uncertainty of our model&amp;rdquo; at this point? Well, we can talk about the accuracy of our predictions. They still blow. The uncertainty of our model is nice and all, but with a predictive accuracy like this why would we trust the model?&lt;/p&gt;
&lt;p&gt;For now, let&amp;rsquo;s turn to the conceptual part of this exercise: &amp;ldquo;Why has the association diminished with the new model?&amp;rdquo; The question comes down to understanding what the gamma distribution does to our model. The gamma distribution allows for a death rate to be calculated for each outcome individually rather than one overall death rate for all hurricanes. These individual rates are sampled from a common distribution which is a function of the femininity of hurricane names. As a matter of fact, we can plot this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;post &amp;lt;- extract.samples(mH2)
par(mfrow = c(1, 3))
for (fem in -1:1) {
  for (i in 1:1e2) {
    curve(dgamma2(
      x, # where to calculate density
      exp(post$a[i] + post$bF[i] * fem), # linear model with inverse link applied
      post$scale[i] # scale for gamma
    ),
    from = 0, to = 70, xlab = &amp;quot;mean deaths&amp;quot;, ylab = &amp;quot;Density&amp;quot;,
    ylim = c(0, 0.19), col = col.alpha(&amp;quot;black&amp;quot;, 0.2),
    add = ifelse(i == 1, FALSE, TRUE)
    )
  }
  mtext(concat(&amp;quot;femininity  =   &amp;quot;, fem))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-25-statistical-rethinking-chapter-12_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;These are the gamma distributions samples from the posterior distribution of death rates when assuming same femininity of name for all of them at three different levels of femininity. Yes, a distribution sampled from another distribution. The above plots simply show the uncertainty of which gamma distribution to settle on.&lt;/p&gt;
&lt;p&gt;Since our model and gamma distributions are informed by &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;bF&lt;/code&gt;, and the scale for the gamma distribution at the same time many combinations of &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;bF&lt;/code&gt; are consistent with the data which results in a wider posterior distribution.&lt;/p&gt;
&lt;p&gt;Finally, let&amp;rsquo;s look at Paraeto-K values and potentially influential data again:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(as.data.frame(PSISk(mH2)), aes(x = PSISk(mH2))) +
  stat_halfeye() +
  theme_bw() +
  labs(title = &amp;quot;Paraeto-K values&amp;quot;, subtitle = &amp;quot;Values &amp;gt; 1 indicate highly influential data&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-25-statistical-rethinking-chapter-12_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;MUCH BETTER than before!&lt;/p&gt;
&lt;h3 id=&#34;practice-h3&#34;&gt;Practice H3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  In order to infer a strong association between deaths and femininity, itâs necessary to include an interaction effect. In the data, there are two measures of a hurricaneâs potential to cause death: &lt;code&gt;damage_norm&lt;/code&gt; and &lt;code&gt;min_pressure&lt;/code&gt;. Consult &lt;code&gt;?Hurricanes&lt;/code&gt; for their meanings. It makes some sense to imagine that femininity of a name matters more when the hurricane is itself deadly. This implies an interaction between &lt;code&gt;femininity&lt;/code&gt; and either or both of &lt;code&gt;damage_norm&lt;/code&gt; and &lt;code&gt;min_pressure&lt;/code&gt;. Fit a series of models evaluating these interactions. Interpret and compare the models. In interpreting the estimates, it may help to generate counterfactual predictions contrasting hurricanes with masculine and feminine names. Are the effect sizes plausible?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  To start this off, I load the library and data again, so much of the exercise and my solutions can stand by itself:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rethinking)
data(Hurricanes)
d &amp;lt;- Hurricanes # load data on object called d
d$fem_std &amp;lt;- (d$femininity - mean(d$femininity)) / sd(d$femininity) # standardised femininity
dat &amp;lt;- list(D = d$deaths, F = d$fem_std)
dat$P &amp;lt;- standardize(d$min_pressure)
dat$S &amp;lt;- standardize(d$damage_norm)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data is ready and I step into my model fitting procedure. Here, I start with a basic model which builds on the previous gamma-Poisson model by adding an interaction between &lt;code&gt;femininity&lt;/code&gt; and &lt;code&gt;min_pressure&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mH3a &amp;lt;- ulam(
  alist(
    D ~ dgampois(lambda, scale),
    log(lambda) &amp;lt;- a + bF * F + bP * P + bFP * F * P,
    a ~ dnorm(1, 1),
    c(bF, bP, bFP) ~ dnorm(0, 1),
    scale ~ dexp(1)
  ),
  data = dat, cores = 4, chains = 4, log_lik = TRUE
)
precis(mH3a)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean         sd        5.5%      94.5%    n_eff     Rhat4
## a      2.7499528 0.13739615  2.53479738  2.9731015 2057.085 0.9990744
## bFP    0.2991240 0.14883183  0.06678460  0.5242145 1849.512 1.0007366
## bP    -0.6715712 0.13597028 -0.88624645 -0.4539281 1894.311 1.0002580
## bF     0.3027293 0.14148646  0.08381879  0.5332170 1953.726 0.9999575
## scale  0.5523986 0.08078761  0.42818191  0.6867295 1985.855 0.9987496
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As minimum pressure gets lower, a storm grows stronger (I was confused by that myself when answering these exercises). Quite obviously, the lower the pressure in a storm, the more severe the storm, and the more people die which is reflected by the negative value in &lt;code&gt;bP&lt;/code&gt;. &lt;code&gt;bF&lt;/code&gt; is still estimated to be positive. This time, the interval doesn&amp;rsquo;t even overlap zero. Meanwhile, the interaction effect &lt;code&gt;bFP&lt;/code&gt; is positive. I find it hard to interpret this so I&amp;rsquo;d rather plot some predictions against real data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;P_seq &amp;lt;- seq(from = -3, to = 2, length.out = 1e2) # pressure sequence
# &#39;masculine&#39; storms
d_pred &amp;lt;- data.frame(F = -1, P = P_seq)
lambda_m &amp;lt;- link(mH3a, data = d_pred)
lambda_m.mu &amp;lt;- apply(lambda_m, 2, mean)
lambda_m.PI &amp;lt;- apply(lambda_m, 2, PI)
# &#39;feminine&#39; storms
d_pred &amp;lt;- data.frame(F = 1, P = P_seq)
lambda_f &amp;lt;- link(mH3a, data = d_pred)
lambda_f.mu &amp;lt;- apply(lambda_f, 2, mean)
lambda_f.PI &amp;lt;- apply(lambda_f, 2, PI)
# Plotting, sqrt() to make differences easier to spot, can&#39;t use log because there are storm with zero deaths
plot(dat$P, sqrt(dat$D),
  pch = 1, lwd = 2, col = ifelse(dat$F &amp;gt; 0, &amp;quot;red&amp;quot;, &amp;quot;dark gray&amp;quot;),
  xlab = &amp;quot;minimum pressure (std)&amp;quot;, ylab = &amp;quot;sqrt(deaths)&amp;quot;
)
lines(P_seq, sqrt(lambda_m.mu), lty = 2)
shade(sqrt(lambda_m.PI), P_seq)
lines(P_seq, sqrt(lambda_f.mu), lty = 1, col = &amp;quot;red&amp;quot;)
shade(sqrt(lambda_f.PI), P_seq, col = col.alpha(&amp;quot;red&amp;quot;, 0.2))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-25-statistical-rethinking-chapter-12_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Our model expects masculine (grey) storms to be less deadly, on average, than feminine (red) ones. As pressure drops (toward the rightward side of the plot above), these differences become smaller and smaller. Quite evidently, some of these storms are influencing what our model predicts much more so than others:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(as.data.frame(PSISk(mH3a)), aes(x = PSISk(mH3a))) +
  stat_halfeye() +
  theme_bw() +
  labs(title = &amp;quot;Paraeto-K values&amp;quot;, subtitle = &amp;quot;Values &amp;gt; 1 indicate highly influential data&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-25-statistical-rethinking-chapter-12_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s turn to the second variable we may want to add &lt;code&gt;damage_norm&lt;/code&gt; - the damage caused by each storm:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mH3b &amp;lt;- ulam(
  alist(
    D ~ dgampois(lambda, scale),
    log(lambda) &amp;lt;- a + bF * F + bS * S + bFS * F * S,
    a ~ dnorm(1, 1),
    c(bF, bS, bFS) ~ dnorm(0, 1),
    scale ~ dexp(1)
  ),
  data = dat, chains = 4, cores = 4, log_lik = TRUE
)
precis(mH3b)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean         sd        5.5%     94.5%    n_eff     Rhat4
## a     2.56677402 0.12946975  2.36261687 2.7748946 1903.061 0.9995145
## bFS   0.30853835 0.20499020 -0.04173134 0.6265386 2243.101 0.9994129
## bS    1.25058627 0.21057791  0.92538673 1.5951083 1956.116 1.0003102
## bF    0.08485749 0.12501328 -0.11660977 0.2820823 1963.489 1.0005148
## scale 0.68529101 0.09803179  0.53525802 0.8466209 2165.010 0.9990485
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That just eradicated the effect of femininity of hurricane name (&lt;code&gt;bF&lt;/code&gt;)! The newly added interaction parameter &lt;code&gt;bFS&lt;/code&gt; is incredibly strong and positive. Again, let&amp;rsquo;s visualise this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;S_seq &amp;lt;- seq(from = -1, to = 5.5, length.out = 1e2) # damage sequence
# &#39;masculine&#39; storms
d_pred &amp;lt;- data.frame(F = -1, S = S_seq)
lambda_m &amp;lt;- link(mH3b, data = d_pred)
lambda_m.mu &amp;lt;- apply(lambda_m, 2, mean)
lambda_m.PI &amp;lt;- apply(lambda_m, 2, PI)
# &#39;feminine&#39; storms
d_pred &amp;lt;- data.frame(F = 1, S = S_seq)
lambda_f &amp;lt;- link(mH3b, data = d_pred)
lambda_f.mu &amp;lt;- apply(lambda_f, 2, mean)
lambda_f.PI &amp;lt;- apply(lambda_f, 2, PI)
# plot
plot(dat$S, sqrt(dat$D),
  pch = 1, lwd = 2, col = ifelse(dat$F &amp;gt; 0, &amp;quot;red&amp;quot;, &amp;quot;dark gray&amp;quot;),
  xlab = &amp;quot;normalized damage (std)&amp;quot;, ylab = &amp;quot;sqrt(deaths)&amp;quot;
)
lines(S_seq, sqrt(lambda_m.mu), lty = 2)
shade(sqrt(lambda_m.PI), S_seq)
lines(S_seq, sqrt(lambda_f.mu), lty = 1, col = &amp;quot;red&amp;quot;)
shade(sqrt(lambda_f.PI), S_seq, col = col.alpha(&amp;quot;red&amp;quot;, 0.2))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-25-statistical-rethinking-chapter-12_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can clearly see how our model makes less of a distinction between masculine and feminine hurricanes overall at this point. Damage norm scales multiplicatively. The distances grow fast as we approach the rightward side of the plot. This is difficult for the model to account for. Hence why the model is underwhelming.&lt;/p&gt;
&lt;p&gt;So why is the interaction effect so strong? Probably because of those 3-4 highly influential feminine storms at the upper-righthand corner of our plot above which implies that feminine storms are especially deadly when they are damaging to begin with. Personally, I don&amp;rsquo;t trust this association and would argue that there is no logical reason for it and most likely an artefact of the limited data availability.&lt;/p&gt;
&lt;h3 id=&#34;practice-h4&#34;&gt;Practice H4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  In the original hurricanes paper, storm damage (&lt;code&gt;damage_norm&lt;/code&gt;) was used directly. This assumption implies that mortality increases exponentially with a linear increase in storm strength, because a Poisson regression uses a log link. So itâs worth exploring an alternative hypothesis: that the logarithm of storm strength is what matters. Explore this by using the logarithm of &lt;code&gt;damage_norm&lt;/code&gt; as a predictor. Using the best model structure from the previous problem, compare a model that uses &lt;code&gt;log(damage_norm)&lt;/code&gt; to a model that uses &lt;code&gt;damage_norm&lt;/code&gt; directly. Compare their DIC/WAIC values as well as their implied predictions. What do you conclude?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; To start this off, I load the library and data again, so much of the exercise and my solutions can stand by itself:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rethinking)
data(Hurricanes)
d &amp;lt;- Hurricanes # load data on object called d
d$fem_std &amp;lt;- (d$femininity - mean(d$femininity)) / sd(d$femininity) # standardised femininity
dat &amp;lt;- list(D = d$deaths, F = d$fem_std)
dat$S2 &amp;lt;- standardize(log(d$damage_norm))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s fit the model as before and compare it to the previously identified best model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mH4 &amp;lt;- ulam(
  alist(
    D ~ dgampois(lambda, scale),
    log(lambda) &amp;lt;- a + bF * F + bS * S2 + bFS * F * S2,
    a ~ dnorm(1, 1),
    c(bF, bS, bFS) ~ dnorm(0, 1),
    scale ~ dexp(1)
  ),
  data = dat, chains = 4, cores = 4, log_lik = TRUE
)
compare(mH3b, mH4, func = PSIS)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          PSIS       SE    dPSIS      dSE    pPSIS       weight
## mH4  630.7019 31.19102  0.00000       NA 5.376913 1.000000e+00
## mH3b 670.6361 34.19897 39.93425 13.67922 6.857192 2.130043e-09
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Model &lt;code&gt;mH4&lt;/code&gt; clearly outperforms the earlier (non-logarithmic) model &lt;code&gt;mH3b&lt;/code&gt;. How do the parameter estimates look in comparison?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(coeftab(mH3b, mH4),
  labels = paste(rep(rownames(coeftab(mH3b, mH4)@coefs), each = 2),
    rep(c(&amp;quot;Norm&amp;quot;, &amp;quot;Log&amp;quot;), nrow(coeftab(mH3b, mH4)@coefs) * 2),
    sep = &amp;quot;-&amp;quot;
  )
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-25-statistical-rethinking-chapter-12_files/figure-html/unnamed-chunk-25-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With the log-transformed input, &lt;code&gt;bFS&lt;/code&gt; has increased in magnitude. What do the resulting predictions look like?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;S2_seq &amp;lt;- seq(from = -3, to = 1.8, length.out = 1e2)
# &#39;masculine&#39; storms
d_pred &amp;lt;- data.frame(F = -1, S2 = S2_seq)
lambda_m &amp;lt;- link(mH4, data = d_pred)
lambda_m.mu &amp;lt;- apply(lambda_m, 2, mean)
lambda_m.PI &amp;lt;- apply(lambda_m, 2, PI)
# &#39;feminine&#39; storms
d_pred &amp;lt;- data.frame(F = 1, S2 = S2_seq)
lambda_f &amp;lt;- link(mH4, data = d_pred)
lambda_f.mu &amp;lt;- apply(lambda_f, 2, mean)
lambda_f.PI &amp;lt;- apply(lambda_f, 2, PI)
# plot
plot(dat$S2, sqrt(dat$D),
  pch = 1, lwd = 2, col = ifelse(dat$F &amp;gt; 0, &amp;quot;red&amp;quot;, &amp;quot;dark gray&amp;quot;),
  xlab = &amp;quot;normalized damage (std)&amp;quot;, ylab = &amp;quot;sqrt(deaths)&amp;quot;
)
lines(S2_seq, sqrt(lambda_m.mu), lty = 2)
shade(sqrt(lambda_m.PI), S2_seq)
lines(S2_seq, sqrt(lambda_f.mu), lty = 1, col = &amp;quot;red&amp;quot;)
shade(sqrt(lambda_f.PI), S2_seq, col = col.alpha(&amp;quot;red&amp;quot;, 0.2))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-25-statistical-rethinking-chapter-12_files/figure-html/unnamed-chunk-26-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now this model fits the data much better! Still not perfect, but much better.&lt;/p&gt;
&lt;h3 id=&#34;practice-h5&#34;&gt;Practice H5&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  One hypothesis from developmental psychology, usually attributed to Carol Gilligan, proposes that women and men have different average tendencies in moral reasoning. Like most hypotheses in social psychology, it is merely descriptive. The notion is that women are more concerned with care (avoiding harm), while men are more concerned with justice and rights. Culture-bound nonsense? Yes. Descriptively accurate? Maybe.&lt;/p&gt;
&lt;p&gt;Evaluate this hypothesis, using the &lt;code&gt;Trolley&lt;/code&gt; data, supposing that contact provides a proxy for physical harm. Are women more or less bothered by contact than are men, in these data? Figure out the model(s) that is needed to address this question.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Again, let&amp;rsquo;s start by preparing the data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rethinking)
data(Trolley)
d &amp;lt;- Trolley
dat &amp;lt;- list(
  R = d$response,
  A = d$action,
  I = d$intention,
  C = d$contact
)
dat$Gid &amp;lt;- ifelse(d$male == 1, 1L, 2L)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now for a model. We use the same model skeleton as provided in the book. However, this time around, we want cutpoints in our ordered, cumulative logit for both genders separately:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dat$F &amp;lt;- 1L - d$male # indicator of femaleness to turn intercepts on and off
mH5 &amp;lt;- ulam(
  alist(
    R ~ dordlogit(phi, cutpoints),
    phi &amp;lt;- a * F + bA[Gid] * A + bC[Gid] * C + BI * I,
    BI &amp;lt;- bI[Gid] + bIA[Gid] * A + bIC[Gid] * C,
    c(bA, bI, bC, bIA, bIC)[Gid] ~ dnorm(0, 0.5),
    a ~ dnorm(0, 0.5),
    cutpoints ~ dnorm(0, 1.5)
  ),
  data = dat, chains = 4, cores = 4
)
precis(mH5, depth = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                     mean         sd        5.5%       94.5%     n_eff    Rhat4
## bIC[1]       -1.29000860 0.13002100 -1.50109161 -1.07614615 1348.3365 1.000146
## bIC[2]       -1.14842366 0.13434342 -1.36319985 -0.93345115 1163.2537 1.006455
## bIA[1]       -0.43821462 0.10411766 -0.60427628 -0.27043071 1201.4372 1.003107
## bIA[2]       -0.42253949 0.11300405 -0.59891439 -0.23526558 1156.5531 1.005345
## bC[1]        -0.47691217 0.09253044 -0.62813726 -0.32838280 1294.9094 1.000744
## bC[2]        -0.20730657 0.09595489 -0.36193206 -0.06036975 1207.3517 1.004505
## bI[1]        -0.33520753 0.07772555 -0.46098056 -0.20740598 1075.2158 1.003142
## bI[2]        -0.25819201 0.08096287 -0.39036524 -0.12766901 1026.1192 1.006898
## bA[1]        -0.59363382 0.07036947 -0.70451058 -0.48492259 1075.2269 1.005641
## bA[2]        -0.33605523 0.07724757 -0.45859501 -0.21053675 1205.2000 1.005800
## a            -0.78364935 0.08006100 -0.90628106 -0.65659045  948.2152 1.003790
## cutpoints[1] -3.02124197 0.06328453 -3.11858791 -2.91868150 1181.9794 1.002080
## cutpoints[2] -2.32745650 0.06074323 -2.42260517 -2.23098876 1175.2672 1.001935
## cutpoints[3] -1.72766666 0.05885444 -1.82181068 -1.63431341 1123.8787 1.002648
## cutpoints[4] -0.67165728 0.05700586 -0.76381567 -0.58315767 1112.3527 1.003052
## cutpoints[5]  0.01889978 0.05633424 -0.07146169  0.10708376 1122.8518 1.003504
## cutpoints[6]  0.94926239 0.05853165  0.85602268  1.04283590 1166.1198 1.002682
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The parameter estimates of interest here are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;a&lt;/code&gt; (-0.78) - main effect of being female on cumulative log-odds. On average women, have more moral qualms about the trolley problem it seems.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bC[2]&lt;/code&gt; (-0.21) - the interaction effect of being both female and in a contact scenario as opposed to &lt;code&gt;bc[1]&lt;/code&gt; (-0.48) which is the same scenario but for men. Women in our set-up had less moral issues with contact events.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The latter goes against the previously stated hypothesis. Why is that? Because the people in our study are much more complex than just their genders.&lt;/p&gt;
&lt;h3 id=&#34;practice-h6&#34;&gt;Practice H6&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  The data in &lt;code&gt;data(Fish)&lt;/code&gt; are records of visits to a national park. See &lt;code&gt;?Fish&lt;/code&gt; for details. The question of interest is how many fish an average visitor takes per hour, when fishing. The problem is that not everyone tried to fish, so the &lt;code&gt;fish_caught&lt;/code&gt; numbers are zero-inflated. As with the monks example in the chapter, there is a process that determines who is fishing (working) and another process that determines fish per hour (manuscripts per day), conditional on fishing (working). We want to model both. Otherwise weâll end up with an underestimate of rate of fish extraction from the park.&lt;/p&gt;
&lt;p&gt;You will model these data using zero-inflated Poisson GLMs. Predict &lt;code&gt;fish_caught&lt;/code&gt; as a function
of any of the other variables you think are relevant. One thing you must do, however, is use a proper Poisson offset/exposure in the Poisson portion of the zero-inflated model. Then use the hours variable to construct the offset. This will adjust the model for the differing amount of time individuals spent in the park.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; One last time, for this week, we prepare some data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rethinking)
data(Fish)
d &amp;lt;- Fish
str(d)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &#39;data.frame&#39;:	250 obs. of  6 variables:
##  $ fish_caught: int  0 0 0 0 1 0 0 0 0 1 ...
##  $ livebait   : int  0 1 1 1 1 1 1 1 0 1 ...
##  $ camper     : int  0 1 0 1 0 1 0 0 1 1 ...
##  $ persons    : int  1 1 1 2 1 4 3 4 3 1 ...
##  $ child      : int  0 0 0 1 0 2 1 3 2 0 ...
##  $ hours      : num  21.124 5.732 1.323 0.548 1.695 ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model I want to build will look at the following variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;fish_caught&lt;/code&gt;. This is our response variable.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;livebait&lt;/code&gt;. I suggest that using livebait increases number of fish caught.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;camper&lt;/code&gt;. I assume that being a camper increases the chances that one goes fishing, but not necessarily the number of fish caught.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;persons&lt;/code&gt;. I assume that the number of people in a group increases how many fish are caught.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;child&lt;/code&gt;. Being a child should reasonably determine both whether one fishes (I assume children fish less - call it personal bias) and also that they are less effective than adults.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hours&lt;/code&gt;. How long one fishes surely determines how many fish are caught. I want to include the base rate of these into my model of fish caught. Since said model will be log-linked, I log-transform them.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let me fit that model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;d$loghours &amp;lt;- log(d$hours)
mH6 &amp;lt;- ulam(
  alist(
    # outcome distribution
    fish_caught ~ dzipois(p, mu),
    # linear model for probability of fishing
    logit(p) &amp;lt;- a0 + bC0 * camper + bc0 * child,
    # linear model of catching a number of fish
    log(mu) &amp;lt;- a + bb * livebait + bp * persons + bc * child + bl * loghours,
    c(a0, a) ~ dnorm(0, 1),
    c(bC0, bc0, bb, bp, bc, bl) ~ dnorm(0, 0.5)
  ),
  data = d, chains = 4, cores = 4, log_lik = TRUE
)
precis(mH6)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           mean         sd       5.5%      94.5%     n_eff     Rhat4
## a   -2.0839466 0.24408479 -2.4814597 -1.6945336  899.9998 1.0016629
## a0  -0.4650216 0.28849629 -0.9436559 -0.0177434 1191.0483 1.0015268
## bl   0.1701858 0.03428906  0.1134792  0.2254237 1255.6829 0.9985511
## bc  -0.8280833 0.10732217 -0.9969186 -0.6569452 1218.3750 1.0024020
## bp   0.8625626 0.04553091  0.7919523  0.9330040 1479.7715 0.9990951
## bb   1.4076543 0.19237711  1.1106049  1.7217943 1169.0384 1.0032455
## bc0  0.9787908 0.23529584  0.6059568  1.3537809 1292.4802 0.9994360
## bC0 -0.6597597 0.29791643 -1.1313113 -0.1803763 1413.3364 1.0017523
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remember that $p$ stands for the probability of &lt;strong&gt;not&lt;/strong&gt; going to fish. Overall, park visitors are pretty likely to fish (&lt;code&gt;a0 =&lt;/code&gt;-0.47, logit scale). In line with my intuition, campers are more likely to fish (&lt;code&gt;bC0&lt;/code&gt;) while children are less likely to fish (&lt;code&gt;bc0&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Once one is actually fishing, one is not likely to catch much fish (&lt;code&gt;a=&lt;/code&gt;-2.08). The parameters pertaining to number of fish caught are expressed on log-scale and so transforming &lt;code&gt;a&lt;/code&gt; into the outcome scale (counts of fish) by exponentiating, an adult who fishes by themselves without livebait (this is what &lt;code&gt;a&lt;/code&gt; refers to) catches around 0.1249302 fish on average. More people catch more fish (&lt;code&gt;bp&lt;/code&gt;). Using livebait is effective (&lt;code&gt;bb&lt;/code&gt;). In line with my intuition, children catch less fish than adults (&lt;code&gt;bc&lt;/code&gt;). Lastly, the more time one spends fishing, the more fish one catches (&lt;code&gt;bl&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s turn to some actual model predictions of &lt;code&gt;p&lt;/code&gt; and &lt;code&gt;mu&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;zip_link &amp;lt;- link(mH6)
str(zip_link)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## List of 2
##  $ p : num [1:2000, 1:250] 0.403 0.388 0.469 0.382 0.443 ...
##  $ mu: num [1:2000, 1:250] 0.588 0.543 0.537 0.592 0.488 ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;p&lt;/code&gt; cases provide estimates for additional zeros not obtained through the actual Poisson-process behind fishing catches whose output lies with &lt;code&gt;mu&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For example, if $p =$0.39 (the inverse logit of &lt;code&gt;a0&lt;/code&gt; in the model above) and $Î¼ = 1$ (much higher than &lt;code&gt;a&lt;/code&gt; in the model above for ease here), then the implied predictive distribution is:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;zeros &amp;lt;- rbinom(
  n = 1e4, # number of samples
  size = 1,
  prob = inv_logit(precis(mH6)[2, 1]) # probability of going fishing
)
obs_fish &amp;lt;- (1 - zeros) * rpois(1e4, 1)
simplehist(obs_fish)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-03-25-statistical-rethinking-chapter-12_files/figure-html/unnamed-chunk-32-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s see what our model would predict given the estimated parameters:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;fish_sim &amp;lt;- sim(mH6)
str(fish_sim)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  num [1:1000, 1:250] 0 0 1 1 0 0 0 2 0 1 ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This now contains a simulation output for each sample in our data! We could now plot ourselves into oblivion with counterfactual plots. We can also produce counterfactual posterior predictions. As an example, I assume a party of 1 adult spending 1 hour in the park without any use of livebait and not staying the night as a camper:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# new data
pred_dat &amp;lt;- list(
  loghours = log(1), # note that this is zero, the baseline rate
  persons = 1,
  child = 0,
  livebait = 0,
  camper = 0
)
# sim predictions - want expected number of fish, but must use both processes
fish_link &amp;lt;- link(mH6, data = pred_dat)
# summarize
p &amp;lt;- fish_link$p
mu &amp;lt;- fish_link$mu
(expected_fish_mean &amp;lt;- mean((1 - p) * mu))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1838057
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(expected_fish_PI &amp;lt;- PI((1 - p) * mu))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        5%       94% 
## 0.1261989 0.2553580
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This tells us that this hypothetical person is expected to catch 0.1838057 fish with the interval displayed above.&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.5 (2021-03-31)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19043)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.1252  LC_CTYPE=English_United Kingdom.1252    LC_MONETARY=English_United Kingdom.1252 LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.1252    
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] tidybayes_2.3.1      rethinking_2.13      rstan_2.21.2         ggplot2_3.3.6        StanHeaders_2.21.0-7
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.7           mvtnorm_1.1-1        lattice_0.20-41      tidyr_1.1.3          prettyunits_1.1.1    ps_1.6.0             assertthat_0.2.1     digest_0.6.27        utf8_1.2.1          
## [10] V8_3.4.1             plyr_1.8.6           R6_2.5.0             backports_1.2.1      stats4_4.0.5         evaluate_0.14        coda_0.19-4          highr_0.9            blogdown_1.3        
## [19] pillar_1.6.0         rlang_0.4.11         curl_4.3.2           callr_3.7.0          jquerylib_0.1.4      R.utils_2.10.1       R.oo_1.24.0          rmarkdown_2.7        styler_1.4.1        
## [28] labeling_0.4.2       stringr_1.4.0        loo_2.4.1            munsell_0.5.0        compiler_4.0.5       xfun_0.22            pkgconfig_2.0.3      pkgbuild_1.2.0       shape_1.4.5         
## [37] htmltools_0.5.1.1    tidyselect_1.1.0     tibble_3.1.1         gridExtra_2.3        bookdown_0.22        arrayhelpers_1.1-0   codetools_0.2-18     matrixStats_0.61.0   fansi_0.4.2         
## [46] crayon_1.4.1         dplyr_1.0.5          withr_2.4.2          MASS_7.3-53.1        R.methodsS3_1.8.1    distributional_0.2.2 ggdist_2.4.0         grid_4.0.5           jsonlite_1.7.2      
## [55] gtable_0.3.0         lifecycle_1.0.0      DBI_1.1.1            magrittr_2.0.1       scales_1.1.1         RcppParallel_5.1.2   cli_3.0.0            stringi_1.5.3        farver_2.1.0        
## [64] bslib_0.2.4          ellipsis_0.3.2       generics_0.1.0       vctrs_0.3.7          rematch2_2.1.2       forcats_0.5.1        tools_4.0.5          svUnit_1.0.6         R.cache_0.14.0      
## [73] glue_1.4.2           purrr_0.3.4          processx_3.5.1       yaml_2.2.1           inline_0.3.17        colorspace_2.0-0     knitr_1.33           sass_0.3.1
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Simple Parametric Tests</title>
      <link>https://www.erikkusch.com/courses/an-introduction-to-biostatistics/simple-parametric-tests/</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/an-introduction-to-biostatistics/simple-parametric-tests/</guid>
      <description>&lt;h2 id=&#34;theory&#34;&gt;Theory&lt;/h2&gt;
&lt;p&gt;Welcome to our sixth practical experience in R. Throughout the following notes, I will introduce you to a couple of simple parametric test. Whilst parametric tests are used extremely often in biological statistics, they can be somewhat challenging to fit to your data as you will see soon. To do so, I will enlist the sparrow data set we handled in our first exercise. Additionally, todays seminar is showing plotting via base plot instead of &lt;code&gt;ggplot2&lt;/code&gt; to highlight the usefulness of base plot and show you the base notation.&lt;/p&gt;
&lt;p&gt;I have prepared some &lt;a href=&#34;https://www.erikkusch.com/courses/an-introduction-to-biostatistics/12---Simple-Parametric-Tests_Handout.html&#34; target=&#34;_blank&#34;&gt; Lecture Slides &lt;/a&gt; for this session.&lt;/p&gt;
&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;p&gt;Find the data for this exercise &lt;a href=&#34;https://www.erikkusch.com/courses/an-introduction-to-biostatistics/Data/1%20-%20Sparrow_Data_READY.rds&#34; target=&#34;_blank&#34;&gt; here&lt;/a&gt; and &lt;a href=&#34;https://www.erikkusch.com/courses/an-introduction-to-biostatistics/Data/2b%20-%20Sparrow_ResettledSIUK_READY.rds&#34; target=&#34;_blank&#34;&gt; here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;preparing-our-procedure&#34;&gt;Preparing Our Procedure&lt;/h2&gt;
&lt;p&gt;To ensure others can reproduce our analysis we run the following three lines of code at the beginning of our &lt;code&gt;R&lt;/code&gt; coding file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rm(list=ls()) # clearing environment
Dir.Base &amp;lt;- getwd() # soft-coding our working directory
Dir.Data &amp;lt;- paste(Dir.Base, &amp;quot;Data&amp;quot;, sep=&amp;quot;/&amp;quot;) # soft-coding our data directory 
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;packages&#34;&gt;Packages&lt;/h3&gt;
&lt;p&gt;Using the following, user-defined function, we install/load all the necessary packages into our current &lt;code&gt;R&lt;/code&gt; session.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# function to load packages and install them if they haven&#39;t been installed yet
install.load.package &amp;lt;- function(x) {
  if (!require(x, character.only = TRUE))
    install.packages(x)
  require(x, character.only = TRUE)
}
package_vec &amp;lt;- c(&amp;quot;car&amp;quot;) # needed for the Levene Test for Homogeneity
sapply(package_vec, install.load.package)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: car
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: carData
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  car 
## TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;loading-data&#34;&gt;Loading Data&lt;/h3&gt;
&lt;p&gt;During our first exercise (Data Mining and Data Handling - Fixing The Sparrow Data Set) we saved our clean data set as an RDS file. To load this, we use the &lt;code&gt;readRDS()&lt;/code&gt; command that comes with base &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df_base &amp;lt;- readRDS(file = paste(Dir.Data, &amp;quot;/1 - Sparrow_Data_READY.rds&amp;quot;, sep=&amp;quot;&amp;quot;))
Data_df &amp;lt;- Data_df_base # duplicate and save initial data on a new object
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;t-test-unpaired&#34;&gt;t-Test (unpaired)&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Assumptions of the unpaired t-Test:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Predictor variable is binary&lt;/li&gt;
&lt;li&gt;Response variable is metric and &lt;strong&gt;normal distributed&lt;/strong&gt; within their groups&lt;/li&gt;
&lt;li&gt;Variable values are &lt;strong&gt;independent&lt;/strong&gt; (not paired)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition, test whether variance of response variable values in groups are equal (&lt;code&gt;var.test()&lt;/code&gt;) and adjust &lt;code&gt;t.test()&lt;/code&gt; argument &lt;code&gt;var.equal&lt;/code&gt; accordingly.&lt;/p&gt;
&lt;h3 id=&#34;testing-for-normality-and-homogeneity&#34;&gt;Testing For Normality And Homogeneity&lt;/h3&gt;
&lt;p&gt;We need to test the distribution of our response variables within each predictor variable group for their normality and variance. Since this involves two Shapiro tests and one variance test per variable for each response variable, we might want to write our own function to do so:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ShapiroTest &amp;lt;- function(Variables, Grouping){
  Output &amp;lt;- data.frame(x = Variables)
  for(i in 1:length(Variables)){
    
    X &amp;lt;- Data_df[,Variables[i]]
    Levels &amp;lt;- levels(factor(Data_df[,Grouping]))
    
    Output[i,2] &amp;lt;- shapiro.test(X[which(Data_df[,Grouping] == Levels[1])])$p.value
    Output[i,3] &amp;lt;- shapiro.test(X[which(Data_df[,Grouping] == Levels[2])])$p.value
    Output[i,4] &amp;lt;- var.test(x = X[which(Data_df[,Grouping] == Levels[1])], 
                            y = X[which(Data_df[,Grouping] == Levels[2])])$p.value
  }
  colnames(Output) &amp;lt;- c(&amp;quot;Variable&amp;quot;, &amp;quot;P.value1&amp;quot;, &amp;quot;P.value2&amp;quot;, &amp;quot;Var.Test&amp;quot;)
  return(Output)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function (&lt;code&gt;ShapiroTest()&lt;/code&gt;) takes two arguments: (1) &lt;code&gt;Variables&lt;/code&gt; - a vector of characters holding the names of the variables we want to have tested, and (2) &lt;code&gt;Grouping&lt;/code&gt; - the binary variable by which to group our variables. The function returns a data frame holding the p-values of the Shapiro tests on each variable group values as well as the &lt;code&gt;var.test()&lt;/code&gt; p-value.&lt;/p&gt;
&lt;h3 id=&#34;climate-warmingextremes&#34;&gt;Climate Warming/Extremes&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Does sparrow morphology change depend on climate?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Using multiple different methods (i.e. Kruskal-Wallis and Mann-Whitney U Test), we have already identified climate (be it in its binary form or when recorded as a three-level variable) is a strong driving force of sparrow morphology. We expect the same results when using a t-Test.&lt;/p&gt;
&lt;p&gt;Take note that we need to limit our analysis to our climate type testing sites again as follows (we include Manitoba this time as it is at the same latitude as the UK and Siberia and holds a semi-coastal climate type):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# prepare climate type testing data
Data_df &amp;lt;- Data_df_base
Index &amp;lt;- Data_df$Index
Rows &amp;lt;- which(Index == &amp;quot;SI&amp;quot; | Index == &amp;quot;UK&amp;quot; | Index == &amp;quot;RE&amp;quot; | Index == &amp;quot;AU&amp;quot; | Index == &amp;quot;MA&amp;quot;)
Data_df &amp;lt;- Data_df[Rows,]
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;testing-for-normality-and-variance&#34;&gt;Testing for Normality and Variance&lt;/h4&gt;
&lt;p&gt;Before we can make use of our data with a t-Test, we need to do an &lt;strong&gt;assumption check&lt;/strong&gt;. To this end, we first turn &lt;code&gt;Climate&lt;/code&gt; records into a binary variable by turning records of a semi-coastal climate into a coastal one.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Make climate binary
Data_df$Climate[which(Data_df$Climate == &amp;quot;Semi-Coastal&amp;quot;)] &amp;lt;- &amp;quot;Coastal&amp;quot;
Data_df$Climate &amp;lt;- droplevels(factor(Data_df$Climate))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s make sure our assumptions are met:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ShapiroTest(Variables = c(&amp;quot;Weight&amp;quot;, &amp;quot;Height&amp;quot;, &amp;quot;Wing.Chord&amp;quot;), Grouping = &amp;quot;Climate&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Variable  P.value1  P.value2    Var.Test
## 1     Weight 0.1699442 0.2521182 0.326240416
## 2     Height 0.1676977 0.3645040 0.010632158
## 3 Wing.Chord 0.0538642 0.1722528 0.002942433
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Luckily, all of our variables allow for the calculation of t-Test. Take note though that some need different specification of the &lt;code&gt;var.equal&lt;/code&gt; argument than others.&lt;/p&gt;
&lt;h4 id=&#34;analyses&#34;&gt;Analyses&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Sparrow Weight&lt;/strong&gt;&lt;br&gt;
Let&amp;rsquo;s start with the weight of &lt;em&gt;Passer domesticus&lt;/em&gt; individuals as grouped by the climate type present at the site weights have been recorded at:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;t.test(Data_df$Weight ~ Data_df$Climate, var.equal = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Two Sample t-test
## 
## data:  Data_df$Weight by Data_df$Climate
## t = -14.852, df = 381, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -2.428439 -1.860640
## sample estimates:
##     mean in group Coastal mean in group Continental 
##                  31.23383                  33.37837
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;According to our analysis, which has us &lt;strong&gt;reject the null hypothesis&lt;/strong&gt;, we conclude that binary climate records are valuable information criteria for predicting sparrow weight with sparrows in coastal climates being lighter than sparrows in continental ones thus effectively varifying the results of our non-parametric approaches (Kruskal-Wallis, Mann-Whitney U).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sparrow Height&lt;/strong&gt;&lt;br&gt;
Let&amp;rsquo;s move on to the height of &lt;em&gt;Passer domesticus&lt;/em&gt; individuals as grouped by the climate type present at the site weights have been recorded at:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;t.test(Data_df$Height ~ Data_df$Climate, var.equal = FALSE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Welch Two Sample t-test
## 
## data:  Data_df$Height by Data_df$Climate
## t = -0.27916, df = 365.69, p-value = 0.7803
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.2329126  0.1750052
## sample estimates:
##     mean in group Coastal mean in group Continental 
##                  13.91670                  13.94565
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Confirming the results of our Mann-Whitney U Test, we &lt;strong&gt;accept the null hypothesis&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sparrow Wing Chord&lt;/strong&gt;&lt;br&gt;
Lastly, we test the wing chords of &lt;em&gt;Passer domesticus&lt;/em&gt; individuals as grouped by the climate type present at the site weights have been recorded at:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;t.test(Data_df$Wing.Chord ~ Data_df$Climate, var.equal = FALSE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Welch Two Sample t-test
## 
## data:  Data_df$Wing.Chord by Data_df$Climate
## t = -0.12285, df = 370.22, p-value = 0.9023
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.03985039  0.03516377
## sample estimates:
##     mean in group Coastal mean in group Continental 
##                  6.898696                  6.901039
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Without confirming the results of our Mann-Whitney U Test, we &lt;strong&gt;accept the null hypothesis&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;br&gt;
Here&amp;rsquo;s what we&amp;rsquo;ve learned from the t-Test so far:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sparrow weight depends on (binary) climate types&lt;/li&gt;
&lt;li&gt;Sparrow height does not depend on (binary) climate types&lt;/li&gt;
&lt;li&gt;Sparrow wing chord does not depend on (binary) climate types&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s end this by viusalising all of the data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;par(mfrow=c(2,2))
plot(Data_df$Weight ~ Data_df$Climate)
plot(Data_df$Height ~ Data_df$Climate)
plot(Data_df$Wing.Chord ~ Data_df$Climate)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;12---Simple-Parametric-Tests_files/figure-html/t-test1-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;sexual-dimorphism&#34;&gt;Sexual Dimorphism&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Does sparrow morphology change depend on Sex?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Using the Mann-Whitney U Test, we have already identified the sex of &lt;em&gt;Passer domesticus&lt;/em&gt; is a good information criterion for understanding sparrow weight but not sparrow height or wing chord. Let&amp;rsquo;s see if we can reproduce this using a t-Test approach.&lt;/p&gt;
&lt;p&gt;We may wish to use the entirety of our data set again for this purpose:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df &amp;lt;- Data_df_base
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;testing-for-normality-and-variance-1&#34;&gt;Testing for Normality and Variance&lt;/h4&gt;
&lt;p&gt;Again, before we can use our data in a t-Test for this purpose, we have to make sure that our assumptions are met. To this end, we can make use of our user defined &lt;code&gt;ShapiroTest()&lt;/code&gt; function as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ShapiroTest(Variables = c(&amp;quot;Weight&amp;quot;, &amp;quot;Height&amp;quot;, &amp;quot;Wing.Chord&amp;quot;), Grouping = &amp;quot;Sex&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Variable     P.value1     P.value2  Var.Test
## 1     Weight 2.878769e-21 1.744517e-21 0.7475085
## 2     Height 4.028475e-17 6.600273e-19 0.4006799
## 3 Wing.Chord 3.438104e-25 1.628147e-26 0.5554935
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As it turns out, our data does not allow for any t-Test (this happens often in real studies). However, we can create sex-driven subgroups within each site and test whether these meet the requirements for our t-Test. This is out of the scope of this course though and so we will skip it. Spoler alert: I have done this and the findings did not reveal anything we didn&amp;rsquo;t uncover so far.&lt;/p&gt;
&lt;!-- In order to do so, we need to do some minor tweaking to our `ShapiroTest()` function: --&gt;
&lt;!-- ```{r ShapiroSex} --&gt;
&lt;!-- ShapiroTestSites &lt;- function(Variables, Grouping){ --&gt;
&lt;!--   list &lt;- list() --&gt;
&lt;!--   for(k in 1:length(unique(Data_df$Index))){ --&gt;
&lt;!--     Output &lt;- data.frame(x = Variables) --&gt;
&lt;!--     Data &lt;- Data_df[which(Data_df$Index == unique(Data_df$Index)[k]), ] --&gt;
&lt;!--     for(i in 1:length(Variables)){ --&gt;
&lt;!--       X &lt;- Data[,Variables[i]] --&gt;
&lt;!--       Levels &lt;- levels(Data[,Grouping]) --&gt;
&lt;!--       Output[i,2] &lt;- shapiro.test(X[which(Data[,Grouping] == Levels[1])])$p.value --&gt;
&lt;!--       Output[i,3] &lt;- shapiro.test(X[which(Data[,Grouping] == Levels[2])])$p.value --&gt;
&lt;!--       Output[i,4] &lt;- var.test(x = X[which(Data_df[,Grouping] == Levels[1])],  --&gt;
&lt;!--                             y = X[which(Data_df[,Grouping] == Levels[2])]) --&gt;
&lt;!--     } --&gt;
&lt;!--     colnames(Output) &lt;- c(&#34;Variable&#34;, &#34;P.value1&#34;, &#34;P.value2&#34;, &#34;Var.Test&#34;) --&gt;
&lt;!--     list[[k]] &lt;- Output --&gt;
&lt;!--   } --&gt;
&lt;!--   return(list) --&gt;
&lt;!-- } --&gt;
&lt;!-- ``` --&gt;
&lt;!-- This function (`ShapiroTestSites()`) takes two arguments: (1) `Variables` - a vector of characters holding the names of the variables we want to have tested, and (2) `Grouping` - the binary variable by which to group our variables. The function returns a list of data frames for each site holding the p-values of the Shapiro tests on each variable group values as well as the `var.test()` p-value.  --&gt;
&lt;!--  --&gt;
&lt;!-- Let&#39;s put this function to the test: --&gt;
&lt;!-- ```{r ShapiroSex1} --&gt;
&lt;!-- ShapiroTestSites(Variables = c(&#34;Weight&#34;, &#34;Height&#34;, &#34;Wing.Chord&#34;), Grouping = &#34;Sex&#34;) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- With the exception for sparrow morphology records at:   --&gt;
&lt;!-- - Siberia (SI, height and wing chord of males)   --&gt;
&lt;!-- - Manitoba (MA, morphology of females)   --&gt;
&lt;!-- - South Africa (SA, morphology of females)   --&gt;
&lt;!-- all of our data groups variables are normal distributed with equal variances between the groups per site.   --&gt;
&lt;!-- Since our problematic sites are still relatively close to fulfilling our requirements of the data, we will use them going forward as if they did. --&gt;
&lt;!-- ### Analyses --&gt;
&lt;!-- Running three t-Tests (Weight, Height, Wing Chord) for each of our eleven sites is absolute mania! Therefore, we write our own function again that let&#39;s us apply the tests exactly the way we want to: --&gt;
&lt;!-- ```{r ttestSex0} --&gt;
&lt;!-- t_testSite &lt;- function(Variables, Grouping, data, VarEqual){ --&gt;
&lt;!--   Data &lt;- data --&gt;
&lt;!--   Index &lt;- unique(Data$Index) --&gt;
&lt;!--   Indexes &lt;- Data$Index --&gt;
&lt;!--   list &lt;- list() --&gt;
&lt;!--   for(i in 1:length(Variables)){ --&gt;
&lt;!--     Output &lt;- data.frame(NA) --&gt;
&lt;!--     for(k in 1:length(Index)){ --&gt;
&lt;!--       # data and test --&gt;
&lt;!--       X &lt;- Data[, Variables[i]][which(Indexes == Index[k])] --&gt;
&lt;!--       Y &lt;- Data[, Grouping][which(Indexes == Index[k])] --&gt;
&lt;!--       Test &lt;- t.test(X ~ Y, paired = FALSE, var.equal = VarEqual) --&gt;
&lt;!--       # filling data frame --&gt;
&lt;!--       Output[1,k] &lt;- Test[[&#34;p.value&#34;]] --&gt;
&lt;!--       Output[2,k] &lt;- Test[[&#34;estimate&#34;]][[1]] --&gt;
&lt;!--       Output[3,k] &lt;- Test[[&#34;estimate&#34;]][[2]] --&gt;
&lt;!--     } --&gt;
&lt;!--     # data frame to list --&gt;
&lt;!--     colnames(Output) &lt;- Index --&gt;
&lt;!--     rownames(Output) &lt;- c(&#34;p&#34;, &#34;Mean1&#34;, &#34;Mean2&#34;) --&gt;
&lt;!--     list[[i]] &lt;- Output --&gt;
&lt;!--   } --&gt;
&lt;!--   return(list) --&gt;
&lt;!-- } --&gt;
&lt;!-- ``` --&gt;
&lt;!-- This `t_testSite()` function takes four arguments: (1) `Variables` - a vector of characters holding the names of the variables we want to have tested, (2) `Grouping` - the binary variable by which to group our variables, (3) `data` - the data frame which contains the `Variables` and the `Grouping` factor, and (4) `VarEqual` - a logical indicator of whether to perform a t-Test assuming equal variance of the groups or not.   --&gt;
&lt;!-- The function returns a list of data frames (one per variable) containing the p-values of the unpaired t-Tests for each variable at every site as well as the predicted group means. --&gt;
&lt;!--  --&gt;
&lt;!-- Although our function `t_testSite()` can handle multiple variables at once, we will now use it on each of our morphological sparrow variables individually to disentangle them a bit easier:   --&gt;
&lt;!-- \begin{center} --&gt;
&lt;!-- \textit{Does sparrow weight depend on sex when assessed at each of our sites individually?} --&gt;
&lt;!-- \end{center} --&gt;
&lt;!-- ```{r tTestSex1} --&gt;
&lt;!-- t_testSite(Variables = &#34;Weight&#34;, Grouping = &#34;Sex&#34;, data = Data_df, VarEqual = TRUE) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- As it turns out, sex is a statistically significant predictor for sparrow weight at each site. This was to be expected. Using a binomial test in our second practical, we identified no bias in sexes for our sparrow populations. In addition, using the Mann-Whitney U-Test in our fourth practical, we identified sex to be an important information criterion for sparrow weight across all of our sites. Given these two conditions, we were expecting a results like the one presented here with males being, on average, heavier than females in *Passer domesticus* and we **reject the null hypothesis**.   --&gt;
&lt;!-- \hfill \linebreak --&gt;
&lt;!-- \begin{center} --&gt;
&lt;!-- \textit{Does sparrow height depend on sex when assessed at each of our sites individually?} --&gt;
&lt;!-- \end{center} --&gt;
&lt;!-- ```{r tTestSex2} --&gt;
&lt;!-- t_testSite(Variables = &#34;Height&#34;, Grouping = &#34;Sex&#34;, data = Data_df, VarEqual = TRUE) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- Like with our Mann-Whitney U-Test, we fail to identify a significant effect of sex on sparrow height records at each of our sites and so we **accept the null hypothesis**. --&gt;
&lt;!-- \hfill \linebreak --&gt;
&lt;!-- \begin{center} --&gt;
&lt;!-- \textit{Does sparrow wing chord depend on sex when assessed at each of our sites individually?} --&gt;
&lt;!-- \end{center} --&gt;
&lt;!-- ```{r tTestSex3} --&gt;
&lt;!-- t_testSite(Variables = &#34;Wing.Chord&#34;, Grouping = &#34;Sex&#34;, data = Data_df, VarEqual = TRUE) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- Like with our Mann-Whitney U-Test, we fail to identify a significant effect of sex on sparrow wing chord records at each of our sites and so we **accept the null hypothesis**. --&gt;
&lt;!--  --&gt;
&lt;h2 id=&#34;t-test-paired&#34;&gt;t-Test (paired)&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Assumptions of the paired t-Test:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Predictor variable is binary&lt;/li&gt;
&lt;li&gt;Response variable is metric&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Difference of response variable pairs&lt;/em&gt; is &lt;strong&gt;normal distributed&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Variable values are &lt;strong&gt;dependent&lt;/strong&gt; (paired)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;preparing-data&#34;&gt;Preparing Data&lt;/h3&gt;
&lt;p&gt;For this purpose, we need an &lt;strong&gt;additional data set with truly paired records&lt;/strong&gt; of sparrows and so we implement the same solution as we&amp;rsquo;ve used within our fourth seminar using the Wilcoxon Signed Rank Test. Within our study set-up, think of a &lt;strong&gt;resettling experiment&lt;/strong&gt;, were you take &lt;em&gt;Passer domesticus&lt;/em&gt; individuals from one site, transfer them to another and check back with them after some time has passed to see whether some of their characteristics have changed in their expression.&lt;br&gt;
To this end, presume we have taken the entire &lt;em&gt;Passer domesticus&lt;/em&gt; population found at our &lt;strong&gt;Siberian&lt;/strong&gt; research station and moved them to the &lt;strong&gt;United Kingdom&lt;/strong&gt;. Whilst this keeps the latitude stable, the sparrows &lt;em&gt;now experience a coastal climate instead of a continental one&lt;/em&gt;. After some time (let&amp;rsquo;s say: a year), we have come back and recorded all the characteristics for the same individuals again.&lt;/p&gt;
&lt;p&gt;You will find the corresponding &lt;em&gt;new data&lt;/em&gt; in &lt;code&gt;2b - Sparrow_ResettledSIUK_READY.rds&lt;/code&gt;. Take note that this set only contains records for the transferred individuals in the &lt;strong&gt;same order&lt;/strong&gt; as in the old data set.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df_Resettled &amp;lt;- readRDS(file = paste(Dir.Data, &amp;quot;/2b - Sparrow_ResettledSIUK_READY.rds&amp;quot;, sep=&amp;quot;&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since earlier analysis such as the Wilcoxon Signed Rank test (fourth practical) and the Friedman Test (fifth practical) showed that height and wing chord records do not change when sparrows are resettled at all, we have excluded these here and &lt;strong&gt;focus solely on sparrow weight&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;testing-for-normality&#34;&gt;Testing for Normality&lt;/h3&gt;
&lt;p&gt;Before being able to run our paired t-Test, we must make sure that the &lt;em&gt;difference of response variable pairs&lt;/em&gt; is &lt;strong&gt;normal distributed&lt;/strong&gt;. We can do so using the &lt;code&gt;shapiro.test()&lt;/code&gt; of base &lt;code&gt;R&lt;/code&gt; as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# selecting pre-resettling weights
DataSI &amp;lt;- Data_df$Weight[which(Data_df$Index == &amp;quot;SI&amp;quot;)]
# calculating difference of before and after resettling weights
WeightDiff &amp;lt;- DataSI-Data_df_Resettled$Weight
# shapiro test
shapiro.test(WeightDiff)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Shapiro-Wilk normality test
## 
## data:  WeightDiff
## W = 0.97361, p-value = 0.1716
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thankfully, the &lt;strong&gt;assumption of normality&lt;/strong&gt; is &lt;strong&gt;met&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Now let&amp;rsquo;s visualise that using a qqplot:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;qqnorm(WeightDiff)
qqline(WeightDiff)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;12---Simple-Parametric-Tests_files/figure-html/Norm3-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;climate-warmingextremes-1&#34;&gt;Climate Warming/Extremes&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Does sparrow morphology change depend on climate?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Now let&amp;rsquo;s go on to test whether sparrow weights change significantly per individual due to our relocation experiment (we expect this from future test in our practicals):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;t.test(DataSI, Data_df_Resettled$Weight, paired = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Paired t-test
## 
## data:  DataSI and Data_df_Resettled$Weight
## t = 8.4762, df = 65, p-value = 4.17e-12
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  1.583629 2.559914
## sample estimates:
## mean of the differences 
##                2.071771
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We were right, individual sparrow weights change significantly after our relocation experiment and we &lt;strong&gt;reject the null hypothesis&lt;/strong&gt;. This is in accordance with the results of the Wilcoxon Signed Rank Test as well as the Friedman Test.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s go on to visualise our data to make better sense of what is going on here:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Select the sparrow weights
Weights &amp;lt;- c(DataSI, Data_df_Resettled$Weight)
# Select the sites
Sites &amp;lt;- factor(rep(c(&amp;quot;SI&amp;quot;, &amp;quot;SI_UK&amp;quot;), each = length(DataSI)))
# Plot
plot(Weights ~ Sites)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;12---Simple-Parametric-Tests_files/figure-html/tpaired1-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Quite obviously sparrows observed in Siberia are heavier than when they are resettled to the United Kingdom (this may be due to the more forgiving climate in the UK). Just like the test stated, the difference of the average weights is roughly 2g between the sparrows at the two sites.&lt;/p&gt;
&lt;h2 id=&#34;one-way-anova&#34;&gt;One-Way ANOVA&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Assumptions of the One-Way ANOVA:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Predictor variable is categorical&lt;/li&gt;
&lt;li&gt;Response variable is metric&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Response variable residuals&lt;/em&gt; are &lt;strong&gt;normal distributed&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Variance of populations/samples are equal (&lt;strong&gt;homogeneity&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;Variable values are &lt;strong&gt;independent&lt;/strong&gt; (not paired)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;testing-for-assumptions&#34;&gt;Testing For Assumptions&lt;/h3&gt;
&lt;p&gt;Firstly, we need to test the assumptions of our One-Way ANOVA. For this purpose, we write another user-defined function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# User-defined function
ANOVACheck &amp;lt;- function(Variables, Grouping, data, plotting){
  Output &amp;lt;- data.frame(x = NA)
  for(i in 1:length(Variables)){
    # data
    Y &amp;lt;- as.numeric(factor(data[,Variables[i]]))
    X &amp;lt;- data[,Grouping]
    Levels &amp;lt;- levels(factor(Data_df[,Grouping]))
    # Residuals?
    model &amp;lt;- lm(Y ~ X)
    Output[1,i] &amp;lt;- shapiro.test(residuals(model))$p.value
    # Homgeneity?
    Levene &amp;lt;- leveneTest(Y ~ X, 
                         center = median, 
                         data = data)
    Output[2,i] &amp;lt;- Levene[1,3]
    # Plotting
    if(plotting == TRUE){
      plot(model, 2)# Normality
      plot(model, 3)# Homogeneity
    }
  }
  colnames(Output) &amp;lt;- Variables
  rownames(Output) &amp;lt;- c(&amp;quot;Residual Normality&amp;quot;, &amp;quot;Homogeneity of Variances&amp;quot;)
  return(Output)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This &lt;code&gt;ANOVACheck()&lt;/code&gt; function takes four arguments: (1) &lt;code&gt;Variables&lt;/code&gt; - a vector of characters holding the names of the variables we want to have tested, (2) &lt;code&gt;Grouping&lt;/code&gt; - the categorical variable by which to group our variables, (3) &lt;code&gt;data&lt;/code&gt; - the data frame which contains the &lt;code&gt;Variables&lt;/code&gt; and the &lt;code&gt;Grouping&lt;/code&gt; factor, and (4) &lt;code&gt;plotting&lt;/code&gt; - a logical indicator of whether to produce plots visualising the test results or not.&lt;br&gt;
The function returns a data frames containing the p-values indexing whether to accept or reject the notion of the normality of residuals per variable (&lt;code&gt;Residual Normality&lt;/code&gt;), and the p-values indexing whether variances between groups are homogeneous or not (&lt;code&gt;Homogeneity of Variances&lt;/code&gt;).&lt;/p&gt;
&lt;h3 id=&#34;climate-warmingextremes-2&#34;&gt;Climate Warming/Extremes&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Does sparrow morphology change depend on climate?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Using the Kruskal-Wallis Test in our last exercise, we already identified climate to be an important factor in determining &lt;em&gt;Passer domesticus&lt;/em&gt; morphology. Let&amp;rsquo;s see if this holds true.&lt;/p&gt;
&lt;p&gt;Take note that we need to limit our analysis to our climate type testing sites again as follows (we include Manitoba this time as it is at the same latitude as the UK and Siberia and holds a semi-coastal climate type):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# prepare climate type testing data
Data_df &amp;lt;- Data_df_base
Index &amp;lt;- Data_df$Index
Rows &amp;lt;- which(Index == &amp;quot;SI&amp;quot; | Index == &amp;quot;UK&amp;quot; | Index == &amp;quot;RE&amp;quot; | Index == &amp;quot;AU&amp;quot; | Index == &amp;quot;MA&amp;quot;)
Data_df &amp;lt;- Data_df[Rows,]
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;assumption-check&#34;&gt;Assumption Check&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s use the &lt;code&gt;ANOVACheck()&lt;/code&gt; function on our data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;par(mfrow=c(3,2))
ANOVACheck(Variables = c(&amp;quot;Weight&amp;quot;, &amp;quot;Height&amp;quot;, &amp;quot;Wing.Chord&amp;quot;), Grouping = &amp;quot;Climate&amp;quot;, data = Data_df, plotting = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in leveneTest.default(y = y, group = group, ...): group coerced to
## factor.

## Warning in leveneTest.default(y = y, group = group, ...): group coerced to
## factor.

## Warning in leveneTest.default(y = y, group = group, ...): group coerced to
## factor.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;12---Simple-Parametric-Tests_files/figure-html/AssCheck1b-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##                               Weight       Height  Wing.Chord
## Residual Normality       0.002521771 2.671414e-05 0.001685579
## Homogeneity of Variances 0.110120912 1.896577e-01 0.013575440
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unfortunately, neither weight nor wing chord records fullfil our requirements.&lt;/p&gt;
&lt;h4 id=&#34;analysis&#34;&gt;Analysis&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s run our analysis for height as grouped by the three-level climate variable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;model &amp;lt;- lm(Data_df$Height ~ Data_df$Climate)
anova(model)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Variance Table
## 
## Response: Data_df$Height
##                  Df Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## Data_df$Climate   2  14.99  7.4942  7.2494 0.0008129 ***
## Residuals       381 393.87  1.0338                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;According to this, climate is a meaningful predictor of height of sparrows and we &lt;strong&gt;reject the null hypothesis&lt;/strong&gt; thus confirming the results of our Kruskall-Wallis analysis.&lt;/p&gt;
&lt;p&gt;Now, let&amp;rsquo;s analyse the output a bit more in-depth:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(model)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Data_df$Height ~ Data_df$Climate)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.98994 -0.69815 -0.01475  0.67142  2.37045 
## 
## Coefficients:
##                             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)                 14.07994    0.07964 176.800  &amp;lt; 2e-16 ***
## Data_df$ClimateContinental  -0.13429    0.11426  -1.175  0.24060    
## Data_df$ClimateSemi-Coastal -0.56039    0.14755  -3.798  0.00017 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.017 on 381 degrees of freedom
## Multiple R-squared:  0.03666,	Adjusted R-squared:  0.0316 
## F-statistic: 7.249 on 2 and 381 DF,  p-value: 0.0008129
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The mean sparrow height in coastal climates is 14.0799387cm (this is our &lt;strong&gt;Intercept&lt;/strong&gt;/&lt;em&gt;Baseline&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;The mean sparrow height in continental climates is -0.1342893cm bigger than the &lt;strong&gt;Intercept&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;The mean sparrow height in semi-coastal climates is -0.5603864cm bigger than the &lt;strong&gt;Intercept&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Only the estimates in coastal and semi-coastal climates are statistically significant&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Personally, I would not place too much confidence in these results due to a couple of reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Our only semi-coastal site is on the northern hemisphere whereas two of our stations are located in the southern hemisphere&lt;/li&gt;
&lt;li&gt;Confounding factors such as population status might have an effect which we are not considering here&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s end this by plotting all of our data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;par(mfrow=c(2,2))
plot(Data_df$Weight ~ factor(Data_df$Climate))
plot(Data_df$Height ~ factor(Data_df$Climate))
plot(Data_df$Wing.Chord ~ factor(Data_df$Climate))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;12---Simple-Parametric-Tests_files/figure-html/Analysis1-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As you can see, the variances are definitely not equal between our groups which explains why part of our assumption test failed.&lt;/p&gt;
&lt;h3 id=&#34;predation&#34;&gt;Predation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Does nesting height depend on predator characteristics?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Again, using the Kruskal-Wallis Test in our last exercise, we already identified predator characteristics to be an important factor in determining &lt;em&gt;Passer domesticus&lt;/em&gt; nesting height. Let&amp;rsquo;s see if this holds true.&lt;/p&gt;
&lt;p&gt;We may wish to use the entirety of our data set again for this purpose:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df &amp;lt;- Data_df_base
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;assumption-check-1&#34;&gt;Assumption Check&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s use our &lt;code&gt;ANOVACeck()&lt;/code&gt; function to test whether we can run our analysis. Before we can do so, however, we need to slightly adjust our predator type variable just like we did in our last exercise and as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# changing levels in predator type
levels(Data_df$Predator.Type) &amp;lt;- c(levels(Data_df$Predator.Type), &amp;quot;None&amp;quot;)
Data_df$Predator.Type[which(is.na(Data_df$Predator.Type))] &amp;lt;- &amp;quot;None&amp;quot;

# Assumption Check
par(mfrow=c(1,2))
ANOVACheck(Variables = &amp;quot;Nesting.Height&amp;quot;, Grouping = &amp;quot;Predator.Type&amp;quot;, data = Data_df, plotting = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;12---Simple-Parametric-Tests_files/figure-html/AssCheck2-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##                          Nesting.Height
## Residual Normality         0.0017160318
## Homogeneity of Variances   0.0005845899
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, our data fails the assumption check. The residuals are definitely not normal distributed and the variance of nesting height records within our groups are not equal.&lt;/p&gt;
&lt;h4 id=&#34;analysis-1&#34;&gt;Analysis&lt;/h4&gt;
&lt;p&gt;Since none of our assumptions are met, we cannot run an ANOVA and therefore resort to data visualisation alone:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(Data_df$Nesting.Height ~ Data_df$Predator.Type)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;12---Simple-Parametric-Tests_files/figure-html/Analysis2-1.png&#34; width=&#34;576&#34; /&gt;
Once more, we can see why our homogeneity of variances test failed.&lt;/p&gt;
&lt;h2 id=&#34;two-way-anova&#34;&gt;Two-Way ANOVA&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Assumptions of the Two-Way ANOVA:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Predictor variables are categorical&lt;/li&gt;
&lt;li&gt;Response variable is metric&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Response variable residuals&lt;/em&gt; are &lt;strong&gt;normal distributed&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Variance of populations/samples are equal (&lt;strong&gt;homogeneity&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;Variable values are &lt;strong&gt;independent&lt;/strong&gt; (not paired)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;testing-for-assumptions-1&#34;&gt;Testing For Assumptions&lt;/h3&gt;
&lt;p&gt;Yet again, we need to check if our assumptions are met first. Automating this procedure is definitely a good idea and only needs slight modification from our &lt;code&gt;ANOVACheck()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# User-defined function
ANOVACheck_TWO &amp;lt;- function(Formulas, data, plotting){
  Output &amp;lt;- data.frame(x = NA)
  for(i in 1:length(Formulas)){
    # Check how many formulas there are
    if(length(Formulas) == 1){
      Expression &amp;lt;- Formulas[[1]]
    }else{
      Expression &amp;lt;- Formulas[[i]]
    }
    # Residuals?
    model &amp;lt;- lm(formula = Expression, data = data)
    Output[1,i] &amp;lt;- shapiro.test(residuals(model))$p.value
    # Homgeneity?
    Levene &amp;lt;- leveneTest(Expression, 
                         center = median, 
                         data = data)
    Output[2,i] &amp;lt;- Levene[1,3]
    # Plotting
    if(plotting == TRUE){
      plot(model, 2)# Normality
      plot(model, 3)# Homogeneity
    }
  }
  colnames(Output) &amp;lt;- as.character(Formulas)
  rownames(Output) &amp;lt;- c(&amp;quot;RN&amp;quot;, &amp;quot;HoV&amp;quot;)
  return(Output)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This &lt;code&gt;ANOVACheck_TWO()&lt;/code&gt; function takes four arguments: (1) &lt;code&gt;Formulas&lt;/code&gt; - a vector of formula specification for our ANOVA models we want to have tested, (2) &lt;code&gt;data&lt;/code&gt; - the data frame which contains the variables and the grouping factor called upon in our &lt;code&gt;Formulas&lt;/code&gt;, and (3) &lt;code&gt;plotting&lt;/code&gt; - a logical indicator of whether to produce plots visualising the test results or not.&lt;br&gt;
The function returns a data frames containing the p-values indexing whether to accept or reject the notion of the normality of residuals per variable (&lt;code&gt;RN&lt;/code&gt;), and the p-values indexing whether variances between groups are homogeneous or not (&lt;code&gt;HoV&lt;/code&gt;).&lt;/p&gt;
&lt;h3 id=&#34;sexual-dimorphism-1&#34;&gt;Sexual Dimorphism&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Does sparrow morphology depend on population status and sex?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Given different factors affecting invasive species, we might expect different patterns of sexual dimorphism for invasive and native populations. Take note that we keep using the northern hemisphere subset our cimate testing sites as these present us with a nice set of invasive/native population records already whilst keeping confounding factors to a minimum.&lt;/p&gt;
&lt;h4 id=&#34;assumption-check-2&#34;&gt;Assumption Check&lt;/h4&gt;
&lt;p&gt;First, we need to check our assumptions:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# prepare climate type testing data
Data_df &amp;lt;- Data_df_base
Index &amp;lt;- Data_df$Index
Rows &amp;lt;- which(Index == &amp;quot;SI&amp;quot; | Index == &amp;quot;UK&amp;quot; | Index == &amp;quot;MA&amp;quot;)
Data_df &amp;lt;- Data_df[Rows,]

# analysis
par(mfrow=c(3,2))
ANOVACheck_TWO(Formulas = c(Weight ~ Population.Status*Sex, 
                            Height ~ Population.Status*Sex,
                            Wing.Chord ~ Population.Status*Sex)
               , data = Data_df, plotting = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;12---Simple-Parametric-Tests_files/figure-html/AssCheck5-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##     Weight ~ Population.Status * Sex Height ~ Population.Status * Sex
## RN                       0.287959531                        0.2171916
## HoV                      0.004492103                        0.9057774
##     Wing.Chord ~ Population.Status * Sex
## RN                             0.1907782
## HoV                            0.9174340
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again our assumptions are not met except for sparrow height and wing chord as a product of sex and population status.&lt;/p&gt;
&lt;h4 id=&#34;analysis-2&#34;&gt;Analysis&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s run our analysis:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# height model
model &amp;lt;- lm(Height ~ Population.Status*Sex, data = Data_df)
anova(model)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Variance Table
## 
## Response: Height
##                        Df  Sum Sq Mean Sq F value Pr(&amp;gt;F)
## Population.Status       1   0.338 0.33820  0.3190 0.5729
## Sex                     1   0.179 0.17896  0.1688 0.6816
## Population.Status:Sex   1   1.786 1.78585  1.6844 0.1959
## Residuals             197 208.865 1.06023
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# wing chord model
model &amp;lt;- lm(Wing.Chord ~ Population.Status*Sex, data = Data_df)
anova(model)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Variance Table
## 
## Response: Wing.Chord
##                        Df Sum Sq  Mean Sq F value Pr(&amp;gt;F)
## Population.Status       1 0.0047 0.004669  0.1955 0.6589
## Sex                     1 0.0041 0.004125  0.1727 0.6782
## Population.Status:Sex   1 0.0399 0.039856  1.6688 0.1979
## Residuals             197 4.7049 0.023883
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# plotting
par(mfrow=c(1,2))
boxplot(Height ~ Population.Status*Sex, data = Data_df, col = c(&amp;quot;blue&amp;quot;, &amp;quot;red&amp;quot;))
boxplot(Wing.Chord ~ Population.Status*Sex, data = Data_df, col = c(&amp;quot;blue&amp;quot;, &amp;quot;red&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;12---Simple-Parametric-Tests_files/figure-html/Analysis5-1.png&#34; width=&#34;576&#34; /&gt;
As it turns out, population status and sex are no viable predictors for sparrow height or wing chord and so we &lt;strong&gt;accept the null hypothesis&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;ancova&#34;&gt;ANCOVA&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Assumptions of the ANCOVA:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Predictor variables are categorical or continuous&lt;/li&gt;
&lt;li&gt;Response variable is metric&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Response variable residuals&lt;/em&gt; are &lt;strong&gt;normal distributed&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Variance of populations/samples are equal (&lt;strong&gt;homogeneity&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;Variable values are &lt;strong&gt;independent&lt;/strong&gt; (not paired)&lt;/li&gt;
&lt;li&gt;Relationship between the response and covariate is &lt;strong&gt;linear&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;climate-warmingextremes-3&#34;&gt;Climate Warming/Extremes&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Do sparrow characteristics depend on climate and latitude?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Latitude may have masked some effects of climate on sparrow morphology in our preceding analyses and vice-versa. At times, we have been able to account for this by including our site records, which can be seen as binned versions of latitude records. Let&amp;rsquo;s test if the inclusion of raw latitude records are meaningful.&lt;/p&gt;
&lt;h4 id=&#34;assumption-check-3&#34;&gt;Assumption Check&lt;/h4&gt;
&lt;p&gt;Again, we need to do an assumption check. However, we need a new function for this, since we now need to test whether our response variable and the covariate are linear or not:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# overwriting prior changes in Data_df
Data_df &amp;lt;- Data_df_base
Data_df$Latitude &amp;lt;- abs(Data_df$Latitude)
# User-defined function
ANCOVACheck &amp;lt;- function(Variables, Grouping, Covariate, data, plotting){
  Output &amp;lt;- data.frame(x = NA)
  for(i in 1:length(Variables)){
    # data
    Y &amp;lt;- as.numeric(factor(data[,Variables[i]]))
    X &amp;lt;- factor(data[,Grouping])
    Z &amp;lt;- data[, Covariate]
    # Residuals?
    model &amp;lt;- lm(Y ~ X*Z)
    Output[1,i] &amp;lt;- shapiro.test(residuals(model))$p.value
    # Homgeneity?
    Levene &amp;lt;- leveneTest(Y ~ X, 
                         center = median, 
                         data = data)
    Output[2,i] &amp;lt;- Levene[1,3]
    # Plotting
    if(plotting == TRUE){
      plot(model, 1)# Linearity
      plot(model, 2)# Normality
      plot(model, 3)# Homogeneity
    }
  }
  colnames(Output) &amp;lt;- Variables
  rownames(Output) &amp;lt;- c(&amp;quot;RN&amp;quot;, &amp;quot;HoV&amp;quot;)
  return(Output)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This &lt;code&gt;ANCOVACheck()&lt;/code&gt; function takes five arguments: (1) &lt;code&gt;Variables&lt;/code&gt; - a vector of response variables used in our models, (2) &lt;code&gt;Grouping&lt;/code&gt; - the categorical variable by which to group our variables, (3) &lt;code&gt;Covariate&lt;/code&gt; - the covariate of our analysis, (4)&lt;code&gt;data&lt;/code&gt; - the data frame which contains the variables, the grouping factor and our covariate, and (5) &lt;code&gt;plotting&lt;/code&gt; - a logical indicator of whether to produce plots visualising the test results or not.&lt;br&gt;
The function returns a data frames containing the p-values indexing whether to accept or reject the notion of the normality of residuals per variable (&lt;code&gt;RN&lt;/code&gt;), and the p-values indexing whether variances between groups are homogeneous or not (&lt;code&gt;HoV&lt;/code&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ANCOVACheck(Variables = c(&amp;quot;Weight&amp;quot;, &amp;quot;Height&amp;quot;, &amp;quot;Wing.Chord&amp;quot;, &amp;quot;Nesting.Height&amp;quot;, &amp;quot;Egg.Weight&amp;quot;, &amp;quot;Number.of.Eggs&amp;quot;, &amp;quot;Home.Range&amp;quot;), 
            Grouping = &amp;quot;Climate&amp;quot;, Covariate = &amp;quot;Latitude&amp;quot;, 
            data = Data_df, plotting = FALSE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           Weight       Height   Wing.Chord Nesting.Height   Egg.Weight
## RN  2.082300e-02 1.502944e-04 4.535941e-07   5.190971e-06 2.943560e-03
## HoV 9.937376e-24 1.929783e-22 1.561040e-33   2.004612e-01 4.816355e-09
##     Number.of.Eggs   Home.Range
## RN    1.809197e-09 3.629841e-20
## HoV   2.750100e-14 1.157673e-08
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The assumptions aren&amp;rsquo;t met. I have set the &lt;code&gt;plotting&lt;/code&gt; argument to &lt;code&gt;FALSE&lt;/code&gt; tu suppress the plotting of model checking visualisation. The would be useful to judge linearity but not necessary here since the other two important assumptions (Homogeneity of variances and Normality of residuals) aren&amp;rsquo;t met to begin with.&lt;/p&gt;
&lt;h4 id=&#34;analysis-3&#34;&gt;Analysis&lt;/h4&gt;
&lt;p&gt;Since none of our assumptions are met, we cannot run an ANOVA and therefore resort to data visualisation alone. We need a new function for this to do our plotting easily and automatically with some colours indicating our grouping factors whilst plotting response variables versus covariates.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;PlotAncovas &amp;lt;- function(Variables, Grouping, Covariate, data){
  for(i in 1:length(Variables)){
    Y &amp;lt;- Data_df[,Variables[i]]
    if(class(Y) == &amp;quot;character&amp;quot;){Y &amp;lt;- factor(Y)}
    X &amp;lt;- Data_df[,Covariate]
    G &amp;lt;- factor(Data_df[, Grouping])
    plot(X, Y, col = G, xlab = Covariate, ylab = Variables[i])
    legend(&amp;quot;top&amp;quot;, # place legend at the top
           inset = -0.35, # move legend away from plot centre
           xpd = TRUE, # allow legend outside of plot area
           legend=levels(G), # what to include in legend
           bg = &amp;quot;white&amp;quot;, col = unique(G), ncol=length(levels(G)), # colours
           pch = 1, # plotting symbols
           title = Variables[i] # title of legend
           )
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;PlotAncovas()&lt;/code&gt; returns a scatter plot and takes four arguments: (1) &lt;code&gt;Variables&lt;/code&gt; - a vector of response variables, (2) &lt;code&gt;Grouping&lt;/code&gt; - the name of the grouping factor according to which to colour the symbols in our plot, (3) &lt;code&gt;Covariate&lt;/code&gt; - the covariate against which to plot individuals variables, and (4) &lt;code&gt;data&lt;/code&gt; - the data frame which holds our variables.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s use our function:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;par(mfrow=c(1,2))
PlotAncovas(Variables = c(&amp;quot;Weight&amp;quot;, &amp;quot;Height&amp;quot;, &amp;quot;Wing.Chord&amp;quot;, &amp;quot;Nesting.Height&amp;quot;, &amp;quot;Egg.Weight&amp;quot;, &amp;quot;Number.of.Eggs&amp;quot;, &amp;quot;Home.Range&amp;quot;), Grouping = &amp;quot;Climate&amp;quot;, Covariate = &amp;quot;Latitude&amp;quot;, data = Data_df)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I will not interpret these plots here in text and leave this to you.&lt;/p&gt;
&lt;p&gt;Take note that this &lt;strong&gt;could&amp;rsquo;ve been achieved much easier with &lt;code&gt;ggplot2&lt;/code&gt;!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;12---Simple-Parametric-Tests_files/figure-html/Analysisc-1.png&#34; width=&#34;576&#34; /&gt;&lt;img src=&#34;12---Simple-Parametric-Tests_files/figure-html/Analysisc-2.png&#34; width=&#34;576&#34; /&gt;&lt;img src=&#34;12---Simple-Parametric-Tests_files/figure-html/Analysisc-3.png&#34; width=&#34;576&#34; /&gt;&lt;img src=&#34;12---Simple-Parametric-Tests_files/figure-html/Analysisc-4.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;sparrow-characteristics-and-sites&#34;&gt;Sparrow Characteristics And Sites&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;This was not part of what we set out to do according to the lecture slides but has been included as a logical conclusion to an earlier analysis.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Unfortunately, our previous attempt at an ANCOVA didn&amp;rsquo;t work. So what other covariate do we have available for sparrow characteristics?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Latitude&lt;/em&gt; doesn&amp;rsquo;t make sense to include when grouping by site index as these two are synonymous&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Longitude&lt;/em&gt; doesn&amp;rsquo;t make sense to include when grouping by site index as these two are synonymous&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Weight&lt;/em&gt; is well explained by other variables and we know the causal links&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Height&lt;/em&gt; is not that well explained by other variables&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Wing.Chord&lt;/em&gt; is not that well explained by other variables&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Of course, there are more within our data set but it has become apparent that &lt;code&gt;Weight&lt;/code&gt; may make for an important covariate in our site-wise ANCOVA set-up. Using the Pearson correlation (third practical), we already identified a causal link between sparrow &lt;code&gt;Weight&lt;/code&gt; and &lt;code&gt;Height&lt;/code&gt; per site.&lt;/p&gt;
&lt;h4 id=&#34;assumption-check-4&#34;&gt;Assumption Check&lt;/h4&gt;
&lt;p&gt;Firstly, we test whether assumptions are met. For brevities sake, we only test four variables:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;par(mfrow=c(1,3))
ANCOVACheck(Variables = c(&amp;quot;Height&amp;quot;, &amp;quot;Wing.Chord&amp;quot;, &amp;quot;Egg.Weight&amp;quot;, &amp;quot;Number.of.Eggs&amp;quot;), Grouping = &amp;quot;Index&amp;quot;, Covariate = &amp;quot;Weight&amp;quot;, data = Data_df, plotting = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;12---Simple-Parametric-Tests_files/figure-html/AssCheck7-1.png&#34; width=&#34;576&#34; /&gt;&lt;img src=&#34;12---Simple-Parametric-Tests_files/figure-html/AssCheck7-2.png&#34; width=&#34;576&#34; /&gt;&lt;img src=&#34;12---Simple-Parametric-Tests_files/figure-html/AssCheck7-3.png&#34; width=&#34;576&#34; /&gt;&lt;img src=&#34;12---Simple-Parametric-Tests_files/figure-html/AssCheck7-4.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##           Height   Wing.Chord   Egg.Weight Number.of.Eggs
## RN  1.499909e-06 5.251393e-08 0.1565038171   9.220862e-07
## HoV 3.594021e-13 2.434880e-01 0.0002015813   2.660146e-02
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As it turns out, we can run our ANCOVA on &lt;code&gt;Egg.Weight&lt;/code&gt; when grouped by site &lt;code&gt;Index&lt;/code&gt; and driven by &lt;code&gt;Weight&lt;/code&gt;.&lt;/p&gt;
&lt;h4 id=&#34;analysis-4&#34;&gt;Analysis&lt;/h4&gt;
&lt;p&gt;First, let&amp;rsquo;s visualise our data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;PlotAncovas(Variables = &amp;quot;Egg.Weight&amp;quot;, Grouping = &amp;quot;Index&amp;quot;, Covariate = &amp;quot;Weight&amp;quot;, data = Data_df)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;12---Simple-Parametric-Tests_files/figure-html/Analysis7-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Quite obviously, Belize (BE) records are very different from the other stations, whose egg weight and weight records are grouped together. There seems to be some evidence for an overall linkage of sparrow weight and egg weight (a positive correlation).&lt;/p&gt;
&lt;p&gt;Now we run the analysis:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;LM_fit5 &amp;lt;- lm(Egg.Weight ~ Weight*Index, data = Data_df)
anova(LM_fit5)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Variance Table
## 
## Response: Egg.Weight
##               Df Sum Sq Mean Sq   F value Pr(&amp;gt;F)    
## Weight         1 52.531  52.531 1442.5616 &amp;lt;2e-16 ***
## Index         10  8.087   0.809   22.2064 &amp;lt;2e-16 ***
## Weight:Index  10  0.129   0.013    0.3536 0.9653    
## Residuals    455 16.569   0.036                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above ANCOVA output tells us that there is no interaction effect between sites and sparrow weights when determining mean egg weight per nest of &lt;em&gt;Passer domesticus&lt;/em&gt; and so we do another iteration of our model and remove the postulated interaction:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;LM_fit6 &amp;lt;- lm(Egg.Weight ~ Weight+Index, data = Data_df)
anova(LM_fit6)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Variance Table
## 
## Response: Egg.Weight
##            Df Sum Sq Mean Sq  F value    Pr(&amp;gt;F)    
## Weight      1 52.531  52.531 1462.898 &amp;lt; 2.2e-16 ***
## Index      10  8.087   0.809   22.519 &amp;lt; 2.2e-16 ***
## Residuals 465 16.698   0.036                       
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By now, all of our model coefficients are significant and we can go on to interpret them:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(LM_fit6)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Egg.Weight ~ Weight + Index, data = Data_df)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.58887 -0.13146 -0.00621  0.12033  0.55135 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  3.345984   0.280826  11.915  &amp;lt; 2e-16 ***
## Weight       0.001081   0.008614   0.125 0.900203    
## IndexBE     -0.708478   0.054864 -12.913  &amp;lt; 2e-16 ***
## IndexFG     -1.281168   0.099135 -12.923  &amp;lt; 2e-16 ***
## IndexFI     -0.625287   0.057442 -10.885  &amp;lt; 2e-16 ***
## IndexLO     -0.550137   0.051754 -10.630  &amp;lt; 2e-16 ***
## IndexMA     -0.513645   0.051352 -10.003  &amp;lt; 2e-16 ***
## IndexNU     -0.517015   0.053365  -9.688  &amp;lt; 2e-16 ***
## IndexRE     -0.612632   0.051418 -11.915  &amp;lt; 2e-16 ***
## IndexSA     -0.806045   0.056685 -14.220  &amp;lt; 2e-16 ***
## IndexSI     -0.272580   0.077868  -3.501 0.000509 ***
## IndexUK     -0.511667   0.051404  -9.954  &amp;lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.1895 on 465 degrees of freedom
##   (590 observations deleted due to missingness)
## Multiple R-squared:  0.784,	Adjusted R-squared:  0.7789 
## F-statistic: 153.5 on 11 and 465 DF,  p-value: &amp;lt; 2.2e-16
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Simple Parametric Tests</title>
      <link>https://www.erikkusch.com/courses/biostat101/simple-parametric-tests/</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/biostat101/simple-parametric-tests/</guid>
      <description>&lt;h2 id=&#34;theory&#34;&gt;Theory&lt;/h2&gt;
&lt;p&gt;Welcome to our sixth practical experience in R. Throughout the following notes, I will introduce you to a couple of simple parametric test. Whilst parametric tests are used extremely often in biological statistics, they can be somewhat challenging to fit to your data as you will see soon. To do so, I will enlist the sparrow data set we handled in our first exercise. Additionally, todays seminar is showing plotting via base plot instead of &lt;code&gt;ggplot2&lt;/code&gt; to highlight the usefulness of base plot and show you the base notation. I have prepared some slides for this session: &lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/biostat101/12---Simple-Parametric-Tests_Handout.html&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://www.erikkusch.com/courses/biostat101/12---BioStat101_featured.png&#34; width=&#34;900&#34; margin-top = &#34;0&#34;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;p&gt;Find the data for this exercise &lt;a href=&#34;https://github.com/ErikKusch/Homepage/raw/master/content/courses/biostat101/Data/1%20-%20Sparrow_Data_READY.rds&#34; target=&#34;_blank&#34;&gt; here&lt;/a&gt; and &lt;a href=&#34;https://github.com/ErikKusch/Homepage/raw/master/content/courses/biostat101/Data/2b%20-%20Sparrow_ResettledSIUK_READY.rds&#34; target=&#34;_blank&#34;&gt; here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;preparing-our-procedure&#34;&gt;Preparing Our Procedure&lt;/h2&gt;
&lt;p&gt;To ensure others can reproduce our analysis we run the following three lines of code at the beginning of our &lt;code&gt;R&lt;/code&gt; coding file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rm(list=ls()) # clearing environment
Dir.Base &amp;lt;- getwd() # soft-coding our working directory
Dir.Data &amp;lt;- paste(Dir.Base, &amp;quot;Data&amp;quot;, sep=&amp;quot;/&amp;quot;) # soft-coding our data directory 
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;packages&#34;&gt;Packages&lt;/h3&gt;
&lt;p&gt;Using the following, user-defined function, we install/load all the necessary packages into our current &lt;code&gt;R&lt;/code&gt; session.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# function to load packages and install them if they haven&#39;t been installed yet
install.load.package &amp;lt;- function(x) {
  if (!require(x, character.only = TRUE))
    install.packages(x)
  require(x, character.only = TRUE)
}
package_vec &amp;lt;- c(&amp;quot;car&amp;quot;) # needed for the Levene Test for Homogeneity
sapply(package_vec, install.load.package)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: car
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: carData
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  car 
## TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;loading-data&#34;&gt;Loading Data&lt;/h3&gt;
&lt;p&gt;During our first exercise (Data Mining and Data Handling - Fixing The Sparrow Data Set) we saved our clean data set as an RDS file. To load this, we use the &lt;code&gt;readRDS()&lt;/code&gt; command that comes with base &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df_base &amp;lt;- readRDS(file = paste(Dir.Data, &amp;quot;/1 - Sparrow_Data_READY.rds&amp;quot;, sep=&amp;quot;&amp;quot;))
Data_df &amp;lt;- Data_df_base # duplicate and save initial data on a new object
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;t-test-unpaired&#34;&gt;t-Test (unpaired)&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Assumptions of the unpaired t-Test:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Predictor variable is binary&lt;/li&gt;
&lt;li&gt;Response variable is metric and &lt;strong&gt;normal distributed&lt;/strong&gt; within their groups&lt;/li&gt;
&lt;li&gt;Variable values are &lt;strong&gt;independent&lt;/strong&gt; (not paired)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition, test whether variance of response variable values in groups are equal (&lt;code&gt;var.test()&lt;/code&gt;) and adjust &lt;code&gt;t.test()&lt;/code&gt; argument &lt;code&gt;var.equal&lt;/code&gt; accordingly.&lt;/p&gt;
&lt;h3 id=&#34;testing-for-normality-and-homogeneity&#34;&gt;Testing For Normality And Homogeneity&lt;/h3&gt;
&lt;p&gt;We need to test the distribution of our response variables within each predictor variable group for their normality and variance. Since this involves two Shapiro tests and one variance test per variable for each response variable, we might want to write our own function to do so:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ShapiroTest &amp;lt;- function(Variables, Grouping){
  Output &amp;lt;- data.frame(x = Variables)
  for(i in 1:length(Variables)){
    
    X &amp;lt;- Data_df[,Variables[i]]
    Levels &amp;lt;- levels(factor(Data_df[,Grouping]))
    
    Output[i,2] &amp;lt;- shapiro.test(X[which(Data_df[,Grouping] == Levels[1])])$p.value
    Output[i,3] &amp;lt;- shapiro.test(X[which(Data_df[,Grouping] == Levels[2])])$p.value
    Output[i,4] &amp;lt;- var.test(x = X[which(Data_df[,Grouping] == Levels[1])], 
                            y = X[which(Data_df[,Grouping] == Levels[2])])$p.value
  }
  colnames(Output) &amp;lt;- c(&amp;quot;Variable&amp;quot;, &amp;quot;P.value1&amp;quot;, &amp;quot;P.value2&amp;quot;, &amp;quot;Var.Test&amp;quot;)
  return(Output)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function (&lt;code&gt;ShapiroTest()&lt;/code&gt;) takes two arguments: (1) &lt;code&gt;Variables&lt;/code&gt; - a vector of characters holding the names of the variables we want to have tested, and (2) &lt;code&gt;Grouping&lt;/code&gt; - the binary variable by which to group our variables. The function returns a data frame holding the p-values of the Shapiro tests on each variable group values as well as the &lt;code&gt;var.test()&lt;/code&gt; p-value.&lt;/p&gt;
&lt;h3 id=&#34;climate-warmingextremes&#34;&gt;Climate Warming/Extremes&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Does sparrow morphology change depend on climate?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Using multiple different methods (i.e. Kruskal-Wallis and Mann-Whitney U Test), we have already identified climate (be it in its binary form or when recorded as a three-level variable) is a strong driving force of sparrow morphology. We expect the same results when using a t-Test.&lt;/p&gt;
&lt;p&gt;Take note that we need to limit our analysis to our climate type testing sites again as follows (we include Manitoba this time as it is at the same latitude as the UK and Siberia and holds a semi-coastal climate type):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# prepare climate type testing data
Data_df &amp;lt;- Data_df_base
Index &amp;lt;- Data_df$Index
Rows &amp;lt;- which(Index == &amp;quot;SI&amp;quot; | Index == &amp;quot;UK&amp;quot; | Index == &amp;quot;RE&amp;quot; | Index == &amp;quot;AU&amp;quot; | Index == &amp;quot;MA&amp;quot;)
Data_df &amp;lt;- Data_df[Rows,]
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;testing-for-normality-and-variance&#34;&gt;Testing for Normality and Variance&lt;/h4&gt;
&lt;p&gt;Before we can make use of our data with a t-Test, we need to do an &lt;strong&gt;assumption check&lt;/strong&gt;. To this end, we first turn &lt;code&gt;Climate&lt;/code&gt; records into a binary variable by turning records of a semi-coastal climate into a coastal one.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Make climate binary
Data_df$Climate[which(Data_df$Climate == &amp;quot;Semi-Coastal&amp;quot;)] &amp;lt;- &amp;quot;Coastal&amp;quot;
Data_df$Climate &amp;lt;- droplevels(factor(Data_df$Climate))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s make sure our assumptions are met:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ShapiroTest(Variables = c(&amp;quot;Weight&amp;quot;, &amp;quot;Height&amp;quot;, &amp;quot;Wing.Chord&amp;quot;), Grouping = &amp;quot;Climate&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Variable  P.value1  P.value2    Var.Test
## 1     Weight 0.1699442 0.2521182 0.326240416
## 2     Height 0.1676977 0.3645040 0.010632158
## 3 Wing.Chord 0.0538642 0.1722528 0.002942433
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Luckily, all of our variables allow for the calculation of t-Test. Take note though that some need different specification of the &lt;code&gt;var.equal&lt;/code&gt; argument than others.&lt;/p&gt;
&lt;h4 id=&#34;analyses&#34;&gt;Analyses&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Sparrow Weight&lt;/strong&gt;&lt;br&gt;
Let&amp;rsquo;s start with the weight of &lt;em&gt;Passer domesticus&lt;/em&gt; individuals as grouped by the climate type present at the site weights have been recorded at:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;t.test(Data_df$Weight ~ Data_df$Climate, var.equal = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Two Sample t-test
## 
## data:  Data_df$Weight by Data_df$Climate
## t = -14.852, df = 381, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true difference in means between group Coastal and group Continental is not equal to 0
## 95 percent confidence interval:
##  -2.428439 -1.860640
## sample estimates:
##     mean in group Coastal mean in group Continental 
##                  31.23383                  33.37837
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;According to our analysis, which has us &lt;strong&gt;reject the null hypothesis&lt;/strong&gt;, we conclude that binary climate records are valuable information criteria for predicting sparrow weight with sparrows in coastal climates being lighter than sparrows in continental ones thus effectively varifying the results of our non-parametric approaches (Kruskal-Wallis, Mann-Whitney U).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sparrow Height&lt;/strong&gt;&lt;br&gt;
Let&amp;rsquo;s move on to the height of &lt;em&gt;Passer domesticus&lt;/em&gt; individuals as grouped by the climate type present at the site weights have been recorded at:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;t.test(Data_df$Height ~ Data_df$Climate, var.equal = FALSE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Welch Two Sample t-test
## 
## data:  Data_df$Height by Data_df$Climate
## t = -0.27916, df = 365.69, p-value = 0.7803
## alternative hypothesis: true difference in means between group Coastal and group Continental is not equal to 0
## 95 percent confidence interval:
##  -0.2329126  0.1750052
## sample estimates:
##     mean in group Coastal mean in group Continental 
##                  13.91670                  13.94565
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Confirming the results of our Mann-Whitney U Test, we &lt;strong&gt;accept the null hypothesis&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sparrow Wing Chord&lt;/strong&gt;&lt;br&gt;
Lastly, we test the wing chords of &lt;em&gt;Passer domesticus&lt;/em&gt; individuals as grouped by the climate type present at the site weights have been recorded at:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;t.test(Data_df$Wing.Chord ~ Data_df$Climate, var.equal = FALSE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Welch Two Sample t-test
## 
## data:  Data_df$Wing.Chord by Data_df$Climate
## t = -0.12285, df = 370.22, p-value = 0.9023
## alternative hypothesis: true difference in means between group Coastal and group Continental is not equal to 0
## 95 percent confidence interval:
##  -0.03985039  0.03516377
## sample estimates:
##     mean in group Coastal mean in group Continental 
##                  6.898696                  6.901039
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Without confirming the results of our Mann-Whitney U Test, we &lt;strong&gt;accept the null hypothesis&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;br&gt;
Here&amp;rsquo;s what we&amp;rsquo;ve learned from the t-Test so far:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sparrow weight depends on (binary) climate types&lt;/li&gt;
&lt;li&gt;Sparrow height does not depend on (binary) climate types&lt;/li&gt;
&lt;li&gt;Sparrow wing chord does not depend on (binary) climate types&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s end this by viusalising all of the data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;par(mfrow=c(2,2))
plot(Data_df$Weight ~ Data_df$Climate)
plot(Data_df$Height ~ Data_df$Climate)
plot(Data_df$Wing.Chord ~ Data_df$Climate)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;12---Simple-Parametric-Tests_files/figure-html/t-test1-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;sexual-dimorphism&#34;&gt;Sexual Dimorphism&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Does sparrow morphology change depend on Sex?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Using the Mann-Whitney U Test, we have already identified the sex of &lt;em&gt;Passer domesticus&lt;/em&gt; is a good information criterion for understanding sparrow weight but not sparrow height or wing chord. Let&amp;rsquo;s see if we can reproduce this using a t-Test approach.&lt;/p&gt;
&lt;p&gt;We may wish to use the entirety of our data set again for this purpose:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df &amp;lt;- Data_df_base
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;testing-for-normality-and-variance-1&#34;&gt;Testing for Normality and Variance&lt;/h4&gt;
&lt;p&gt;Again, before we can use our data in a t-Test for this purpose, we have to make sure that our assumptions are met. To this end, we can make use of our user defined &lt;code&gt;ShapiroTest()&lt;/code&gt; function as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ShapiroTest(Variables = c(&amp;quot;Weight&amp;quot;, &amp;quot;Height&amp;quot;, &amp;quot;Wing.Chord&amp;quot;), Grouping = &amp;quot;Sex&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Variable     P.value1     P.value2  Var.Test
## 1     Weight 2.878769e-21 1.744517e-21 0.7475085
## 2     Height 4.028475e-17 6.600273e-19 0.4006799
## 3 Wing.Chord 3.438104e-25 1.628147e-26 0.5554935
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As it turns out, our data does not allow for any t-Test (this happens often in real studies). However, we can create sex-driven subgroups within each site and test whether these meet the requirements for our t-Test. This is out of the scope of this course though and so we will skip it. Spoler alert: I have done this and the findings did not reveal anything we didn&amp;rsquo;t uncover so far.&lt;/p&gt;
&lt;!-- In order to do so, we need to do some minor tweaking to our `ShapiroTest()` function: --&gt;
&lt;!-- ```{r ShapiroSex} --&gt;
&lt;!-- ShapiroTestSites &lt;- function(Variables, Grouping){ --&gt;
&lt;!--   list &lt;- list() --&gt;
&lt;!--   for(k in 1:length(unique(Data_df$Index))){ --&gt;
&lt;!--     Output &lt;- data.frame(x = Variables) --&gt;
&lt;!--     Data &lt;- Data_df[which(Data_df$Index == unique(Data_df$Index)[k]), ] --&gt;
&lt;!--     for(i in 1:length(Variables)){ --&gt;
&lt;!--       X &lt;- Data[,Variables[i]] --&gt;
&lt;!--       Levels &lt;- levels(Data[,Grouping]) --&gt;
&lt;!--       Output[i,2] &lt;- shapiro.test(X[which(Data[,Grouping] == Levels[1])])$p.value --&gt;
&lt;!--       Output[i,3] &lt;- shapiro.test(X[which(Data[,Grouping] == Levels[2])])$p.value --&gt;
&lt;!--       Output[i,4] &lt;- var.test(x = X[which(Data_df[,Grouping] == Levels[1])],  --&gt;
&lt;!--                             y = X[which(Data_df[,Grouping] == Levels[2])]) --&gt;
&lt;!--     } --&gt;
&lt;!--     colnames(Output) &lt;- c(&#34;Variable&#34;, &#34;P.value1&#34;, &#34;P.value2&#34;, &#34;Var.Test&#34;) --&gt;
&lt;!--     list[[k]] &lt;- Output --&gt;
&lt;!--   } --&gt;
&lt;!--   return(list) --&gt;
&lt;!-- } --&gt;
&lt;!-- ``` --&gt;
&lt;!-- This function (`ShapiroTestSites()`) takes two arguments: (1) `Variables` - a vector of characters holding the names of the variables we want to have tested, and (2) `Grouping` - the binary variable by which to group our variables. The function returns a list of data frames for each site holding the p-values of the Shapiro tests on each variable group values as well as the `var.test()` p-value.  --&gt;
&lt;!--  --&gt;
&lt;!-- Let&#39;s put this function to the test: --&gt;
&lt;!-- ```{r ShapiroSex1} --&gt;
&lt;!-- ShapiroTestSites(Variables = c(&#34;Weight&#34;, &#34;Height&#34;, &#34;Wing.Chord&#34;), Grouping = &#34;Sex&#34;) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- With the exception for sparrow morphology records at:   --&gt;
&lt;!-- - Siberia (SI, height and wing chord of males)   --&gt;
&lt;!-- - Manitoba (MA, morphology of females)   --&gt;
&lt;!-- - South Africa (SA, morphology of females)   --&gt;
&lt;!-- all of our data groups variables are normal distributed with equal variances between the groups per site.   --&gt;
&lt;!-- Since our problematic sites are still relatively close to fulfilling our requirements of the data, we will use them going forward as if they did. --&gt;
&lt;!-- ### Analyses --&gt;
&lt;!-- Running three t-Tests (Weight, Height, Wing Chord) for each of our eleven sites is absolute mania! Therefore, we write our own function again that let&#39;s us apply the tests exactly the way we want to: --&gt;
&lt;!-- ```{r ttestSex0} --&gt;
&lt;!-- t_testSite &lt;- function(Variables, Grouping, data, VarEqual){ --&gt;
&lt;!--   Data &lt;- data --&gt;
&lt;!--   Index &lt;- unique(Data$Index) --&gt;
&lt;!--   Indexes &lt;- Data$Index --&gt;
&lt;!--   list &lt;- list() --&gt;
&lt;!--   for(i in 1:length(Variables)){ --&gt;
&lt;!--     Output &lt;- data.frame(NA) --&gt;
&lt;!--     for(k in 1:length(Index)){ --&gt;
&lt;!--       # data and test --&gt;
&lt;!--       X &lt;- Data[, Variables[i]][which(Indexes == Index[k])] --&gt;
&lt;!--       Y &lt;- Data[, Grouping][which(Indexes == Index[k])] --&gt;
&lt;!--       Test &lt;- t.test(X ~ Y, paired = FALSE, var.equal = VarEqual) --&gt;
&lt;!--       # filling data frame --&gt;
&lt;!--       Output[1,k] &lt;- Test[[&#34;p.value&#34;]] --&gt;
&lt;!--       Output[2,k] &lt;- Test[[&#34;estimate&#34;]][[1]] --&gt;
&lt;!--       Output[3,k] &lt;- Test[[&#34;estimate&#34;]][[2]] --&gt;
&lt;!--     } --&gt;
&lt;!--     # data frame to list --&gt;
&lt;!--     colnames(Output) &lt;- Index --&gt;
&lt;!--     rownames(Output) &lt;- c(&#34;p&#34;, &#34;Mean1&#34;, &#34;Mean2&#34;) --&gt;
&lt;!--     list[[i]] &lt;- Output --&gt;
&lt;!--   } --&gt;
&lt;!--   return(list) --&gt;
&lt;!-- } --&gt;
&lt;!-- ``` --&gt;
&lt;!-- This `t_testSite()` function takes four arguments: (1) `Variables` - a vector of characters holding the names of the variables we want to have tested, (2) `Grouping` - the binary variable by which to group our variables, (3) `data` - the data frame which contains the `Variables` and the `Grouping` factor, and (4) `VarEqual` - a logical indicator of whether to perform a t-Test assuming equal variance of the groups or not.   --&gt;
&lt;!-- The function returns a list of data frames (one per variable) containing the p-values of the unpaired t-Tests for each variable at every site as well as the predicted group means. --&gt;
&lt;!--  --&gt;
&lt;!-- Although our function `t_testSite()` can handle multiple variables at once, we will now use it on each of our morphological sparrow variables individually to disentangle them a bit easier:   --&gt;
&lt;!-- \begin{center} --&gt;
&lt;!-- \textit{Does sparrow weight depend on sex when assessed at each of our sites individually?} --&gt;
&lt;!-- \end{center} --&gt;
&lt;!-- ```{r tTestSex1} --&gt;
&lt;!-- t_testSite(Variables = &#34;Weight&#34;, Grouping = &#34;Sex&#34;, data = Data_df, VarEqual = TRUE) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- As it turns out, sex is a statistically significant predictor for sparrow weight at each site. This was to be expected. Using a binomial test in our second practical, we identified no bias in sexes for our sparrow populations. In addition, using the Mann-Whitney U-Test in our fourth practical, we identified sex to be an important information criterion for sparrow weight across all of our sites. Given these two conditions, we were expecting a results like the one presented here with males being, on average, heavier than females in *Passer domesticus* and we **reject the null hypothesis**.   --&gt;
&lt;!-- \hfill \linebreak --&gt;
&lt;!-- \begin{center} --&gt;
&lt;!-- \textit{Does sparrow height depend on sex when assessed at each of our sites individually?} --&gt;
&lt;!-- \end{center} --&gt;
&lt;!-- ```{r tTestSex2} --&gt;
&lt;!-- t_testSite(Variables = &#34;Height&#34;, Grouping = &#34;Sex&#34;, data = Data_df, VarEqual = TRUE) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- Like with our Mann-Whitney U-Test, we fail to identify a significant effect of sex on sparrow height records at each of our sites and so we **accept the null hypothesis**. --&gt;
&lt;!-- \hfill \linebreak --&gt;
&lt;!-- \begin{center} --&gt;
&lt;!-- \textit{Does sparrow wing chord depend on sex when assessed at each of our sites individually?} --&gt;
&lt;!-- \end{center} --&gt;
&lt;!-- ```{r tTestSex3} --&gt;
&lt;!-- t_testSite(Variables = &#34;Wing.Chord&#34;, Grouping = &#34;Sex&#34;, data = Data_df, VarEqual = TRUE) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- Like with our Mann-Whitney U-Test, we fail to identify a significant effect of sex on sparrow wing chord records at each of our sites and so we **accept the null hypothesis**. --&gt;
&lt;!--  --&gt;
&lt;h2 id=&#34;t-test-paired&#34;&gt;t-Test (paired)&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Assumptions of the paired t-Test:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Predictor variable is binary&lt;/li&gt;
&lt;li&gt;Response variable is metric&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Difference of response variable pairs&lt;/em&gt; is &lt;strong&gt;normal distributed&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Variable values are &lt;strong&gt;dependent&lt;/strong&gt; (paired)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;preparing-data&#34;&gt;Preparing Data&lt;/h3&gt;
&lt;p&gt;For this purpose, we need an &lt;strong&gt;additional data set with truly paired records&lt;/strong&gt; of sparrows and so we implement the same solution as we&amp;rsquo;ve used within our fourth seminar using the Wilcoxon Signed Rank Test. Within our study set-up, think of a &lt;strong&gt;resettling experiment&lt;/strong&gt;, were you take &lt;em&gt;Passer domesticus&lt;/em&gt; individuals from one site, transfer them to another and check back with them after some time has passed to see whether some of their characteristics have changed in their expression.&lt;br&gt;
To this end, presume we have taken the entire &lt;em&gt;Passer domesticus&lt;/em&gt; population found at our &lt;strong&gt;Siberian&lt;/strong&gt; research station and moved them to the &lt;strong&gt;United Kingdom&lt;/strong&gt;. Whilst this keeps the latitude stable, the sparrows &lt;em&gt;now experience a coastal climate instead of a continental one&lt;/em&gt;. After some time (let&amp;rsquo;s say: a year), we have come back and recorded all the characteristics for the same individuals again.&lt;/p&gt;
&lt;p&gt;You will find the corresponding &lt;em&gt;new data&lt;/em&gt; in &lt;code&gt;2b - Sparrow_ResettledSIUK_READY.rds&lt;/code&gt;. Take note that this set only contains records for the transferred individuals in the &lt;strong&gt;same order&lt;/strong&gt; as in the old data set.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df_Resettled &amp;lt;- readRDS(file = paste(Dir.Data, &amp;quot;/2b - Sparrow_ResettledSIUK_READY.rds&amp;quot;, sep=&amp;quot;&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since earlier analysis such as the Wilcoxon Signed Rank test (fourth practical) and the Friedman Test (fifth practical) showed that height and wing chord records do not change when sparrows are resettled at all, we have excluded these here and &lt;strong&gt;focus solely on sparrow weight&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;testing-for-normality&#34;&gt;Testing for Normality&lt;/h3&gt;
&lt;p&gt;Before being able to run our paired t-Test, we must make sure that the &lt;em&gt;difference of response variable pairs&lt;/em&gt; is &lt;strong&gt;normal distributed&lt;/strong&gt;. We can do so using the &lt;code&gt;shapiro.test()&lt;/code&gt; of base &lt;code&gt;R&lt;/code&gt; as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# selecting pre-resettling weights
DataSI &amp;lt;- Data_df$Weight[which(Data_df$Index == &amp;quot;SI&amp;quot;)]
# calculating difference of before and after resettling weights
WeightDiff &amp;lt;- DataSI-Data_df_Resettled$Weight
# shapiro test
shapiro.test(WeightDiff)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Shapiro-Wilk normality test
## 
## data:  WeightDiff
## W = 0.97361, p-value = 0.1716
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thankfully, the &lt;strong&gt;assumption of normality&lt;/strong&gt; is &lt;strong&gt;met&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Now let&amp;rsquo;s visualise that using a qqplot:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;qqnorm(WeightDiff)
qqline(WeightDiff)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;12---Simple-Parametric-Tests_files/figure-html/Norm3-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;climate-warmingextremes-1&#34;&gt;Climate Warming/Extremes&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Does sparrow morphology change depend on climate?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Now let&amp;rsquo;s go on to test whether sparrow weights change significantly per individual due to our relocation experiment (we expect this from future test in our practicals):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;t.test(DataSI, Data_df_Resettled$Weight, paired = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Paired t-test
## 
## data:  DataSI and Data_df_Resettled$Weight
## t = 8.4762, df = 65, p-value = 4.17e-12
## alternative hypothesis: true mean difference is not equal to 0
## 95 percent confidence interval:
##  1.583629 2.559914
## sample estimates:
## mean difference 
##        2.071771
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We were right, individual sparrow weights change significantly after our relocation experiment and we &lt;strong&gt;reject the null hypothesis&lt;/strong&gt;. This is in accordance with the results of the Wilcoxon Signed Rank Test as well as the Friedman Test.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s go on to visualise our data to make better sense of what is going on here:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Select the sparrow weights
Weights &amp;lt;- c(DataSI, Data_df_Resettled$Weight)
# Select the sites
Sites &amp;lt;- factor(rep(c(&amp;quot;SI&amp;quot;, &amp;quot;SI_UK&amp;quot;), each = length(DataSI)))
# Plot
plot(Weights ~ Sites)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;12---Simple-Parametric-Tests_files/figure-html/tpaired1-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Quite obviously sparrows observed in Siberia are heavier than when they are resettled to the United Kingdom (this may be due to the more forgiving climate in the UK). Just like the test stated, the difference of the average weights is roughly 2g between the sparrows at the two sites.&lt;/p&gt;
&lt;h2 id=&#34;one-way-anova&#34;&gt;One-Way ANOVA&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Assumptions of the One-Way ANOVA:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Predictor variable is categorical&lt;/li&gt;
&lt;li&gt;Response variable is metric&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Response variable residuals&lt;/em&gt; are &lt;strong&gt;normal distributed&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Variance of populations/samples are equal (&lt;strong&gt;homogeneity&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;Variable values are &lt;strong&gt;independent&lt;/strong&gt; (not paired)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;testing-for-assumptions&#34;&gt;Testing For Assumptions&lt;/h3&gt;
&lt;p&gt;Firstly, we need to test the assumptions of our One-Way ANOVA. For this purpose, we write another user-defined function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# User-defined function
ANOVACheck &amp;lt;- function(Variables, Grouping, data, plotting){
  Output &amp;lt;- data.frame(x = NA)
  for(i in 1:length(Variables)){
    # data
    Y &amp;lt;- as.numeric(factor(data[,Variables[i]]))
    X &amp;lt;- data[,Grouping]
    Levels &amp;lt;- levels(factor(Data_df[,Grouping]))
    # Residuals?
    model &amp;lt;- lm(Y ~ X)
    Output[1,i] &amp;lt;- shapiro.test(residuals(model))$p.value
    # Homgeneity?
    Levene &amp;lt;- leveneTest(Y ~ X, 
                         center = median, 
                         data = data)
    Output[2,i] &amp;lt;- Levene[1,3]
    # Plotting
    if(plotting == TRUE){
      plot(model, 2)# Normality
      plot(model, 3)# Homogeneity
    }
  }
  colnames(Output) &amp;lt;- Variables
  rownames(Output) &amp;lt;- c(&amp;quot;Residual Normality&amp;quot;, &amp;quot;Homogeneity of Variances&amp;quot;)
  return(Output)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This &lt;code&gt;ANOVACheck()&lt;/code&gt; function takes four arguments: (1) &lt;code&gt;Variables&lt;/code&gt; - a vector of characters holding the names of the variables we want to have tested, (2) &lt;code&gt;Grouping&lt;/code&gt; - the categorical variable by which to group our variables, (3) &lt;code&gt;data&lt;/code&gt; - the data frame which contains the &lt;code&gt;Variables&lt;/code&gt; and the &lt;code&gt;Grouping&lt;/code&gt; factor, and (4) &lt;code&gt;plotting&lt;/code&gt; - a logical indicator of whether to produce plots visualising the test results or not.&lt;br&gt;
The function returns a data frames containing the p-values indexing whether to accept or reject the notion of the normality of residuals per variable (&lt;code&gt;Residual Normality&lt;/code&gt;), and the p-values indexing whether variances between groups are homogeneous or not (&lt;code&gt;Homogeneity of Variances&lt;/code&gt;).&lt;/p&gt;
&lt;h3 id=&#34;climate-warmingextremes-2&#34;&gt;Climate Warming/Extremes&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Does sparrow morphology change depend on climate?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Using the Kruskal-Wallis Test in our last exercise, we already identified climate to be an important factor in determining &lt;em&gt;Passer domesticus&lt;/em&gt; morphology. Let&amp;rsquo;s see if this holds true.&lt;/p&gt;
&lt;p&gt;Take note that we need to limit our analysis to our climate type testing sites again as follows (we include Manitoba this time as it is at the same latitude as the UK and Siberia and holds a semi-coastal climate type):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# prepare climate type testing data
Data_df &amp;lt;- Data_df_base
Index &amp;lt;- Data_df$Index
Rows &amp;lt;- which(Index == &amp;quot;SI&amp;quot; | Index == &amp;quot;UK&amp;quot; | Index == &amp;quot;RE&amp;quot; | Index == &amp;quot;AU&amp;quot; | Index == &amp;quot;MA&amp;quot;)
Data_df &amp;lt;- Data_df[Rows,]
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;assumption-check&#34;&gt;Assumption Check&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s use the &lt;code&gt;ANOVACheck()&lt;/code&gt; function on our data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;par(mfrow=c(3,2))
ANOVACheck(Variables = c(&amp;quot;Weight&amp;quot;, &amp;quot;Height&amp;quot;, &amp;quot;Wing.Chord&amp;quot;), Grouping = &amp;quot;Climate&amp;quot;, data = Data_df, plotting = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in leveneTest.default(y = y, group = group, ...): group coerced to
## factor.

## Warning in leveneTest.default(y = y, group = group, ...): group coerced to
## factor.

## Warning in leveneTest.default(y = y, group = group, ...): group coerced to
## factor.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;12---Simple-Parametric-Tests_files/figure-html/AssCheck1b-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##                               Weight       Height  Wing.Chord
## Residual Normality       0.002521771 2.671414e-05 0.001685579
## Homogeneity of Variances 0.110120912 1.896577e-01 0.013575440
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unfortunately, neither weight nor wing chord records fullfil our requirements.&lt;/p&gt;
&lt;h4 id=&#34;analysis&#34;&gt;Analysis&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s run our analysis for height as grouped by the three-level climate variable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;model &amp;lt;- lm(Data_df$Height ~ Data_df$Climate)
anova(model)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Variance Table
## 
## Response: Data_df$Height
##                  Df Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## Data_df$Climate   2  14.99  7.4942  7.2494 0.0008129 ***
## Residuals       381 393.87  1.0338                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;According to this, climate is a meaningful predictor of height of sparrows and we &lt;strong&gt;reject the null hypothesis&lt;/strong&gt; thus confirming the results of our Kruskall-Wallis analysis.&lt;/p&gt;
&lt;p&gt;Now, let&amp;rsquo;s analyse the output a bit more in-depth:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(model)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Data_df$Height ~ Data_df$Climate)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.98994 -0.69815 -0.01475  0.67142  2.37045 
## 
## Coefficients:
##                             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)                 14.07994    0.07964 176.800  &amp;lt; 2e-16 ***
## Data_df$ClimateContinental  -0.13429    0.11426  -1.175  0.24060    
## Data_df$ClimateSemi-Coastal -0.56039    0.14755  -3.798  0.00017 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.017 on 381 degrees of freedom
## Multiple R-squared:  0.03666,	Adjusted R-squared:  0.0316 
## F-statistic: 7.249 on 2 and 381 DF,  p-value: 0.0008129
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The mean sparrow height in coastal climates is 14.0799387cm (this is our &lt;strong&gt;Intercept&lt;/strong&gt;/&lt;em&gt;Baseline&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;The mean sparrow height in continental climates is -0.1342893cm bigger than the &lt;strong&gt;Intercept&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;The mean sparrow height in semi-coastal climates is -0.5603864cm bigger than the &lt;strong&gt;Intercept&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Only the estimates in coastal and semi-coastal climates are statistically significant&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Personally, I would not place too much confidence in these results due to a couple of reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Our only semi-coastal site is on the northern hemisphere whereas two of our stations are located in the southern hemisphere&lt;/li&gt;
&lt;li&gt;Confounding factors such as population status might have an effect which we are not considering here&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s end this by plotting all of our data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;par(mfrow=c(2,2))
plot(Data_df$Weight ~ factor(Data_df$Climate))
plot(Data_df$Height ~ factor(Data_df$Climate))
plot(Data_df$Wing.Chord ~ factor(Data_df$Climate))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;12---Simple-Parametric-Tests_files/figure-html/Analysis1-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As you can see, the variances are definitely not equal between our groups which explains why part of our assumption test failed.&lt;/p&gt;
&lt;h3 id=&#34;predation&#34;&gt;Predation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Does nesting height depend on predator characteristics?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Again, using the Kruskal-Wallis Test in our last exercise, we already identified predator characteristics to be an important factor in determining &lt;em&gt;Passer domesticus&lt;/em&gt; nesting height. Let&amp;rsquo;s see if this holds true.&lt;/p&gt;
&lt;p&gt;We may wish to use the entirety of our data set again for this purpose:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Data_df &amp;lt;- Data_df_base
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;assumption-check-1&#34;&gt;Assumption Check&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s use our &lt;code&gt;ANOVACeck()&lt;/code&gt; function to test whether we can run our analysis. Before we can do so, however, we need to slightly adjust our predator type variable just like we did in our last exercise and as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# changing levels in predator type
levels(Data_df$Predator.Type) &amp;lt;- c(levels(Data_df$Predator.Type), &amp;quot;None&amp;quot;)
Data_df$Predator.Type[which(is.na(Data_df$Predator.Type))] &amp;lt;- &amp;quot;None&amp;quot;

# Assumption Check
par(mfrow=c(1,2))
ANOVACheck(Variables = &amp;quot;Nesting.Height&amp;quot;, Grouping = &amp;quot;Predator.Type&amp;quot;, data = Data_df, plotting = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;12---Simple-Parametric-Tests_files/figure-html/AssCheck2-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##                          Nesting.Height
## Residual Normality         0.0017160318
## Homogeneity of Variances   0.0005845899
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, our data fails the assumption check. The residuals are definitely not normal distributed and the variance of nesting height records within our groups are not equal.&lt;/p&gt;
&lt;h4 id=&#34;analysis-1&#34;&gt;Analysis&lt;/h4&gt;
&lt;p&gt;Since none of our assumptions are met, we cannot run an ANOVA and therefore resort to data visualisation alone:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(Data_df$Nesting.Height ~ Data_df$Predator.Type)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;12---Simple-Parametric-Tests_files/figure-html/Analysis2-1.png&#34; width=&#34;576&#34; /&gt;
Once more, we can see why our homogeneity of variances test failed.&lt;/p&gt;
&lt;h2 id=&#34;two-way-anova&#34;&gt;Two-Way ANOVA&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Assumptions of the Two-Way ANOVA:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Predictor variables are categorical&lt;/li&gt;
&lt;li&gt;Response variable is metric&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Response variable residuals&lt;/em&gt; are &lt;strong&gt;normal distributed&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Variance of populations/samples are equal (&lt;strong&gt;homogeneity&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;Variable values are &lt;strong&gt;independent&lt;/strong&gt; (not paired)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;testing-for-assumptions-1&#34;&gt;Testing For Assumptions&lt;/h3&gt;
&lt;p&gt;Yet again, we need to check if our assumptions are met first. Automating this procedure is definitely a good idea and only needs slight modification from our &lt;code&gt;ANOVACheck()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# User-defined function
ANOVACheck_TWO &amp;lt;- function(Formulas, data, plotting){
  Output &amp;lt;- data.frame(x = NA)
  for(i in 1:length(Formulas)){
    # Check how many formulas there are
    if(length(Formulas) == 1){
      Expression &amp;lt;- Formulas[[1]]
    }else{
      Expression &amp;lt;- Formulas[[i]]
    }
    # Residuals?
    model &amp;lt;- lm(formula = Expression, data = data)
    Output[1,i] &amp;lt;- shapiro.test(residuals(model))$p.value
    # Homgeneity?
    Levene &amp;lt;- leveneTest(Expression, 
                         center = median, 
                         data = data)
    Output[2,i] &amp;lt;- Levene[1,3]
    # Plotting
    if(plotting == TRUE){
      plot(model, 2)# Normality
      plot(model, 3)# Homogeneity
    }
  }
  colnames(Output) &amp;lt;- as.character(Formulas)
  rownames(Output) &amp;lt;- c(&amp;quot;RN&amp;quot;, &amp;quot;HoV&amp;quot;)
  return(Output)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This &lt;code&gt;ANOVACheck_TWO()&lt;/code&gt; function takes four arguments: (1) &lt;code&gt;Formulas&lt;/code&gt; - a vector of formula specification for our ANOVA models we want to have tested, (2) &lt;code&gt;data&lt;/code&gt; - the data frame which contains the variables and the grouping factor called upon in our &lt;code&gt;Formulas&lt;/code&gt;, and (3) &lt;code&gt;plotting&lt;/code&gt; - a logical indicator of whether to produce plots visualising the test results or not.&lt;br&gt;
The function returns a data frames containing the p-values indexing whether to accept or reject the notion of the normality of residuals per variable (&lt;code&gt;RN&lt;/code&gt;), and the p-values indexing whether variances between groups are homogeneous or not (&lt;code&gt;HoV&lt;/code&gt;).&lt;/p&gt;
&lt;h3 id=&#34;sexual-dimorphism-1&#34;&gt;Sexual Dimorphism&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Does sparrow morphology depend on population status and sex?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Given different factors affecting invasive species, we might expect different patterns of sexual dimorphism for invasive and native populations. Take note that we keep using the northern hemisphere subset our cimate testing sites as these present us with a nice set of invasive/native population records already whilst keeping confounding factors to a minimum.&lt;/p&gt;
&lt;h4 id=&#34;assumption-check-2&#34;&gt;Assumption Check&lt;/h4&gt;
&lt;p&gt;First, we need to check our assumptions:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# prepare climate type testing data
Data_df &amp;lt;- Data_df_base
Index &amp;lt;- Data_df$Index
Rows &amp;lt;- which(Index == &amp;quot;SI&amp;quot; | Index == &amp;quot;UK&amp;quot; | Index == &amp;quot;MA&amp;quot;)
Data_df &amp;lt;- Data_df[Rows,]

# analysis
par(mfrow=c(3,2))
ANOVACheck_TWO(Formulas = c(Weight ~ Population.Status*Sex, 
                            Height ~ Population.Status*Sex,
                            Wing.Chord ~ Population.Status*Sex)
               , data = Data_df, plotting = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;12---Simple-Parametric-Tests_files/figure-html/AssCheck5-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##     Weight ~ Population.Status * Sex Height ~ Population.Status * Sex
## RN                       0.287959531                        0.2171916
## HoV                      0.004492103                        0.9057774
##     Wing.Chord ~ Population.Status * Sex
## RN                             0.1907782
## HoV                            0.9174340
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again our assumptions are not met except for sparrow height and wing chord as a product of sex and population status.&lt;/p&gt;
&lt;h4 id=&#34;analysis-2&#34;&gt;Analysis&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s run our analysis:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# height model
model &amp;lt;- lm(Height ~ Population.Status*Sex, data = Data_df)
anova(model)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Variance Table
## 
## Response: Height
##                        Df  Sum Sq Mean Sq F value Pr(&amp;gt;F)
## Population.Status       1   0.338 0.33820  0.3190 0.5729
## Sex                     1   0.179 0.17896  0.1688 0.6816
## Population.Status:Sex   1   1.786 1.78585  1.6844 0.1959
## Residuals             197 208.865 1.06023
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# wing chord model
model &amp;lt;- lm(Wing.Chord ~ Population.Status*Sex, data = Data_df)
anova(model)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Variance Table
## 
## Response: Wing.Chord
##                        Df Sum Sq  Mean Sq F value Pr(&amp;gt;F)
## Population.Status       1 0.0047 0.004669  0.1955 0.6589
## Sex                     1 0.0041 0.004125  0.1727 0.6782
## Population.Status:Sex   1 0.0399 0.039856  1.6688 0.1979
## Residuals             197 4.7049 0.023883
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# plotting
par(mfrow=c(1,2))
boxplot(Height ~ Population.Status*Sex, data = Data_df, col = c(&amp;quot;blue&amp;quot;, &amp;quot;red&amp;quot;))
boxplot(Wing.Chord ~ Population.Status*Sex, data = Data_df, col = c(&amp;quot;blue&amp;quot;, &amp;quot;red&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;12---Simple-Parametric-Tests_files/figure-html/Analysis5-1.png&#34; width=&#34;576&#34; /&gt;
As it turns out, population status and sex are no viable predictors for sparrow height or wing chord and so we &lt;strong&gt;accept the null hypothesis&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;ancova&#34;&gt;ANCOVA&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Assumptions of the ANCOVA:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Predictor variables are categorical or continuous&lt;/li&gt;
&lt;li&gt;Response variable is metric&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Response variable residuals&lt;/em&gt; are &lt;strong&gt;normal distributed&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Variance of populations/samples are equal (&lt;strong&gt;homogeneity&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;Variable values are &lt;strong&gt;independent&lt;/strong&gt; (not paired)&lt;/li&gt;
&lt;li&gt;Relationship between the response and covariate is &lt;strong&gt;linear&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;climate-warmingextremes-3&#34;&gt;Climate Warming/Extremes&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Do sparrow characteristics depend on climate and latitude?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Latitude may have masked some effects of climate on sparrow morphology in our preceding analyses and vice-versa. At times, we have been able to account for this by including our site records, which can be seen as binned versions of latitude records. Let&amp;rsquo;s test if the inclusion of raw latitude records are meaningful.&lt;/p&gt;
&lt;h4 id=&#34;assumption-check-3&#34;&gt;Assumption Check&lt;/h4&gt;
&lt;p&gt;Again, we need to do an assumption check. However, we need a new function for this, since we now need to test whether our response variable and the covariate are linear or not:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# overwriting prior changes in Data_df
Data_df &amp;lt;- Data_df_base
Data_df$Latitude &amp;lt;- abs(Data_df$Latitude)
# User-defined function
ANCOVACheck &amp;lt;- function(Variables, Grouping, Covariate, data, plotting){
  Output &amp;lt;- data.frame(x = NA)
  for(i in 1:length(Variables)){
    # data
    Y &amp;lt;- as.numeric(factor(data[,Variables[i]]))
    X &amp;lt;- factor(data[,Grouping])
    Z &amp;lt;- data[, Covariate]
    # Residuals?
    model &amp;lt;- lm(Y ~ X*Z)
    Output[1,i] &amp;lt;- shapiro.test(residuals(model))$p.value
    # Homgeneity?
    Levene &amp;lt;- leveneTest(Y ~ X, 
                         center = median, 
                         data = data)
    Output[2,i] &amp;lt;- Levene[1,3]
    # Plotting
    if(plotting == TRUE){
      plot(model, 1)# Linearity
      plot(model, 2)# Normality
      plot(model, 3)# Homogeneity
    }
  }
  colnames(Output) &amp;lt;- Variables
  rownames(Output) &amp;lt;- c(&amp;quot;RN&amp;quot;, &amp;quot;HoV&amp;quot;)
  return(Output)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This &lt;code&gt;ANCOVACheck()&lt;/code&gt; function takes five arguments: (1) &lt;code&gt;Variables&lt;/code&gt; - a vector of response variables used in our models, (2) &lt;code&gt;Grouping&lt;/code&gt; - the categorical variable by which to group our variables, (3) &lt;code&gt;Covariate&lt;/code&gt; - the covariate of our analysis, (4)&lt;code&gt;data&lt;/code&gt; - the data frame which contains the variables, the grouping factor and our covariate, and (5) &lt;code&gt;plotting&lt;/code&gt; - a logical indicator of whether to produce plots visualising the test results or not.&lt;br&gt;
The function returns a data frames containing the p-values indexing whether to accept or reject the notion of the normality of residuals per variable (&lt;code&gt;RN&lt;/code&gt;), and the p-values indexing whether variances between groups are homogeneous or not (&lt;code&gt;HoV&lt;/code&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ANCOVACheck(Variables = c(&amp;quot;Weight&amp;quot;, &amp;quot;Height&amp;quot;, &amp;quot;Wing.Chord&amp;quot;, &amp;quot;Nesting.Height&amp;quot;, &amp;quot;Egg.Weight&amp;quot;, &amp;quot;Number.of.Eggs&amp;quot;, &amp;quot;Home.Range&amp;quot;), 
            Grouping = &amp;quot;Climate&amp;quot;, Covariate = &amp;quot;Latitude&amp;quot;, 
            data = Data_df, plotting = FALSE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           Weight       Height   Wing.Chord Nesting.Height   Egg.Weight
## RN  2.082300e-02 1.502944e-04 4.535941e-07   5.190971e-06 2.943560e-03
## HoV 9.937376e-24 1.929783e-22 1.561040e-33   2.004612e-01 4.816355e-09
##     Number.of.Eggs   Home.Range
## RN    1.809197e-09 3.629841e-20
## HoV   2.750100e-14 1.157673e-08
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The assumptions aren&amp;rsquo;t met. I have set the &lt;code&gt;plotting&lt;/code&gt; argument to &lt;code&gt;FALSE&lt;/code&gt; tu suppress the plotting of model checking visualisation. The would be useful to judge linearity but not necessary here since the other two important assumptions (Homogeneity of variances and Normality of residuals) aren&amp;rsquo;t met to begin with.&lt;/p&gt;
&lt;h4 id=&#34;analysis-3&#34;&gt;Analysis&lt;/h4&gt;
&lt;p&gt;Since none of our assumptions are met, we cannot run an ANOVA and therefore resort to data visualisation alone. We need a new function for this to do our plotting easily and automatically with some colours indicating our grouping factors whilst plotting response variables versus covariates.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;PlotAncovas &amp;lt;- function(Variables, Grouping, Covariate, data){
  for(i in 1:length(Variables)){
    Y &amp;lt;- Data_df[,Variables[i]]
    if(class(Y) == &amp;quot;character&amp;quot;){Y &amp;lt;- factor(Y)}
    X &amp;lt;- Data_df[,Covariate]
    G &amp;lt;- factor(Data_df[, Grouping])
    plot(X, Y, col = G, xlab = Covariate, ylab = Variables[i])
    legend(&amp;quot;top&amp;quot;, # place legend at the top
           inset = -0.35, # move legend away from plot centre
           xpd = TRUE, # allow legend outside of plot area
           legend=levels(G), # what to include in legend
           bg = &amp;quot;white&amp;quot;, col = unique(G), ncol=length(levels(G)), # colours
           pch = 1, # plotting symbols
           title = Variables[i] # title of legend
           )
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;PlotAncovas()&lt;/code&gt; returns a scatter plot and takes four arguments: (1) &lt;code&gt;Variables&lt;/code&gt; - a vector of response variables, (2) &lt;code&gt;Grouping&lt;/code&gt; - the name of the grouping factor according to which to colour the symbols in our plot, (3) &lt;code&gt;Covariate&lt;/code&gt; - the covariate against which to plot individuals variables, and (4) &lt;code&gt;data&lt;/code&gt; - the data frame which holds our variables.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s use our function:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;par(mfrow=c(1,2))
PlotAncovas(Variables = c(&amp;quot;Weight&amp;quot;, &amp;quot;Height&amp;quot;, &amp;quot;Wing.Chord&amp;quot;, &amp;quot;Nesting.Height&amp;quot;, &amp;quot;Egg.Weight&amp;quot;, &amp;quot;Number.of.Eggs&amp;quot;, &amp;quot;Home.Range&amp;quot;), Grouping = &amp;quot;Climate&amp;quot;, Covariate = &amp;quot;Latitude&amp;quot;, data = Data_df)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I will not interpret these plots here in text and leave this to you.&lt;/p&gt;
&lt;p&gt;Take note that this &lt;strong&gt;could&amp;rsquo;ve been achieved much easier with &lt;code&gt;ggplot2&lt;/code&gt;!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;12---Simple-Parametric-Tests_files/figure-html/Analysisc-1.png&#34; width=&#34;576&#34; /&gt;&lt;img src=&#34;12---Simple-Parametric-Tests_files/figure-html/Analysisc-2.png&#34; width=&#34;576&#34; /&gt;&lt;img src=&#34;12---Simple-Parametric-Tests_files/figure-html/Analysisc-3.png&#34; width=&#34;576&#34; /&gt;&lt;img src=&#34;12---Simple-Parametric-Tests_files/figure-html/Analysisc-4.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;sparrow-characteristics-and-sites&#34;&gt;Sparrow Characteristics And Sites&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;This was not part of what we set out to do according to the lecture slides but has been included as a logical conclusion to an earlier analysis.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Unfortunately, our previous attempt at an ANCOVA didn&amp;rsquo;t work. So what other covariate do we have available for sparrow characteristics?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Latitude&lt;/em&gt; doesn&amp;rsquo;t make sense to include when grouping by site index as these two are synonymous&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Longitude&lt;/em&gt; doesn&amp;rsquo;t make sense to include when grouping by site index as these two are synonymous&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Weight&lt;/em&gt; is well explained by other variables and we know the causal links&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Height&lt;/em&gt; is not that well explained by other variables&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Wing.Chord&lt;/em&gt; is not that well explained by other variables&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Of course, there are more within our data set but it has become apparent that &lt;code&gt;Weight&lt;/code&gt; may make for an important covariate in our site-wise ANCOVA set-up. Using the Pearson correlation (third practical), we already identified a causal link between sparrow &lt;code&gt;Weight&lt;/code&gt; and &lt;code&gt;Height&lt;/code&gt; per site.&lt;/p&gt;
&lt;h4 id=&#34;assumption-check-4&#34;&gt;Assumption Check&lt;/h4&gt;
&lt;p&gt;Firstly, we test whether assumptions are met. For brevities sake, we only test four variables:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;par(mfrow=c(1,3))
ANCOVACheck(Variables = c(&amp;quot;Height&amp;quot;, &amp;quot;Wing.Chord&amp;quot;, &amp;quot;Egg.Weight&amp;quot;, &amp;quot;Number.of.Eggs&amp;quot;), Grouping = &amp;quot;Index&amp;quot;, Covariate = &amp;quot;Weight&amp;quot;, data = Data_df, plotting = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;12---Simple-Parametric-Tests_files/figure-html/AssCheck7-1.png&#34; width=&#34;576&#34; /&gt;&lt;img src=&#34;12---Simple-Parametric-Tests_files/figure-html/AssCheck7-2.png&#34; width=&#34;576&#34; /&gt;&lt;img src=&#34;12---Simple-Parametric-Tests_files/figure-html/AssCheck7-3.png&#34; width=&#34;576&#34; /&gt;&lt;img src=&#34;12---Simple-Parametric-Tests_files/figure-html/AssCheck7-4.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##           Height   Wing.Chord   Egg.Weight Number.of.Eggs
## RN  1.499909e-06 5.251393e-08 0.1565038171   9.220862e-07
## HoV 3.594021e-13 2.434880e-01 0.0002015813   2.660146e-02
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As it turns out, we can run our ANCOVA on &lt;code&gt;Egg.Weight&lt;/code&gt; when grouped by site &lt;code&gt;Index&lt;/code&gt; and driven by &lt;code&gt;Weight&lt;/code&gt;.&lt;/p&gt;
&lt;h4 id=&#34;analysis-4&#34;&gt;Analysis&lt;/h4&gt;
&lt;p&gt;First, let&amp;rsquo;s visualise our data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;PlotAncovas(Variables = &amp;quot;Egg.Weight&amp;quot;, Grouping = &amp;quot;Index&amp;quot;, Covariate = &amp;quot;Weight&amp;quot;, data = Data_df)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;12---Simple-Parametric-Tests_files/figure-html/Analysis7-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Quite obviously, Belize (BE) records are very different from the other stations, whose egg weight and weight records are grouped together. There seems to be some evidence for an overall linkage of sparrow weight and egg weight (a positive correlation).&lt;/p&gt;
&lt;p&gt;Now we run the analysis:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;LM_fit5 &amp;lt;- lm(Egg.Weight ~ Weight*Index, data = Data_df)
anova(LM_fit5)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Variance Table
## 
## Response: Egg.Weight
##               Df Sum Sq Mean Sq   F value Pr(&amp;gt;F)    
## Weight         1 52.531  52.531 1442.5616 &amp;lt;2e-16 ***
## Index         10  8.087   0.809   22.2064 &amp;lt;2e-16 ***
## Weight:Index  10  0.129   0.013    0.3536 0.9653    
## Residuals    455 16.569   0.036                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above ANCOVA output tells us that there is no interaction effect between sites and sparrow weights when determining mean egg weight per nest of &lt;em&gt;Passer domesticus&lt;/em&gt; and so we do another iteration of our model and remove the postulated interaction:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;LM_fit6 &amp;lt;- lm(Egg.Weight ~ Weight+Index, data = Data_df)
anova(LM_fit6)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Variance Table
## 
## Response: Egg.Weight
##            Df Sum Sq Mean Sq  F value    Pr(&amp;gt;F)    
## Weight      1 52.531  52.531 1462.898 &amp;lt; 2.2e-16 ***
## Index      10  8.087   0.809   22.519 &amp;lt; 2.2e-16 ***
## Residuals 465 16.698   0.036                       
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By now, all of our model coefficients are significant and we can go on to interpret them:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(LM_fit6)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Egg.Weight ~ Weight + Index, data = Data_df)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.58887 -0.13146 -0.00621  0.12033  0.55135 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  3.345984   0.280826  11.915  &amp;lt; 2e-16 ***
## Weight       0.001081   0.008614   0.125 0.900203    
## IndexBE     -0.708478   0.054864 -12.913  &amp;lt; 2e-16 ***
## IndexFG     -1.281168   0.099135 -12.923  &amp;lt; 2e-16 ***
## IndexFI     -0.625287   0.057442 -10.885  &amp;lt; 2e-16 ***
## IndexLO     -0.550137   0.051754 -10.630  &amp;lt; 2e-16 ***
## IndexMA     -0.513645   0.051352 -10.003  &amp;lt; 2e-16 ***
## IndexNU     -0.517015   0.053365  -9.688  &amp;lt; 2e-16 ***
## IndexRE     -0.612632   0.051418 -11.915  &amp;lt; 2e-16 ***
## IndexSA     -0.806045   0.056685 -14.220  &amp;lt; 2e-16 ***
## IndexSI     -0.272580   0.077868  -3.501 0.000509 ***
## IndexUK     -0.511667   0.051404  -9.954  &amp;lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.1895 on 465 degrees of freedom
##   (590 observations deleted due to missingness)
## Multiple R-squared:  0.784,	Adjusted R-squared:  0.7789 
## F-statistic: 153.5 on 11 and 465 DF,  p-value: &amp;lt; 2.2e-16
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 13</title>
      <link>https://www.erikkusch.com/courses/rethinking/chapter-13/</link>
      <pubDate>Wed, 07 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/rethinking/chapter-13/</guid>
      <description>&lt;h1 id=&#34;models-with-memory&#34;&gt;Models with Memory&lt;/h1&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/rethinking/15__09-04-2021_SUMMARY_-Multi-Level-Models.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides Chapter 13&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to the exercises at the end of chapter 13 in 
&lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Satistical Rethinking 2&lt;/a&gt; by Richard McElreath. I have created these notes as a part of my ongoing involvement in the 
&lt;a href=&#34;https://www.erikkusch.com/project/aubayes/&#34;&gt;AU Bayes Study Group&lt;/a&gt;. Much of my inspiration for these solutions, where necessary, has been obtained from&lt;/p&gt;
&lt;!-- [Taras Svirskyi](https://github.com/jffist/statistical-rethinking-solutions/blob/master/ch10_hw.R), [William Wolf](https://github.com/cavaunpeu/statistical-rethinking/blob/master/chapter-10/homework.R), and [Corrie Bartelheimer](https://www.samples-of-thoughts.com/projects/statistical-rethinking/chapter_10/chp10-ex/) as well as  --&gt;
&lt;p&gt;the solutions provided to instructors by Richard McElreath himself.&lt;/p&gt;
&lt;h2 id=&#34;r-environment&#34;&gt;&lt;code&gt;R&lt;/code&gt; Environment&lt;/h2&gt;
&lt;p&gt;For today&amp;rsquo;s exercise, I load the following packages:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rethinking)
library(rstan)
library(ggplot2)
library(tidybayes)
library(cowplot)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;easy-exercises&#34;&gt;Easy Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-e1&#34;&gt;Practice E1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Which of the following priors will produce more shrinkage in the estimates?&lt;/p&gt;
&lt;p&gt;(a) $Î±_{tank} â¼ Normal(0, 1)$&lt;br&gt;
(b) $Î±_{tank} â¼ Normal(0, 2)$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  Shrinkage is introduced by regularising/informative priors. This means that option (a) will introduce more shrinkage because it&amp;rsquo;s distribution is narrower than that of (b) and thus more informative/regularising.&lt;/p&gt;
&lt;h3 id=&#34;practice-e2&#34;&gt;Practice E2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Make the following model into a multilevel model.&lt;/p&gt;
&lt;p&gt;$$y_i â¼ Binomial(1, p_i)$$
$$logit(p_i) = Î±_{group[i]} + Î²x_i$$ 
$$Î±_{group} â¼ Normal(0, 10)$$ 
$$Î² â¼ Normal(0, 1)$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  To make the above model into a multi-level model, we need to assign some hyperpriors. These are priors on parameters of parameters. In this case, we express $\alpha_{group[i]}$ (a parameter) through a prior with another set of parameters ($\bar\alpha, \sigma_\alpha$). These parameters, in turn, require priors themselves - so called hyperpriors.&lt;/p&gt;
&lt;p&gt;$$y_i â¼ Binomial(1, p_i)$$
$$logit(p_i) = Î±_{group[i]} + Î²x_i$$ 
$$Î±_{group} â¼ Normal(\bar\alpha, \sigma_\alpha)$$ 
$$\bar\alpha \sim Normal(0, 2)$$
$$\sigma_\alpha \sim Exponential(1)$$
$$Î² â¼ Normal(0, 1)$$&lt;/p&gt;
&lt;p&gt;The numbers we feed into our hyperpriors here are difficult to assess for sensibility since we don&amp;rsquo;t have any data to test the performance of both models and their assumptions.&lt;/p&gt;
&lt;h3 id=&#34;practice-e3&#34;&gt;Practice E3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Make the following model into a multilevel model.&lt;/p&gt;
&lt;p&gt;$$y_i â¼ Normal(\mu, \sigma)$$
$$logit(p_i) = Î±_{group[i]} + Î²x_i$$ 
$$Î±_{group} â¼ Normal(0, 10)$$ 
$$\beta \sim Normal(0, 1)$$
$$\sigma â¼ HalfCauchy(0, 2)$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  Well this is just a repeat of the previous problem:&lt;/p&gt;
&lt;p&gt;$$y_i â¼ Normal(\mu_i, \sigma)$$
$$logit(p_i) = Î±_{group[i]} + Î²x_i$$ 
$$Î±_{group} â¼ Normal(\bar\alpha, \sigma_\alpha)$$ 
$$\bar\alpha \sim Normal(0, 2)$$
$$\sigma_\alpha \sim Exponential(1)$$
$$\beta \sim Normal(0, 1)$$
$$\sigma â¼ HalfCauchy(0, 2)$$&lt;/p&gt;
&lt;h3 id=&#34;practice-e4&#34;&gt;Practice E4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Write an example mathematical model formula for a Poisson regression with varying intercepts.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  This is simply just the solution to E2 with a change to the outcome distribution (now Poisson) and link function (now log):&lt;/p&gt;
&lt;p&gt;$$y_i â¼ Poisson(\lambda_i)$$
$$log(\lambda_i) = Î±_{group[i]} + Î²x_i$$ 
$$Î±_{group} â¼ Normal(\bar\alpha, \sigma_\alpha)$$ 
$$\bar\alpha \sim Normal(0, 2)$$
$$\sigma_\alpha \sim Exponential(1)$$
$$Î² â¼ Normal(0, 1)$$&lt;/p&gt;
&lt;p&gt;Again, I would like to highlight that I can&amp;rsquo;t set any meaningful priors here because I have no idea what we are analysing. These exercises are just about model structure, I wager.&lt;/p&gt;
&lt;h3 id=&#34;practice-e5&#34;&gt;Practice E5&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Write an example mathematical model formula for a Poisson regression with two different kinds of varying intercepts, a cross-classified model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  I start with the solution to E4 and add another intercept group ($\gamma$) which target a cluster of days like the example in the book:&lt;/p&gt;
&lt;p&gt;$$y_i â¼ Poisson(\lambda_i)$$
$$log(\lambda_i) = Î±_{group[i]}+ \gamma_{day[i]} + Î²x_i$$ 
$$Î±_{group} â¼ Normal(\bar\alpha, \sigma_\alpha)$$ 
$$\bar\alpha \sim Normal(0, 2)$$
$$\sigma_\alpha \sim Exponential(1)$$
$$\gamma_{day} â¼ Normal(\bar\gamma, \sigma_\gamma)$$ 
$$\bar\gamma \sim Normal(0, 2)$$
$$\sigma_\gamma \sim Exponential(1)$$&lt;/p&gt;
&lt;p&gt;$$Î² â¼ Normal(0, 1)$$&lt;/p&gt;
&lt;h2 id=&#34;medium-exercises&#34;&gt;Medium Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-m1&#34;&gt;Practice M1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Revisit the Reed frog survival data, &lt;code&gt;data(reedfrogs)&lt;/code&gt;, and add the predation and size treatment variables to the varying intercepts model. Consider models with either main effect alone, both main effects, as well as a model including both and their interaction. Instead of focusing on inferences about these two predictor variables, focus on the inferred variation across tanks. Explain why it changes as it does across models.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  This corresponds to the multi-level tadpole example in the book (starting in section 13.1 on page 415). First, I load the data and prepare the data list as was done in the book and add in the data about predation (binary - yes/no) and size treatment (binary - small/large):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(reedfrogs)
d &amp;lt;- reedfrogs
dat &amp;lt;- list(
  S = d$surv,
  n = d$density,
  tank = 1:nrow(d),
  pred = ifelse(d$pred == &amp;quot;no&amp;quot;, 0L, 1L),
  size_ = ifelse(d$size == &amp;quot;small&amp;quot;, 1L, 2L)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;ulam()&lt;/code&gt; doesn&amp;rsquo;t like when any data value is called &lt;code&gt;size&lt;/code&gt;. That&amp;rsquo;s why I call it &lt;code&gt;size_&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now, I can define the models. Note that I am running all of them with &lt;code&gt;log_lik = TRUE&lt;/code&gt; so I can compare them later:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Tank-only&lt;/strong&gt; model which will serve as our baseline.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_Tank &amp;lt;- ulam(
  alist(
    S ~ dbinom(n, p),
    logit(p) &amp;lt;- a[tank],
    a[tank] ~ dnorm(a_bar, sigma),
    a_bar ~ dnorm(0, 1.5),
    sigma ~ dexp(1)
  ),
  data = dat, chains = 4, cores = 4, log_lik = TRUE, iter = 2e3
)
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;Predation&lt;/strong&gt; model:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_Pred &amp;lt;- ulam(
  alist(
    S ~ dbinom(n, p),
    logit(p) &amp;lt;- a[tank] + bp * pred,
    a[tank] ~ dnorm(a_bar, sigma),
    bp ~ dnorm(-0.5, 1),
    a_bar ~ dnorm(0, 1.5),
    sigma ~ dexp(1)
  ),
  data = dat, chains = 4, cores = 4, log_lik = TRUE, iter = 2e3
)
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;strong&gt;Size&lt;/strong&gt; model:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_Size &amp;lt;- ulam(
  alist(
    S ~ dbinom(n, p),
    logit(p) &amp;lt;- a[tank] + s[size_],
    a[tank] ~ dnorm(a_bar, sigma),
    s[size_] ~ dnorm(0, 0.5),
    a_bar ~ dnorm(0, 1.5),
    sigma ~ dexp(1)
  ),
  data = dat, chains = 4, cores = 4, log_lik = TRUE, iter = 2e3
)
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;&lt;strong&gt;Predation + Size&lt;/strong&gt; model:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_Additive &amp;lt;- ulam(
  alist(
    S ~ dbinom(n, p),
    logit(p) &amp;lt;- a[tank] + bp * pred + s[size_],
    a[tank] ~ dnorm(a_bar, sigma),
    bp ~ dnorm(-0.5, 1),
    s[size_] ~ dnorm(0, 0.5),
    a_bar ~ dnorm(0, 1.5),
    sigma ~ dexp(1)
  ),
  data = dat, chains = 4, cores = 4, log_lik = TRUE, iter = 2e3
)
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;&lt;strong&gt;Predation-Size-Interaction&lt;/strong&gt; model:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# this is a con-centred parametrisation for giggles:
m_Interaction &amp;lt;- ulam(
  alist(
    S ~ dbinom(n, p),
    logit(p) &amp;lt;- a_bar + a[tank] * sigma + bp[size_] * pred + s[size_], # interaction comes in via bP[size_]
    a[tank] ~ dnorm(0, 1),
    bp[size_] ~ dnorm(-0.5, 1),
    s[size_] ~ dnorm(0, 0.5),
    a_bar ~ dnorm(0, 1.5),
    sigma ~ dexp(1)
  ),
  data = dat, chains = 4, cores = 4, log_lik = TRUE, iter = 2e3
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have all the models ready, we can assess the variation among tanks. This information is contained within the &lt;code&gt;sigma&lt;/code&gt; parameter in all of the models:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(coeftab(m_Tank, m_Pred, m_Size, m_Additive, m_Interaction),
  pars = &amp;quot;sigma&amp;quot;,
  labels = c(&amp;quot;Tank&amp;quot;, &amp;quot;Predation&amp;quot;, &amp;quot;Size&amp;quot;, &amp;quot;Additive&amp;quot;, &amp;quot;Interaction&amp;quot;)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-04-07-statistical-rethinking-chapter-13_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Quite evidently, omitting &lt;code&gt;pred&lt;/code&gt; (predation) from our models assigns a lot of variation to the &lt;code&gt;tank&lt;/code&gt; variable. Conclusively, we can say that predation explains a lot of the variation across tanks and helps to explain it. Omitting predation from our models simply assigns this variation to the tank intercepts without explaining it.&lt;/p&gt;
&lt;h3 id=&#34;practice-m2&#34;&gt;Practice M2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Compare the models you fit just above, using WAIC. Can you reconcile the differences in WAIC with the posterior distributions of the models?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;compare(m_Tank, m_Pred, m_Size, m_Additive, m_Interaction)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                   WAIC       SE     dWAIC      dSE    pWAIC    weight
## m_Interaction 199.0758 9.089561 0.0000000       NA 18.81776 0.2771353
## m_Pred        199.5219 8.995153 0.4460306 3.177855 19.44211 0.2217367
## m_Additive    199.9935 8.737508 0.9176872 2.209613 19.16555 0.1751534
## m_Tank        200.0751 7.259051 0.9992752 6.180324 20.94365 0.1681520
## m_Size        200.2019 7.140956 1.1260679 5.992834 20.97228 0.1578226
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Evidently, all of our models are expected to perform similarly in out-of-sample predictions. So how do the posterior samples look like? Here, I write a function to extract all parameter samples from the posterior given any of our models except the $\alpha$ parameters and feed them into a &lt;code&gt;ggplot&lt;/code&gt; using the beautiful &lt;code&gt;stat_halfeye()&lt;/code&gt; from the &lt;code&gt;tidybayes&lt;/code&gt; package:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;na.omit.list &amp;lt;- function(y) {
  return(y[!sapply(y, function(x) all(is.na(x)))])
}
Halfeyes_NoAs &amp;lt;- function(model = NULL, N = 1e4) {
  Samples &amp;lt;- extract.samples(model, n = N)
  list &amp;lt;- as.list(rep(NA, sum(!startsWith(names(model@coef), &amp;quot;a[&amp;quot;))))
  names(list) &amp;lt;- names(model@coef)[!startsWith(names(model@coef), &amp;quot;a[&amp;quot;)]
  for (i in names(Samples)) {
    if (i == &amp;quot;a&amp;quot;) {
      next
    } # skip all &amp;quot;a&amp;quot; parameters
    if (is.na(dim(Samples[[i]])[2])) {
      list[[i]] &amp;lt;- data.frame(
        Posterior = Samples[[i]],
        Parameter = rep(i, length(Samples[[i]]))
      )
    } else { # if there are multiple parameter levels
      list[[i]] &amp;lt;- data.frame(
        Posterior = Samples[[i]][, 1],
        Parameter = rep(paste(i, 1, sep = &amp;quot;_&amp;quot;), length(Samples[[i]]))
      )
      for (k in 2:dim(Samples[[i]])[2]) {
        list[[i]] &amp;lt;- rbind(
          list[[i]],
          data.frame(
            Posterior = Samples[[i]][, k],
            Parameter = rep(paste(i, k, sep = &amp;quot;_&amp;quot;), length(Samples[[i]]))
          )
        )
      }
    }
  } # Samples-loop
  Plot_df &amp;lt;- do.call(&amp;quot;rbind&amp;quot;, na.omit.list(list))
  Plot_gg &amp;lt;- ggplot(Plot_df, aes(y = Parameter, x = Posterior)) +
    stat_halfeye() +
    labs(x = &amp;quot;Parameter Estimate&amp;quot;, y = &amp;quot;Parameter&amp;quot;) +
    geom_vline(xintercept = 0, color = &amp;quot;red&amp;quot;) +
    theme_bw(base_size = 20)
  return(Plot_gg)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I don&amp;rsquo;t claim that this is beautiful code. There&amp;rsquo;s probably and easier way of doing this. Basically, this is a botch job. I am aware that it is, but it works for now.&lt;/p&gt;
&lt;p&gt;Let me apply this to our models and then show you the plots:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot_ls &amp;lt;- lapply(list(m_Tank, m_Pred, m_Size, m_Additive, m_Interaction), Halfeyes_NoAs, N = 1e4)
plot_grid(
  plotlist = plot_ls, labels = c(&amp;quot;Tank&amp;quot;, &amp;quot;Predation&amp;quot;, &amp;quot;Size&amp;quot;, &amp;quot;Additive&amp;quot;, &amp;quot;Interaction&amp;quot;),
  ncol = 1, vjust = 1.25, hjust = -0.1
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-04-07-statistical-rethinking-chapter-13_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;These plots only tell us what our models have sampled from the posterior in terms of parameter estimates. They do not tell us how accurate the models are when predicting data. However, they do tell us loads about what the models use to make their predictions.&lt;/p&gt;
&lt;p&gt;For now, I will focus on the posterior distributions of our predation parameter (&lt;code&gt;bp&lt;/code&gt;) and size parameter (&lt;code&gt;s&lt;/code&gt;). When inspecting these, it is apparent that the parameter estimates of &lt;code&gt;bp&lt;/code&gt; are much further from 0 than those for &lt;code&gt;s&lt;/code&gt;. This holds true across all models. In addition, anytime &lt;code&gt;bp&lt;/code&gt; is contained in a model, &lt;code&gt;sigma&lt;/code&gt; (the variation in tank intercepts) decreases drastically.&lt;/p&gt;
&lt;p&gt;This is consistent with the model rankings. The tank-only model does not because size and predation are meaningless predictors. The posterior distributions above show us that they do contain important information. The tank-only model does well because there exists variation among tanks for a multitude of reasons. Prediction and inference of causality are not the same thing, after all.&lt;/p&gt;
&lt;h3 id=&#34;practice-m3&#34;&gt;Practice M3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Re-estimate the basic Reed frog varying intercept model, but now using a Cauchy distribution in place of the Gaussian distribution for the varying intercepts. That is, fit this model:&lt;/p&gt;
&lt;p&gt;$$s_i â¼ Binomial(n_i, p_i)$$
$$logit(p_i) = Î±_{tank[i]}$$ 
$$Î±_{tank} â¼ Cauchy(\alpha, \sigma)$$ 
$$\alpha â¼ Normal(0, 1)$$
$$\sigma â¼ Exponential(1)$$&lt;/p&gt;
&lt;p&gt;Compare the posterior means of the intercepts, $Î±_{tank}$, to the posterior means produced in the chapter, using the customary Gaussian prior. Can you explain the pattern of differences?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  This is simply the &lt;code&gt;m_Tank&lt;/code&gt; model we ran previously, but with a &lt;code&gt;dcauchy()&lt;/code&gt; prior on $\alpha_{tank}$. Because the Cauchy distribution comes with very long tails, we run into a few issues of divergent transitions with default parameters and so I add &lt;code&gt;control=list(adapt_delta=0.99)&lt;/code&gt; to the call to &lt;code&gt;ulam()&lt;/code&gt; for more measured sampling of the posterior space:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_TankCauchy &amp;lt;- ulam(
  alist(
    S ~ dbinom(n, p),
    logit(p) &amp;lt;- a[tank],
    a[tank] ~ dcauchy(a_bar, sigma),
    a_bar ~ dnorm(0, 1.5),
    sigma ~ dexp(1)
  ),
  data = dat, chains = 4, cores = 4, log_lik = TRUE,
  iter = 2e3, control = list(adapt_delta = 0.99)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let&amp;rsquo;s compare the posterior means of &lt;code&gt;m_Tank&lt;/code&gt; and the new &lt;code&gt;m_TankCauchy&lt;/code&gt; for their estimates of the $\alpha_{tank}$ parameters:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;a_Tank &amp;lt;- apply(extract.samples(m_Tank)$a, 2, mean)
a_TankCauchy &amp;lt;- apply(extract.samples(m_TankCauchy)$a, 2, mean)
plot(a_Tank, a_TankCauchy,
  pch = 16, col = rangi2,
  xlab = &amp;quot;intercept (Gaussian prior)&amp;quot;, ylab = &amp;quot; intercept (Cauchy prior)&amp;quot;
)
abline(a = 0, b = 1, lty = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-04-07-statistical-rethinking-chapter-13_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;1440&#34; /&gt;
For most of our intercepts ($\alpha_{tank}$), both the Cauchy-prior model and the Gaussian-prior model are basically creating the same results (i.e. points on the dashed line). However, once we hit extreme $\alpha_{tank}$ under the Gaussian prior, the $\alpha_{tank}$ estimates of the Cauchy prior are even more extreme by comparison. This is because of how much adaptive shrinkage is going on. In the tanks on the right-hand side of the plot above, extreme proportions of tadpoles survived the experiment. These estimates are shrunk towards the population (i.e. all tanks) mean. Since the Gaussian distribution is more concentrated than the Cauchy distribution, the Gaussian estimates have more shrinkage applied to them and so fall to lower values.&lt;/p&gt;
&lt;h3 id=&#34;practice-m4&#34;&gt;Practice M4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Modify the cross-classified chimpanzees model &lt;code&gt;m13.4&lt;/code&gt; so that the adaptive prior for blocks contains a parameter $\bar\gamma$ for its mean:&lt;/p&gt;
&lt;p&gt;$$Î³_i \sim Normal(\bar\gamma,  \sigma_Î³)$$
$$\bar\gamma \sim Normal(0, 1.5)$$&lt;/p&gt;
&lt;p&gt;Compare this model to &lt;code&gt;m13.4&lt;/code&gt;. What has including $\bar\gamma$ done?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; First, I load the data again and prepare it like it was done in the book:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(chimpanzees)
d &amp;lt;- chimpanzees
d$treatment &amp;lt;- 1 + d$prosoc_left + 2 * d$condition
dat_list &amp;lt;- list(
  pulled_left = d$pulled_left,
  actor = d$actor,
  block_id = d$block,
  treatment = as.integer(d$treatment)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here&amp;rsquo;s model &lt;code&gt;m13.4&lt;/code&gt; from the book:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m13.4 &amp;lt;- ulam(
  alist(
    pulled_left ~ dbinom(1, p),
    logit(p) &amp;lt;- a[actor] + g[block_id] + b[treatment],
    b[treatment] ~ dnorm(0, 0.5),
    ## adaptive priors
    a[actor] ~ dnorm(a_bar, sigma_a),
    g[block_id] ~ dnorm(0, sigma_g),
    ## hyper-priors
    a_bar ~ dnorm(0, 1.5),
    sigma_a ~ dexp(1),
    sigma_g ~ dexp(1)
  ),
  data = dat_list, chains = 4, cores = 4, log_lik = TRUE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now for the modification with the adaptive prior on blocks with $\bar\gamma$ (&lt;code&gt;g_bar&lt;/code&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_M4 &amp;lt;- ulam(
  alist(
    pulled_left ~ dbinom(1, p),
    logit(p) &amp;lt;- a[actor] + g[block_id] + b[treatment],
    b[treatment] ~ dnorm(0, 0.5),
    ## adaptive priors
    a[actor] ~ dnorm(a_bar, sigma_a),
    g[block_id] ~ dnorm(g_bar, sigma_g),
    ## hyper-priors
    a_bar ~ dnorm(0, 1.5),
    g_bar ~ dnorm(0, 1.5),
    sigma_a ~ dexp(1),
    sigma_g ~ dexp(1)
  ),
  data = dat_list, chains = 4, cores = 4, log_lik = TRUE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, let&amp;rsquo;s compare these two models:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m13.4, 2, pars = c(&amp;quot;a_bar&amp;quot;, &amp;quot;b&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean        sd        5.5%      94.5%    n_eff    Rhat4
## a_bar  0.5572916 0.7400381 -0.61815978 1.71465486 846.1133 1.000954
## b[1]  -0.1117336 0.3049828 -0.59688685 0.38352742 537.8580 1.005063
## b[2]   0.4117441 0.3013615 -0.07603188 0.90041231 541.1696 1.004054
## b[3]  -0.4583805 0.3050711 -0.94912651 0.02402336 525.1723 1.003261
## b[4]   0.3022693 0.2992465 -0.19185582 0.76269842 496.3259 1.002969
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m_M4, 2, pars = c(&amp;quot;a_bar&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;g_bar&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean        sd        5.5%        94.5%    n_eff    Rhat4
## a_bar  0.4579123 1.1562369 -1.32920915  2.276252493 288.8963 1.000056
## b[1]  -0.1180502 0.2922779 -0.58177755  0.345807340 623.5734 1.004741
## b[2]   0.4021677 0.2935470 -0.08422107  0.874494410 653.4873 1.001191
## b[3]  -0.4628938 0.2963843 -0.93668728 -0.005844908 640.5164 1.004185
## b[4]   0.2897304 0.2867230 -0.16069746  0.758941565 611.7289 1.002156
## g_bar  0.1704292 1.1696618 -1.77806340  1.926636466 209.8574 1.001308
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Oof. That new model (&lt;code&gt;m_M4&lt;/code&gt;) did not work well. I gleam that its sampling was extremely inefficient from looking at the number of effective samples (&lt;code&gt;n_eff&lt;/code&gt;) and Gelman-Rubin statistic (&lt;code&gt;Rhat&lt;/code&gt;) above. The numbers of effective samples are much worse for all of our parameters in &lt;code&gt;m_M4&lt;/code&gt; when compared to the original model (&lt;code&gt;m13.4&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Why is that? Well, &lt;code&gt;m_M4&lt;/code&gt; is what is called &lt;em&gt;over-parameterised&lt;/em&gt;. Both means of our intercepts (&lt;code&gt;a[actor]&lt;/code&gt;, &lt;code&gt;g[block_id]&lt;/code&gt;) are defined via varying priors now. So since there are two parameters for our means, one inside each adaptive prior, we end up with an infinite number of combinations of values of $\bar\alpha$ and $\bar\gamma$ to produce the same sum. This makes the posterior poorly defined and hard to sample. It is worth pointing out, however, that the estimated parameters are almost exactly the same between the two models. Conclusively, over-parameterisation is inefficient in sampling, but will land on similar values if run long enough. We should still avoid coding our models this way in the first place, of course.&lt;/p&gt;
&lt;h2 id=&#34;hard-exercises&#34;&gt;Hard Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-h1&#34;&gt;Practice H1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; In 1980, a typical Bengali woman could have 5 or more children in her lifetime. By the year 2000, a typical Bengali woman had only 2 or 3. Youâre going to look at a historical set of data, when contraception was widely available but many families chose not to use it. These data reside in &lt;code&gt;data(bangladesh)&lt;/code&gt; and come from the 1988 Bangladesh Fertility Survey. Each row is one of 1934 women. There are six variables, but you can focus on three of them for this practice problem:&lt;/p&gt;
&lt;p&gt;(1) &lt;code&gt;district&lt;/code&gt;: ID number of administrative district each woman resided in&lt;br&gt;
(2) &lt;code&gt;use.contraception&lt;/code&gt;: An indicator (0/1) of whether the woman was using contraception&lt;br&gt;
(3) &lt;code&gt;urban&lt;/code&gt;: An indicator (0/1) of whether the woman lived in a city, as opposed to living in a rural area&lt;/p&gt;
&lt;p&gt;The first thing to do is ensure that the cluster variable, district, is a contiguous set of integers. Recall that these values will be index values inside the model. If there are gaps, youâll have parameters for which there is no data to inform them. Worse, the model probably wonât run. Look at the unique values of the district variable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(bangladesh)
d &amp;lt;- bangladesh
sort(unique(d$district))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 55 56 57 58 59 60 61
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;District 54 is absent. So district isn&amp;rsquo;t yet a good index variable, because itâs not contiguous. This is easy to fix. Just make a new variable that is contiguous. This is enough to do it:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;d$district_id &amp;lt;- as.integer(as.factor(d$district))
sort(unique(d$district_id))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now there are 60 values, contiguous integers 1 to 60. Now, focus on predicting &lt;code&gt;use.contraception&lt;/code&gt;, clustered by &lt;code&gt;district_id&lt;/code&gt;. Do not include &lt;code&gt;urban&lt;/code&gt; just yet. Fit both (1) a traditional fixed-effects model that uses dummy variables for district and (2) a multilevel model with varying intercepts for district. Plot the predicted proportions of women in each district using contraception, for both the fixed-effects model and the varying-effects model. That is, make a plot in which district ID is on the horizontal axis and expected proportion using contraception is on the vertical. Make one plot for each model, or layer them on the same plot, as you prefer. How do the models disagree? Can you explain the pattern of disagreement? In particular, can you explain the most extreme cases of disagreement, both why they happen where they do and why the models reach different inferences?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  First, I prep the data into a list:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dat_list &amp;lt;- list(
  C = d$use.contraception,
  D = d$district_id
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now for the models:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Fixed-Effect&lt;/strong&gt;:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_Fixed &amp;lt;- ulam(
  alist(
    C ~ bernoulli(p), # this is the same as dbinom(1, p)
    logit(p) &amp;lt;- a[D],
    a[D] ~ dnorm(0, 1.5)
  ),
  data = dat_list, chains = 4, cores = 4, log_lik = TRUE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;Varying-Intercept&lt;/strong&gt;:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_Varying &amp;lt;- ulam(
  alist(
    C ~ dbinom(1, p), # this is the same as bernoulli(p)
    logit(p) &amp;lt;- a[D],
    a[D] ~ normal(a_bar, sigma),
    a_bar ~ normal(0, 1.5),
    sigma ~ exponential(1)
  ),
  data = dat_list, chains = 4, cores = 4, log_lik = TRUE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now to make our predictions:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## compute posterior means
p_Fixed &amp;lt;- apply(inv_logit(extract.samples(m_Fixed)$a), 2, mean)
p_Varying &amp;lt;- apply(inv_logit(extract.samples(m_Varying)$a), 2, mean)
## compute raw estimate from data in each district
tab &amp;lt;- table(d$use.contraception, d$district_id) # contraception no and yes per district
n_per_district &amp;lt;- colSums(tab) # number of observations per district
p_raw &amp;lt;- as.numeric(tab[2, ] / n_per_district) # raw proportion per district
nd &amp;lt;- max(dat_list$D) # number of districts
plot(NULL, xlim = c(1, nd), ylim = c(0, 1), ylab = &amp;quot;prob use contraception&amp;quot;, xlab = &amp;quot;district&amp;quot;)
points(1:nd, p_Fixed, pch = 16, col = rangi2, cex = 3)
points(1:nd, p_Varying, cex = 3)
points(1:nd, p_raw, pch = 3, cex = 3)
abline(
  h = mean(inv_logit(extract.samples(m_Varying)$a_bar)), # population mean
  lty = 2
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-04-07-statistical-rethinking-chapter-13_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As expected, the varying-intercept-estimates (open circles) are shrunk towards the population mean (dashed line) when compared to the fixed-intercept-estimates (blue circles) and the raw proportion (cross symbols). Some are shrunk more than others. Those which are shrunk more have been shrunk because:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Their sample sizes were small&lt;/li&gt;
&lt;li&gt;Their raw proportions were far from the population mean&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Shrinkage is also introduced due to large variation in the values within each clustering variable which is much easier to demonstrate with continuous observations rather than a binary outcome (contraception used: yes/no).&lt;/p&gt;
&lt;h3 id=&#34;practice-h2&#34;&gt;Practice H2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Return to the Trolley data, &lt;code&gt;data(Trolley)&lt;/code&gt;, from Chapter 12. Define and fit a varying intercepts model for these data. Cluster intercepts on individual participants, as indicated by the unique values in the &lt;code&gt;id&lt;/code&gt; variable. Include &lt;code&gt;action&lt;/code&gt;, &lt;code&gt;intention&lt;/code&gt;, and &lt;code&gt;contact&lt;/code&gt; as ordinary terms. Compare the varying intercepts model and a model that ignores individuals, using both WAIC and posterior predictions. What is the impact of individual variation in these data?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  Again, let&amp;rsquo;s start with loading the data and preparing it into a list:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(Trolley)
d &amp;lt;- Trolley
dat &amp;lt;- list(
  R = d$response,
  A = d$action,
  I = d$intention,
  C = d$contact
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To run the varying intercept model, the &lt;code&gt;id&lt;/code&gt; variable needs to be a simple index variable. Currently that is not the case, so let fix that, too:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dat$id &amp;lt;- coerce_index(d$id)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here&amp;rsquo;s the model from chapter 12 (&lt;code&gt;m12.5&lt;/code&gt;) which will be our baseline model for comparison:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m12.5 &amp;lt;- ulam(
  alist(
    R ~ dordlogit(phi, cutpoints),
    phi &amp;lt;- bA * A + bC * C + BI * I,
    BI &amp;lt;- bI + bIA * A + bIC * C,
    c(bA, bI, bC, bIA, bIC) ~ dnorm(0, 0.5),
    cutpoints ~ dnorm(0, 1.5)
  ),
  data = dat, chains = 4, cores = 4, log_lik = TRUE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now for the varying intercepts model (I have added a new parameter: &lt;code&gt;a[id]&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_H2 &amp;lt;- ulam(
  alist(
    R ~ dordlogit(phi, cutpoints),
    phi &amp;lt;- a[id] + bA * A + bC * C + BI * I,
    BI &amp;lt;- bI + bIA * A + bIC * C,
    a[id] ~ normal(0, sigma),
    c(bA, bI, bC, bIA, bIC) ~ dnorm(0, 0.5),
    cutpoints ~ dnorm(0, 1.5),
    sigma ~ exponential(1)
  ),
  data = dat, chains = 4, cores = 4, log_lik = TRUE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s start comparing these two models by looking at their parameter estimates:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m12.5)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           mean         sd       5.5%      94.5%    n_eff    Rhat4
## bIC -1.2332275 0.09736975 -1.3882391 -1.0769963 999.3059 1.000883
## bIA -0.4325889 0.07986057 -0.5621930 -0.3043263 937.5476 1.000899
## bC  -0.3430801 0.06858197 -0.4535116 -0.2286056 999.4107 1.000974
## bI  -0.2919059 0.05770286 -0.3857312 -0.2020194 855.6081 1.001664
## bA  -0.4733247 0.05380776 -0.5606800 -0.3883240 892.1070 1.001730
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m_H2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean         sd       5.5%      94.5%    n_eff     Rhat4
## bIC   -1.6593276 0.10069140 -1.8242617 -1.4993492 1345.530 0.9989020
## bIA   -0.5524349 0.07997312 -0.6840528 -0.4238708 1185.324 0.9994280
## bC    -0.4584264 0.07028331 -0.5692494 -0.3468558 1299.616 0.9997609
## bI    -0.3892700 0.05888992 -0.4845553 -0.2932161 1044.587 0.9994699
## bA    -0.6511028 0.05543980 -0.7386650 -0.5631939 1256.216 0.9988687
## sigma  1.9176768 0.08248496  1.7902506  2.0537969 2291.638 0.9997130
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When moving to varying intercepts, in this case, all parameter estimates have become stronger in magnitude while remaining negative in sign. Why is that? Because there is a lot of variation among the individual intercepts. &lt;code&gt;sigma&lt;/code&gt; tells us that. Remember that is on the logit scale, so there is a lot of variation here in probability scale. Conclusively, the average formulation we explored in chapter 12 (&lt;code&gt;m12.5&lt;/code&gt;) hid a lot of the effect of the different treatments.&lt;/p&gt;
&lt;p&gt;Finally, let&amp;rsquo;s compare our models using WAIC:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;compare(m12.5, m_H2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           WAIC        SE    dWAIC      dSE     pWAIC weight
## m_H2  31057.47 179.39150    0.000       NA 355.68843      1
## m12.5 36929.69  80.66443 5872.216 173.5441  11.17174      0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now it&amp;rsquo;s official. Conditioning on the individual (&lt;code&gt;id&lt;/code&gt;) really made a massive difference here in understanding the assignments of morality among our data. Effectively, this tells us that our few variables which we used previously to understand how people of different backgrounds and genders perceive morality are not enough to fully understand the matter at hand.&lt;/p&gt;
&lt;h3 id=&#34;practice-h3&#34;&gt;Practice H3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; The Trolley data are also clustered by &lt;code&gt;story&lt;/code&gt;, which indicates a unique narrative for each vignette. Define and fit a cross-classified varying intercepts model with both &lt;code&gt;id&lt;/code&gt; and &lt;code&gt;story&lt;/code&gt;. Use the same ordinary terms as in the previous problem. Compare this model to the previous models. What do you infer about the impact of different stories on responses?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;  I continue with the data as used before, but add the information about &lt;code&gt;story&lt;/code&gt; which needs to be coerced into a proper index, too:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dat$Sid &amp;lt;- coerce_index(d$story)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now for the cross-classified model. All I do here is just add a varying intercept for &lt;code&gt;story&lt;/code&gt;/&lt;code&gt;Sid&lt;/code&gt;. This is a non-centred parametrisation - it probably explores posterior space less efficiently than the centred counterpart, but I find it easier to write and am under a time crunch when writing these solutions:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_H3 &amp;lt;- ulam(
  alist(
    R ~ dordlogit(phi, cutpoints),
    phi &amp;lt;- z[id] * sigma + s[Sid] + bA * A + bC * C + BI * I,
    BI &amp;lt;- bI + bIA * A + bIC * C,
    z[id] ~ normal(0, 1),
    s[Sid] ~ normal(0, tau),
    c(bA, bI, bC, bIA, bIC) ~ dnorm(0, 0.5),
    cutpoints ~ dnorm(0, 1.5),
    sigma ~ exponential(1),
    tau ~ exponential(1)
  ),
  data = dat, chains = 4, cores = 4, log_lik = TRUE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m_H3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean         sd       5.5%      94.5%    n_eff     Rhat4
## bIC   -1.2842990 0.11302619 -1.4633731 -1.1053741 1298.018 1.0012462
## bIA   -0.5259545 0.08510626 -0.6602232 -0.3885573 1328.205 0.9999519
## bC    -1.0802958 0.09536571 -1.2312331 -0.9262213 1306.242 0.9997597
## bI    -0.4587315 0.06843039 -0.5681288 -0.3517746 1287.560 1.0026441
## bA    -0.8941359 0.06928219 -1.0008664 -0.7831957 1316.376 1.0001676
## sigma  1.9602712 0.08455294  1.8277702  2.0951087  170.422 1.0206646
## tau    0.5404331 0.13271393  0.3704167  0.7853622 1857.443 0.9987709
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The treatment variable estimates (&lt;code&gt;bIC&lt;/code&gt;, &lt;code&gt;bIA&lt;/code&gt;, etc.) are changed from the previous model (&lt;code&gt;m_H2&lt;/code&gt;). Interestingly, the estimate for &lt;code&gt;sigma&lt;/code&gt; (variation among individuals) has not changed much. The added variation among stories (&lt;code&gt;tau&lt;/code&gt;) is noticeable, albeit much smaller than &lt;code&gt;sigma&lt;/code&gt;. Let&amp;rsquo;s visualise this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(coeftab(m_H2, m_H3), pars = c(&amp;quot;bIC&amp;quot;, &amp;quot;bIA&amp;quot;, &amp;quot;bC&amp;quot;, &amp;quot;bI&amp;quot;, &amp;quot;bA&amp;quot;, &amp;quot;sigma&amp;quot;, &amp;quot;tau&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-04-07-statistical-rethinking-chapter-13_files/figure-html/unnamed-chunk-33-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This means that there is probably rather meaningful information contained within the story variable when trying to understand morality of decision in the trolley data. We cannot meaningfully compare these models using WAIC, however, and so this will remain a qualitative statement&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.5 (2021-03-31)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19043)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.1252  LC_CTYPE=English_United Kingdom.1252    LC_MONETARY=English_United Kingdom.1252 LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.1252    
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] cowplot_1.1.1        tidybayes_2.3.1      rethinking_2.13      rstan_2.21.2         ggplot2_3.3.6        StanHeaders_2.21.0-7
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.7           mvtnorm_1.1-1        lattice_0.20-41      tidyr_1.1.3          prettyunits_1.1.1    ps_1.6.0             assertthat_0.2.1     digest_0.6.27        utf8_1.2.1          
## [10] V8_3.4.1             plyr_1.8.6           R6_2.5.0             backports_1.2.1      stats4_4.0.5         evaluate_0.14        coda_0.19-4          highr_0.9            blogdown_1.3        
## [19] pillar_1.6.0         rlang_0.4.11         curl_4.3.2           callr_3.7.0          jquerylib_0.1.4      R.utils_2.10.1       R.oo_1.24.0          rmarkdown_2.7        styler_1.4.1        
## [28] labeling_0.4.2       stringr_1.4.0        loo_2.4.1            munsell_0.5.0        compiler_4.0.5       xfun_0.22            pkgconfig_2.0.3      pkgbuild_1.2.0       shape_1.4.5         
## [37] htmltools_0.5.1.1    tidyselect_1.1.0     tibble_3.1.1         gridExtra_2.3        bookdown_0.22        arrayhelpers_1.1-0   codetools_0.2-18     matrixStats_0.61.0   fansi_0.4.2         
## [46] crayon_1.4.1         dplyr_1.0.5          withr_2.4.2          MASS_7.3-53.1        R.methodsS3_1.8.1    distributional_0.2.2 ggdist_2.4.0         grid_4.0.5           jsonlite_1.7.2      
## [55] gtable_0.3.0         lifecycle_1.0.0      DBI_1.1.1            magrittr_2.0.1       scales_1.1.1         RcppParallel_5.1.2   cli_3.0.0            stringi_1.5.3        farver_2.1.0        
## [64] bslib_0.2.4          ellipsis_0.3.2       generics_0.1.0       vctrs_0.3.7          rematch2_2.1.2       forcats_0.5.1        tools_4.0.5          svUnit_1.0.6         R.cache_0.14.0      
## [73] glue_1.4.2           purrr_0.3.4          processx_3.5.1       yaml_2.2.1           inline_0.3.17        colorspace_2.0-0     knitr_1.33           sass_0.3.1
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Closing &amp; Summary</title>
      <link>https://www.erikkusch.com/courses/an-introduction-to-biostatistics/13-an-outlook-on-advanced-statistics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/an-introduction-to-biostatistics/13-an-outlook-on-advanced-statistics/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m afraid, I haven&amp;rsquo;t found the time to create this one yet.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Closing &amp; Summary</title>
      <link>https://www.erikkusch.com/courses/biostat101/13-an-outlook-on-advanced-statistics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/biostat101/13-an-outlook-on-advanced-statistics/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m afraid, I haven&amp;rsquo;t found the time to create this one yet.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 14</title>
      <link>https://www.erikkusch.com/courses/rethinking/chapter-14/</link>
      <pubDate>Fri, 16 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/rethinking/chapter-14/</guid>
      <description>&lt;h1 id=&#34;adventures-in-covariance&#34;&gt;Adventures in Covariance&lt;/h1&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/rethinking/16__16-04-2021_SUMMARY_-Multi-Level-Models-II.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides Chapter 14&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to the exercises at the end of chapter 14 in 
&lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Satistical Rethinking 2&lt;/a&gt; by Richard McElreath. I have created these notes as a part of my ongoing involvement in the 
&lt;a href=&#34;https://www.erikkusch.com/project/aubayes/&#34;&gt;AU Bayes Study Group&lt;/a&gt;. Much of my inspiration for these solutions, where necessary, has been obtained from&lt;/p&gt;
&lt;!-- [Taras Svirskyi](https://github.com/jffist/statistical-rethinking-solutions/blob/master/ch10_hw.R), [William Wolf](https://github.com/cavaunpeu/statistical-rethinking/blob/master/chapter-10/homework.R), and [Corrie Bartelheimer](https://www.samples-of-thoughts.com/projects/statistical-rethinking/chapter_10/chp10-ex/) as well as  --&gt;
&lt;p&gt;the solutions provided to instructors by Richard McElreath himself.&lt;/p&gt;
&lt;h2 id=&#34;r-environment&#34;&gt;&lt;code&gt;R&lt;/code&gt; Environment&lt;/h2&gt;
&lt;p&gt;For today&amp;rsquo;s exercise, I load the following packages:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rethinking)
library(rstan)
library(MASS)
library(ellipse)
library(ape)
library(ggplot2)
library(tidybayes)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;easy-exercises&#34;&gt;Easy Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-e1&#34;&gt;Practice E1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Add to the following model varying slopes on the predictor $x$.&lt;/p&gt;
&lt;p&gt;$$y_i â¼ Normal(Âµi, Ï)$$ 
$$Âµ_i = Î±_{group[i]} + Î²x_i$$
$$Î±_{group} â¼ Normal(Î±, Ï_Î±)$$ 
$$Î± â¼ Normal(0, 10)$$ 
$$Î² â¼ Normal(0, 1)$$ 
$$Ï â¼ HalfCauchy(0, 2)$$ 
$$Ï_Î± â¼ HalfCauchy(0, 2)$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; To do this, our outcome distribution does not change. So keep it as is:&lt;/p&gt;
&lt;p&gt;$$y_i â¼ Normal(Î¼_i, Ï)$$&lt;/p&gt;
&lt;p&gt;Next, we come to the linear model. This needs changing. Since we are now interested in a varying slope for each group ($\beta_{group}$), we need to exchange the original $\beta$ with $\beta_{group}$:&lt;/p&gt;
&lt;p&gt;$$Î¼_i = Î±_{group[i]} + Î²_{group[i]}x_i$$&lt;/p&gt;
&lt;p&gt;Consequently, we also need to change our prior. Since $\alpha_{group}$ and $\beta_{group}$ now stem from a joint distribution, we need to express them as such. $\alpha$ is still the average intercept. However, $\beta$ now turns into the average slope. Both of these serve as the mean expectations for $\alpha_{group}$ and $\beta_{group}$ in a multivariate normal distribution ($MVNormal()$) with a covariance matrix ($S$) defining how they are linked.&lt;/p&gt;
&lt;p&gt;$$\begin{bmatrix} \alpha_{group} \ \beta_{group} \ \end{bmatrix}  \sim MVNormal \left(\begin{bmatrix} \alpha \ \beta \ \end{bmatrix}, S \right)$$&lt;/p&gt;
&lt;p&gt;Since we have just introduced the need for a covariance matrix, we now need to define it. A covariance matrix is the product of a variance matrix and a correlation matrix ($R$). What we can do when determining the covariance matrix ($S$) is setting our variances for $\alpha$ and $\beta$ - $\sigma_\alpha$ and $\sigma_\beta$, respectively - and subsequently multiplying this with the correlation matrix ($R$):&lt;/p&gt;
&lt;p&gt;$$S = \begin{pmatrix} \sigma_\alpha &amp;amp; 0 \ 0 &amp;amp; \sigma_\beta  \ \end{pmatrix} R \begin{pmatrix} \sigma_\alpha &amp;amp; 0 \ 0 &amp;amp; \sigma_\beta  \ \end{pmatrix} $$&lt;/p&gt;
&lt;p&gt;The variances and correlation matrix referenced above need priors of their own - so called hyperpriors. Let&amp;rsquo;s start with the priors of the variances:&lt;/p&gt;
&lt;p&gt;$$Ï_Î± â¼ HalfCauchy(0, 2)$$
$$Ï_\beta â¼ HalfCauchy(0, 2)$$&lt;/p&gt;
&lt;p&gt;And also add a somewhat regularising prior for $R$:&lt;/p&gt;
&lt;p&gt;$$R â¼ LKJcorr(2)$$&lt;/p&gt;
&lt;p&gt;Lastly, we simply keep the priors for $\alpha$, $\beta$, and $\sigma$ from the original model.
$$Î± â¼ Normal(0, 10)$$ 
$$Î² â¼ Normal(0, 1)$$ 
$$Ï â¼ HalfCauchy(0, 2)$$&lt;/p&gt;
&lt;h3 id=&#34;practice-e2&#34;&gt;Practice E2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Think up a context in which varying intercepts will be positively correlated with varying slopes. Provide a mechanistic explanation for the correlation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; A setting within which there is positive correlation between varying intercepts and varying slopes can be put in laymen-terms as: &amp;ldquo;A setting within which high intercepts come with steep slopes&amp;rdquo;. With that in mind, what could be such a setting?&lt;/p&gt;
&lt;p&gt;There are many settings which would meet this criterion. I am a biologist by training and the first thing that came to mind was that of an ant colony. Let&amp;rsquo;s say we are interested studying ant hill size as a function of food availability. Ignoring the carrying capacity of a system, we can reasonably expect larger ant hills (higher intercepts) to benefit more strongly from increased food availability as their foraging will be much more efficient (steeper slope).&lt;/p&gt;
&lt;p&gt;Of course, I realise that this thought experiment ignores some crucial bits of biological reality such as diminishing returns and structural integrity of ant hills after a certain size is reached. For the sake of keeping this example simple, I neglect them.&lt;/p&gt;
&lt;h3 id=&#34;practice-e3&#34;&gt;Practice E3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  When is it possible for a varying slopes model to have fewer effective parameters (as estimated by WAIC or DIC) than the corresponding model with fixed (unpooled) slopes? Explain.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; When there is little or next-to-no variation among clusters. The absence of this among-cluster variation induces very strong shrinkage. As a result, albeit containing more actual parameters in the posterior distribution, the varying slopes model may end up less flexible in fitting to the data because of adaptive regularisation forcing strong shrinkage. Consequently, our number of effective parameters - a proxy of overfitting risk and posterior flexibility - decreases.&lt;/p&gt;
&lt;p&gt;For an example, consult the comparison of models &lt;code&gt;m13.1&lt;/code&gt; and &lt;code&gt;m13.2&lt;/code&gt; in R Code &lt;code&gt;13.4&lt;/code&gt; in the book.&lt;/p&gt;
&lt;h2 id=&#34;medium-exercises&#34;&gt;Medium Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-m1&#34;&gt;Practice M1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Repeat the cafÃ© robot simulation from the beginning of the chapter. This time, set &lt;code&gt;rho&lt;/code&gt; to zero, so that there is no correlation between intercepts and slopes. How does the posterior distribution of the correlation reflect this change in the underlying simulation?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; This is what was done in the book. &lt;code&gt;rho&lt;/code&gt; has been adjusted to be $0$ now:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# set up parameters of population
a &amp;lt;- 3.5 # average morning wait time
b &amp;lt;- (-1) # average difference afternoon wait time
sigma_a &amp;lt;- 1 # std dev in intercepts
sigma_b &amp;lt;- 0.5 # std dev in slopes
rho &amp;lt;- 0 # correlation between intercepts and slopes
Mu &amp;lt;- c(a, b)
cov_ab &amp;lt;- sigma_a * sigma_b * rho
Sigma &amp;lt;- matrix(c(sigma_a^2, cov_ab, cov_ab, sigma_b^2), ncol = 2)
# simulate observations
N_cafes &amp;lt;- 20
set.seed(6)
vary_effects &amp;lt;- mvrnorm(N_cafes, Mu, Sigma)
a_cafe &amp;lt;- vary_effects[, 1]
b_cafe &amp;lt;- vary_effects[, 2]
N_visits &amp;lt;- 10
afternoon &amp;lt;- rep(0:1, N_visits * N_cafes / 2)
cafe_id &amp;lt;- rep(1:N_cafes, each = N_visits)
mu &amp;lt;- a_cafe[cafe_id] + b_cafe[cafe_id] * afternoon
sigma &amp;lt;- 0.5 # std dev within cafes
wait &amp;lt;- rnorm(N_visits * N_cafes, mu, sigma)
# package into  data frame
d &amp;lt;- data.frame(cafe = cafe_id, afternoon = afternoon, wait = wait)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now to run our model (&lt;code&gt;m14.1&lt;/code&gt;) with the exact same specification as in the book:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_M1 &amp;lt;- ulam(
  alist(
    wait ~ normal(mu, sigma),
    mu &amp;lt;- a_cafe[cafe] + b_cafe[cafe] * afternoon,
    c(a_cafe, b_cafe)[cafe] ~ multi_normal(c(a, b), Rho, sigma_cafe),
    a ~ normal(5, 2),
    b ~ normal(-1, 0.5),
    sigma_cafe ~ exponential(1),
    sigma ~ exponential(1),
    Rho ~ lkj_corr(2)
  ),
  data = d, chains = 6, cores = 6
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So what about that posterior distribution for &lt;code&gt;Rho&lt;/code&gt;?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;post &amp;lt;- extract.samples(m_M1)
ggplot() +
  stat_halfeye(aes(x = post$Rho[, 1, 2])) +
  theme_bw() +
  labs(x = &amp;quot;Rho&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-04-15-statistical-rethinking-chapter-14_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Jup. That accurately represents our underlying correlation of $0$. The &lt;code&gt;precis&lt;/code&gt; output agrees:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m_M1, pars = &amp;quot;Rho[1,2]&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              result
## mean     0.01110476
## sd       0.23468860
## 5.5%    -0.36400650
## 94.5%    0.38982346
## n_eff 2833.41771477
## Rhat     1.00023118
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practice-m2&#34;&gt;Practice M2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Fit this multilevel model to the simulated cafÃ© data: 
$$W_i â¼ Normal(Âµ_i, Ï)$$ 
$$Âµ_i = Î±_{cafÃ©[i]} + Î²_{cafÃ©[i]}A_i$$
$$Î±_{cafÃ©} â¼ Normal(Î±, Ï_Î±)$$ 
$$Î²_{cafÃ©} â¼ Normal(Î², Ï_Î²)$$ 
$$Î± â¼ Normal(0, 10)$$ 
$$Î² â¼ Normal(0, 10)$$ 
$$Ï â¼ HalfCauchy(0, 1)$$
$$Ï_Î± â¼ HalfCauchy(0, 1)$$ 
$$Ï_Î² â¼ HalfCauchy(0, 1)$$&lt;/p&gt;
&lt;p&gt;Use WAIC to compare this model to the model from the chapter, the one that uses a multi-variate Gaussian prior. Explain the result.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; I am strongly assuming that this question is targeting the simulated cafÃ© data used in the book. I create that data again here:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# set up parameters of population
a &amp;lt;- 3.5 # average morning wait time
b &amp;lt;- -1 # average difference afternoon wait time
sigma_a &amp;lt;- 1 # std dev in intercepts
sigma_b &amp;lt;- 0.5 # std dev in slopes
rho &amp;lt;- -0.7 # correlation between intercepts and slopes
Mu &amp;lt;- c(a, b)
cov_ab &amp;lt;- sigma_a * sigma_b * rho
Sigma &amp;lt;- matrix(c(sigma_a^2, cov_ab, cov_ab, sigma_b^2), ncol = 2)
# simulate observations
N_cafes &amp;lt;- 20
set.seed(42)
vary_effects &amp;lt;- mvrnorm(N_cafes, Mu, Sigma)
a_cafe &amp;lt;- vary_effects[, 1]
b_cafe &amp;lt;- vary_effects[, 2]
N_visits &amp;lt;- 10
afternoon &amp;lt;- rep(0:1, N_visits * N_cafes / 2)
cafe_id &amp;lt;- rep(1:N_cafes, each = N_visits)
mu &amp;lt;- a_cafe[cafe_id] + b_cafe[cafe_id] * afternoon
sigma &amp;lt;- 0.5 # std dev within cafes
wait &amp;lt;- rnorm(N_visits * N_cafes, mu, sigma)
# package into  data frame
d &amp;lt;- data.frame(cafe = cafe_id, afternoon = afternoon, wait = wait)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the data at hand, I now run our baseline model which is, again, &lt;code&gt;m14.1&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_M2Baseline &amp;lt;- ulam(
  alist(
    wait ~ normal(mu, sigma),
    mu &amp;lt;- a_cafe[cafe] + b_cafe[cafe] * afternoon,
    c(a_cafe, b_cafe)[cafe] ~ multi_normal(c(a, b), Rho, sigma_cafe),
    a ~ normal(5, 2),
    b ~ normal(-1, 0.5),
    sigma_cafe ~ exponential(1),
    sigma ~ exponential(1),
    Rho ~ lkj_corr(2)
  ),
  data = d, chains = 4, cores = 4
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now onto our new model for this task. What is already striking is the use of independent intercepts and slopes. There is no correlation parameter between them so the assumed correlation, by the model, is $0$:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_M2 &amp;lt;- ulam(
  alist(
    wait ~ dnorm(mu, sigma),
    mu &amp;lt;- a_cafe[cafe] + b_cafe[cafe] * afternoon,
    a_cafe[cafe] ~ dnorm(a, sigma_alpha),
    b_cafe[cafe] ~ dnorm(b, sigma_beta),
    a ~ dnorm(0, 10),
    b ~ dnorm(0, 10),
    sigma ~ dexp(1),
    sigma_alpha ~ dexp(1),
    sigma_beta ~ dexp(1)
  ),
  data = d, chains = 4, cores = 4
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But what actually distinguishes the model outputs now? Let&amp;rsquo;s extract posterior samples for intercepts and slopes from both models and investigate visually:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;post_Base &amp;lt;- extract.samples(m_M2Baseline)
a_Base &amp;lt;- apply(post_Base$a_cafe, 2, mean)
b_Base &amp;lt;- apply(post_Base$b_cafe, 2, mean)
post_M2 &amp;lt;- extract.samples(m_M2)
a_M2 &amp;lt;- apply(post_M2$a_cafe, 2, mean)
b_M2 &amp;lt;- apply(post_M2$b_cafe, 2, mean)
plot(a_M2, b_M2,
  xlab = &amp;quot;intercept&amp;quot;, ylab = &amp;quot;slope&amp;quot;,
  pch = 16, col = rangi2, ylim = c(min(b_M2) - 0.05, max(b_M2) + 0.05),
  xlim = c(min(a_M2) - 0.1, max(a_M2) + 0.1), cex = 2
)
points(a_Base, b_Base, pch = 1, cex = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-04-15-statistical-rethinking-chapter-14_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I have stuck to McElreath&amp;rsquo;s colour scheme here once more. The filled circles represent samples from our new model, while the open circles represent samples from the posterior obtained via the model which accounts for correlation of slopes and intercepts. First and foremost, these are pretty similar I must say. This agreement is particularly pronounced towards the centre of the plot with increasing divergences of the posterior samples at the fringes of the intercept and slope ranges. This comes down to what the underlying models assume. Our baseline model assumes that slopes and intercepts are inherently related to one another and finds a negative correlation between them. This can be seen when looking at the lower right-hand and the upper left-hand corner of the plot above. Given the baseline model assumption, large intercepts are associated with strongly negative slopes and vice versa.&lt;/p&gt;
&lt;p&gt;The correlation-informed model does better here because it leverages more information from the entire population and just so happens to exactly mirror the data generation process.&lt;/p&gt;
&lt;h3 id=&#34;practice-m3&#34;&gt;Practice M3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Re-estimate the varying slopes model for the &lt;code&gt;UCBadmit&lt;/code&gt; data, now using a non-centered parametrization. Compare the efficiency of the forms of the model, using &lt;code&gt;n_eff&lt;/code&gt;. Which is better? Which chain sampled faster?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Ok&amp;hellip; This is a headache because there is no varying slopes model for the &lt;code&gt;UCBadmit&lt;/code&gt; data in the bookchapter. So let&amp;rsquo;s make one ourselves and then re-parameterise it.&lt;/p&gt;
&lt;p&gt;We start by loading and preparing the data. By defining an indicator variable for &lt;code&gt;male&lt;/code&gt; we make it easier to fit a varying slopes model based on gender of applicant:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(UCBadmit)
d &amp;lt;- UCBadmit
dat_list &amp;lt;- list(
  admit = d$admit,
  applications = d$applications,
  male = ifelse(d$applicant.gender == &amp;quot;male&amp;quot;, 1, 0),
  dept_id = rep(1:6, each = 2)
)
str(dat_list)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## List of 4
##  $ admit       : int [1:12] 512 89 353 17 120 202 138 131 53 94 ...
##  $ applications: int [1:12] 825 108 560 25 325 593 417 375 191 393 ...
##  $ male        : num [1:12] 1 0 1 0 1 0 1 0 1 0 ...
##  $ dept_id     : int [1:12] 1 1 2 2 3 3 4 4 5 5 ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now with the data in hand, we can fit our own model on varying slopes. Let&amp;rsquo;s think about this in theory first. What would this look like?&lt;/p&gt;
&lt;p&gt;We start out with a binomial outcome distribution for our admitted applications:&lt;/p&gt;
&lt;p&gt;$$admit_i â¼ Binomial(Applications, p_i)$$&lt;/p&gt;
&lt;p&gt;Our next line is the linear model again. This time, the admittance rate ($p_i$) is a product of a department-specific intercept ($\alpha_{deptId}$) and slope ($\beta_{deptId}$):&lt;/p&gt;
&lt;p&gt;$$p_i = \alpha_{deptID} + \beta_{deptID}*male$$&lt;/p&gt;
&lt;p&gt;Since the varying slopes and intercepts are certain to be correlated, we specify a multivariate normal prior again:&lt;/p&gt;
&lt;p&gt;$$\begin{bmatrix} \alpha_{deptID} \ \beta_{deptID} \ \end{bmatrix}  \sim MVNormal \left(\begin{bmatrix} \alpha \ \beta \ \end{bmatrix}, S \right)$$&lt;/p&gt;
&lt;p&gt;And now for the covariance matrix:&lt;/p&gt;
&lt;p&gt;$$S = \begin{pmatrix} \sigma_\alpha &amp;amp; 0 \ 0 &amp;amp; \sigma_\beta  \ \end{pmatrix} R \begin{pmatrix} \sigma_\alpha &amp;amp; 0 \ 0 &amp;amp; \sigma_\beta  \ \end{pmatrix} $$&lt;/p&gt;
&lt;p&gt;Finally, we just need some priors and hyperpriors:&lt;/p&gt;
&lt;p&gt;$$Ï_Î± â¼ Exponential(1)$$
$$Ï_\beta â¼ Exponential(1)$$&lt;/p&gt;
&lt;p&gt;And also add a somewhat regularising prior for $R$:&lt;/p&gt;
&lt;p&gt;$$R â¼ LKJcorr(2)$$&lt;/p&gt;
&lt;p&gt;Lastly, we simply keep the priors for $\alpha$, $\beta$:
$$Î± â¼ Normal(0, 1)$$ 
$$Î² â¼ Normal(0, 1)$$&lt;/p&gt;
&lt;p&gt;And now to do all of this in &lt;code&gt;R&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Begin_C &amp;lt;- Sys.time()
m_M3 &amp;lt;- ulam(
  alist(
    admit ~ dbinom(applications, p),
    logit(p) &amp;lt;- a[dept_id] + bm[dept_id] * male,
    c(a, bm)[dept_id] ~ multi_normal(c(a_bar, bm_bar), Rho, sigma_dept),
    a_bar ~ dnorm(0, 1),
    bm_bar ~ dnorm(0, 1),
    sigma_dept ~ dexp(1),
    Rho ~ dlkjcorr(2)
  ),
  data = dat_list, chains = 4, cores = 4
)
End_C &amp;lt;- Sys.time()
precis(m_M3, 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                      mean           sd       5.5%      94.5%     n_eff     Rhat4
## bm[1]         -0.75467135 2.642890e-01 -1.1974575 -0.3419394  762.8361 1.0035396
## bm[2]         -0.21546963 3.141762e-01 -0.7172490  0.2702785 1444.8377 1.0007292
## bm[3]          0.07788729 1.395628e-01 -0.1457046  0.3005848 1519.8738 1.0005234
## bm[4]         -0.09706014 1.389302e-01 -0.3171928  0.1276948 1760.1554 0.9999004
## bm[5]          0.11606582 1.793774e-01 -0.1610577  0.4022199 1726.0739 1.0005376
## bm[6]         -0.10986997 2.643276e-01 -0.5302442  0.3009293 1397.2613 1.0030231
## a[1]           1.27128456 2.477604e-01  0.8819218  1.6722364  778.7511 1.0032979
## a[2]           0.74963140 3.154061e-01  0.2626239  1.2484753 1436.8953 1.0004966
## a[3]          -0.64605733 8.582742e-02 -0.7808221 -0.5086328 1617.3253 0.9995321
## a[4]          -0.61501279 1.019817e-01 -0.7774726 -0.4515837 1708.0376 0.9992410
## a[5]          -1.13005620 1.096386e-01 -1.3054035 -0.9559542 1993.4173 1.0004230
## a[6]          -2.60481876 2.045530e-01 -2.9411431 -2.2759003 1799.7746 1.0015531
## a_bar         -0.39106882 5.307969e-01 -1.2265922  0.4709463 1698.0086 0.9994382
## bm_bar        -0.16176143 2.137141e-01 -0.4947019  0.1694293 1403.2870 1.0018595
## sigma_dept[1]  1.48653705 4.716297e-01  0.9088349  2.3531967 1292.0985 1.0008335
## sigma_dept[2]  0.44566910 2.144169e-01  0.1819589  0.8410725  988.9796 1.0015728
## Rho[1,1]       1.00000000 0.000000e+00  1.0000000  1.0000000       NaN       NaN
## Rho[1,2]      -0.32154165 3.408218e-01 -0.8124723  0.2811236 1522.8640 1.0015113
## Rho[2,1]      -0.32154165 3.408218e-01 -0.8124723  0.2811236 1522.8640 1.0015113
## Rho[2,2]       1.00000000 8.265587e-17  1.0000000  1.0000000 1811.6478 0.9979980
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s just acknowledge that the &lt;code&gt;precis()&lt;/code&gt; output is here, but move on for now to the re-parametrised model.&lt;/p&gt;
&lt;p&gt;I am not even going to attempt to come up with the mathematical notation of the non-centred version of the above model. Luckily, I don&amp;rsquo;t have to because &lt;code&gt;ulam()&lt;/code&gt; has helper functions which can do this for me:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Begin_NC &amp;lt;- Sys.time()
m_M3NonCent &amp;lt;- ulam(
  alist(
    admit ~ dbinom(applications, p),
    logit(p) &amp;lt;- a_bar + v[dept_id, 1] + (bm_bar + v[dept_id, 2]) * male,
    transpars &amp;gt; matrix[dept_id, 2]:v &amp;lt;- compose_noncentered(sigma_dept, L_Rho, z),
    matrix[2, dept_id]:z ~ dnorm(0, 1),
    a_bar ~ dnorm(0, 1.5),
    bm_bar ~ dnorm(0, 1),
    vector[2]:sigma_dept ~ dexp(1),
    cholesky_factor_corr[2]:L_Rho ~ lkj_corr_cholesky(2)
  ),
  data = dat_list, chains = 4, cores = 4
)
End_NC &amp;lt;- Sys.time()
precis(m_M3NonCent, 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                      mean        sd       5.5%       94.5%     n_eff     Rhat4
## z[1,1]         1.27847990 0.5262323  0.4705305  2.13326205  727.0227 1.0011087
## z[1,2]         0.88269092 0.4857079  0.1388867  1.68994558  719.7999 1.0031813
## z[1,3]        -0.12528283 0.3849925 -0.7106325  0.50669799  612.7119 1.0140975
## z[1,4]        -0.10433526 0.3866020 -0.6958663  0.51682268  601.9049 1.0154629
## z[1,5]        -0.48074322 0.4055438 -1.1153447  0.16675545  643.6182 1.0175388
## z[1,6]        -1.56200990 0.5688037 -2.4838992 -0.69872746  737.3167 1.0145782
## z[2,1]        -1.23644880 0.8164974 -2.6022559  0.05071129 1271.1801 1.0026798
## z[2,2]         0.18312727 0.8096899 -1.0820859  1.45719856 1573.6340 1.0003249
## z[2,3]         0.60192227 0.6276861 -0.3439741  1.64761142 1203.4125 1.0020469
## z[2,4]         0.10146041 0.5799030 -0.8100848  1.01529134 1291.1294 1.0015204
## z[2,5]         0.53484163 0.6684946 -0.4889643  1.64962250 1165.4873 1.0009753
## z[2,6]        -0.49127010 0.8038063 -1.8068248  0.74735922 1885.4880 1.0003286
## a_bar         -0.46792318 0.5611715 -1.3795380  0.39745615  588.0941 1.0121940
## bm_bar        -0.13969516 0.2136100 -0.4695941  0.18952741  799.7150 1.0034511
## sigma_dept[1]  1.46852953 0.4505571  0.9255641  2.31989495  833.1351 1.0030067
## sigma_dept[2]  0.44493698 0.2388977  0.1818530  0.86089979  757.1626 0.9991599
## L_Rho[1,1]     1.00000000 0.0000000  1.0000000  1.00000000       NaN       NaN
## L_Rho[1,2]     0.00000000 0.0000000  0.0000000  0.00000000       NaN       NaN
## L_Rho[2,1]    -0.31981947 0.3440124 -0.8022403  0.29104897 1687.4781 1.0001054
## L_Rho[2,2]     0.87223636 0.1365401  0.5970006  0.99912792 1190.3266 1.0002817
## v[1,1]         1.73536453 0.6011856  0.8128695  2.70174932  648.8099 1.0094272
## v[1,2]        -0.61370995 0.3128848 -1.1315494 -0.14674125 1028.2946 1.0005503
## v[2,1]         1.19934441 0.6279810  0.2169330  2.22040335  675.6765 1.0101770
## v[2,2]        -0.06176349 0.3328516 -0.5897528  0.46209909 1539.0956 1.0001901
## v[3,1]        -0.17733549 0.5673456 -1.0542767  0.75415175  593.0556 1.0109594
## v[3,2]         0.21846891 0.2395483 -0.1438607  0.60551012 1035.9533 1.0016130
## v[4,1]        -0.14832190 0.5675488 -1.0127159  0.76606136  575.1370 1.0125444
## v[4,2]         0.04602378 0.2363775 -0.3298587  0.41695086  890.5646 1.0022504
## v[5,1]        -0.65983984 0.5702765 -1.5558841  0.26053379  610.1289 1.0114797
## v[5,2]         0.25080258 0.2583262 -0.1081353  0.67319529 1159.0924 1.0024436
## v[6,1]        -2.12809664 0.5847887 -3.0186504 -1.19350046  633.0943 1.0111973
## v[6,2]         0.02683558 0.3048067 -0.4676814  0.50643307 1645.9028 1.0004084
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First of all, we can see that the number of effective samples (&lt;code&gt;n_eff&lt;/code&gt;) is higher for the centred model (&lt;code&gt;m_M3&lt;/code&gt;) which is surprising to me. I thought that non-centred models were supposed to sample more efficiently. Maybe the underlying data just doesn&amp;rsquo;t suffer from abrupt changes in posterior slope?&lt;/p&gt;
&lt;p&gt;So what about running time?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Centred
End_C - Begin_C
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Time difference of 55.36985 secs
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Non-Centred
End_NC - Begin_NC
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Time difference of 51.25241 secs
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ok. The non-centred model ran slightly faster.&lt;/p&gt;
&lt;h3 id=&#34;practice-m4&#34;&gt;Practice M4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Use WAIC to compare the Gaussian process model of Oceanic tools to the models fit to the same data in Chapter 11. Pay special attention to the effective numbers of parameters, as estimated by WAIC.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; So this needed some digging. The models in question are &lt;code&gt;m11.11&lt;/code&gt; for the simpler model of CHapter 11 and &lt;code&gt;m14.8&lt;/code&gt; from Chapter 14.&lt;/p&gt;
&lt;p&gt;Before we can do any modelling, we need to load and prepare the data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(Kline)
d &amp;lt;- Kline
# Chapter 11 stuff
d$P &amp;lt;- scale(log(d$population))
d$contact_id &amp;lt;- ifelse(d$contact == &amp;quot;high&amp;quot;, 2, 1)
# Chapter 14 stuff
d$society &amp;lt;- 1:10
data(islandsDistMatrix)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the data at hand, I simply use the exact same code as in the book to execute the respective models. Note that I have set the &lt;code&gt;ulam()&lt;/code&gt; argument &lt;code&gt;log_lik=TRUE&lt;/code&gt; for comparison with WAIC in the next step.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Chapter 11 Model
dat2 &amp;lt;- list(T = d$total_tools, P = d$population, cid = d$contact_id)
m11.11 &amp;lt;- ulam(
  alist(
    T ~ dpois(lambda),
    lambda &amp;lt;- exp(a[cid]) * P^b[cid] / g,
    a[cid] ~ dnorm(1, 1),
    b[cid] ~ dexp(1),
    g ~ dexp(1)
  ),
  data = dat2, chains = 4, cores = 4, log_lik = TRUE
)
## Chapter 14 Model
dat_list &amp;lt;- list(T = d$total_tools, P = d$population, society = d$society, Dmat = islandsDistMatrix)
m14.8 &amp;lt;- ulam(
  alist(
    T ~ dpois(lambda),
    lambda &amp;lt;- (a * P^b / g) * exp(k[society]),
    vector[10]:k ~ multi_normal(0, SIGMA),
    matrix[10, 10]:SIGMA &amp;lt;- cov_GPL2(Dmat, etasq, rhosq, 0.01), c(a, b, g) ~ dexp(1), etasq ~ dexp(2), rhosq ~ dexp(0.5)
  ),
  data = dat_list, chains = 4, cores = 4, iter = 2000, log_lik = TRUE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have both models at the ready, let&amp;rsquo;s do what the task asked of us and compare their out-of-sample accuracy predictions:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;compare(m11.11, m14.8)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            WAIC        SE    dWAIC      dSE    pWAIC      weight
## m14.8  67.87559  2.370998  0.00000       NA 4.196489 0.998336383
## m11.11 80.66978 11.103427 12.79419 10.95919 5.148678 0.001663617
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The more complex model taking into account spatial distances of societies - &lt;code&gt;m14.8&lt;/code&gt; - outperforms the previously held &amp;ldquo;best&amp;rdquo; model (&lt;code&gt;m11.11&lt;/code&gt;). We also see that the Gaussian process model has less effective parameters (&lt;code&gt;pWAIC&lt;/code&gt;) than the simpler model. This is a sign of intense regularisation on the part of the Gaussian Process model.&lt;/p&gt;
&lt;h3 id=&#34;practice-m5&#34;&gt;Practice M5&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Modify the phylogenetic distance example to use group size as the outcome and brain size as a predictor. Assuming brain size influences group size, what is your estimate of the effect? How does phylogeny influence the estimate?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; This is the example from the book, but simply just switching the positions of group size and brain size in the model specification. Coincidentally, this is the model shown in the 
&lt;a href=&#34;https://www.youtube.com/watch?v=pwMRbt2CbSU&amp;amp;list=PLDcUM9US4XdNM4Edgs7weiyIguLSToZRI&amp;amp;index=19&amp;amp;ab_channel=RichardMcElreath&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;YouTube Lecture series&lt;/a&gt; by Richard McElreath.&lt;/p&gt;
&lt;p&gt;For now, we start by loading the data as was done in the book and establish our first data list for subsequent modelling:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(Primates301)
d &amp;lt;- Primates301
d$name &amp;lt;- as.character(d$name)
dstan &amp;lt;- d[complete.cases(d$group_size, d$body, d$brain), ]
spp_obs &amp;lt;- dstan$name
dat_list &amp;lt;- list(
  N_spp = nrow(dstan),
  M = standardize(log(dstan$body)),
  B = standardize(log(dstan$brain)),
  G = standardize(log(dstan$group_size)),
  Imat = diag(nrow(dstan))
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this part of the observational data ready, I now turn to phylogenetic data which we can obtain and attach to our data list like so:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(Primates301_nex)
tree_trimmed &amp;lt;- keep.tip(Primates301_nex, spp_obs) # only keep tree that&#39;s relevant to our species
Rbm &amp;lt;- corBrownian(phy = tree_trimmed) # calculate expected covariance given a Brownian model
V &amp;lt;- vcv(Rbm) # compute expected variances and covariances
Dmat &amp;lt;- cophenetic(tree_trimmed) # cophenetic distance matrix
dat_list$V &amp;lt;- V[spp_obs, spp_obs] # covariances in speciesXspecies matrix
dat_list$R &amp;lt;- dat_list$V / max(V) # relative covariances of speciesXspecies matrix
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And we are ready to run our first model! Because they book went through multiple candidate models so do I.&lt;/p&gt;
&lt;p&gt;Here, I start off with the basic, ordinary regression:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_M5Ordi &amp;lt;- ulam(
  alist(
    G ~ multi_normal(mu, SIGMA),
    mu &amp;lt;- a + bM * M + bB * B,
    matrix[N_spp, N_spp]:SIGMA &amp;lt;- Imat * sigma_sq,
    a ~ normal(0, 1),
    c(bM, bB) ~ normal(0, 0.5),
    sigma_sq ~ exponential(1)
  ),
  data = dat_list, chains = 4, cores = 4
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, I run the Brownian motion model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_M5Brown &amp;lt;- ulam(
  alist(
    G ~ multi_normal(mu, SIGMA),
    mu &amp;lt;- a + bM * M + bB * B,
    matrix[N_spp, N_spp]:SIGMA &amp;lt;- R * sigma_sq,
    a ~ normal(0, 1),
    c(bM, bB) ~ normal(0, 0.5),
    sigma_sq ~ exponential(1)
  ),
  data = dat_list, chains = 4, cores = 4
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lastly, I execute a Gaussian Process model. To do so, we need to convert our phylogenetic distance matrix into a relative measure of distance among our species:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dat_list$Dmat &amp;lt;- Dmat[spp_obs, spp_obs] / max(Dmat)
m_M5GP &amp;lt;- ulam(
  alist(
    G ~ multi_normal(mu, SIGMA),
    mu &amp;lt;- a + bM * M + bB * B,
    matrix[N_spp, N_spp]:SIGMA &amp;lt;- cov_GPL1(Dmat, etasq, rhosq, 0.01),
    a ~ normal(0, 1),
    c(bM, bB) ~ normal(0, 0.5),
    etasq ~ half_normal(1, 0.25),
    rhosq ~ half_normal(3, 0.25)
  ),
  data = dat_list, chains = 4, cores = 4
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(coeftab(m_M5Ordi, m_M5Brown, m_M5GP), pars = c(&amp;quot;bM&amp;quot;, &amp;quot;bB&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-04-15-statistical-rethinking-chapter-14_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From the above, we clearly see that model which does not take into account our phylogeny - &lt;code&gt;m_M5Ordi&lt;/code&gt; - finds a clearly non-zero dependence of brain size on group size. However, both model which do include phylogenetic information - &lt;code&gt;m_M5Brown&lt;/code&gt; and &lt;code&gt;m_M5GP&lt;/code&gt; - do not show this relationship. Adding phylogenetic information seems to reduce the evidence for a causal link between brain size and group size.&lt;/p&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- post &lt;- extract.samples(m_M5GP) --&gt;
&lt;!-- plot(NULL, xlim=c(0,max(dat_list$Dmat)), ylim=c(0,1.5), --&gt;
&lt;!--     xlab=&#34;phylogenetic distance&#34;, ylab=&#34;covariance&#34;) --&gt;
&lt;!-- # posterior --&gt;
&lt;!-- for (i in 1:30) --&gt;
&lt;!--     curve(post$etasq[i]*exp(-post$rhosq[i]*x), add=TRUE, col=rangi2) --&gt;
&lt;!-- # prior mean and 89% interval --&gt;
&lt;!-- eta &lt;- abs(rnorm(1e3,1,0.25)) --&gt;
&lt;!-- rho &lt;- abs(rnorm(1e3,3,0.25)) --&gt;
&lt;!-- d_seq &lt;- seq(from=0,to=1,length.out=50) --&gt;
&lt;!-- K &lt;- sapply(d_seq, function(x) eta*exp(-rho*x)) --&gt;
&lt;!-- lines(d_seq, colMeans(K), lwd=2) --&gt;
&lt;!-- shade(apply(K,2,PI), d_seq) --&gt;
&lt;!-- text(0.5, 0.5, &#34;prior&#34;) --&gt;
&lt;!-- text(0.2, 0.1, &#34;posterior&#34;, col=rangi2) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- # OU model with different GP prior --&gt;
&lt;!-- m14M5.3 &lt;- ulam( --&gt;
&lt;!--     alist( --&gt;
&lt;!--         G ~ multi_normal(mu, SIGMA), --&gt;
&lt;!--         mu &lt;- a + bM*M + bB*B, --&gt;
&lt;!--         matrix[N_spp,N_spp]: SIGMA &lt;- cov_GPL1(Dmat, etasq, rhosq, 0.01), --&gt;
&lt;!--         a ~ normal(0,1), --&gt;
&lt;!--         c(bM,bB) ~ normal(0,0.5), --&gt;
&lt;!--         etasq ~ half_normal(0.25,0.25), --&gt;
&lt;!--         rhosq ~ half_normal(3,0.25) --&gt;
&lt;!--  ), data=dat_list, chains=4, cores=4) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- post &lt;- extract.samples(m14M5.3) --&gt;
&lt;!-- plot(NULL, xlim=c(0,max(dat_list$Dmat)), ylim=c(0,1.5), --&gt;
&lt;!--     xlab=&#34;phylogenetic distance&#34;, ylab=&#34;covariance&#34;) --&gt;
&lt;!-- # posterior --&gt;
&lt;!-- for (i in 1:30) --&gt;
&lt;!--     curve(post$etasq[i]*exp(-post$rhosq[i]*x), add=TRUE, col=rangi2) --&gt;
&lt;!-- # prior mean and 89% interval --&gt;
&lt;!-- eta &lt;- abs(rnorm(1e3,1,0.25)) --&gt;
&lt;!-- rho &lt;- abs(rnorm(1e3,3,0.25)) --&gt;
&lt;!-- d_seq &lt;- seq(from=0,to=1,length.out=50) --&gt;
&lt;!-- K &lt;- sapply(d_seq, function(x) eta*exp(-rho*x)) --&gt;
&lt;!-- lines(d_seq, colMeans(K), lwd=2) --&gt;
&lt;!-- shade(apply(K,2,PI), d_seq) --&gt;
&lt;!-- text(0.5, 0.5, &#34;prior&#34;) --&gt;
&lt;!-- text(0.2, 0.1, &#34;posterior&#34;, col=rangi2) --&gt;
&lt;!-- ``` --&gt;
&lt;h2 id=&#34;hard-exercises&#34;&gt;Hard Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-h1&#34;&gt;Practice H1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Letâs revisit the Bangladesh fertility data, &lt;code&gt;data(bangladesh)&lt;/code&gt;, from the practice problems for Chapter 13. Fit a model with both varying intercepts by &lt;code&gt;district_id&lt;/code&gt; and varying slopes of urban by &lt;code&gt;district_id&lt;/code&gt;. You are still predicting &lt;code&gt;use.contraception&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Inspect the correlation between the intercepts and slopes. Can you interpret this correlation, in terms of what it tells you about the pattern of contraceptive use in the sample? It might help to plot the mean (or median) varying effect estimates for both the intercepts and slopes, by district. Then you can visualize the correlation and maybe more easily think through what it means to have a particular correlation. Plotting predicted proportion of women using contraception, with urban women on one axis and rural on the other, might also help.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Once more, I start by loading the data and preparing it as was done in a previous chapter:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(bangladesh)
d &amp;lt;- bangladesh
dat_list &amp;lt;- list(
  C = d$use.contraception,
  did = as.integer(as.factor(d$district)),
  urban = d$urban
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I can run my model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_H1 &amp;lt;- ulam(
  alist(
    C ~ bernoulli(p),
    logit(p) &amp;lt;- a[did] + b[did] * urban,
    c(a, b)[did] ~ multi_normal(c(abar, bbar), Rho, Sigma),
    abar ~ normal(0, 1),
    bbar ~ normal(0, 0.5),
    Rho ~ lkj_corr(2),
    Sigma ~ exponential(1)
  ),
  data = dat_list, chains = 4, cores = 4, iter = 4000
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now look at the posterior estimates of average effects:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m_H1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            mean        sd       5.5%      94.5%    n_eff    Rhat4
## abar -0.6842219 0.1003302 -0.8475050 -0.5301731 6158.562 1.000395
## bbar  0.6369826 0.1590421  0.3866908  0.8900233 4474.285 1.000145
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unsurprisingly, I find a positive effect for &lt;code&gt;bbar&lt;/code&gt; which indicates that contraception is used more frequently in urban areas.&lt;/p&gt;
&lt;p&gt;Looking deeper into the posterior estimates:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m_H1, depth = 3, pars = c(&amp;quot;Rho&amp;quot;, &amp;quot;Sigma&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                mean           sd       5.5%      94.5%     n_eff     Rhat4
## Rho[1,1]  1.0000000 0.000000e+00  1.0000000  1.0000000       NaN       NaN
## Rho[1,2] -0.6512997 1.696069e-01 -0.8680436 -0.3443487 1431.4457 1.0003693
## Rho[2,1] -0.6512997 1.696069e-01 -0.8680436 -0.3443487 1431.4457 1.0003693
## Rho[2,2]  1.0000000 5.972661e-17  1.0000000  1.0000000 7485.2487 0.9994999
## Sigma[1]  0.5762512 9.710360e-02  0.4308600  0.7404430 1990.0617 1.0021542
## Sigma[2]  0.7731623 1.991282e-01  0.4617857  1.0991674  967.7497 1.0034399
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;shows a negative correlation between the intercepts and slopes (&lt;code&gt;Rho[1,2]&lt;/code&gt; or &lt;code&gt;Rho[2,1]&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s plot this relationship between the varying effects to get a better understanding of what is happening:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;post &amp;lt;- extract.samples(m_H1)
a &amp;lt;- apply(post$a, 2, mean)
b &amp;lt;- apply(post$b, 2, mean)
plot(a, b, xlab = &amp;quot;a (intercept)&amp;quot;, ylab = &amp;quot;b (urban slope)&amp;quot;)
abline(h = 0, lty = 2)
abline(v = 0, lty = 2)
R &amp;lt;- apply(post$Rho, 2:3, mean)
s &amp;lt;- apply(post$Sigma, 2, mean)
S &amp;lt;- diag(s) %*% R %*% diag(s)
ll &amp;lt;- c(0.5, 0.67, 0.89, 0.97)
for (l in ll) {
  el &amp;lt;- ellipse(S, centre = c(mean(post$abar), mean(post$bbar)), level = l)
  lines(el, col = &amp;quot;black&amp;quot;, lwd = 0.5)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-04-15-statistical-rethinking-chapter-14_files/figure-html/unnamed-chunk-27-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So districts with higher use of contraception outside of urban areas come with smaller slopes. Basically, what this means is that districts which boast a high use of contraception outside of urban areas do not have a marked shift in use of contraceptives when moving to urban areas.&lt;/p&gt;
&lt;p&gt;We can also show this in probability scale by applying inverse logit transformation to our estimates:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;u0 &amp;lt;- inv_logit(a)
u1 &amp;lt;- inv_logit(a + b)
plot(u0, u1, xlim = c(0, 1), ylim = c(0, 1), xlab = &amp;quot;urban = 0&amp;quot;, ylab = &amp;quot;urban = 1&amp;quot;)
abline(h = 0.5, lty = 2)
abline(v = 0.5, lty = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-04-15-statistical-rethinking-chapter-14_files/figure-html/unnamed-chunk-28-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;practice-h2&#34;&gt;Practice H2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Varying effects models are useful for modelling time series, as well as spatial clustering. In a time series, the observations cluster by entities that have continuity through time, such as individuals. Since observations within individuals are likely highly correlated, the multilevel structure can help quite a lot. Youâll use the data in &lt;code&gt;data(Oxboys)&lt;/code&gt;, which is 234 height measurements on 26 boys from an Oxford Boys Club (I think these were like youth athletic leagues?), at 9 different ages (centred and standardized) per boy.&lt;/p&gt;
&lt;p&gt;Youâll be interested in predicting &lt;code&gt;height&lt;/code&gt;, using &lt;code&gt;age&lt;/code&gt;, clustered by &lt;code&gt;Subject&lt;/code&gt; (individual boy). Fit a model with varying intercepts and slopes (on &lt;code&gt;age&lt;/code&gt;), clustered by &lt;code&gt;Subject&lt;/code&gt;. Present and interpret the parameter estimates. Which varying effect contributes more variation to the heights, the intercept or the slope?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; I start with loading the data, standardising the age data, and making the subject IDs into an index:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(Oxboys)
d &amp;lt;- Oxboys
d$A &amp;lt;- standardize(d$age)
d$id &amp;lt;- coerce_index(d$Subject)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Armed with my data, I can now turn to modelling:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_H2 &amp;lt;- ulam(
  alist(
    height ~ dnorm(mu, sigma),
    mu &amp;lt;- a_bar + a[id] + (b_bar + b[id]) * A,
    a_bar ~ dnorm(150, 10),
    b_bar ~ dnorm(0, 10),
    c(a, b)[id] ~ multi_normal(0, Rho_id, sigma_id),
    sigma_id ~ dexp(1),
    Rho_id ~ dlkjcorr(2),
    sigma ~ dexp(1)
  ),
  data = d, chains = 4, cores = 4, iter = 4000
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model has compiled and I am interested in the output it produced concerning average effects and variation:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m_H2, depth = 2, pars = c(&amp;quot;a_bar&amp;quot;, &amp;quot;b_bar&amp;quot;, &amp;quot;sigma_id&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                   mean        sd        5.5%      94.5%     n_eff    Rhat4
## a_bar       149.523993 1.3844230 147.2887666 151.690432  293.0144 1.011856
## b_bar         4.230315 0.2073667   3.8999943   4.554123  414.4092 1.003962
## sigma_id[1]   7.331832 0.8789976   6.0675622   8.840290 4173.7730 1.001038
## sigma_id[2]   1.065676 0.1525730   0.8508899   1.336301 4141.6398 1.000750
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since age is standardised, &lt;code&gt;a_bar&lt;/code&gt; represent the average height at average age in the data set. The average slope &lt;code&gt;b_bar&lt;/code&gt; represents change in height for a one-unit change in standardised age.&lt;/p&gt;
&lt;p&gt;I don&amp;rsquo;t like interpreting standardised data coefficients like that. Let&amp;rsquo;s rather plot it:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(height ~ age, type = &amp;quot;n&amp;quot;, data = d)
for (i in 1:26) {
  h &amp;lt;- d$height[d$Subject == i]
  a &amp;lt;- d$age[d$Subject == i]
  lines(a, h, col = col.alpha(&amp;quot;slateblue&amp;quot;, 0.5), lwd = 2)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-04-15-statistical-rethinking-chapter-14_files/figure-html/unnamed-chunk-32-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now the the task at hand. Which effect contributes more to the overall heights of our individuals? Given the plot above, I&amp;rsquo;d argue that it is the varying intercepts which provide us with most of the variation in heights. However, this is very much down to the data and should not be generalised beyond this data set. It might completely fall apart if we had longer time-series of data values.&lt;/p&gt;
&lt;h3 id=&#34;practice-h3&#34;&gt;Practice H3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Now consider the correlation between the varying intercepts and slopes. Can you explain its value? How would this estimated correlation influence your predictions about a new sample of boys?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; For this, we look at the correlation matrix &lt;code&gt;Rho_id&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m_H2, depth = 3, pars = &amp;quot;Rho_id&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                  mean           sd      5.5%     94.5%    n_eff     Rhat4
## Rho_id[1,1] 1.0000000 0.000000e+00 1.0000000 1.0000000      NaN       NaN
## Rho_id[1,2] 0.5307351 1.283373e-01 0.3046972 0.7170573 4627.842 1.0007918
## Rho_id[2,1] 0.5307351 1.283373e-01 0.3046972 0.7170573 4627.842 1.0007918
## Rho_id[2,2] 1.0000000 7.886203e-17 1.0000000 1.0000000 7829.277 0.9994999
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So there is a positive correlation. Let&amp;rsquo;s visualise that:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot() +
  stat_halfeye(aes(x = extract.samples(m_H2)$Rho_id[, 1, 2])) +
  theme_bw() +
  labs(x = &amp;quot;Rho&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-04-15-statistical-rethinking-chapter-14_files/figure-html/unnamed-chunk-34-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The positive correlation implies that larger intercepts are associated with steeper slopes. What this means is that taller boys grow faster.&lt;/p&gt;
&lt;h3 id=&#34;practice-h4&#34;&gt;Practice H4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Use &lt;code&gt;mvrnorm&lt;/code&gt; (in &lt;code&gt;library(MASS)&lt;/code&gt;) or &lt;code&gt;rmvnorm&lt;/code&gt; (in &lt;code&gt;library(mvtnorm)&lt;/code&gt;) to simulate a new sample of boys, based upon the posterior mean values of the parameters.&lt;/p&gt;
&lt;p&gt;That is, try to simulate varying intercepts and slopes, using the relevant parameter estimates, and then plot the predicted trends of height on age, one trend for each simulated boy you produce. A sample of 10 simulated boys is plenty, to illustrate the lesson. You can ignore uncertainty in the posterior, just to make the problem a little easier. But if you want to include the uncertainty about the parameters, go for it.&lt;/p&gt;
&lt;p&gt;Note that you can construct an arbitrary variance-covariance matrix to pass to either &lt;code&gt;mvrnorm&lt;/code&gt; or &lt;code&gt;rmvnorm&lt;/code&gt; with something like:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;S &amp;lt;- matrix(c(sa^2, sa * sb * rho, sa * sb * rho, sb^2), nrow = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where &lt;code&gt;sa&lt;/code&gt; is the standard deviation of the first variable, &lt;code&gt;sb&lt;/code&gt; is the standard deviation of the second variable, and &lt;code&gt;rho&lt;/code&gt; is the correlation between them.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; To simulate new observations we need to obtain the estimates of our model so far:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;post &amp;lt;- extract.samples(m_H2)
rho &amp;lt;- mean(post$Rho_id[, 1, 2])
sb &amp;lt;- mean(post$sigma_id[, 2])
sa &amp;lt;- mean(post$sigma_id[, 1])
sigma &amp;lt;- mean(post$sigma)
a &amp;lt;- mean(post$a_bar)
b &amp;lt;- mean(post$b_bar)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can define the variance-covariance matrix:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;S &amp;lt;- matrix(c(sa^2, sa * sb * rho, sa * sb * rho, sb^2), nrow = 2)
round(S, 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       [,1] [,2]
## [1,] 53.76 4.15
## [2,]  4.15 1.14
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Subsequently, we can sample from the multivariate normal distribution given our variance-covariance matrix to obtain a bivariate distribution of intercepts and slopes:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ve &amp;lt;- mvrnorm(10, c(0, 0), Sigma = S)
ve
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             [,1]       [,2]
##  [1,] -0.2914409 -0.8985609
##  [2,]  8.8137562  0.3436168
##  [3,] -1.5233131  1.5530926
##  [4,] -9.5179212 -0.6967038
##  [5,]  7.6547083 -0.3622056
##  [6,]  5.4710535 -0.3060008
##  [7,] -0.3548003  0.1445644
##  [8,]  7.2706590  3.0081540
##  [9,]  2.8143313  0.1653603
## [10,] -6.3582595 -1.0162374
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These are individual intercepts and slopes of 10 random boys have which we only need to add to the average intercept and slope values to generate predicted heights for them. Here, we simulate the trend for each boy and add it to a plot:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;age.seq &amp;lt;- seq(from = -1, to = 1, length.out = 9)
plot(0, 0, type = &amp;quot;n&amp;quot;, xlim = range(d$age), ylim = range(d$height), xlab = &amp;quot;age (centered)&amp;quot;, ylab = &amp;quot;height&amp;quot;)
for (i in 1:nrow(ve)) {
  h &amp;lt;- rnorm(9,
    mean = a + ve[i, 1] + (b + ve[i, 2]) * age.seq,
    sd = sigma
  )
  lines(age.seq, h, col = col.alpha(&amp;quot;slateblue&amp;quot;, 0.5))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-04-15-statistical-rethinking-chapter-14_files/figure-html/unnamed-chunk-39-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.5 (2021-03-31)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19043)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.1252  LC_CTYPE=English_United Kingdom.1252    LC_MONETARY=English_United Kingdom.1252 LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.1252    
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] tidybayes_2.3.1      ape_5.5              ellipse_0.4.2        MASS_7.3-53.1        rethinking_2.13      rstan_2.21.2         ggplot2_3.3.6        StanHeaders_2.21.0-7
## 
## loaded via a namespace (and not attached):
##  [1] sass_0.3.1           tidyr_1.1.3          jsonlite_1.7.2       R.utils_2.10.1       bslib_0.2.4          RcppParallel_5.1.2   assertthat_0.2.1     distributional_0.2.2 highr_0.9           
## [10] stats4_4.0.5         ggdist_2.4.0         yaml_2.2.1           pillar_1.6.0         backports_1.2.1      lattice_0.20-41      glue_1.4.2           arrayhelpers_1.1-0   digest_0.6.27       
## [19] colorspace_2.0-0     htmltools_0.5.1.1    R.oo_1.24.0          plyr_1.8.6           pkgconfig_2.0.3      svUnit_1.0.6         bookdown_0.22        purrr_0.3.4          mvtnorm_1.1-1       
## [28] scales_1.1.1         processx_3.5.1       tibble_3.1.1         styler_1.4.1         generics_0.1.0       farver_2.1.0         ellipsis_0.3.2       withr_2.4.2          cli_3.0.0           
## [37] magrittr_2.0.1       crayon_1.4.1         evaluate_0.14        ps_1.6.0             R.methodsS3_1.8.1    fansi_0.4.2          R.cache_0.14.0       nlme_3.1-152         forcats_0.5.1       
## [46] pkgbuild_1.2.0       blogdown_1.3         tools_4.0.5          loo_2.4.1            prettyunits_1.1.1    lifecycle_1.0.0      matrixStats_0.61.0   stringr_1.4.0        V8_3.4.1            
## [55] munsell_0.5.0        callr_3.7.0          compiler_4.0.5       jquerylib_0.1.4      rlang_0.4.11         grid_4.0.5           labeling_0.4.2       rmarkdown_2.7        gtable_0.3.0        
## [64] codetools_0.2-18     inline_0.3.17        DBI_1.1.1            curl_4.3.2           rematch2_2.1.2       R6_2.5.0             gridExtra_2.3        knitr_1.33           dplyr_1.0.5         
## [73] utf8_1.2.1           shape_1.4.5          stringi_1.5.3        Rcpp_1.0.7           vctrs_0.3.7          tidyselect_1.1.0     xfun_0.22            coda_0.19-4
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Statistical Downscaling</title>
      <link>https://www.erikkusch.com/courses/krigr/kriging/</link>
      <pubDate>Thu, 26 May 2022 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/krigr/kriging/</guid>
      <description>&lt;p&gt;&lt;div class=&#34;alert alert-danger&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;&lt;code&gt;KrigR&lt;/code&gt; is currently undergoing development. As a result, this part of the workshop has become deprecated. Please refer to the &lt;a href=&#34;https://www.erikkusch.com/courses/krigr/setup/&#34;&gt;setup&lt;/a&gt; &lt;a href=&#34;https://www.erikkusch.com/courses/krigr/quickstart/&#34;&gt;quick guide&lt;/a&gt; portions of this material as these are up-to-date. &lt;/strong&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;div class=&#34;alert alert-danger&#34;&gt;
  &lt;div&gt;
    This part of the workshop is dependant on set-up and preparation done previously &lt;a href=&#34;https://www.erikkusch.com/courses/krigr/prep/&#34;&gt;here&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;First, we load &lt;code&gt;KrigR&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(KrigR)
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-info&#34;&gt;
  &lt;div&gt;
    Statistical downscaling with &lt;code&gt;KrigR&lt;/code&gt; is handled via the &lt;code&gt;krigR()&lt;/code&gt; function and requires a set of spatial covariates.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    For an introduction to the statistical downscaling process, I will first walk you through the &lt;code&gt;SpatialPolygons&lt;/code&gt; spatial preference.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;First, we load the data we wish to statistically downscale. We established these data 
&lt;a href=&#34;https://www.erikkusch.com/courses/krigr/download//#climate-data&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;SpatialPolygonsRaw &amp;lt;- stack(file.path(Dir.Data, &amp;quot;SpatialPolygonsRaw.nc&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are now ready to begin our journey to high-spatial resolution data products!&lt;/p&gt;
&lt;h2 id=&#34;covariates&#34;&gt;Covariates&lt;/h2&gt;
&lt;p&gt;First, we use the &lt;code&gt;download_DEM()&lt;/code&gt; function which comes with &lt;code&gt;KrigR&lt;/code&gt; to obtain elevation data as our covariate of choice. This produces two rasters:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A raster of &lt;strong&gt;training&lt;/strong&gt; resolution which matches the input data in all attributes except for the data in each cell.&lt;/li&gt;
&lt;li&gt;A raster of &lt;strong&gt;target&lt;/strong&gt; resolution which matches the input data as closely as possible in all attributes except for the resolution (which is specified by the user).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Both of these products are bundled into a &lt;code&gt;list&lt;/code&gt; where the first element corresponds to the &lt;em&gt;training&lt;/em&gt; resolution and the second element contains the &lt;em&gt;target&lt;/em&gt; resolution covariate data. Here, we specify a target resolution of &lt;code&gt;.02&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This is how we specify &lt;code&gt;download_DEM()&lt;/code&gt; to prepare DEM covariates for us:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Covs_ls &amp;lt;- download_DEM(Train_ras = SpatialPolygonsRaw, # the data we want to downscale
                        Target_res = .02, # the resolution we want to downscale to
                        Shape = Shape_shp, # extra spatial preferences
                        Dir = Dir.Covariates # where to store the covariate files
                        )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For now, let&amp;rsquo;s simply inspect our list of covariate rasters:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Covs_ls
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## class      : RasterLayer 
## dimensions : 34, 54, 1836  (nrow, ncol, ncell)
## resolution : 0.1000189, 0.09999998  (x, y)
## extent     : 9.726991, 15.12801, 49.75, 53.15  (xmin, xmax, ymin, ymax)
## crs        : +proj=longlat +datum=WGS84 +no_defs 
## source     : memory
## names      : DEM 
## values     : 20.11554, 861.7248  (min, max)
## 
## 
## [[2]]
## class      : RasterLayer 
## dimensions : 204, 324, 66096  (nrow, ncol, ncell)
## resolution : 0.01666667, 0.01666667  (x, y)
## extent     : 9.72486, 15.12486, 49.74986, 53.14986  (xmin, xmax, ymin, ymax)
## crs        : +proj=longlat +datum=WGS84 +no_defs 
## source     : memory
## names      : DEM 
## values     : 15.75, 1128  (min, max)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You will find that the target resolution covariate data comes at a resolution of 0.017 instead of the 0.02 resolution we specified. This happens because &lt;code&gt;download_DEM()&lt;/code&gt; calls upon the &lt;code&gt;raster::aggregate()&lt;/code&gt; function when aggregating the high-resolution covariate data to your desired target resolution and is thus only capable of creating target-resolution covariates in multiples of the base resolution of the GMTED 2010 DEM we are using as our default covariate. This happens only when the &lt;code&gt;Target_res&lt;/code&gt; argument is specified to be a number.&lt;/p&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    Specifying the &lt;code&gt;Target_res&lt;/code&gt; argument as a number will lead to best approximation of the desired resolution due to usage of the &lt;code&gt;raster::aggregate()&lt;/code&gt; within &lt;code&gt;download_DEM()&lt;/code&gt;. If you need an exact resolution to match pre-existing data, please refer to &lt;a href=&#34;https://www.erikkusch.com/courses/krigr/third-party/#matching-third-party-data&#34;&gt;this part of the workshop&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Notice that despite the covariate rasters (and input rasters, for that matter) containing 1836 and 6.6096\times 10^{4} for training and target resolution respectively, we only obtain data for 826 and 26247 cells respectively due to our specification of &lt;code&gt;SpatialPolygons&lt;/code&gt;. This will come in handy when doing the statistical interpolation (see 
&lt;a href=&#34;https://www.erikkusch.com/courses/krigr/kriging/#spatial-limitation&#34;&gt;this section&lt;/a&gt; for details).&lt;/p&gt;
&lt;p&gt;Before moving on, let&amp;rsquo;s visualise the covariate data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Plot_Covs(Covs_ls, Shape_shp)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-downscaling_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notice just how much more clearly the mountainous areas in our study region show up at our target resolution.&lt;/p&gt;
&lt;h3 id=&#34;considerations-for-download_dem&#34;&gt;Considerations for &lt;code&gt;download_DEM()&lt;/code&gt;&lt;/h3&gt;
&lt;h4 id=&#34;target_res&#34;&gt;&lt;code&gt;Target_res&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;Alternatively to specifying a target resolution, you can specify a different raster which should be matched in all attributes by the raster at target resolution. We get to this again when discussing 
&lt;a href=&#34;https://www.erikkusch.com/courses/krigr/third-party/#matching-third-party-data&#34;&gt;third-party&lt;/a&gt; data usage.&lt;/p&gt;
&lt;div class=&#34;alert alert-&#34;&gt;
  &lt;div&gt;
    &lt;code&gt;Target_res&lt;/code&gt; can be used for a numeric input or to match a pre-existing &lt;code&gt;raster&lt;/code&gt; object.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h4 id=&#34;shape&#34;&gt;&lt;code&gt;Shape&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;Spatial preferences with &lt;code&gt;download_DEM()&lt;/code&gt; are specified slightly differently when compared to &lt;code&gt;download_ERA()&lt;/code&gt;. Whereas &lt;code&gt;download_ERA()&lt;/code&gt; uses the &lt;code&gt;Extent&lt;/code&gt; argument, &lt;code&gt;download_DEM()&lt;/code&gt; uses the &lt;code&gt;Shape&lt;/code&gt; argument. The reason? &lt;code&gt;download_DEM()&lt;/code&gt; automatically reads out the extent of the input raster and carries out &lt;code&gt;extent&lt;/code&gt; limitation according to this. &lt;code&gt;SpatialPolygons&lt;/code&gt; and &lt;code&gt;data.frame&lt;/code&gt; inputs are supported. For clarity, we simply recognise them with the &lt;code&gt;Shape&lt;/code&gt; argument to avoid confusion and unnecessary &lt;code&gt;extent&lt;/code&gt; inputs.&lt;/p&gt;
&lt;div class=&#34;alert alert-&#34;&gt;
  &lt;div&gt;
    Spatial preferences are handed to &lt;code&gt;download_DEM()&lt;/code&gt; using the &lt;code&gt;Shape&lt;/code&gt; argument.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h4 id=&#34;keep_temporary&#34;&gt;&lt;code&gt;Keep_Temporary&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;By default, this argument is set to &lt;code&gt;FALSE&lt;/code&gt; and raw, global DEM data will be deleted when the covariates you queried have been established. Setting this argument to &lt;code&gt;TRUE&lt;/code&gt; will retain the raw data and make it so you do not have to re-download the DEM data for later use.&lt;/p&gt;
&lt;div class=&#34;alert alert-&#34;&gt;
  &lt;div&gt;
    Setting &lt;code&gt;Keep_Temporary = TRUE&lt;/code&gt; will retain global DEM data on your hard drive.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h4 id=&#34;source&#34;&gt;&lt;code&gt;Source&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;This argument specifies where to download the DEM data from. By default, we query the data from the official USGS website. However, this website has given some users issues with connection instabilities. Consequently, the raw DEM data is also available from a dropbox which you can query download from by setting &lt;code&gt;Source = &amp;quot;Drive&amp;quot;&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;alert alert-&#34;&gt;
  &lt;div&gt;
    When experiencing connection issues with the USGS servers, we recommend setting &lt;code&gt;Source = &amp;quot;Drive&amp;quot;&lt;/code&gt; to obtain covariate data.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;kriging&#34;&gt;Kriging&lt;/h2&gt;
&lt;div class=&#34;alert alert-danger&#34;&gt;
  &lt;div&gt;
    Kriging can be a very &lt;strong&gt;computationally expensive&lt;/strong&gt; exercise.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The expense of kriging is largely determined by three factors:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Change in spatial resolution.&lt;/li&gt;
&lt;li&gt;Number of cells containing data; i.e. 
&lt;a href=&#34;https://www.erikkusch.com/courses/krigr/kriging/#spatial-limitation&#34;&gt;Spatial Limitation&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Localisation of Kriging; i.e. 
&lt;a href=&#34;https://www.erikkusch.com/courses/krigr/kriging/#spatial-limitation&#34;&gt;Localisation of Results&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    We explore two of these further down in this workshop material. For more information, please consult &lt;a href=&#34;https://iopscience.iop.org/article/10.1088/1748-9326/ac39bf&#34;&gt;this publication (Figure 4)&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Finally, we are ready to interpolate our input data given our covariates with the &lt;code&gt;krigR()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;SpatialPolygonsKrig &amp;lt;- krigR(Data = SpatialPolygonsRaw, # data we want to krig as a raster object
      Covariates_coarse = Covs_ls[[1]], # training covariate as a raster object
      Covariates_fine = Covs_ls[[2]], # target covariate as a raster object
      Keep_Temporary = FALSE, # we don&#39;t want to retain the individually kriged layers on our hard-drive
      Cores = 1, # we want to krig on just one core
      FileName = &amp;quot;SpatialPolygonsKrig&amp;quot;, # the file name for our full kriging output
      Dir = Dir.Exports # which directory to save our final input in
      )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Commencing Kriging
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Kriging of remaining 3 data layers should finish around: 2023-04-03 16:35:08
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
  |                                                                                      
  |                                                                                |   0%
  |                                                                                      
  |====================                                                            |  25%
  |                                                                                      
  |========================================                                        |  50%
  |                                                                                      
  |============================================================                    |  75%
  |                                                                                      
  |================================================================================| 100%
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Just like with the &lt;code&gt;download_ERA()&lt;/code&gt; function, &lt;code&gt;krigR()&lt;/code&gt; updates you on what it is currently working on. Again, I implemented this to make sure people don&amp;rsquo;t get too anxious staring at an empty console in &lt;code&gt;R&lt;/code&gt;. If this feature is not appealing to you, you can turn this progress tracking off by setting &lt;code&gt;verbose = FALSE&lt;/code&gt; in the function call to &lt;code&gt;krigR()&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    For the rest of this workshop, I suppress messages from &lt;code&gt;krigR()&lt;/code&gt; via other means so that when you execute, you get progress tracking.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;There we go. As output of the &lt;code&gt;krigR()&lt;/code&gt; function, we obtain a list of downscaled data as the first element and downscaling standard errors as the second list element. Let&amp;rsquo;s look at that:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;SpatialPolygonsKrig[-3] # we will talk later about the third element
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $Kriging_Output
## class      : RasterBrick 
## dimensions : 175, 309, 54075, 4  (nrow, ncol, ncell, nlayers)
## resolution : 0.01666667, 0.01666667  (x, y)
## extent     : 9.87486, 15.02486, 50.14986, 53.06653  (xmin, xmax, ymin, ymax)
## crs        : +proj=longlat +datum=WGS84 +no_defs 
## source     : memory
## names      : var1.pred.1, var1.pred.2, var1.pred.3, var1.pred.4 
## min values :    269.3269,    266.6584,    265.8426,    261.2555 
## max values :    275.0150,    273.3421,    272.1410,    270.0713 
## 
## 
## $Kriging_SE
## class      : RasterBrick 
## dimensions : 175, 309, 54075, 4  (nrow, ncol, ncell, nlayers)
## resolution : 0.01666667, 0.01666667  (x, y)
## extent     : 9.87486, 15.02486, 50.14986, 53.06653  (xmin, xmax, ymin, ymax)
## crs        : +proj=longlat +datum=WGS84 +no_defs 
## source     : memory
## names      : var1.stdev.1, var1.stdev.2, var1.stdev.3, var1.stdev.4 
## min values :    0.1184605,    0.1265206,    0.1142046,    0.1283697 
## max values :    0.1308865,    0.1426154,    0.1535409,    0.2638671
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All the data has been downscaled and we do have uncertainties recorded for all of our outputs. Let&amp;rsquo;s visualise the data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Plot_Krigs(SpatialPolygonsKrig, 
           Shp = Shape_shp,
           Dates = c(&amp;quot;01-1995&amp;quot;, &amp;quot;02-1995&amp;quot;, &amp;quot;03-1995&amp;quot;, &amp;quot;04-1995&amp;quot;)
           )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-downscaling_files/figure-html/KrigPlot-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As you can see, the elevation patterns show up clearly in our kriged air temperature output. Furthermore, you can see that our certainty of Kriging predictions drops on the 04/01/1995 in comparison to the preceding days. However, do keep in mind that a maximum standard error of 0.131, 0.143, 0.154, 0.264 (for each layer of our output respectively) on a total range of data of 5.688, 6.684, 6.298, 8.816 (again, for each layer in the output respectively) is evident of a downscaling result we can be confident in. We also demonstrated reliability of kriging in 
&lt;a href=&#34;https://iopscience.iop.org/article/10.1088/1748-9326/ac39bf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this publication (Figure 3)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Finally, this &lt;code&gt;SpatialPolygons&lt;/code&gt;-informed downscaling took roughly 57 minutes on my machine (this may vary drastically on other devices).&lt;/p&gt;
&lt;h3 id=&#34;spatial-limitation&#34;&gt;Spatial Limitation&lt;/h3&gt;
&lt;div class=&#34;alert alert-info&#34;&gt;
  &lt;div&gt;
    Kriging can be sped up tremendously by limiting downscaling efforts to smaller regions.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;To demonstrate how spatial limitation affects computational time, we downscale all of our remaining 
&lt;a href=&#34;https://www.erikkusch.com/courses/krigr/download//#climate-data&#34;&gt;target data&lt;/a&gt; (i.e., &lt;code&gt;extent&lt;/code&gt; and &lt;code&gt;data.frame&lt;/code&gt; time-series specifications).&lt;/p&gt;
&lt;details&gt;&lt;summary&gt; Click here for kriging calls &lt;/summary&gt;
&lt;h4 id=&#34;extent-data&#34;&gt;&lt;code&gt;extent&lt;/code&gt; Data&lt;/h4&gt;
&lt;h4 id=&#34;point-data-dataframe&#34;&gt;Point-Data (&lt;code&gt;data.frame&lt;/code&gt;)&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;PtsRaw &amp;lt;- stack(file.path(Dir.Data, &amp;quot;PointsRaw.nc&amp;quot;))
Covs_ls &amp;lt;- download_DEM(Train_ras = PtsRaw,
                        Target_res = .02,
                        Shape = Mountains_df,
                        Buffer = 0.5,
                        ID = &amp;quot;Mountain&amp;quot;,
                        Dir = Dir.Covariates,
                        Keep_Temporary = TRUE)
PtsKrig &amp;lt;- krigR(Data = PtsRaw, 
      Covariates_coarse = Covs_ls[[1]], 
      Covariates_fine = Covs_ls[[2]], 
      Keep_Temporary = FALSE, 
      Cores = 1, 
      FileName = &amp;quot;PointsKrig&amp;quot;,
      Dir = Dir.Exports
      )
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;How long did the kriging for each data set take? Let me list these out to highlight just how much of a difference the spatial limitation makes here:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;extent&lt;/code&gt; specification (7344 data cells in training resolution) - roughly 30 minutes&lt;/li&gt;
&lt;li&gt;&lt;code&gt;SpatialPolygons&lt;/code&gt; specification (3752 data cells in training resolution) - roughly 4 minutes&lt;/li&gt;
&lt;li&gt;Point (&lt;code&gt;data.frame&lt;/code&gt;) specification (1908 data cells in training resolution) - roughly 30 seconds&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As you can see, there is a huge benefit to reducing the cells containing data to speed up computation. But what is the impact of doing so for our 
&lt;a href=&#34;https://www.erikkusch.com/courses/krigr/prep/#points-of-interest-dataframe&#34;&gt;points of interest&lt;/a&gt;?&lt;/p&gt;
&lt;details&gt;&lt;summary&gt; Click here for data extraction and plotting &lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Extract_df &amp;lt;- data.frame(
  AirTemp = c(
    raster::extract(
      x = SpatialPolygonsKrig[[1]][[1]],
      y = Mountains_df[, c(&amp;quot;Lon&amp;quot;, &amp;quot;Lat&amp;quot;)]), 
    raster::extract(
      x = ExtKrig[[1]][[1]],
      y = Mountains_df[, c(&amp;quot;Lon&amp;quot;, &amp;quot;Lat&amp;quot;)]), 
    raster::extract(
      x = PtsKrig[[1]][[1]],
      y = Mountains_df[, c(&amp;quot;Lon&amp;quot;, &amp;quot;Lat&amp;quot;)])
  ), 
  Uncertainty = c(
    raster::extract(
      x = SpatialPolygonsKrig[[2]][[1]],
      y = Mountains_df[, c(&amp;quot;Lon&amp;quot;, &amp;quot;Lat&amp;quot;)]), 
    raster::extract(
      x = ExtKrig[[2]][[1]],
      y = Mountains_df[, c(&amp;quot;Lon&amp;quot;, &amp;quot;Lat&amp;quot;)]), 
    raster::extract(
      x = PtsKrig[[2]][[1]],
      y = Mountains_df[, c(&amp;quot;Lon&amp;quot;, &amp;quot;Lat&amp;quot;)])
  ), 
  Mountain = rep(Mountains_df$Mountain, 3),
  Spatial = rep(c(&amp;quot;Polygons&amp;quot;, &amp;quot;Extent&amp;quot;, &amp;quot;Points&amp;quot;), 
                each = nrow(Mountains_df))
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(data = Extract_df, aes(y = Mountain, x = AirTemp, col = Spatial)) +
  geom_point(cex = 5, pch = 18) +
  geom_errorbar(aes(xmin = AirTemp - Uncertainty/2, 
                    xmax = AirTemp + Uncertainty/2)) +
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;&lt;img src=&#34;krigr-downscaling_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As you can see, the differences between the different data sets at our points of interest are noticeable and often times not negligible (as far as statistical interpolation uncertainty, i.e., error bars) are concerned.&lt;/p&gt;
&lt;div class=&#34;alert alert-danger&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;When statistically downscaling data products it is &lt;strong&gt;vital you inspect the output data&lt;/strong&gt; for inconsistencies or other issues.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Kriging is not a one-size-fits all solution to spatial resolution needs!&lt;/strong&gt;&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;localisation-of-results&#34;&gt;Localisation of Results&lt;/h3&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    By default Kriging of the &lt;code&gt;krigR()&lt;/code&gt; function uses all cells in a spatial product to downscale individual cells of rasters.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-info&#34;&gt;
  &lt;div&gt;
    The &lt;code&gt;nmax&lt;/code&gt; argument can circumvent this.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Letâs build further on our above example by adding the &lt;code&gt;nmax&lt;/code&gt; argument (passed on to &lt;code&gt;gstat::krige()&lt;/code&gt;) to our &lt;code&gt;krigR()&lt;/code&gt; function call. This argument controls how many of the closest cells the Kriging algorithm should consider in the downscaling of individual coarse, training cells.&lt;/p&gt;
&lt;p&gt;First, we need to re-establish our covariate data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Covs_ls &amp;lt;- download_DEM(Train_ras = SpatialPolygonsRaw,
                        Target_res = .02,
                        Shape = Shape_shp,
                        Dir = Dir.Covariates,
                        Keep_Temporary = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we may use locally weighted kriging:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;SpatialPolygonsLocalKrig &amp;lt;- krigR(Data = SpatialPolygonsRaw,
      Covariates_coarse = Covs_ls[[1]],
      Covariates_fine = Covs_ls[[2]],
      Keep_Temporary = FALSE,
      Cores = 1, 
      nmax = 10,
      FileName = &amp;quot;SpatialPolygonsLocalKrig&amp;quot;,
      Dir = Dir.Exports
      )
Plot_Krigs(SpatialPolygonsLocalKrig, 
           Shp = Shape_shp,
           Dates = c(&amp;quot;01-1995&amp;quot;, &amp;quot;02-1995&amp;quot;, &amp;quot;03-1995&amp;quot;, &amp;quot;04-1995&amp;quot;)
           )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-downscaling_files/figure-html/KrigShpLocal-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The air temperature prediction/downscaling results look just like the ones that we obtained above (we will investigate this claim in a second here). However, we seriously improved our localised understanding of Kriging uncertainties (i.e., we see much more localised patterns of Kriging standard error). In the case of our study region, uncertainties seem to be highest for areas where the landscape is dominated by large, abrupt changes in elevation (e.g. around the mountainous areas) and water-dominated areas such as streams and lakes (e.g. the lakes around Leipzig in the North of Saxony).&lt;/p&gt;
&lt;div class=&#34;alert alert-info&#34;&gt;
  &lt;div&gt;
    Using the &lt;code&gt;nmax&lt;/code&gt; argument helps to identify highly localised patterns in the Kriging uncertainty as well as predictions!
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Now let&amp;rsquo;s investigate how much of a difference there is between our two predictions of statistically downscaled air temperature when using locally weighted kriging or domain-average kriging as before:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Plot_Raw(SpatialPolygonsLocalKrig[[1]]-SpatialPolygonsKrig[[1]], 
         Shp = Shape_shp,
         Dates = c(&amp;quot;01-1995&amp;quot;, &amp;quot;02-1995&amp;quot;, &amp;quot;03-1995&amp;quot;, &amp;quot;04-1995&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-downscaling_files/figure-html/KrigShpLocalDiff-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Again, limiting the number of data points that the Kriging algorithm has access to changes the data we obtain. Therefore, let me reiterate:&lt;/p&gt;
&lt;div class=&#34;alert alert-danger&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;When statistically downscaling data products it is &lt;strong&gt;vital you inspect the output data&lt;/strong&gt; for inconsistencies or other issues.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Kriging is not a one-size-fits all solution to spatial resolution needs!&lt;/strong&gt;&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;considerations-for-krigr&#34;&gt;Considerations for &lt;code&gt;krigR()&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;krigR()&lt;/code&gt; is a complex function with many things happening under the hood. To make sure you have the best experience with this function, I have compiled a few bits of &lt;em&gt;good-to-know&lt;/em&gt; information about the workings of &lt;code&gt;krigR()&lt;/code&gt;.&lt;/p&gt;
&lt;h4 id=&#34;cores&#34;&gt;&lt;code&gt;Cores&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;Kriging is computationally expensive and can be a time-consuming exercise first and foremost. However, the &lt;code&gt;gstat::krige()&lt;/code&gt; function which &lt;code&gt;krigR()&lt;/code&gt; makes calls to, and which carries out the kriging itself, does not support multi-core processing. Conclusively, we can hand separate kriging jobs to separate cores in our machines and drastically reduce computation time. We do so via the &lt;code&gt;Cores&lt;/code&gt; argument.&lt;/p&gt;
&lt;div class=&#34;alert alert-&#34;&gt;
  &lt;div&gt;
    Using the &lt;code&gt;Cores&lt;/code&gt; argument, &lt;code&gt;krigR()&lt;/code&gt; carries out parallel kriging of multi-layer rasters.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h4 id=&#34;nmax-and-maxdist&#34;&gt;&lt;code&gt;nmax&lt;/code&gt; and &lt;code&gt;maxdist&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;Localised kriging is achieved through either &lt;code&gt;nmax&lt;/code&gt; or &lt;code&gt;maxdist&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    When using &lt;code&gt;nmax&lt;/code&gt; or &lt;code&gt;maxdist&lt;/code&gt;, we recommend you ensure that the distance represented by these arguments approximates the area of typical weather system (around 150km).
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;For the purpose of showing clear patterns in the localisation of uncertainty patterns, we did not to so in the above.&lt;/p&gt;
&lt;h4 id=&#34;keep_temporary-1&#34;&gt;&lt;code&gt;Keep_Temporary&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;Kriging is time-consuming. Particularly for multi-layer rasters with many layers. To make it so you can interrupt kriging of multi-layer rasters and resume the process at a later time, we have implemented temporary file saving. &lt;code&gt;krigR()&lt;/code&gt; checks for presence of temporary files and only loads already kriged layers rather than kriging them again. Upon completion and saving of the final output, you may choose to delete the temporary files or keep them.&lt;/p&gt;
&lt;h4 id=&#34;krigingequation&#34;&gt;&lt;code&gt;KrigingEquation&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;krigR()&lt;/code&gt; can accommodate any covariate pair (training and target resolution) you supply. However, when using 
&lt;a href=&#34;https://www.erikkusch.com/courses/krigr/third-party/#third-party-data-covariates&#34;&gt;third-party covariates&lt;/a&gt; in non-linear combinations, you will need to use the &lt;code&gt;KrigingEquation&lt;/code&gt; argument to do so.&lt;/p&gt;
&lt;div class=&#34;alert alert-&#34;&gt;
  &lt;div&gt;
    With the &lt;code&gt;KrigingEquation&lt;/code&gt; argument, you may specify non-linear combinations of covariates for your call to &lt;code&gt;krigR()&lt;/code&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h4 id=&#34;kriging-reliability&#34;&gt;Kriging Reliability&lt;/h4&gt;
&lt;p&gt;Kriging reliability and robustness is largely dependant on the statistical link between your target variable and covariates of your choice.&lt;/p&gt;
&lt;div class=&#34;alert alert-danger&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Elevation will not be a useful covariate for all climate variables!&lt;/strong&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;We demonstrate that Kriging is a reliable interpolation method when carefully choosing covariates in 
&lt;a href=&#34;https://iopscience.iop.org/article/10.1088/1748-9326/ac39bf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this publication (Figure 3)&lt;/a&gt;. One large factor in reliability of kriging is the change in resolution between training and target resolutions - as a rule of thumb, we do not recommend downscaling representing more than roughly one order of magnitude. If you attempt to do so &lt;code&gt;krigR()&lt;/code&gt; will throw a warning message, but proceed regardless.&lt;/p&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    Kriging is a very flexible tool for statistical interpolation. Consider your choice of covariates and change in resolutions carefully. &lt;strong&gt;Always inspect your data&lt;/strong&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h4 id=&#34;call-list&#34;&gt;Call List&lt;/h4&gt;
&lt;p&gt;So far, we have only ever looked at the first two elements in the list returned by &lt;code&gt;krigR()&lt;/code&gt;. A quick look at the help file, the code, or this guide reveals that there is a third list element - the &lt;em&gt;call list&lt;/em&gt;. When coding this feature into &lt;code&gt;krigR()&lt;/code&gt; I intended for this to be a neat, clean, storage-friendly way of keeping track of how the spatial product was created. It does so without storing additional spatial products. Let&amp;rsquo;s have a look at it:&lt;/p&gt;
&lt;details&gt;&lt;summary&gt; Click here for call list query and output &lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;SpatialPolygonsKrig[[3]]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $Data
## $Data$Class
## [1] &amp;quot;RasterStack&amp;quot;
## attr(,&amp;quot;package&amp;quot;)
## [1] &amp;quot;raster&amp;quot;
## 
## $Data$Dimensions
## $Data$Dimensions$nrow
## [1] 34
## 
## $Data$Dimensions$ncol
## [1] 54
## 
## $Data$Dimensions$ncell
## [1] 1836
## 
## 
## $Data$Extent
## class      : Extent 
## xmin       : 9.726991 
## xmax       : 15.12801 
## ymin       : 49.75 
## ymax       : 53.15 
## 
## $Data$CRS
## Coordinate Reference System:
## Deprecated Proj.4 representation: +proj=longlat +datum=WGS84 +no_defs 
## WKT2 2019 representation:
## GEOGCRS[&amp;quot;unknown&amp;quot;,
##     DATUM[&amp;quot;World Geodetic System 1984&amp;quot;,
##         ELLIPSOID[&amp;quot;WGS 84&amp;quot;,6378137,298.257223563,
##             LENGTHUNIT[&amp;quot;metre&amp;quot;,1]],
##         ID[&amp;quot;EPSG&amp;quot;,6326]],
##     PRIMEM[&amp;quot;Greenwich&amp;quot;,0,
##         ANGLEUNIT[&amp;quot;degree&amp;quot;,0.0174532925199433],
##         ID[&amp;quot;EPSG&amp;quot;,8901]],
##     CS[ellipsoidal,2],
##         AXIS[&amp;quot;longitude&amp;quot;,east,
##             ORDER[1],
##             ANGLEUNIT[&amp;quot;degree&amp;quot;,0.0174532925199433,
##                 ID[&amp;quot;EPSG&amp;quot;,9122]]],
##         AXIS[&amp;quot;latitude&amp;quot;,north,
##             ORDER[2],
##             ANGLEUNIT[&amp;quot;degree&amp;quot;,0.0174532925199433,
##                 ID[&amp;quot;EPSG&amp;quot;,9122]]]] 
## 
## $Data$layers
## [1] &amp;quot;X1&amp;quot; &amp;quot;X2&amp;quot; &amp;quot;X3&amp;quot; &amp;quot;X4&amp;quot;
## 
## 
## $Covariates_coarse
## $Covariates_coarse$Class
## [1] &amp;quot;RasterLayer&amp;quot;
## attr(,&amp;quot;package&amp;quot;)
## [1] &amp;quot;raster&amp;quot;
## 
## $Covariates_coarse$Dimensions
## $Covariates_coarse$Dimensions$nrow
## [1] 34
## 
## $Covariates_coarse$Dimensions$ncol
## [1] 54
## 
## $Covariates_coarse$Dimensions$ncell
## [1] 1836
## 
## 
## $Covariates_coarse$Extent
## class      : Extent 
## xmin       : 9.726991 
## xmax       : 15.12801 
## ymin       : 49.75 
## ymax       : 53.15 
## 
## $Covariates_coarse$CRS
## Coordinate Reference System:
## Deprecated Proj.4 representation: +proj=longlat +datum=WGS84 +no_defs 
## WKT2 2019 representation:
## GEOGCRS[&amp;quot;unknown&amp;quot;,
##     DATUM[&amp;quot;World Geodetic System 1984&amp;quot;,
##         ELLIPSOID[&amp;quot;WGS 84&amp;quot;,6378137,298.257223563,
##             LENGTHUNIT[&amp;quot;metre&amp;quot;,1]],
##         ID[&amp;quot;EPSG&amp;quot;,6326]],
##     PRIMEM[&amp;quot;Greenwich&amp;quot;,0,
##         ANGLEUNIT[&amp;quot;degree&amp;quot;,0.0174532925199433],
##         ID[&amp;quot;EPSG&amp;quot;,8901]],
##     CS[ellipsoidal,2],
##         AXIS[&amp;quot;longitude&amp;quot;,east,
##             ORDER[1],
##             ANGLEUNIT[&amp;quot;degree&amp;quot;,0.0174532925199433,
##                 ID[&amp;quot;EPSG&amp;quot;,9122]]],
##         AXIS[&amp;quot;latitude&amp;quot;,north,
##             ORDER[2],
##             ANGLEUNIT[&amp;quot;degree&amp;quot;,0.0174532925199433,
##                 ID[&amp;quot;EPSG&amp;quot;,9122]]]] 
## 
## $Covariates_coarse$layers
## [1] &amp;quot;DEM&amp;quot;
## 
## 
## $Covariates_fine
## $Covariates_fine$Class
## [1] &amp;quot;RasterLayer&amp;quot;
## attr(,&amp;quot;package&amp;quot;)
## [1] &amp;quot;raster&amp;quot;
## 
## $Covariates_fine$Dimensions
## $Covariates_fine$Dimensions$nrow
## [1] 204
## 
## $Covariates_fine$Dimensions$ncol
## [1] 324
## 
## $Covariates_fine$Dimensions$ncell
## [1] 66096
## 
## 
## $Covariates_fine$Extent
## class      : Extent 
## xmin       : 9.72486 
## xmax       : 15.12486 
## ymin       : 49.74986 
## ymax       : 53.14986 
## 
## $Covariates_fine$CRS
## Coordinate Reference System:
## Deprecated Proj.4 representation: +proj=longlat +datum=WGS84 +no_defs 
## WKT2 2019 representation:
## GEOGCRS[&amp;quot;unknown&amp;quot;,
##     DATUM[&amp;quot;World Geodetic System 1984&amp;quot;,
##         ELLIPSOID[&amp;quot;WGS 84&amp;quot;,6378137,298.257223563,
##             LENGTHUNIT[&amp;quot;metre&amp;quot;,1]],
##         ID[&amp;quot;EPSG&amp;quot;,6326]],
##     PRIMEM[&amp;quot;Greenwich&amp;quot;,0,
##         ANGLEUNIT[&amp;quot;degree&amp;quot;,0.0174532925199433],
##         ID[&amp;quot;EPSG&amp;quot;,8901]],
##     CS[ellipsoidal,2],
##         AXIS[&amp;quot;longitude&amp;quot;,east,
##             ORDER[1],
##             ANGLEUNIT[&amp;quot;degree&amp;quot;,0.0174532925199433,
##                 ID[&amp;quot;EPSG&amp;quot;,9122]]],
##         AXIS[&amp;quot;latitude&amp;quot;,north,
##             ORDER[2],
##             ANGLEUNIT[&amp;quot;degree&amp;quot;,0.0174532925199433,
##                 ID[&amp;quot;EPSG&amp;quot;,9122]]]] 
## 
## $Covariates_fine$layers
## [1] &amp;quot;DEM&amp;quot;
## 
## 
## $KrigingEquation
## ERA ~ DEM
## &amp;lt;environment: 0x7fee0d4450e8&amp;gt;
## 
## $Cores
## [1] 1
## 
## $FileName
## [1] &amp;quot;SpatialPolygonsKrig&amp;quot;
## 
## $Keep_Temporary
## [1] FALSE
## 
## $nmax
## [1] Inf
## 
## $Data_Retrieval
## [1] &amp;quot;None needed. Data was not queried via krigR function, but supplied by user.&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;This lengthy list should contain all information you need to trace how you created a certain data set using &lt;code&gt;krigR()&lt;/code&gt;. If you feel like anything is missing in this list, please contact us.&lt;/p&gt;
&lt;h2 id=&#34;aggregate-uncertainty&#34;&gt;Aggregate Uncertainty&lt;/h2&gt;
&lt;div class=&#34;alert alert-danger&#34;&gt;
  &lt;div&gt;
    Every climate data product is subject to an error-rate / range of data uncertainty. Unfortunately, &lt;strong&gt;almost none&lt;/strong&gt; of the established climate data products &lt;strong&gt;communicate associated uncertainties&lt;/strong&gt;. This leads to a dangerous &lt;strong&gt;overestimation of data credibility&lt;/strong&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-info&#34;&gt;
  &lt;div&gt;
    With the &lt;code&gt;KrigR&lt;/code&gt; workflow, it is trivial to obtain uncertainty flags for all of your data - no matter the spatial or temporal resolution.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;To understand the full certainty of our data obtained via the &lt;code&gt;KrigR&lt;/code&gt; workflow, we should combine 
&lt;a href=&#34;https://www.erikkusch.com/courses/krigr/download/#dynamical-data-uncertainty&#34;&gt;dynamical uncertainty&lt;/a&gt; with the statistical uncertainty we obtained from the &lt;code&gt;krigR()&lt;/code&gt; function call above.&lt;/p&gt;
&lt;p&gt;To do so, we require two data sets:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;SpatialPoylgonsKrig&lt;/code&gt; - 
&lt;a href=&#34;https://www.erikkusch.com/courses/krigr/kriging/#kriging&#34;&gt;created above&lt;/a&gt; containing statistical uncertainty in the second list position&lt;/li&gt;
&lt;li&gt;&lt;code&gt;SpatialPoylgonsEns&lt;/code&gt; - 
&lt;a href=&#34;https://www.erikkusch.com/courses/krigr/download/#dynamical-data-uncertainty-1&#34;&gt;created here&lt;/a&gt;; 
&lt;a href=&#34;https://www.erikkusch.com/courses/krigr/Data/SpatialPolygonsEns.nc&#34;&gt;download here&lt;/a&gt; containing dynamical uncertainty&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First, we load the data and assign them to objects with shorter names:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;SpatialPolygonsEns &amp;lt;- stack(file.path(Dir.Data, &amp;quot;SpatialPolygonsEns.nc&amp;quot;))
DynUnc &amp;lt;- SpatialPolygonsEns
KrigUnc &amp;lt;- SpatialPolygonsKrig[[2]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we need to align the rasters of statistical uncertainty (resolution: 0.017) and dynamical uncertainty (resolution: 0.5). As you can see, these are of differing resolutions and so cannot easily be combined using raster math. Instead, we first disaggregate the coarser-resolution raster (&lt;code&gt;DynUnc&lt;/code&gt;) as disaggregation does not attempt any interpolation thus preserving the data, but representing it with smaller cells. To fix final remaining alignment issues, we allow for some resampling between the two raster:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;EnsDisagg &amp;lt;- disaggregate(DynUnc, fact=res(DynUnc)[1]/res(KrigUnc)[1])
DynUnc &amp;lt;- resample(EnsDisagg, KrigUnc)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we combine the two uncertainty data products to form an aggregate uncertainty product:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;FullUnc &amp;lt;- DynUnc + KrigUnc
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we are ready to plot our aggregate uncertainty:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Plot_Raw(FullUnc, 
         Shp = Shape_shp, 
         Dates = c(&amp;quot;01-1995&amp;quot;, &amp;quot;02-1995&amp;quot;, &amp;quot;03-1995&amp;quot;, &amp;quot;04-1995&amp;quot;), 
         COL = rev(viridis(100)))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-downscaling_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As you can see, at short time-scales dynamic uncertainty eclipses statistical uncertainty. However, this phenomenon reverses at longer time-scales as shown in 
&lt;a href=&#34;https://iopscience.iop.org/article/10.1088/1748-9326/ac39bf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this publication (Figure 1)&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.2.3 (2023-03-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Big Sur ... 10.16
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] mapview_2.11.0          rnaturalearthdata_0.1.0 rnaturalearth_0.3.2    
##  [4] gimms_1.2.1             ggmap_3.0.2             cowplot_1.1.1          
##  [7] viridis_0.6.2           viridisLite_0.4.1       ggplot2_3.4.1          
## [10] tidyr_1.3.0             KrigR_0.1.2             terra_1.7-21           
## [13] httr_1.4.5              stars_0.6-0             abind_1.4-5            
## [16] fasterize_1.0.4         sf_1.0-12               lubridate_1.9.2        
## [19] automap_1.1-9           doSNOW_1.0.20           snow_0.4-4             
## [22] doParallel_1.0.17       iterators_1.0.14        foreach_1.5.2          
## [25] rgdal_1.6-5             raster_3.6-20           sp_1.6-0               
## [28] stringr_1.5.0           keyring_1.3.1           ecmwfr_1.5.0           
## [31] ncdf4_1.21             
## 
## loaded via a namespace (and not attached):
##  [1] leafem_0.2.0             colorspace_2.1-0         class_7.3-21            
##  [4] leaflet_2.1.2            satellite_1.0.4          base64enc_0.1-3         
##  [7] rstudioapi_0.14          proxy_0.4-27             farver_2.1.1            
## [10] fansi_1.0.4              codetools_0.2-19         cachem_1.0.7            
## [13] knitr_1.42               jsonlite_1.8.4           png_0.1-8               
## [16] Kendall_2.2.1            compiler_4.2.3           assertthat_0.2.1        
## [19] fastmap_1.1.1            cli_3.6.0                htmltools_0.5.4         
## [22] tools_4.2.3              gtable_0.3.1             glue_1.6.2              
## [25] dplyr_1.1.0              Rcpp_1.0.10              jquerylib_0.1.4         
## [28] vctrs_0.6.1              blogdown_1.16            crosstalk_1.2.0         
## [31] lwgeom_0.2-11            xfun_0.37                timechange_0.2.0        
## [34] lifecycle_1.0.3          rnaturalearthhires_0.2.1 zoo_1.8-11              
## [37] scales_1.2.1             gstat_2.1-0              yaml_2.3.7              
## [40] curl_5.0.0               memoise_2.0.1            gridExtra_2.3           
## [43] sass_0.4.5               reshape_0.8.9            stringi_1.7.12          
## [46] highr_0.10               e1071_1.7-13             boot_1.3-28.1           
## [49] intervals_0.15.3         RgoogleMaps_1.4.5.3      rlang_1.1.0             
## [52] pkgconfig_2.0.3          bitops_1.0-7             evaluate_0.20           
## [55] lattice_0.20-45          purrr_1.0.1              htmlwidgets_1.6.1       
## [58] labeling_0.4.2           tidyselect_1.2.0         plyr_1.8.8              
## [61] magrittr_2.0.3           bookdown_0.33            R6_2.5.1                
## [64] generics_0.1.3           DBI_1.1.3                pillar_1.8.1            
## [67] withr_2.5.0              units_0.8-1              xts_0.13.0              
## [70] tibble_3.2.1             spacetime_1.2-8          KernSmooth_2.23-20      
## [73] utf8_1.2.3               rmarkdown_2.20           jpeg_0.1-10             
## [76] grid_4.2.3               zyp_0.11-1               FNN_1.1.3.2             
## [79] digest_0.6.31            classInt_0.4-9           webshot_0.5.4           
## [82] stats4_4.2.3             munsell_0.5.0            bslib_0.4.2
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 15</title>
      <link>https://www.erikkusch.com/courses/rethinking/chapter-15/</link>
      <pubDate>Thu, 06 May 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/rethinking/chapter-15/</guid>
      <description>&lt;h1 id=&#34;missing-data-and-other-opportunities&#34;&gt;Missing Data and Other Opportunities&lt;/h1&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/rethinking/18__07-05-2021_SUMMARY_-Measurement-Error-and-Missing-Data.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides Chapter 15&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to the exercises at the end of chapter 15 in 
&lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Satistical Rethinking 2&lt;/a&gt; by Richard McElreath. I have created these notes as a part of my ongoing involvement in the 
&lt;a href=&#34;https://www.erikkusch.com/project/aubayes/&#34;&gt;AU Bayes Study Group&lt;/a&gt;. Much of my inspiration for these solutions, where necessary, has been obtained from&lt;/p&gt;
&lt;!-- [Taras Svirskyi](https://github.com/jffist/statistical-rethinking-solutions/blob/master/ch10_hw.R), [William Wolf](https://github.com/cavaunpeu/statistical-rethinking/blob/master/chapter-10/homework.R), and [Corrie Bartelheimer](https://www.samples-of-thoughts.com/projects/statistical-rethinking/chapter_10/chp10-ex/) as well as  --&gt;
&lt;p&gt;the solutions provided to instructors by Richard McElreath himself.&lt;/p&gt;
&lt;h2 id=&#34;r-environment&#34;&gt;&lt;code&gt;R&lt;/code&gt; Environment&lt;/h2&gt;
&lt;p&gt;For today&amp;rsquo;s exercise, I load the following packages:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rethinking)
library(gtools)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;easy-exercises&#34;&gt;Easy Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-e1&#34;&gt;Practice E1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Rewrite the Oceanic tools model (from Chapter 11) below so that it assumes measured error on the log population sizes of each society. You donât need to fit the model to data. Just modify the mathematical formula below.
$$T_i â¼ Poisson(Âµ_i)$$
$$log(Âµ_i) = Î± + Î²*log(P_i)$$
$$Î± â¼ Normal(0, 1.5)$$ 
$$Î² â¼ Normal(0, 1)$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; The population variable ($P_i$) is a predictor in this model. In order to estimate/account for measurement error in a predictor variable, all we need to do is add a distribution to the observed values ($P^\star_i$) with a given error ($Ï_P$):&lt;/p&gt;
&lt;p&gt;$$log(P_i) â¼ Normal(P^\star_i, Ï_P)$$&lt;/p&gt;
&lt;p&gt;The final model specification combines the above line with the previous model specification and substitutes $P^\star_i$ in place of $P_i$:&lt;/p&gt;
&lt;p&gt;$$T_i â¼ Poisson(Âµ_i)$$&lt;/p&gt;
&lt;p&gt;$$log(Âµ_i) = Î± + Î²* P^\star_i $$&lt;/p&gt;
&lt;p&gt;$$log(P_i) â¼ Normal(P^\star_i, Ï_P)$$&lt;/p&gt;
&lt;p&gt;$$Î± â¼ Normal(0, 1.5)$$&lt;/p&gt;
&lt;p&gt;$$Î² â¼ Normal(0, 1)$$&lt;/p&gt;
&lt;p&gt;$$Ï_P \sim Exponential(1)$$&lt;/p&gt;
&lt;p&gt;Of course, we also need a prior for $Ï_P$. I don&amp;rsquo;t know enough about the data to take a good educated guess for this parameter and so I just run the usual prior for standard deviations used in the book.&lt;/p&gt;
&lt;h3 id=&#34;practice-e2&#34;&gt;Practice E2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Rewrite the same model so that it allows imputation of missing values for log population. There arenât any missing values in the variable, but you can still write down a model formula that would imply imputation, if any values were missing.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Imputation comes into play when measurement error is so intense that we have missing data - &amp;ldquo;missing data is grown-up measurement error&amp;rdquo;. The trick with missing data is to establish adaptive priors for the missing data which is informed by the observations for which we do have data:&lt;/p&gt;
&lt;p&gt;$$T_i â¼ Poisson(Âµ_i)$$&lt;/p&gt;
&lt;p&gt;$$log(Âµ_i) = Î± + Î² * P^\star_i$$&lt;/p&gt;
&lt;p&gt;$$P^\star_i â¼ Normal(\overline{ P^\star }, Ï_P)$$&lt;/p&gt;
&lt;p&gt;$$Î± â¼ Normal(0, 1.5)$$ 
$$Î² â¼ Normal(0, 1)$$
$$P^\star \sim Normal(0, 1)$$
$$Ï_P \sim Exponential(1)$$&lt;/p&gt;
&lt;p&gt;With the new specification, values of $P^\star_i$ (observed log-populations) are either assumed to be data or parameters according to whether data is present for observation $i$ or not.&lt;/p&gt;
&lt;h2 id=&#34;medium-exercises&#34;&gt;Medium Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-m1&#34;&gt;Practice M1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Using the mathematical form of the imputation model in the chapter, explain what is being assumed about how the missing values were generated.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; As a reminder, the mathematical form of the imputation model in the chapter is as follows:&lt;/p&gt;
&lt;p&gt;$$K_i â¼ Normal(Âµ_i, Ï)$$
$$Âµ_i = Î± + Î²_BB_i + Î²_M * log(M_i)$$
$$B_i â¼ Normal(Î½, Ï_B)$$
$$Î± â¼ Normal(0, 0.5)$$ 
$$Î²_B â¼ Normal(0, 0.5)$$
$$Î²_M â¼ Normal(0, 0.5)$$ 
$$Ï â¼ Exponential(1)$$ 
$$Î½ â¼ Normal(0.5, 1)$$ 
$$Ï_B â¼ Exponential(1)$$&lt;/p&gt;
&lt;p&gt;The assumption about which distribution our predictor with missing data ($B$) does not contain any information about individual cases. It simply just assumes that missing values are randomly placed across the cases. As such, the model assumes that there is no causation at play for how the data came to be missing/not reported, but only states that information that is missing follows a certain distribution which is the same distribution against which to test the data which we do have.&lt;/p&gt;
&lt;h3 id=&#34;practice-m2&#34;&gt;Practice M2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  In earlier chapters, we threw away cases from the primate milk data, so we could use the neocortex variable. Now repeat the WAIC model comparison example from Chapter 6, but use imputation on the neocortex variable so that you can include all of the cases in the original data. The simplest form of imputation is acceptable. How are the model comparison results affected by being able to include all of the cases?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Unfortunately, chapter 6 does not include a neocortex model in the version of the book I am working with and pulling these exercises from. However, chapter 5 does. To begin with this exercise, I load the data and prepare it the same way we did back in chapter 5, by standardising our variables for energy content of milk (&lt;code&gt;K&lt;/code&gt;), and body mass (&lt;code&gt;M&lt;/code&gt;). Contrary to chapter 5, I do not standardise the neocortex portion (&lt;code&gt;P&lt;/code&gt;), but leave it as a proportion between 0 and 1:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(milk)
d &amp;lt;- milk
d$neocortex.prop &amp;lt;- d$neocortex.perc / 100
d$logmass &amp;lt;- log(d$mass)
## Incomplete cases allowed
dat_list &amp;lt;- list(
  K = standardize(d$kcal.per.g),
  P = d$neocortex.prop,
  M = standardize(d$logmass)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why did I set the neocortex variable (&lt;code&gt;P&lt;/code&gt;) to be non-standardised? So I could use priors more readily and make sure this proportion always stays between 0 and 1 - everything outside these bounds would be biological nonsense.&lt;/p&gt;
&lt;p&gt;With the data ready, we can now run our three models from chapter 5, but this time, in a way so as to account for missing data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Mass effect (not the video game franchise); no imputation needed here
m_M2_5.6 &amp;lt;- ulam(
  alist(
    K ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bM * M,
    a ~ dnorm(0, 0.2),
    bM ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = dat_list[-2], chains = 4, cores = 4, iter = 2000, log_lik = TRUE
)

## Neocortex effect
m_M2_5.5 &amp;lt;- ulam(
  alist(
    K ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bP * (P - 0.67), # 0.67 is the average value of P --&amp;gt; Intercept now represents K at average P
    P ~ dbeta2(nu, theta), # bound between 0 and 1, but wide
    nu ~ dbeta(2, 2), # bound between 0 and 1
    a ~ dnorm(0, 0.2), # same as before
    bP ~ dnorm(0, 10), # another wide prior, since there is little variation in values of P
    theta ~ dexp(1), # standard stdev prior
    sigma ~ dexp(1), # same as before
    vector[12]:P_impute ~ uniform(0, 1) # there are 12 NA-values for P, we bound them between 0 and 1
  ),
  data = dat_list, chains = 4, cores = 4, iter = 2000, log_lik = TRUE
)

## Both predictors
m_M2_5.7 &amp;lt;- ulam(
  alist(
    K ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bP * (P - 0.67) + bM * M, # 0.67 is the average value of P --&amp;gt; Intercept now represents K at average P
    P ~ dbeta2(nu, theta), # bound between 0 and 1, but wide
    nu ~ dbeta(2, 2), # bound between 0 and 1
    a ~ dnorm(0, 0.2), # same as before
    bM ~ dnorm(0, 0.5), # same as before
    bP ~ dnorm(0, 10), # another wide prior, since there is little variation in values of P
    theta ~ dexp(1), # standard stdev prior
    sigma ~ dexp(1), # same as before
    vector[12]:P_impute ~ uniform(0, 1) # there are 12 NA-values for P, we bound them between 0 and 1
  ),
  data = dat_list, chains = 4, cores = 4, iter = 2000, log_lik = TRUE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All three models are compiled. Time to compare how they perform:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;compare(m_M2_5.5, m_M2_5.6, m_M2_5.7)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              WAIC       SE    dWAIC      dSE    pWAIC     weight
## m_M2_5.7 79.50850 5.831506 0.000000       NA 4.565835 0.74491310
## m_M2_5.6 82.17410 5.870788 2.665598 1.455532 1.678415 0.19646189
## m_M2_5.5 84.59271 5.291591 5.084212 3.492726 2.344498 0.05862501
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unsurprisingly, the full model outperforms both one-effect models here. Interestingly, the mass-only model still pulls ahead of the (now imputation-driven) neocortex-only model.&lt;/p&gt;
&lt;p&gt;Visualising what our full imputation model sees, we obtain:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;post &amp;lt;- extract.samples(m_M2_5.7)
P_impute_mu &amp;lt;- apply(post$P_impute, 2, mean)
P_impute_ci &amp;lt;- apply(post$P_impute, 2, PI)
par(mfrow = c(1, 2))
# P vs K
plot(dat_list$P,
  dat_list$K,
  pch = 16, col = rangi2,
  xlab = &amp;quot;neocortex percent&amp;quot;, ylab = &amp;quot;kcal milk (std)&amp;quot;, xlim = c(0, 1)
)
miss_idx &amp;lt;- which(is.na(dat_list$P))
Ki &amp;lt;- dat_list$K[miss_idx]
points(P_impute_mu, Ki)
for (i in 1:12) lines(P_impute_ci[, i], rep(Ki[i], 2))
# M vs B
plot(dat_list$M, dat_list$P, pch = 16, col = rangi2, ylab = &amp;quot;neocortex percent (std)&amp;quot;, xlab = &amp;quot;log body mass (std)&amp;quot;, ylim = c(0, 1))
Mi &amp;lt;- dat_list$M[miss_idx]
points(Mi, P_impute_mu)
for (i in 1:12) lines(rep(Mi[i], 2), P_impute_ci[, i])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-07-statistical-rethinking-chapter-15_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;These are the same plots as in the book in chapter 15. The only difference is that our imputed neocortex percent values now fall into clearly readable (and sensible) ranges between 0 and 1.&lt;/p&gt;
&lt;h3 id=&#34;practice-m3&#34;&gt;Practice M3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Repeat the divorce data measurement error models, but this time double the standard errors. Can you explain how doubling the standard errors impacts inference?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Again, I prepare the data the same way as the book does it:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(WaffleDivorce)
d &amp;lt;- WaffleDivorce
dlist &amp;lt;- list(
  D_obs = standardize(d$Divorce),
  D_sd = d$Divorce.SE / sd(d$Divorce),
  M = standardize(d$Marriage),
  A = standardize(d$MedianAgeMarriage),
  N = nrow(d)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, I simply take the model from the book and run it:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m15.1 &amp;lt;- ulam(
  alist(
    D_obs ~ dnorm(D_true, D_sd),
    vector[N]:D_true ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bA * A + bM * M,
    a ~ dnorm(0, 0.2),
    bA ~ dnorm(0, 0.5),
    bM ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = dlist, chains = 4, cores = 4
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have our baseline model, it is time to double the standard error variable &lt;code&gt;D_sd&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_M3 &amp;lt;- ulam(
  alist(
    D_obs ~ dnorm(D_true, D_sd * 2.0),
    vector[N]:D_true ~ dnorm(mu, sigma),
    mu &amp;lt;- a + bA * A + bM * M,
    a ~ dnorm(0, 0.2),
    bA ~ dnorm(0, 0.5),
    bM ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = dlist, chains = 4, cores = 4, iter = 4000
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s compare the two models for now and see what is happening:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m15.1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              mean         sd       5.5%      94.5%     n_eff    Rhat4
## a     -0.05218494 0.09615369 -0.2026636  0.1001795 1835.1415 1.001424
## bA    -0.61413500 0.16450611 -0.8828791 -0.3543928  916.8593 1.002451
## bM     0.05837404 0.16475940 -0.2017307  0.3267334  961.6816 1.002555
## sigma  0.58800945 0.10284935  0.4284475  0.7570348  784.5056 1.000690
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m_M3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean        sd        5.5%       94.5%     n_eff    Rhat4
## a     -0.1177004 0.1007595 -0.27064731  0.04738969 401.47170 1.008286
## bA    -0.6323619 0.1522011 -0.88505387 -0.38786285 510.81804 1.015609
## bM     0.2072101 0.1808927 -0.08675262  0.47906692 420.57867 1.022198
## sigma  0.1541612 0.1131281  0.02439099  0.36471832  97.19502 1.065839
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Oof. Without going into any detail on the parameter estimates, I have to point out that I don&amp;rsquo;t like the effective sample sizes (&lt;code&gt;n_eff&lt;/code&gt;) on our new model one bit. They are much, MUCH smaller than those of our baseline model. This highlights that out second model struggled with efficient exploration of posterior parameter space. I reckon this is a result of the increased standard deviation making the posterior landscape less easy to identify.&lt;/p&gt;
&lt;p&gt;One way to work around this issue is to rewrite the model in a non-centred parametrisation:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_M3B &amp;lt;- ulam(
  alist(
    D_obs ~ dnorm(mu + z_true * sigma, D_sd * 2.0),
    vector[N]:z_true ~ dnorm(0, 1), # gotten rid of the prior dependency here
    mu &amp;lt;- a + bA * A + bM * M,
    a ~ dnorm(0, 0.2),
    bA ~ dnorm(0, 0.5),
    bM ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data = dlist, chains = 4, cores = 4, iter = 4000,
  control = list(max_treedepth = 14)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now, let&amp;rsquo;s compare these again:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m15.1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              mean         sd       5.5%      94.5%     n_eff    Rhat4
## a     -0.05218494 0.09615369 -0.2026636  0.1001795 1835.1415 1.001424
## bA    -0.61413500 0.16450611 -0.8828791 -0.3543928  916.8593 1.002451
## bM     0.05837404 0.16475940 -0.2017307  0.3267334  961.6816 1.002555
## sigma  0.58800945 0.10284935  0.4284475  0.7570348  784.5056 1.000690
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m_M3B)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean         sd       5.5%       94.5%     n_eff     Rhat4
## a     -0.1185044 0.09953924 -0.2778272  0.03741466 10901.282 0.9996237
## bA    -0.6444853 0.16468812 -0.9085568 -0.37653366  7171.900 0.9996660
## bM     0.1948245 0.19042617 -0.1056664  0.50037111  8322.124 1.0000969
## sigma  0.1431878 0.10844390  0.0117172  0.34538107  4706.223 0.9998725
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Nice. That got rid off our issues of non-effective sampling of posteriors. Now we can actually compare the model results. The biggest difference between these two models is found in the estimates for &lt;code&gt;bM&lt;/code&gt; (the effect of marriage rate on divorce rate) and &lt;code&gt;sigma&lt;/code&gt; (the standard deviation of the normal distribution from which the divorce rates are pulled). By increasing the standard error, we have effectively allowed individual states to exert much greater influence on the regression slope estimates thus shifting the result around.&lt;/p&gt;
&lt;p&gt;It is also worth pointing out right now that the non-centred model performs much more effective sampling, but the parameter estimates are ultimately the same irrespective of parametrisation in this example.&lt;/p&gt;
&lt;h2 id=&#34;hard-exercises&#34;&gt;Hard Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-h1&#34;&gt;Practice H1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  The data in &lt;code&gt;data(elephants)&lt;/code&gt; are counts of matings observed for bull elephants of differing ages. There is a strong positive relationship between age and matings. However, age is not always assessed accurately. First, fit a Poisson model predicting &lt;code&gt;MATINGS&lt;/code&gt; with &lt;code&gt;AGE&lt;/code&gt; as a predictor. Second, assume that the observed &lt;code&gt;AGE&lt;/code&gt; values are uncertain and have a standard error of $\pm$ 5 years. Re-estimate the relationship between &lt;code&gt;MATINGS&lt;/code&gt; and &lt;code&gt;AGE&lt;/code&gt;, incorporating this measurement error. Compare the inferences of the two models.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; First, I load the data and take a glance at its contents:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(elephants)
d &amp;lt;- elephants
str(d)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &#39;data.frame&#39;:	41 obs. of  2 variables:
##  $ AGE    : int  27 28 28 28 28 29 29 29 29 29 ...
##  $ MATINGS: int  0 1 1 1 3 0 0 0 2 2 ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can run some models. Before we get started, it is worth pointing out that there are a multitude of ways in which age could influence number of matings - exponential, logarithmic, poisson, etc. Here, I run with a poisson-approach. If this were a real-world research problem, I should probably test all three variations of the model. Alas, ain&amp;rsquo;t nobody got time fo&#39; that in an exercise.&lt;/p&gt;
&lt;p&gt;The data starts with &lt;code&gt;AGE&lt;/code&gt; values at 27. This suggests to me that this must be roughly around when elephants reach sexual maturity and will start to mate. Hence, I subtract 25 from all &lt;code&gt;AGE&lt;/code&gt; values in my model - just to be safe and interpret the number of matings as &amp;ldquo;number of matings since reaching sexual maturity&amp;rdquo;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Basic Model without uncertainty:
m_H1_A &amp;lt;- ulam(
  alist(
    MATINGS ~ dpois(lambda),
    lambda &amp;lt;- exp(a) * (AGE - 25)^bA,
    a ~ dnorm(0, 1),
    bA ~ dnorm(0, 1)
  ),
  data = d, chains = 4, cores = 4
)
precis(m_H1_A)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          mean        sd       5.5%      94.5%    n_eff     Rhat4
## a  -0.6922235 0.3429011 -1.2307922 -0.1347450 388.4313 0.9986860
## bA  0.7191531 0.1353220  0.5000447  0.9213017 379.1290 0.9988198
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, another not-so-efficient sampling model. How does it see the relationship between &lt;code&gt;AGE&lt;/code&gt; and &lt;code&gt;MATINGS&lt;/code&gt;?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# ages in the data range from 27 to 53
A_seq &amp;lt;- seq(from = 25, to = 55, length.out = 30)
lambda &amp;lt;- link(m_H1_A, data = list(AGE = A_seq))
lambda_mu &amp;lt;- apply(lambda, 2, mean)
lambda_PI &amp;lt;- apply(lambda, 2, PI)
plot(d$AGE, d$MATINGS,
  pch = 16, col = rangi2,
  xlab = &amp;quot;age&amp;quot;, ylab = &amp;quot;matings&amp;quot;
)
lines(A_seq, lambda_mu)
shade(lambda_PI, A_seq)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-07-statistical-rethinking-chapter-15_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s a pretty reliably positive relationship. Older elephants mate more.&lt;/p&gt;
&lt;p&gt;On to the measurement error model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;d$AGE0 &amp;lt;- d$AGE - 25 # add the sexual maturity consideration to the data
m_H1_B &amp;lt;- ulam(
  alist(
    MATINGS ~ dpois(lambda), # same outcome as before
    lambda &amp;lt;- exp(a) * AGE_est[i]^bA, # log-scale predictors
    AGE0 ~ dnorm(AGE_est, 5), # Gaussian distribution with error 5
    vector[41]:AGE_est ~ dunif(0, 50), # prior for individual observed ages
    a ~ dnorm(0, 1),
    bA ~ dnorm(0, 1)
  ),
  data = d, chains = 4, cores = 4
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m_H1_B)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          mean        sd       5.5%       94.5%    n_eff    Rhat4
## a  -0.7742834 0.4801451 -1.6114596 -0.04176099 1049.624 1.003248
## bA  0.7360628 0.1803691  0.4590526  1.05008454 1060.533 1.003109
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Interestingly enough, the estimate of &lt;code&gt;bA&lt;/code&gt; has not changed between these models. Why? Because we added completely symmetric measurement error that remains unchanged across all ages of our elephants. Hence, we don&amp;rsquo;t end up biasing our model because the error in the data is not biased (at least we assume so).&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s finish this off by looking at what our model expects the ages to be like for different matings:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;post &amp;lt;- extract.samples(m_H1_B) # extract samples
AGE_est &amp;lt;- apply(post$AGE_est, 2, mean) + 25 # add 25 back to ages
MATINGS_j &amp;lt;- jitter(d$MATINGS) # jitter MATINGS for better readability
plot(d$AGE, MATINGS_j, pch = 16, col = rangi2, xlab = &amp;quot;age&amp;quot;, ylab = &amp;quot;matings&amp;quot;, xlim = c(23, 55)) # observed ages
points(AGE_est, MATINGS_j) # estimated ages
for (i in 1:nrow(d)) lines(c(d$AGE[i], AGE_est[i]), rep(MATINGS_j[i], 2)) # shrinkage lines
lines(A_seq, lambda_mu) # linear regression from previous model
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-07-statistical-rethinking-chapter-15_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The blue dots represent the observed ages, while the open circles depict the estimated true ages from our model. We see some shrinkage. Fascinatingly, the shrinkage appears to switch direction around the regression line, however. Values above the regression line are shrunk to higher age ranges, while the reverse is true below the regression line. What this means is that the model assumed elephants with unexpectedly high mating numbers for their observed age to be older than our data implies and vice versa.&lt;/p&gt;
&lt;h3 id=&#34;practice-h2&#34;&gt;Practice H2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Repeat the model fitting problem above, now increasing the assumed standard error on &lt;code&gt;AGE&lt;/code&gt;. How large does the standard error have to get before the posterior mean for the coefficient on &lt;code&gt;AGE&lt;/code&gt; reaches zero?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; To solve this, I just run the model above again, but increase the standard error. I did several times with ever-increasing standard errors. Finally I landed on a standard error of &lt;code&gt;100&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_H2 &amp;lt;- ulam(
  alist(
    MATINGS ~ dpois(lambda),
    lambda &amp;lt;- exp(a) * AGE_est[i]^bA,
    AGE0 ~ dnorm(AGE_est, 100), # increase standard error here
    vector[41]:AGE_est ~ dunif(0, 50),
    a ~ dnorm(0, 1),
    bA ~ dnorm(0, 1)
  ),
  data = d, chains = 4, cores = 4
)
precis(m_H2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          mean        sd       5.5%     94.5%    n_eff    Rhat4
## a  -0.3487762 1.1358880 -1.7265552 1.9908366 6.393092 1.409652
## bA  0.4246873 0.3829174 -0.4010691 0.8592536 5.932438 1.454539
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Albeit not having reached 0, the mean estimate of &lt;code&gt;bA&lt;/code&gt; is closer to 0 now and the percentile interval around it is so large that we would not be able to identify the effect here.&lt;/p&gt;
&lt;h3 id=&#34;practice-h3&#34;&gt;Practice H3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  The fact that information flows in all directions among parameters sometimes leads to rather unintuitive conclusions. Hereâs an example from missing data imputation, in which imputation of a single datum reverses the direction of an inferred relationship. Use these data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(100)
x &amp;lt;- c(rnorm(10), NA)
y &amp;lt;- c(rnorm(10, x), 100)
d &amp;lt;- list(x = x, y = y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These data comprise 11 cases, one of which has a missing predictor value. You can quickly confirm that a regression of $y$ on $x$ for only the complete cases indicates a strong positive relationship between the two variables. But now fit this model, imputing the one missing value for $x$:&lt;/p&gt;
&lt;p&gt;$$y_i â¼ Normal(Âµ_i, Ï)$$ 
$$Âµ_i = Î± + Î²x_i$$ 
$$x_i â¼ Normal(0, 1)$$ 
$$Î± â¼ Normal(0, 100)$$ 
$$Î² â¼ Normal(0, 100)$$ 
$$Ï â¼ HalfCauchy(0, 1)$$&lt;/p&gt;
&lt;p&gt;What has happened to the posterior distribution of $Î²$? Be sure to inspect the full density. Can you explain the change in inference?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Interestingly, the &lt;code&gt;rethinking&lt;/code&gt; functions also work on basic &lt;code&gt;lm&lt;/code&gt; objects:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(lm(y ~ x, d))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                  mean        sd       5.5%     94.5%
## (Intercept) 0.2412995 0.2774524 -0.2021231 0.6847221
## x           1.4236779 0.5209135  0.5911574 2.2561983
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On to the imputation model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_H3 &amp;lt;- ulam(
  alist(
    y ~ dnorm(mu, sigma),
    mu &amp;lt;- a + b * x,
    x ~ dnorm(0, 1),
    c(a, b) ~ dnorm(0, 100),
    sigma ~ dexp(1)
  ),
  data = d, chains = 4, cores = 4, iter = 4000,
  control = list(adapt_delta = 0.99)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m_H3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mean        sd       5.5%     94.5%       n_eff    Rhat4
## b     -10.972553 19.300190 -27.944113 24.271345    2.066259 5.340370
## a       1.869767  3.346405  -3.361028  7.168704 4172.209841 1.003406
## sigma  10.294366  2.054437   7.383010 13.871396   75.915321 1.033777
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Well those percentile intervals look bad. The joint posterior distributions might help solve this mystery:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;pairs(m_H3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-07-statistical-rethinking-chapter-15_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We have a few bi-modal distributions which place the plausible values for &lt;code&gt;b&lt;/code&gt; and &lt;code&gt;x_impute&lt;/code&gt; either strongly in the negative or strongly in the positive realm. This feels like the issue of unidentifiable parameters all over again.&lt;/p&gt;
&lt;p&gt;The outcome variable value for which we are missing the predictor variable value is very extreme given the range of all other outcome variable values. This means, we can flip our predictor value to either extreme and still be consistent with the data and model thus forcing the regression line to be either positive or negative.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s extract positive and negative regression estimates and their positions in our extracted samples from the posterior:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;post &amp;lt;- extract.samples(m_H3)
post_pos &amp;lt;- post
post_neg &amp;lt;- post
for (i in 1:length(post)) {
  post_pos[[i]] &amp;lt;- post[[i]][post$b &amp;gt; 0]
  post_neg[[i]] &amp;lt;- post[[i]][post$b &amp;lt; 0]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this at hand, we can now compute the two regression lines and plot them:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;par(mfrow = c(1, 2))
## positive
x_seq &amp;lt;- seq(from = -2.6, to = 4, length.out = 30)
mu_link &amp;lt;- function(x, post) post$a + post$b * x
mu &amp;lt;- sapply(x_seq, mu_link, post = post_pos)
mu_mu &amp;lt;- apply(mu, 2, mean)
mu_PI &amp;lt;- apply(mu, 2, PI)
x_impute &amp;lt;- mean(post_pos$x_impute)
plot(y ~ x, d, pch = 16, col = rangi2, xlim = c(-0.85, x_impute))
points(x_impute, 100)
lines(x_seq, mu_mu)
shade(mu_PI, x_seq)
## negative
x_seq &amp;lt;- seq(from = -4, to = 4, length.out = 50)
mu &amp;lt;- sapply(x_seq, mu_link, post = post_neg)
mu_mu &amp;lt;- apply(mu, 2, mean)
mu_PI &amp;lt;- apply(mu, 2, PI)
x_impute &amp;lt;- mean(post_neg$x_impute)
plot(y ~ x, d, pch = 16, col = rangi2, xlim = c(-3.7, 0.9))
points(x_impute, 100)
lines(x_seq, mu_mu)
shade(mu_PI, x_seq)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-07-statistical-rethinking-chapter-15_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;1440&#34; /&gt;
This should make it obvious just how extreme the outcome variable value is and how our model could agree with either extreme imputed variable.&lt;/p&gt;
&lt;h3 id=&#34;practice-h4&#34;&gt;Practice H4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Some lad named Andrew made an eight-sided spinner. He wanted to know if it is fair. So he spun it a bunch of times, recording the counts of each value. Then he accidentally spilled coffee over the 4s and 5s. The surviving data are summarized below.&lt;/p&gt;
&lt;p&gt;| Value      |  1 |  2 |  3 |  4 |  5 |  6 |  7 |  8 |
| Frequency  | 18 | 19 | 22 |  ? |  ? | 19 | 20 | 22 |&lt;/p&gt;
&lt;p&gt;Your job is to impute the two missing values in the table above. Andrew doesnât remember how many times he spun the spinner. So you will have to assign a prior distribution for the total number of spins and then marginalize over the unknown total. Andrew is not sure the spinner is fair (every value is equally likely), but heâs confident that none of the values is twice as likely as any other. Use a Dirichlet distribution to capture this prior belief. Plot the joint posterior distribution of 4s and 5s.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; First, I enter the data into &lt;code&gt;R&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;y &amp;lt;- c(18, 19, 22, NA, NA, 19, 20, 22)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What data do I need to somehow get to for my model?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;N&lt;/code&gt; - total number of spins&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For &lt;code&gt;N&lt;/code&gt;, we can say that is no smaller than 120 - the sum of all spins which we have observed outcomes for. The number of spins would be a count variable and so it would make sense to assign a Poisson distribution to them - especially seeing how we lack a sensible upper bound to the total number of spins. So what should our expected value be? Well, from the data above, it would be sensible to expect that the spins for sides 4 and 5 are 20 respectively - this is just a guess. As such, we could set a prior as:&lt;/p&gt;
&lt;p&gt;$$N \sim Poisson(40) + 120$$
Why 40 and why 120? 40 is the expected number of missing spins from our data table, 120 defines the lower bound of our total spins. We have data for 120 spins.&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;code&gt;Probs&lt;/code&gt; - vector of probabilities for each side of the spinner&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As for the vector of probabilities, we want to use the Dirichlet prior as outlined by the exercise text. The Dirichlet prior is used for categorical outcomes like these. We know that none of the outcomes is twice as likely as any other. Dirichlet doesn&amp;rsquo;t give us that control directly, unfortunately. What we can do is simulate:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;p &amp;lt;- rdirichlet(1e3, alpha = rep(4, 8))
plot(NULL, xlim = c(1, 8), ylim = c(0, 0.3), xlab = &amp;quot;outcome&amp;quot;, ylab = &amp;quot;probability&amp;quot;)
for (i in 1:10) lines(1:8, p[i, ], type = &amp;quot;b&amp;quot;, col = grau(), lwd = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-07-statistical-rethinking-chapter-15_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;1440&#34; /&gt;
It is difficult to judge from this what our prior is assuming and whether our assumption is met. We can identify this numerically though:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;twicer &amp;lt;- function(p) {
  o &amp;lt;- order(p)
  if (p[o][8] / p[o][1] &amp;gt; 2) {
    return(TRUE)
  } else {
    return(FALSE)
  }
}
sum(apply(p, 1, twicer))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 977
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our prior clearly needs to be tighter since our criterion of no category being twice as likely as any other category is being violated quite heavily.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;p &amp;lt;- rdirichlet(1e3, alpha = rep(50, 8))
sum(apply(p, 1, twicer))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 17
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That looks much better! Let&amp;rsquo;s plot that:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(NULL, xlim = c(1, 8), ylim = c(0, 0.3), xlab = &amp;quot;outcome&amp;quot;, ylab = &amp;quot;probability&amp;quot;)
for (i in 1:10) lines(1:8, p[i, ], type = &amp;quot;b&amp;quot;, col = grau(), lwd = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-07-statistical-rethinking-chapter-15_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;code&gt;N4&lt;/code&gt; and &lt;code&gt;N5&lt;/code&gt; - the counts of observations of the side 4 and 5, respectively&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This is what we want to get to to help Andrew get around his coffee-spillage mishap. What we need to do here is to marginalize over all combinations of 4s and 5s. I will freely admit that I was completely lost here and took the STAN code directly from the solutions by Richard McElreath. Looking at it, there are some loops in here, which I couldn&amp;rsquo;t have been able to code myself (yet). I have added some comments to indciate what I understood:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;code15H7 &amp;lt;- &amp;quot;
data{
    int N;
    int y[N];
    int y_max; // consider at most this many spins for y4 and y5
    int S_mean;
}
parameters{
    simplex[N] p;   // probabilities of each outcome
}
model{
    vector[(1+y_max)*(1+y_max)] terms; // all combinations of spins for 4 and 5
    int k = 1; // counter to index above vector of combinations

    p ~ dirichlet(rep_vector(50, N)); // Dirichlet prior

    // loop over possible values for unknown cells 4 and 5
    // this code updates posterior of p
    for(y4 in 0:y_max){
        for(y5 in 0:y_max){
            int Y[N] = y;  // probability of complete vector of individual spins
            Y[4] = y4;  // spins for 4s
            Y[5] = y5; // spins for 5s
            terms[k] = poisson_lpmf(y4+y5|S_mean-120) + multinomial_lpmf(Y|p);  // poisson prior for individual spins and multinomial prior for vector of counts conditional on number of spins n and prior p
            k = k + 1;
        }//y5
    }//y4
    target += log_sum_exp(terms);
}
generated quantities{  // repeates much of the above to compute posterior probability
    matrix[y_max+1, y_max+1] P45; // prob y4, y5 takes joint values
    // now compute Prob(y4, y5|p)
   {
        matrix[(1+y_max), (1+y_max)] terms;
        int k = 1;
        real Z;
        for(y4 in 0:y_max){
            for(y5 in 0:y_max){
              int Y[N] = y;
              Y[4] = y4;
              Y[5] = y5;
              terms[y4+1, y5+1] = poisson_lpmf(y4+y5|S_mean-120) + multinomial_lpmf(Y|p);
            }//y5
        }//y4
        Z = log_sum_exp(to_vector(terms));
        for(y4 in 0:y_max)
            for(y5 in 0:y_max)
                P45[y4+1, y5+1] = exp(terms[y4+1, y5+1] - Z);  //  make sure all probabilities sum to 1
    }
}
&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here&amp;rsquo;s the data that the model needs. STAN doesn&amp;rsquo;t accept &lt;code&gt;NA&lt;/code&gt;s, hence why the &lt;code&gt;NA&lt;/code&gt; values below are now encoded as -1:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;y &amp;lt;- c(18, 19, 22, -1, -1, 19, 20, 22)
dat &amp;lt;- list(
  N = length(y),
  y = y,
  S_mean = 160,
  y_max = 40
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, let&amp;rsquo;s run the model and plot some samples from it:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m15H7 &amp;lt;- stan(model_code = code15H7, data = dat, chains = 4, cores = 4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;post &amp;lt;- extract.samples(m15H7)
y_max &amp;lt;- dat$y_max
plot(NULL,
  xlim = c(10, y_max - 10), ylim = c(10, y_max - 10),
  xlab = &amp;quot;number of 4s&amp;quot;, ylab = &amp;quot;number of 5s&amp;quot;
)
mtext(&amp;quot;posterior distribution of 4s and 5s&amp;quot;)
for (y4 in 0:y_max) {
  for (y5 in 0:y_max) {
    k &amp;lt;- grau(mean(post$P45[, y4 + 1, y5 + 1]) / 0.01)
    points(y4, y5, col = k, pch = 16, cex = 1.5)
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-07-statistical-rethinking-chapter-15_files/figure-html/final plot-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From this, it is apparent that 20 spins for the 4s and 5s respectively is the most likely and that there is a negative correlation between these respective spins - more spins resulting in side 4 make less spins resulting in side 5 more likely.&lt;/p&gt;
&lt;p&gt;Andrew - don&amp;rsquo;t spill your coffee again.&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.5 (2021-03-31)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19043)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.1252  LC_CTYPE=English_United Kingdom.1252    LC_MONETARY=English_United Kingdom.1252 LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.1252    
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] gtools_3.8.2         rethinking_2.13      rstan_2.21.2         ggplot2_3.3.6        StanHeaders_2.21.0-7
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.7         mvtnorm_1.1-1      lattice_0.20-41    prettyunits_1.1.1  ps_1.6.0           assertthat_0.2.1   digest_0.6.27      utf8_1.2.1         V8_3.4.1           R6_2.5.0          
## [11] backports_1.2.1    stats4_4.0.5       evaluate_0.14      coda_0.19-4        highr_0.9          blogdown_1.3       pillar_1.6.0       rlang_0.4.11       curl_4.3.2         callr_3.7.0       
## [21] jquerylib_0.1.4    R.utils_2.10.1     R.oo_1.24.0        rmarkdown_2.7      styler_1.4.1       stringr_1.4.0      loo_2.4.1          munsell_0.5.0      compiler_4.0.5     xfun_0.22         
## [31] pkgconfig_2.0.3    pkgbuild_1.2.0     shape_1.4.5        htmltools_0.5.1.1  tidyselect_1.1.0   tibble_3.1.1       gridExtra_2.3      bookdown_0.22      codetools_0.2-18   matrixStats_0.61.0
## [41] fansi_0.4.2        crayon_1.4.1       dplyr_1.0.5        withr_2.4.2        MASS_7.3-53.1      R.methodsS3_1.8.1  grid_4.0.5         jsonlite_1.7.2     gtable_0.3.0       lifecycle_1.0.0   
## [51] DBI_1.1.1          magrittr_2.0.1     scales_1.1.1       KernSmooth_2.23-18 RcppParallel_5.1.2 cli_3.0.0          stringi_1.5.3      bslib_0.2.4        ellipsis_0.3.2     generics_0.1.0    
## [61] vctrs_0.3.7        rematch2_2.1.2     tools_4.0.5        R.cache_0.14.0     glue_1.4.2         purrr_0.3.4        processx_3.5.1     yaml_2.2.1         inline_0.3.17      colorspace_2.0-0  
## [71] knitr_1.33         sass_0.3.1
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 16</title>
      <link>https://www.erikkusch.com/courses/rethinking/chapter-16/</link>
      <pubDate>Thu, 13 May 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/rethinking/chapter-16/</guid>
      <description>&lt;h1 id=&#34;generalised-linear-madness&#34;&gt;Generalised Linear Madness&lt;/h1&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/rethinking/19__14-05-2021_SUMMARY_-GLM.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides Chapter 16&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;These are answers and solutions to the exercises at the end of chapter 15 in 
&lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Satistical Rethinking 2&lt;/a&gt; by Richard McElreath. I have created these notes as a part of my ongoing involvement in the 
&lt;a href=&#34;https://www.erikkusch.com/project/aubayes/&#34;&gt;AU Bayes Study Group&lt;/a&gt;. Much of my inspiration for these solutions, where necessary, has been obtained from&lt;/p&gt;
&lt;!-- [Taras Svirskyi](https://github.com/jffist/statistical-rethinking-solutions/blob/master/ch10_hw.R), [William Wolf](https://github.com/cavaunpeu/statistical-rethinking/blob/master/chapter-10/homework.R), and [Corrie Bartelheimer](https://www.samples-of-thoughts.com/projects/statistical-rethinking/chapter_10/chp10-ex/) as well as  --&gt;
&lt;p&gt;the solutions provided to instructors by Richard McElreath himself.&lt;/p&gt;
&lt;h2 id=&#34;r-environment&#34;&gt;&lt;code&gt;R&lt;/code&gt; Environment&lt;/h2&gt;
&lt;p&gt;For today&amp;rsquo;s exercise, I load the following packages:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rethinking)
library(ggplot2)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;easy-exercises&#34;&gt;Easy Exercises&lt;/h2&gt;
&lt;p&gt;Unfortunately, the PDF version of 
&lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Satistical Rethinking 2&lt;/a&gt;, I am working with does not list any easy practice exercises for this chapter.&lt;/p&gt;
&lt;h2 id=&#34;medium-exercises&#34;&gt;Medium Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-m1&#34;&gt;Practice M1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Modify the cylinder height model, &lt;code&gt;m16.1&lt;/code&gt;, so that the exponent 3 on height is instead a free parameter. Do you recover the value of 3 or not? Plot the posterior predictions for the new model. How do they differ from those of &lt;code&gt;m16.1&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Before we move on, let me just remind all of us of the model itself:&lt;/p&gt;
&lt;p&gt;$W_i â¼ Log-Normal(Âµ_i, Ï)$ [Distribution for weight]&lt;br&gt;
$exp(Âµ_i) = kÏp^2h^3_i$ [expected median of weight]&lt;br&gt;
$k â¼ Beta(2, 18)$ [prior relation between weight and volume]&lt;br&gt;
$p â¼ Exponential(0.5)$ [prior proportionality of radius to height]&lt;br&gt;
$Ï â¼ Exponential(1)$ [our old friend, sigma]&lt;/p&gt;
&lt;p&gt;As for the exercise, I start by loading the data and rescaling the &lt;code&gt;weight&lt;/code&gt; and &lt;code&gt;height&lt;/code&gt; variables as was done in the chapter:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(&amp;quot;Howell1&amp;quot;)
d &amp;lt;- Howell1
d$w &amp;lt;- d$weight / mean(d$weight)
d$h &amp;lt;- d$height / mean(d$height)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that the data is prepared, we can run the model. Before we run the model that we are asked for, however, I want to run the model from the chapter for later comparison:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m16.1 &amp;lt;- ulam(
  alist(
    w ~ dlnorm(mu, sigma),
    exp(mu) &amp;lt;- 3.141593 * k * p^2 * h^3,
    p ~ beta(2, 18),
    k ~ exponential(0.5),
    sigma ~ exponential(1)
  ),
  data = d, chains = 4, cores = 4, log_lik = TRUE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I run this model as well as the subsequent one with &lt;code&gt;log_lik = TRUE&lt;/code&gt; to allow for model comparison with WAIC.&lt;/p&gt;
&lt;p&gt;To assign a free parameter to the exponent 3 of the chapter, I simply substitute the value 3 in the model code with a letter (&lt;code&gt;e&lt;/code&gt;) to indicate a parameter. I also have to define a prior (I reckon the exponent should definitely be positive) for this new parameter, of course:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_M1 &amp;lt;- ulam(
  alist(
    w ~ dlnorm(mu, sigma),
    exp(mu) &amp;lt;- 3.141593 * k * p^2 * h^e,
    p ~ beta(2, 18),
    k ~ exponential(0.5),
    sigma ~ exponential(1),
    e ~ exponential(1)
  ),
  data = d, chains = 4, cores = 4, log_lik = TRUE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m_M1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            mean          sd      5.5%      94.5%     n_eff    Rhat4
## p     0.2440124 0.056092318 0.1699713  0.3450251  692.9784 1.002353
## k     5.7390657 2.482731435 2.5008291 10.2474462  812.8841 1.003970
## sigma 0.1263219 0.003660893 0.1206056  0.1321993 1004.6100 1.000673
## e     2.3234522 0.022774994 2.2877691  2.3598172 1108.6018 1.003564
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the new model, we obtain an estimates of 2.32 for the exponent rather than the value of 3 that was assumed in the chapter.&lt;/p&gt;
&lt;p&gt;So let&amp;rsquo;s get started with model comparison:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;compare(m16.1, m_M1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            WAIC       SE    dWAIC      dSE    pWAIC        weight
## m_M1  -845.6597 36.86898   0.0000       NA 3.441582  1.000000e+00
## m16.1 -310.4014 44.39907 535.2583 54.80355 3.775032 5.890284e-117
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Seems like model comparison strongly favours our new model &lt;code&gt;m_M1&lt;/code&gt;. What brings this difference about? Let&amp;rsquo;s look at the posterior predictions for the answer:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Getting the data
h_seq &amp;lt;- seq(from = 0, to = max(d$h), length.out = nrow(d))
# m_M1
w_sim &amp;lt;- sim(m_M1, data = list(h = h_seq))
m1_mean &amp;lt;- apply(w_sim, 2, mean)
m1_CI &amp;lt;- apply(w_sim, 2, PI)
# m16.1
w_sim &amp;lt;- sim(m16.1, data = list(h = h_seq))
m16.1_mean &amp;lt;- apply(w_sim, 2, mean)
m16.1_CI &amp;lt;- apply(w_sim, 2, PI)
## Making a data frame for plotting
plot_df &amp;lt;- data.frame(
  seq = rep(h_seq, 2),
  mean = c(m1_mean, m16.1_mean),
  CI_l = c(m1_CI[1, ], m16.1_CI[1, ]),
  CI_U = c(m1_CI[2, ], m16.1_CI[2, ]),
  y = rep(d$w, 2),
  x = rep(d$h, 2),
  model = rep(c(&amp;quot;m_M1&amp;quot;, &amp;quot;m16.1&amp;quot;), each = length(h_seq))
)
## Plotting posterior
ggplot(plot_df, aes(x = x, y = y)) +
  geom_point(col = &amp;quot;blue&amp;quot;) +
  geom_line(aes(x = seq, y = mean)) +
  geom_ribbon(aes(x = seq, ymin = CI_l, ymax = CI_U), alpha = 0.2) +
  labs(x = &amp;quot;height (scaled)&amp;quot;, y = &amp;quot;weight (scaled)&amp;quot;) +
  facet_wrap(~model) +
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-13-statistical-rethinking-chapter-16_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Compared to the original model &lt;code&gt;m16.1&lt;/code&gt;, the new model &lt;code&gt;m_M1&lt;/code&gt; fits shorter individuals much better than the original model which comes at the detriment of fitting taller individuals correctly. Overall, the posterior uncertainty is tighter for our new model &lt;code&gt;m_M1&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;practice-m2&#34;&gt;Practice M2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Conduct a prior predictive simulation for the cylinder height model. Begin with the priors in the chapter. Do these produce reasonable prior height distributions? If not, which modifications do you suggest?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Remember our priors from the chapter:&lt;/p&gt;
&lt;p&gt;$$p â¼ Beta(2, 18)$$
$$k â¼ Exponential(0.5)$$
$$\sigma \sim Exponential(1)$$
Now let&amp;rsquo;s simulate priors for a number of &lt;code&gt;N&lt;/code&gt; cases:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;N &amp;lt;- 1e2
p &amp;lt;- rbeta(N, 2, 18) # p ~ Beta(2, 18)
k &amp;lt;- rexp(N, 0.5) # k ~ Exponential(0.5)
sigma &amp;lt;- rexp(N, 1)
prior &amp;lt;- list(p = p, k = k, sigma = sigma)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The priors are all compiled into one list, now all we have to do is run the prior predictive check:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Making a data frame for plotting
plot_df &amp;lt;- with(prior, 3.141593 * k[1] * p[1]^2 * d$h^3)
for (i in 2:N) {
  plot_df &amp;lt;- c(plot_df, with(prior, 3.141593 * k[i] * p[i]^2 * d$h^3))
}
plot_df &amp;lt;- data.frame(
  w = plot_df,
  seq = rep(d$h, N),
  prior = rep(1:N, each = nrow(d))
)
## Plotting
ggplot() +
  geom_point(data = d, aes(x = h, y = w), col = &amp;quot;blue&amp;quot;) +
  geom_line(data = plot_df, aes(x = seq, y = w, group = prior)) +
  labs(x = &amp;quot;height (scaled)&amp;quot;, y = &amp;quot;weight (scaled)&amp;quot;) +
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-13-statistical-rethinking-chapter-16_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The combination of these priors seems to be too flat - weight is not increasing fast enough with height. Either $p$ or $k$ need to be larger on average:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## New priors
p &amp;lt;- rbeta(N, 4, 18)
k &amp;lt;- rexp(N, 1 / 4)
sigma &amp;lt;- rexp(N, 1)
prior &amp;lt;- list(p = p, k = k, sigma = sigma)
## Making a data frame for plotting
plot_df &amp;lt;- with(prior, 3.141593 * k[1] * p[1]^2 * d$h^3)
for (i in 2:N) {
  plot_df &amp;lt;- c(plot_df, with(prior, 3.141593 * k[i] * p[i]^2 * d$h^3))
}
plot_df &amp;lt;- data.frame(
  w = plot_df,
  seq = rep(d$h, N),
  prior = rep(1:N, each = nrow(d))
)
## Plotting
ggplot() +
  geom_point(data = d, aes(x = h, y = w), col = &amp;quot;blue&amp;quot;) +
  geom_line(data = plot_df, aes(x = seq, y = w, group = prior)) +
  labs(x = &amp;quot;height (scaled)&amp;quot;, y = &amp;quot;weight (scaled)&amp;quot;) +
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-13-statistical-rethinking-chapter-16_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;1440&#34; /&gt;
There are some prior combinations here that are definitely way too extreme, but most priors still bunch up too much along the x-axis. Let&amp;rsquo;s alter the prior for $k$ (the density) some more by making it log-Normal:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## New priors
N &amp;lt;- 1e2
p &amp;lt;- rbeta(N, 4, 18)
k &amp;lt;- rlnorm(N, log(7), 0.2) # median of log(7)
sigma &amp;lt;- rexp(N, 1)
prior &amp;lt;- list(p = p, k = k, sigma = sigma)
## Making a data frame for plotting
plot_df &amp;lt;- with(prior, 3.141593 * k[1] * p[1]^2 * d$h^3)
for (i in 2:N) {
  plot_df &amp;lt;- c(plot_df, with(prior, 3.141593 * k[i] * p[i]^2 * d$h^3))
}
plot_df &amp;lt;- data.frame(
  w = plot_df,
  seq = rep(d$h, N),
  prior = rep(1:N, each = nrow(d))
)
## Plotting
ggplot() +
  geom_point(data = d, aes(x = h, y = w), col = &amp;quot;blue&amp;quot;) +
  geom_line(data = plot_df, aes(x = seq, y = w, group = prior)) +
  labs(x = &amp;quot;height (scaled)&amp;quot;, y = &amp;quot;weight (scaled)&amp;quot;) +
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-13-statistical-rethinking-chapter-16_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is much better! Remember - priors are not about fitting the data exactly but informing the model about plausibility.&lt;/p&gt;
&lt;h3 id=&#34;practice-m3&#34;&gt;Practice M3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Use prior predictive simulations to investigate the Lynx-hare model. Begin with the priors in the chapter. Which population dynamics do these produce? Can you suggest any improvements to the priors, on the basis of your simulations?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Again, let me remind us of the model in the chapter:&lt;/p&gt;
&lt;p&gt;$h_t â¼ LogNormal(log(p_HH_t), Ï_H)$ [Probability of observed hare pelts]&lt;br&gt;
$â_t â¼ LogNormal(log(p_LL_t), Ï_L)$ [Probability observed lynx pelts]&lt;br&gt;
$H_1 â¼ LogNormal(log(10), 1)$ [Prior for initial hare population]&lt;br&gt;
$L_1 â¼ LogNormal(log(10), 1)$ [Prior for initial lynx population]&lt;br&gt;
$H_{T&amp;gt;1} = H_1 + \int_1^TH_t(b_H âm_HL_t)d_t$ [Model for hare population]&lt;br&gt;
$L_{T&amp;gt;1} = L_1 + \int_1^T L_t(b_LH_t âm_L)d_t$ [Model for lynx population]&lt;br&gt;
$Ï_H â¼ Exponential(1)$ [Prior for measurement dispersion]&lt;br&gt;
$Ï_L â¼ Exponential(1)$ [Prior for measurement dispersion]&lt;br&gt;
$p_H â¼ Beta(Î±_H, Î²_H)$ [Prior for hare trap probability]&lt;br&gt;
$p_L â¼ Beta(Î±_L, Î²_L)$ [Prior for lynx trap probability]&lt;br&gt;
$b_H â¼ HalfNormal(1, 0.5)$ [Prior hare birth rate]&lt;br&gt;
$b_L â¼ HalfNormal(0.05, 0.05)$ [Prior lynx birth rate]&lt;br&gt;
$m_H â¼ HalfNormal(0.05, 0.05)$ [Prior hare mortality rate]&lt;br&gt;
$m_L â¼ HalfNormal(1, 0.5)$ [Prior lynx mortality rate]&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s get started on our exercise now by loading the data and preparing our priors. Here, we simply just draw theta parameters (these define halfnormal distributions) from normal distributions as defined above:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(Lynx_Hare)
N &amp;lt;- 12
theta &amp;lt;- matrix(NA, nrow = N, ncol = 4)
theta[, 1] &amp;lt;- rnorm(N, 1, 0.5) # b_H
theta[, 2] &amp;lt;- rnorm(N, 0.05, 0.05) # b_L
theta[, 3] &amp;lt;- rnorm(N, 1, 0.5) # m_L
theta[, 4] &amp;lt;- rnorm(N, 0.05, 0.05) # m_H
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now use these priors in combination with the &lt;code&gt;sim_lynx_hare()&lt;/code&gt; function from the book:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sim_lynx_hare &amp;lt;- function(n_steps, init, theta, dt = 0.002) {
  L &amp;lt;- rep(NA, n_steps)
  H &amp;lt;- rep(NA, n_steps)
  L[1] &amp;lt;- init[1]
  H[1] &amp;lt;- init[2]
  for (i in 2:n_steps) {
    H[i] &amp;lt;- H[i - 1] + dt * H[i - 1] * (theta[1] - theta[2] * L[i - 1])
    L[i] &amp;lt;- L[i - 1] + dt * L[i - 1] * (theta[3] * H[i - 1] - theta[4])
  }
  return(cbind(L, H))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the above function registered in our &lt;code&gt;R&lt;/code&gt; environment, we are ready to simulate with our priors and produce some plots:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Simulate for first prior
plot_df &amp;lt;- sim_lynx_hare(1e4, as.numeric(Lynx_Hare[1, 2:3]), theta[1, ])
plot_df &amp;lt;- data.frame(plot_df)
plot_df$prior &amp;lt;- rep(1, 1e4)
## simulate for all other priors
for (i in 2:N) {
  z &amp;lt;- sim_lynx_hare(1e4, as.numeric(Lynx_Hare[1, 2:3]), theta[i, ])
  z &amp;lt;- data.frame(z)
  z$prior &amp;lt;- rep(i, 1e4)
  plot_df &amp;lt;- rbind(plot_df, z)
}
plot_df$seq &amp;lt;- rep(1:1e4, N)
## Plotting
ggplot(plot_df, aes(x = seq)) +
  geom_line(aes(y = L), col = &amp;quot;brown&amp;quot;) +
  geom_line(aes(y = H), col = &amp;quot;blue&amp;quot;) +
  facet_wrap(~prior, scales = &amp;quot;free&amp;quot;) +
  theme_bw() +
  labs(x = &amp;quot;Time&amp;quot;, y = &amp;quot;Population&amp;quot;) +
  theme(axis.text.y = element_blank())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-13-statistical-rethinking-chapter-16_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Hare population estimates are shown in blue while Lynx population estimates are portrayed in brown. Nevermind that, however, these priors are clearly not good as far as building biological plausibility into our model. Why? There is no cycling in the blue trends depending on the brown trends (lynx eat hares and are thus coupled to them). In addition to that, although I have hidden the actual population estimates, I think it is evident from these plots that some of the prior estimates are just outlandish in terms of population size.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s see if we can do better. How would we do that? By making our priors more informative, of course. We should probably take this step-by-step:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Hare birth rate&lt;/em&gt; - $b_H$:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I want to make this more conservative by lowering the expected birth rate of hares. To do so, the theta parameter for my halfnormal distribution will now be drawn from $Normal(0.5, 0.1)$ as opposed to the previous $Normal(1, 0.5)$.&lt;/p&gt;
&lt;p&gt;$$b_H â¼ HalfNormal(0.5, 0.1)$$&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;em&gt;Lynx birth rate&lt;/em&gt; - $b_L$:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This one, I keep as it was previously. I strongly suspect that the base birth rate of lynx should be much smaller than that of hares. The new prior reflects that:&lt;/p&gt;
&lt;p&gt;$$b_L â¼ HalfNormal(0.05, 0.05)$$&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;em&gt;Lynx mortality rate&lt;/em&gt; - $m_L$:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I want to drastically decrease the estimated lynx mortality rate since lynx don&amp;rsquo;t die as much as hares do (longer life, no predators, etc.):&lt;/p&gt;
&lt;p&gt;$$m_H â¼ HalfNormal(0.025, 0.05)$$&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;&lt;em&gt;Hare mortality rate&lt;/em&gt; - $m_H$:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I increase the mortality rate of hares to reflect that they die much more frequently than lynx do for the aforementioned reasons:&lt;/p&gt;
&lt;p&gt;$$m_L â¼ HalfNormal(0.5, 0.1)$$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s simulate with these priors&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## New priors
N &amp;lt;- 12
theta &amp;lt;- matrix(NA, nrow = N, ncol = 4)
theta[, 1] &amp;lt;- rnorm(N, 0.5, 0.1) # b_H
theta[, 2] &amp;lt;- rnorm(N, 0.05, 0.05) # b_L
theta[, 3] &amp;lt;- rnorm(N, 0.025, 0.05) # m_L
theta[, 4] &amp;lt;- rnorm(N, 0.5, 0.1) # m_H
## Simulate for first prior
plot_df &amp;lt;- sim_lynx_hare(1e4, as.numeric(Lynx_Hare[1, 2:3]), theta[1, ])
plot_df &amp;lt;- data.frame(plot_df)
plot_df$prior &amp;lt;- rep(1, 1e4)
## simulate for all other priors
for (i in 2:N) {
  z &amp;lt;- sim_lynx_hare(1e4, as.numeric(Lynx_Hare[1, 2:3]), theta[i, ])
  z &amp;lt;- data.frame(z)
  z$prior &amp;lt;- rep(i, 1e4)
  plot_df &amp;lt;- rbind(plot_df, z)
}
plot_df$seq &amp;lt;- rep(1:1e4, N)
## Plotting
ggplot(plot_df, aes(x = seq)) +
  geom_line(aes(y = L), col = &amp;quot;brown&amp;quot;) +
  geom_line(aes(y = H), col = &amp;quot;blue&amp;quot;) +
  facet_wrap(~prior, scales = &amp;quot;free&amp;quot;) +
  theme_bw() +
  labs(x = &amp;quot;Time&amp;quot;, y = &amp;quot;Population&amp;quot;) +
  theme(axis.text.y = element_blank())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-13-statistical-rethinking-chapter-16_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Still, there are some populations here that experience explosive growth, but at least we now have properly cycling population trends for both species!&lt;/p&gt;
&lt;h2 id=&#34;hard-exercises&#34;&gt;Hard Exercises&lt;/h2&gt;
&lt;h3 id=&#34;practice-h1&#34;&gt;Practice H1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Modify the Panda nut opening model so that male and female chimpanzees have different maximum adult body mass. The &lt;code&gt;sex&lt;/code&gt; variable in &lt;code&gt;data(Panda_nuts)&lt;/code&gt; provides the information you need. Be sure to incorporate the fact that you know, prior to seeing the data, that males are on average larger than females at maturity.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Once more, let me include the model specification from the chapter:&lt;/p&gt;
&lt;p&gt;$$n_i â¼ Poisson(Î»_i)$$ 
$$Î»_i = d_iÏ(1 â exp(âkt_i))^Î¸$$
$$Ï â¼ LogNormal(log(1), 0.1)$$ 
$$k â¼ LogNormal(log(2), 0.25)$$ 
$$Î¸ â¼ LogNormal(log(5), 0.25)$$&lt;/p&gt;
&lt;p&gt;Once more, we move on to loading the data as was done in the chapter and creating an index variable for male individuals:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(Panda_nuts)
dat_list &amp;lt;- list(
  n = as.integer(Panda_nuts$nuts_opened),
  age = Panda_nuts$age / max(Panda_nuts$age),
  seconds = Panda_nuts$seconds
)
dat_list$male_id &amp;lt;- ifelse(Panda_nuts$sex == &amp;quot;m&amp;quot;, 1L, 0L)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We need to alter the model above to allow for the effect of sex to take hold. How do we go about this? Well, the exercise states that the effect of sex is supposed to come about through the effect of body mass which, in turn, is included in the model through $\phi$ which handles the conversion of body mass into strength. We can add the effect of sex to the model as such:&lt;/p&gt;
&lt;p&gt;$$n_i â¼ Poisson(Î»_i)$$ 
$$Î»_i = d_i&lt;em&gt;p_mS&lt;/em&gt;Ï(1 â exp(âkt_i))^Î¸$$
$$p_m â¼ Exponential(2)$$ 
$$Ï â¼ LogNormal(log(1), 0.1)$$ 
$$k â¼ LogNormal(log(2), 0.25)$$ 
$$Î¸ â¼ LogNormal(log(5), 0.25)$$&lt;/p&gt;
&lt;p&gt;where $S$ stands for the maleness indicator we built above:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_H1 &amp;lt;- ulam(
  alist(
    n ~ poisson(lambda),
    lambda &amp;lt;- seconds * (1 + pm * male_id) * phi * (1 - exp(-k * age))^theta, # 1+ addedd for baseline of effect of sex
    phi ~ lognormal(log(1), 0.1),
    pm ~ exponential(2),
    k ~ lognormal(log(2), 0.25),
    theta ~ lognormal(log(5), 0.25)
  ),
  data = dat_list, chains = 4, cores = 4
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m_H1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            mean         sd      5.5%      94.5%    n_eff     Rhat4
## phi   0.5996689 0.04786004 0.5284066  0.6779957 976.2037 1.0013342
## pm    0.6681319 0.13966484 0.4576800  0.9047816 991.7476 0.9993602
## k     5.1615999 0.66777159 4.0568926  6.2455157 743.6989 1.0060706
## theta 7.5940540 1.82343038 4.9655187 10.9163027 819.2285 1.0040457
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Due to how we built our model, the interpretation of $p_m$ is as follows: &amp;ldquo;Males are 0.67 times stronger than their female counterparts at maximum.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;How does this look when we plot it? I use the plotting scheme outlined by Richard McElreath in the chapter and modified in his solutions:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;post &amp;lt;- extract.samples(m_H1)
plot(NULL, xlim = c(0, 1), ylim = c(0, 1.5), xlab = &amp;quot;age&amp;quot;, ylab = &amp;quot;nuts per second&amp;quot;, xaxt = &amp;quot;n&amp;quot;)
at &amp;lt;- c(0, 0.25, 0.5, 0.75, 1, 1.25, 1.5)
axis(1, at = at, labels = round(at * max(Panda_nuts$age)))
pts &amp;lt;- dat_list$n / dat_list$seconds
point_size &amp;lt;- normalize(dat_list$seconds)
points(jitter(dat_list$age), pts,
  lwd = 2, cex = point_size * 3,
  col = ifelse(dat_list$male_id == 1, &amp;quot;black&amp;quot;, &amp;quot;red&amp;quot;)
)
# 10 female curves
for (i in 1:10) {
  with(
    post,
    curve(phi[i] * (1 - exp(-k[i] * x))^theta[i], add = TRUE, col = &amp;quot;red&amp;quot;)
  )
}
# 10 male curves
for (i in 1:10) {
  with(
    post,
    curve((1 + pm[i]) * phi[i] * (1 - exp(-k[i] * x))^theta[i], add = TRUE, col = grau())
  )
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-13-statistical-rethinking-chapter-16_files/figure-html/unnamed-chunk-25-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There is clearly quite the difference between males (black) and females (red) here. Males benefit more from age in opening nuts most likely due to their higher strength at maximum body mass. It is also worth pointing out that females have not been observed often or for long in this study as is apparent by the few, small circles in red.&lt;/p&gt;
&lt;h3 id=&#34;practice-h2&#34;&gt;Practice H2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Now return to the Panda nut model and try to incorporate individual differences. There are two parameters, $Ï$ and $k$, which plausibly vary by individual. Pick one of these, allow it to vary by individual, and use partial pooling to avoid overfitting. The variable chimpanzee in &lt;code&gt;data(Panda_nuts)&lt;/code&gt; tells you which observations belong to which individuals.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; This works off of the same model as we just used:&lt;/p&gt;
&lt;p&gt;$$n_i â¼ Poisson(Î»_i)$$ 
$$Î»_i = d_iÏ(1 â exp(âkt_i))^Î¸$$
$$Ï â¼ LogNormal(log(1), 0.1)$$ 
$$k â¼ LogNormal(log(2), 0.25)$$ 
$$Î¸ â¼ LogNormal(log(5), 0.25)$$&lt;/p&gt;
&lt;p&gt;To incorporate individual effects here, we need to add our data about individuals into our data list:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dat_list$id &amp;lt;- Panda_nuts$chimpanzee
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To incorporate this ID variable into our model, we want to create a different mass-strength conversion parameter $\phi$ for each individual. Importantly, the average expected rate of opened nuts ($\lambda$) has to stay positive for each individual - otherwise, Poisson will fail us. For this reason, we will want to use a distribution for our individual, varying intercepts that is constrained to be positive. Here, I settle on the exponential. Consequently, I envision to alter the model like this:&lt;/p&gt;
&lt;p&gt;$$n_i â¼ Poisson(Î»_i)$$ 
$$Î»_i = d_i*(Ïz_{ID}*\tau)*(1 â exp(âkt_i))^Î¸$$
$$z_{ID} ~ Exponential(1)$$
$$\tau ~ Exponential(1)$$
$$Ï â¼ LogNormal(log(1), 0.1)$$ 
$$k â¼ LogNormal(log(2), 0.25)$$ 
$$Î¸ â¼ LogNormal(log(5), 0.25)$$&lt;/p&gt;
&lt;p&gt;Given our linear model and our constraint for positive values of $z_{ID}$ with a mean of 1, each value of $z_{ID}$ is a multiplicative effect with $\phi$. Due to the mean of 1, we expect on average no effect of individuals. The data may tell us otherwise.&lt;/p&gt;
&lt;p&gt;The model below bears two more important oddities:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It is parametrised in a &lt;strong&gt;non-centred&lt;/strong&gt; form to allow for more effective sampling of the posterior.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;gq&amp;gt;&lt;/code&gt; part rescales our non-centred estimates of &lt;code&gt;z&lt;/code&gt; and &lt;code&gt;tau&lt;/code&gt; back to our scale of origin for better interpretation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s run this model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_H2 &amp;lt;- ulam(
  alist(
    n ~ poisson(lambda),
    lambda &amp;lt;- seconds * (phi * z[id] * tau) * (1 - exp(-k * age))^theta,
    phi ~ lognormal(log(1), 0.1),
    z[id] ~ exponential(1),
    tau ~ exponential(1),
    k ~ lognormal(log(2), 0.25),
    theta ~ lognormal(log(5), 0.25),
    gq &amp;gt; vector[id]:zz &amp;lt;&amp;lt;- z * tau # rescaled
  ),
  data = dat_list, chains = 4, cores = 4,
  control = list(adapt_delta = 0.99), iter = 4000
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m_H2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            mean        sd      5.5%    94.5%    n_eff     Rhat4
## phi   1.0013518 0.1004075 0.8481803 1.169319 8877.555 0.9998477
## tau   0.6519607 0.2101021 0.3852493 1.030786 1702.305 1.0006857
## k     3.0816213 0.7406724 2.0076499 4.342410 3954.259 1.0007966
## theta 3.1730030 0.6639074 2.2740025 4.341821 5314.860 1.0009539
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;tau&lt;/code&gt; tells us whether there are individual differences or not and it firmly identifies that there are some. To understand these effects, it is easiest to use our rescaled estimates stored in &lt;code&gt;zz&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(precis(m_H2, 2, pars = &amp;quot;zz&amp;quot;))
abline(v = 1, lty = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-13-statistical-rethinking-chapter-16_files/figure-html/unnamed-chunk-31-1.png&#34; width=&#34;1440&#34; /&gt;
Above, we see the proportion of $\phi$ for each individual. Average values are found at 1. Values above 1 indicate stronger-than-average individuals.&lt;/p&gt;
&lt;h3 id=&#34;practice-h3&#34;&gt;Practice H3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  The chapter asserts that a typical, geocentric time series model might be one that uses lag variables. Here youâll fit such a model and compare it to ODE model in the chapter. An autoregressive time series uses earlier values of the state variables to predict new values of the same variables. These earlier values are called &lt;em&gt;lag variables&lt;/em&gt;. You can construct the lag variables here with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(Lynx_Hare)
dat_ar1 &amp;lt;- list(
  L = Lynx_Hare$Lynx[2:21],
  L_lag1 = Lynx_Hare$Lynx[1:20],
  H = Lynx_Hare$Hare[2:21],
  H_lag1 = Lynx_Hare$Hare[1:20]
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you can use &lt;code&gt;L_lag1&lt;/code&gt; and &lt;code&gt;H_lag1&lt;/code&gt; as predictors of the outcomes &lt;code&gt;L&lt;/code&gt; and &lt;code&gt;H&lt;/code&gt;. Like this:&lt;/p&gt;
&lt;p&gt;$$L_t â¼ LogNormal(log (Âµ_{L,t}), Ï_L)$$
$$Âµ_{L,t} = Î±_L + Î²_{LL}L_{tâ1} + Î²_{LH}H_{tâ1}$$ 
$$H_t â¼ LogNormal(log(Âµ_{H,t}), Ï_H)$$ 
$$Âµ_{H,t} = Î±_H + Î²_{HH}H_{tâ1} + Î²_{HL}L_{tâ1}$$&lt;/p&gt;
&lt;p&gt;where $L_{tâ1}$ and $H_{tâ1}$ are the lag variables. Use &lt;code&gt;ulam()&lt;/code&gt; to fit this model. Be careful of the priors of the $Î±$ and $Î²$ parameters. Compare the posterior predictions of the autoregressive model to the ODE model in the chapter. How do the predictions differ? Can you explain why, using the structures of the models?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Let&amp;rsquo;s quickly complete the model above in mathematical notation before we code it:&lt;/p&gt;
&lt;p&gt;$$H_t â¼ LogNormal(log(Âµ_{H,t}), Ï_H)$$ 
$$L_t â¼ LogNormal(log (Âµ_{L,t}), Ï_L)$$
$$Âµ_{H,t} = Î±_H + Î²_{HH}H_{tâ1} + Î²_{HL}L_{tâ1}$$
$$Âµ_{L,t} = Î±_L + Î²_{LL}L_{tâ1} + Î²_{LH}H_{tâ1}$$&lt;/p&gt;
&lt;p&gt;$$H_{T&amp;gt;1} = H_1 + \int_1^TH_t(b_H âm_HL_t)d_t$$
$$L_{T&amp;gt;1} = L_1 + \int_1^T L_t(b_LH_t âm_L)d_t$$&lt;/p&gt;
&lt;p&gt;Now on to the priors:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Mean population size of hares&lt;/em&gt; - $\alpha_H$. I don&amp;rsquo;t have any strong idea about this one except for the fact that it has to be positive:&lt;br&gt;
$$\alpha_H â¼ Exponential(1)$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Mean population size of lynx&lt;/em&gt; - $\alpha_L$. Same as above - must be positive:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$\alpha_L â¼ Exponential(1)$$&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;em&gt;Effect of hares on hares through lag&lt;/em&gt; - $\beta_{HH}$. This one is probably rather positive than negative, but negative values are thinkable:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$\beta_{HH} \sim Normal(1, 0.5)$$&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;&lt;em&gt;Effect of lynx on hares&lt;/em&gt; - $\beta_{HL}$. Lynx eat hares, therefore I assume lynx have a negative effect on hare populations:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$\beta_{HL} \sim Normal(-1, 0.5)$$&lt;/p&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;&lt;em&gt;Effect of lynx on lynx through lag&lt;/em&gt; - $\beta_{LL}$. This one is probably rather positive than negative, but negative values are thinkable:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$\beta_{LL} \sim Normal(1, 0.5)$$&lt;/p&gt;
&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;&lt;em&gt;Effect of hares on lynx&lt;/em&gt; - $\beta_{HL}$. Lynx eat hares, therefore I assume lynx populations grow when hares are abundant:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$\beta_{LH} \sim Normal(1, 0.5)$$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s put this all into effect in a model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_H3_A &amp;lt;- ulam(
  alist(
    H ~ lognormal(log(muh), sigmah),
    L ~ lognormal(log(mul), sigmal),
    muh &amp;lt;- ah + b_hh * H_lag1 + b_hl * L_lag1,
    mul &amp;lt;- al + b_ll * L_lag1 + b_lh * H_lag1,
    c(ah, al) ~ normal(0, 1),
    b_hh ~ normal(1, 0.5),
    b_hl ~ normal(-1, 0.5),
    b_ll ~ normal(1, 0.5),
    b_lh ~ normal(1, 0.5),
    c(sigmah, sigmal) ~ exponential(1)
  ),
  data = dat_ar1, chains = 4, cores = 4
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m_H3_A)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              mean         sd       5.5%       94.5%     n_eff     Rhat4
## al     -0.9086472 0.92283343 -2.3478379  0.58408534 1766.8222 0.9994495
## ah      0.8030823 1.01577410 -0.7686795  2.39673796 1543.2587 1.0012806
## b_hh    1.1575260 0.15494168  0.9220842  1.41288452 1165.4372 0.9993601
## b_hl   -0.1898141 0.10424140 -0.3427948 -0.01818273  955.5289 1.0004735
## b_ll    0.5386323 0.09142832  0.3991584  0.68642887 1275.2896 1.0007976
## b_lh    0.2555668 0.05003324  0.1773263  0.33463902 1033.0136 1.0021123
## sigmal  0.3102309 0.06066444  0.2289059  0.42422181 1117.1826 1.0024006
## sigmah  0.4482276 0.08050156  0.3354154  0.58483481 1407.6199 1.0005659
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Turns out, the data agree with my prior intuition here. The implied time-series looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;post &amp;lt;- extract.samples(m_H3_A)
plot(dat_ar1$H, pch = 16, xlab = &amp;quot;Year&amp;quot;, ylab = &amp;quot;pelts (thousands)&amp;quot;, ylim = c(0, 100))
points(dat_ar1$L, pch = 16, col = rangi2)
mu &amp;lt;- link(m_H3_A)
for (s in 1:21) {
  lines(1:20, mu$muh[s, ], col = col.alpha(&amp;quot;black&amp;quot;, 0.2), lwd = 2) # hares
  lines(1:20, mu$mul[s, ], col = col.alpha(rangi2, 0.4), lwd = 2) # lynx
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-13-statistical-rethinking-chapter-16_files/figure-html/unnamed-chunk-37-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Lynx are portrayed in blue while hares are shown in black. The model in the chapter clearly does a better job at replicating these time-series, particularly that of lynx. Why is that? For starters, our model fails to appreciate measurement error on reported population sizes. Secondly, the effects are modelled as linear when we know them not to be.&lt;/p&gt;
&lt;p&gt;What about a lagged interaction model to resolve the issue of linear effects? Let&amp;rsquo;s try it by modelling as follows:&lt;/p&gt;
&lt;p&gt;$$Âµ_{H,t} = Î±_H + Î²_{HH}H_{tâ1} + Î²_{HL}L_{tâ1}H_{tâ1}$$
$$Âµ_{L,t} = Î±_L + Î²_{LL}L_{tâ1} + Î²_{LH}H_{tâ1}L_{tâ1}$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_H3_B &amp;lt;- ulam(
  alist(
    H ~ lognormal(log(muh), sigmah),
    L ~ lognormal(log(mul), sigmal),
    muh &amp;lt;- ah + b_hh * H_lag1 + b_hl * L_lag1 * H_lag1, # interaction here
    mul &amp;lt;- al + b_ll * L_lag1 + b_lh * H_lag1 * L_lag1, # interaction here
    c(ah, al) ~ normal(0, 1),
    b_hh ~ normal(1, 0.5),
    b_hl ~ normal(-1, 0.5),
    b_ll ~ normal(1, 0.5),
    b_lh ~ normal(1, 0.5),
    c(sigmah, sigmal) ~ exponential(1)
  ),
  data = dat_ar1, chains = 4, cores = 4
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;post &amp;lt;- extract.samples(m_H3_B)
plot(dat_ar1$H, pch = 16, xlab = &amp;quot;Year&amp;quot;, ylab = &amp;quot;pelts (thousands)&amp;quot;, ylim = c(0, 100))
points(dat_ar1$L, pch = 16, col = rangi2)
mu &amp;lt;- link(m_H3_B)
for (s in 1:21) {
  lines(1:20, mu$muh[s, ], col = col.alpha(&amp;quot;black&amp;quot;, 0.2), lwd = 2) # hares
  lines(1:20, mu$mul[s, ], col = col.alpha(rangi2, 0.4), lwd = 2) # lynx
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-13-statistical-rethinking-chapter-16_files/figure-html/unnamed-chunk-40-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s already a lot better, but our lines now overshoot the peaks of lynx populations. I will leave it at that for this exercise although this model is far from perfect. The better model is in the book.&lt;/p&gt;
&lt;h3 id=&#34;practice-h4&#34;&gt;Practice H4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Adapt the autoregressive model to use a two-step lag variable. This means that $L_{tâ2}$ and $H_{tâ2}$, in addition to $L_{tâ1}$ and $H_{tâ1}$, will appear in the equation for $Âµ$. This implies that prediction depends upon not only what happened just before now, but also on what happened two time steps ago. How does this model perform, compared to the ODE model?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; Let&amp;rsquo;s prepare the data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dat_ar2 &amp;lt;- list(
  L = Lynx_Hare$Lynx[3:21],
  L_lag1 = Lynx_Hare$Lynx[2:20],
  L_lag2 = Lynx_Hare$Lynx[1:19],
  H = Lynx_Hare$Hare[3:21],
  H_lag1 = Lynx_Hare$Hare[2:20],
  H_lag2 = Lynx_Hare$Hare[1:19]
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Starting off with the basic, linear model we used above:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_H4_A &amp;lt;- ulam(
  alist(
    H ~ lognormal(log(muh), sigmah),
    L ~ lognormal(log(mul), sigmal),
    muh &amp;lt;- ah + phi_hh * H_lag1 + phi_hl * L_lag1 +
      phi2_hh * H_lag2 + phi2_hl * L_lag2,
    mul &amp;lt;- al + phi_ll * L_lag1 + phi_lh * H_lag1 +
      phi2_ll * L_lag2 + phi2_lh * H_lag2,
    c(ah, al) ~ normal(0, 1),
    phi_hh ~ normal(1, 0.5),
    phi_hl ~ normal(-1, 0.5),
    phi_ll ~ normal(1, 0.5),
    phi_lh ~ normal(1, 0.5),
    phi2_hh ~ normal(0, 0.5),
    phi2_hl ~ normal(0, 0.5),
    phi2_ll ~ normal(0, 0.5),
    phi2_lh ~ normal(0, 0.5),
    c(sigmah, sigmal) ~ exponential(1)
  ),
  data = dat_ar2, chains = 4, cores = 4
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m_H4_A)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               mean         sd       5.5%       94.5%     n_eff     Rhat4
## al      -0.4183636 0.91246723 -1.8486107  1.05960250 1606.2951 0.9992228
## ah       0.3719553 0.99475941 -1.1590825  1.95140989 1990.4075 1.0009188
## phi_hh   1.0099326 0.19248834  0.7119274  1.32914392 1045.2694 1.0034079
## phi_hl  -0.7399365 0.32827054 -1.2783735 -0.20488913  877.9196 1.0017976
## phi_ll   0.9271154 0.24347415  0.5527803  1.31868956  939.7483 1.0037273
## phi_lh   0.3897833 0.13124341  0.1862378  0.60413394  950.0423 1.0039032
## phi2_hh  0.1847126 0.27004715 -0.2318049  0.62006465  917.6192 1.0019516
## phi2_hl  0.3975250 0.15804542  0.1439605  0.64872790  998.1928 1.0016893
## phi2_ll -0.1939459 0.10830081 -0.3702380 -0.02048142 1180.9072 1.0017758
## phi2_lh -0.2402054 0.20086962 -0.5597479  0.06914398  876.4958 1.0041831
## sigmal   0.3020739 0.06006007  0.2193412  0.40635405 1238.3004 0.9997003
## sigmah   0.3949292 0.07801180  0.2906307  0.53267445 1440.4280 1.0019419
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All of these make sense still. As does the implied time-series:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(dat_ar2$H, pch = 16, xlab = &amp;quot;Year&amp;quot;, ylab = &amp;quot;pelts (thousands)&amp;quot;, ylim = c(0, 100))
points(dat_ar2$L, pch = 16, col = rangi2)
mu &amp;lt;- link(m_H4_A)
for (s in 1:21) {
  lines(1:19, mu$muh[s, ], col = col.alpha(&amp;quot;black&amp;quot;, 0.2), lwd = 2)
  lines(1:19, mu$mul[s, ], col = col.alpha(rangi2, 0.4), lwd = 2)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-13-statistical-rethinking-chapter-16_files/figure-html/unnamed-chunk-46-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This time-series hasn&amp;rsquo;t benefited much from including the second-order time-lag.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s try the interaction effect model with two lags:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_H4_B &amp;lt;- ulam(
  alist(
    H ~ lognormal(log(muh), sigmah),
    L ~ lognormal(log(mul), sigmal),
    muh &amp;lt;- ah + phi_hh * H_lag1 + phi_hl * L_lag1 * H_lag1 +
      phi2_hh * H_lag2 + phi2_hl * L_lag2 * H_lag2,
    mul &amp;lt;- al + phi_ll * L_lag1 + phi_lh * H_lag1 * L_lag1 +
      phi2_ll * L_lag2 + phi2_lh * H_lag2 * L_lag2,
    c(ah, al) ~ normal(0, 1),
    phi_hh ~ normal(1, 0.5),
    phi_hl ~ normal(-1, 0.5),
    phi_ll ~ normal(1, 0.5),
    phi_lh ~ normal(1, 0.5),
    phi2_hh ~ normal(0, 0.5),
    phi2_hl ~ normal(0, 0.5),
    phi2_ll ~ normal(0, 0.5),
    phi2_lh ~ normal(0, 0.5),
    c(sigmah, sigmal) ~ exponential(1)
  ),
  data = dat_ar2, chains = 4, cores = 4
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(dat_ar2$H, pch = 16, xlab = &amp;quot;Year&amp;quot;, ylab = &amp;quot;pelts (thousands)&amp;quot;, ylim = c(0, 100))
points(dat_ar2$L, pch = 16, col = rangi2)
mu &amp;lt;- link(m_H4_B)
for (s in 1:21) {
  lines(1:19, mu$muh[s, ], col = col.alpha(&amp;quot;black&amp;quot;, 0.2), lwd = 2)
  lines(1:19, mu$mul[s, ], col = col.alpha(rangi2, 0.4), lwd = 2)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-13-statistical-rethinking-chapter-16_files/figure-html/unnamed-chunk-50-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This time-series also didn&amp;rsquo;t gain anything from adding second-order lag effects, I&amp;rsquo;m afraid.&lt;/p&gt;
&lt;p&gt;I reckon this exercise was designed to highlight that higher-order lag effects don&amp;rsquo;t have any causal meaning.&lt;/p&gt;
&lt;h3 id=&#34;practice-h5&#34;&gt;Practice H5&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Population dynamic models are typically very difficult to fit to empirical data. The Lynx-hare example in the chapter was easy, partly because the data are unusually simple and partly because the chapter did the difficult prior selection for you. Hereâs another data set that will impress upon you both how hard the task can be and how badly Lotka-Volterra fits empirical data in general. The data in &lt;code&gt;data(Mites)&lt;/code&gt; are numbers of predator and prey mites living on fruit. Model these data using the same Lotka-Volterra ODE system from the chapter. These data are actual counts of individuals, not just their pelts. You will need to adapt the Stan code in &lt;code&gt;data(Lynx_Hare_model)&lt;/code&gt;. Note that the priors will need to be rescaled, because the outcome variables are on a different scale. Prior predictive simulation will help. Keep in mind as well that the time variable and the birth and death parameters go together. If you rescale the time dimension, that implies you must also rescale the parameters.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; We have not worked with this data set before and so bet practise would have us load and plot it:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(Mites)
plot(Mites$day, Mites$prey)
points(Mites$day, Mites$predator, pch = 16)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-13-statistical-rethinking-chapter-16_files/figure-html/unnamed-chunk-52-1.png&#34; width=&#34;1440&#34; /&gt;
Open circles show prey. Closed circles show predators. One could definitely argue that there are cycles here.&lt;/p&gt;
&lt;p&gt;Luckily, so the solutions by McElreath tell me, there is no measurement error here. Thank the heavens!&lt;/p&gt;
&lt;p&gt;For prior predictive checks of our upcoming model and its priors we will want to repurpose the simulation function from the chapter that I used above:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sim_mites &amp;lt;- function(n_steps, init, theta, dt = 0.002) {
  L &amp;lt;- rep(NA, n_steps)
  H &amp;lt;- rep(NA, n_steps)
  L[1] &amp;lt;- init[1]
  H[1] &amp;lt;- init[2]
  for (i in 2:n_steps) {
    L[i] &amp;lt;- L[i - 1] + dt * L[i - 1] * (theta[3] * H[i - 1] - theta[4])
    H[i] &amp;lt;- H[i - 1] + dt * H[i - 1] * (theta[1] - theta[2] * L[i - 1])
  }
  return(cbind(L, H))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we need to define some priors for:  (1) prey birth rate &lt;code&gt;theta[1]&lt;/code&gt;, (2) prey mortality rate &lt;code&gt;theta[2]&lt;/code&gt;, (3) predator mortality rate &lt;code&gt;theta[3]&lt;/code&gt;, and (4) predator birth rate &lt;code&gt;theta[4]&lt;/code&gt;. Unfortunately, I lack a good understanding of mites and their prey to build intuitive priors.&lt;/p&gt;
&lt;p&gt;Playing around with the code below will lead you to identifying some priors that look right (the code below just report what we settle on for this exercise):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(41)
## Priors
N &amp;lt;- 16
theta &amp;lt;- matrix(NA, N, 4)
theta[, 1] &amp;lt;- rnorm(N, 1.5, 1) # prey birth rate
theta[, 2] &amp;lt;- rnorm(N, 0.005, 0.1) # prey mortality rate
theta[, 3] &amp;lt;- rnorm(N, 0.0005, 0.1) # predator mortality rate
theta[, 4] &amp;lt;- rnorm(N, 0.5, 1) # predator birth rate
## Simulate for first prior
plot_df &amp;lt;- sim_mites(1e4, as.numeric(Mites[1, 3:2]), theta[1, ])
plot_df &amp;lt;- data.frame(plot_df)
plot_df$prior &amp;lt;- rep(1, 1e4)
## simulate for all other priors
for (i in 2:N) {
  z &amp;lt;- sim_mites(1e4, as.numeric(Mites[1, 3:2]), theta[i, ])
  z &amp;lt;- data.frame(z)
  z$prior &amp;lt;- rep(i, 1e4)
  plot_df &amp;lt;- rbind(plot_df, z)
}
plot_df$seq &amp;lt;- rep(1:1e4, N)
## Plotting
ggplot(plot_df, aes(x = seq)) +
  geom_line(aes(y = L), col = &amp;quot;brown&amp;quot;) +
  geom_line(aes(y = H), col = &amp;quot;blue&amp;quot;) +
  facet_wrap(~prior, scales = &amp;quot;free&amp;quot;) +
  theme_bw() +
  labs(x = &amp;quot;Time&amp;quot;, y = &amp;quot;Population&amp;quot;) +
  theme(axis.text.y = element_blank())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-13-statistical-rethinking-chapter-16_files/figure-html/unnamed-chunk-55-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;These are decent enough, some show nice cycles for a few.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s run with these anyways and take them forward to a model. The model below is just a broken-back version of the STAN model in the chapter:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Mites_STAN &amp;lt;- &amp;quot;// Mites model, L is the predator, H is the prey
functions{
  real[] dpop_dt(real t,                    // time
                  real[] pop_init,          // initial state{lynx, hares}
                  real[] theta,             // parameters
                  real[] x_r, int[] x_i){   // unused
    real L = pop_init[1];                   // prey population initialisation
    real H = pop_init[2];                   // predator population initialisation
    real bh = theta[1];                     // prey birth rate
    real mh = theta[2];                     // prey mortality
    real ml = theta[3];                     // predator mortality
    real bl = theta[4];                     // predator birth rate
    // differential equations
    real dH_dt = (bh - mh * L) * H;
    real dL_dt = (bl * H - ml) * L;
    return{ dL_dt, dH_dt };
  }
}
data{
  int&amp;lt;lower=0&amp;gt; N;            // number of measurement times
  int&amp;lt;lower=0&amp;gt; mites[N,2];   // measured populations
  real&amp;lt;lower=0&amp;gt; days[N];     // days from start of experiment
}
parameters{
  real&amp;lt;lower=0&amp;gt; theta[4];     //{ bh, mh, ml, bl }
  real&amp;lt;lower=0&amp;gt; pop_init[2];  // initial population state
  real&amp;lt;lower=0&amp;gt; sigma[2];     // measurement errors
}
transformed parameters{
  real pop[N, 2];
  pop[1,1] = pop_init[1];     // prey population initialisation
  pop[1,2] = pop_init[2];     // predator population initialisation
  pop[2:N,1:2] = integrate_ode_rk45(
    dpop_dt, pop_init, 0, days[2:N], theta,
    rep_array(0.0, 0), rep_array(0, 0), 1e-5, 1e-3, 5e2);
}
model{
  // priors
  theta[1] ~ normal(1.5, 1);       // prey birth rate
  theta[2] ~ normal(0.005, 0.1);     // prey mortality
  theta[3] ~ normal(0.0005, 0.1);   // predator mortality
  theta[4] ~ normal(0.5, 1);       // predator birth rate
  sigma ~ exponential(1);
  pop_init[1] ~ normal(mites[1,1], 50);
  pop_init[2] ~ normal(mites[1,2], 50);
  // observation model
  // connect latent population state to observed pelts
  for (t in 1:N)
    for (k in 1:2)
      mites[t,k] ~ lognormal(log(pop[t,k]), sigma[k]);
  }
generated quantities{
  real mites_pred[N,2];
  for (t in 1:N)
    for (k in 1:2)
      mites_pred[t,k] = lognormal_rng(log(pop[t,k]), sigma[k]);
}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Preparing the data and running the model is quite straight-forward now:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dat_mites &amp;lt;- list(
  N = nrow(Mites),
  mites = as.matrix(Mites[, 3:2]),
  days = Mites[, 1] / 7
)
m_H5 &amp;lt;- stan(
  model_code = Mites_STAN, data = dat_mites, chains = 4, cores = 4, iter = 2000,
  control = list(adapt_delta = 0.99)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;precis(m_H5, 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                     mean           sd         5.5%        94.5%    n_eff     Rhat4
## theta[1]    1.288983e+00 3.126271e-01 9.175982e-01 1.852993e+00  984.656 1.0012770
## theta[2]    6.533764e-03 2.216549e-03 3.977740e-03 1.067107e-02 1095.219 1.0010345
## theta[3]    3.250613e-01 7.149778e-02 2.071048e-01 4.392285e-01 1218.141 1.0008058
## theta[4]    4.802551e-04 1.592542e-04 2.589479e-04 7.581328e-04 1473.000 1.0011075
## pop_init[1] 1.164473e+02 1.961122e+01 8.879640e+01 1.502520e+02 1822.214 1.0000536
## pop_init[2] 2.481791e+02 3.982614e+01 1.866272e+02 3.136870e+02 2611.777 0.9994916
## sigma[1]    7.293284e-01 1.209180e-01 5.645224e-01 9.408383e-01 1600.917 1.0004918
## sigma[2]    1.071276e+00 1.464701e-01 8.665494e-01 1.327090e+00 2048.149 1.0016011
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Without trying to interpret the parameters here, let&amp;rsquo;s just jump straight into the posterior predictions:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;post &amp;lt;- extract.samples(m_H5)
mites &amp;lt;- dat_mites$mites
plot(dat_mites$days, mites[, 2],
  pch = 16, ylim = c(0, 3000),
  xlab = &amp;quot;time (week)&amp;quot;, ylab = &amp;quot;mites&amp;quot;
)
points(dat_mites$days, mites[, 1], col = rangi2, pch = 16)
for (s in 1:21) {
  lines(dat_mites$days, post$pop[s, , 2], col = col.alpha(&amp;quot;black&amp;quot;, 0.2), lwd = 2)
  lines(dat_mites$days, post$pop[s, , 1], col = col.alpha(rangi2, 0.3), lwd = 2)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2021-05-13-statistical-rethinking-chapter-16_files/figure-html/unnamed-chunk-61-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Yet again, our model struggles to reconstruct the underlying time-series. This is certainly what McElreath referred to in the chapter when he said we would come face-to-face with the limitations of Lotka-Volterra models in the exercises. Why does the model do so baldy then? Well, it assumes equal cycle times which the data does not support. In addition our model is purely deterministic and lacks stochasticity which could help fit closer to the underlying cycles.&lt;/p&gt;
&lt;p&gt;I would have loved to end this series of blogposts on a more upbeat note, I must say. If you have found any use out of this series of posts and/or my summary slides linked at the top of these, then I am very happy. I must say I personally enjoyed working through this book a lot and hope my posts will come in handy for others looking to validate their solutions. Take care!&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.5 (2021-03-31)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19043)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.1252  LC_CTYPE=English_United Kingdom.1252    LC_MONETARY=English_United Kingdom.1252 LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.1252    
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] rethinking_2.13      rstan_2.21.2         ggplot2_3.3.6        StanHeaders_2.21.0-7
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.7         mvtnorm_1.1-1      lattice_0.20-41    prettyunits_1.1.1  ps_1.6.0           assertthat_0.2.1   digest_0.6.27      utf8_1.2.1         V8_3.4.1           R6_2.5.0          
## [11] backports_1.2.1    stats4_4.0.5       evaluate_0.14      coda_0.19-4        highr_0.9          blogdown_1.3       pillar_1.6.0       rlang_0.4.11       curl_4.3.2         callr_3.7.0       
## [21] jquerylib_0.1.4    R.utils_2.10.1     R.oo_1.24.0        rmarkdown_2.7      styler_1.4.1       labeling_0.4.2     stringr_1.4.0      loo_2.4.1          munsell_0.5.0      compiler_4.0.5    
## [31] xfun_0.22          pkgconfig_2.0.3    pkgbuild_1.2.0     shape_1.4.5        htmltools_0.5.1.1  tidyselect_1.1.0   tibble_3.1.1       gridExtra_2.3      bookdown_0.22      codetools_0.2-18  
## [41] matrixStats_0.61.0 fansi_0.4.2        crayon_1.4.1       dplyr_1.0.5        withr_2.4.2        MASS_7.3-53.1      R.methodsS3_1.8.1  grid_4.0.5         jsonlite_1.7.2     gtable_0.3.0      
## [51] lifecycle_1.0.0    DBI_1.1.1          magrittr_2.0.1     scales_1.1.1       RcppParallel_5.1.2 cli_3.0.0          stringi_1.5.3      farver_2.1.0       bslib_0.2.4        ellipsis_0.3.2    
## [61] generics_0.1.0     vctrs_0.3.7        rematch2_2.1.2     tools_4.0.5        R.cache_0.14.0     glue_1.4.2         purrr_0.3.4        processx_3.5.1     yaml_2.2.1         inline_0.3.17     
## [71] colorspace_2.0-0   knitr_1.33         sass_0.3.1
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Bioclimatic Variables</title>
      <link>https://www.erikkusch.com/courses/krigr/bioclim/</link>
      <pubDate>Thu, 26 May 2022 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/krigr/bioclim/</guid>
      <description>&lt;p&gt;&lt;div class=&#34;alert alert-danger&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;&lt;code&gt;KrigR&lt;/code&gt; is currently undergoing development. As a result, this part of the workshop has become deprecated. Please refer to the &lt;a href=&#34;https://www.erikkusch.com/courses/krigr/setup/&#34;&gt;setup&lt;/a&gt; &lt;a href=&#34;https://www.erikkusch.com/courses/krigr/quickstart/&#34;&gt;quick guide&lt;/a&gt; portions of this material as these are up-to-date. &lt;/strong&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;div class=&#34;alert alert-danger&#34;&gt;
  &lt;div&gt;
    This part of the workshop is dependant on set-up and preparation done previously &lt;a href=&#34;https://www.erikkusch.com/courses/krigr/prep/&#34;&gt;here&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;For bioclimatic variable calculation, this workshop makes use of the  &lt;code&gt;SpatialPolygons&lt;/code&gt; spatial preferences which we set up 
&lt;a href=&#34;https://www.erikkusch.com/courses/krigr/prep/#shape-of-interest-spatialpolygons&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;First, we load &lt;code&gt;KrigR&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(KrigR)
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-info&#34;&gt;
  &lt;div&gt;
    To obtain bioclimatic data with &lt;code&gt;KrigR&lt;/code&gt; we want to use the &lt;code&gt;BioClim()&lt;/code&gt; function.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;In the next sections, I will show you how to use it and how the resulting data objects may differ and why.&lt;/p&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    Bioclimatic variables are often treated as very robust metrics - I do not believe so and hope the following will demonstrate the nuance in bioclimatic metrics.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;our-first-bioclimatic-data-set&#34;&gt;Our First Bioclimatic Data Set&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s start with the most basic of bioclimatic data products. So what are the specifications? Well, we:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Query data for the period between 2010 (&lt;code&gt;Y_start&lt;/code&gt;) and 2020 (&lt;code&gt;Y_end&lt;/code&gt;, including 2020).&lt;/li&gt;
&lt;li&gt;Obtain data from the era5-land (&lt;code&gt;DataSet&lt;/code&gt;) catalogue of data.&lt;/li&gt;
&lt;li&gt;Approximate water availability through precipitation (&lt;code&gt;Water_Var&lt;/code&gt;) in keeping with typical practices.&lt;/li&gt;
&lt;li&gt;Extreme metrics for temperature minimum and maximum are calculated from daily (&lt;code&gt;T_res&lt;/code&gt;) aggregates of the underlying hourly temperature data.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    You will see function call to &lt;code&gt;BioClim()&lt;/code&gt; wrapped in if statements which check for whether the output is already present or not. &lt;code&gt;BioClim&lt;/code&gt; compilation can take significant time and I do this here to avoid recompilation on changes to the text of the blogpost on my end.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-&#34;&gt;
  &lt;div&gt;
    Setting the argument &lt;code&gt; Keep_Monthly = TRUE&lt;/code&gt; will prompt the function to retain monthly aggregates of temperature and water availability alongside the final output. When &lt;code&gt;BioClim()&lt;/code&gt; recognises that any of the underlying data is already present, it will skip the steps necessary to create this data.
  &lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for file if download &amp; processing takes too long:&lt;/summary&gt;
    Download 
&lt;a href=&#34;https://github.com/ErikKusch/Homepage/raw/master/content/courses/krigr/Data/Present_BC.nc&#34;&gt;Present_BC.nc&lt;/a&gt; and place it into your data directory.
&lt;/details&gt; 
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;if(!file.exists(file.path(Dir.Data, &amp;quot;Present_BC.nc&amp;quot;))){
  BC2010_ras &amp;lt;- BioClim(
      Water_Var = &amp;quot;total_precipitation&amp;quot;,
      Y_start = 2010,
      Y_end = 2020,
      DataSet = &amp;quot;era5-land&amp;quot;,
      T_res = &amp;quot;day&amp;quot;,
      Extent = Shape_shp,
      Dir = Dir.Data,
      Keep_Monthly = FALSE,
      FileName = &amp;quot;Present_BC&amp;quot;,
      API_User = API_User,
      API_Key = API_Key,
      Cores = numberOfCores,
      TimeOut = 60^2*48,
      SingularDL = TRUE,
      verbose = TRUE,
      Keep_Raw = FALSE,
      TryDown = 5
    )
}else{
  BC2010_ras &amp;lt;- stack(file.path(Dir.Data, &amp;quot;Present_BC.nc&amp;quot;))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let&amp;rsquo;s plot our results. Note that temperature is recorded in Kelvin and precipitation in cubic metres (i.e. litres). To do so, we use one of our 
&lt;a href=&#34;https://www.erikkusch.com/courses/krigr/prep/#visualising-our-study-setting&#34;&gt;user-defined plotting functions&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Plot_BC(BC2010_ras, Shp = Shape_shp)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-bioclim_files/figure-html/BC1b-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There&amp;rsquo;s not much commenting on the output above as the output should look familiar to most macroecologists.&lt;/p&gt;
&lt;h2 id=&#34;time-frames&#34;&gt;Time-Frames&lt;/h2&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    Time window of baseline climate data (e.g; climatology time frames) ought to be adjusted to the specific needs of each study. This is true also for bioclimatic data. &lt;strong&gt;Pre-made data sets do not deliver on this need!&lt;/strong&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-info&#34;&gt;
  &lt;div&gt;
    With &lt;code&gt;KrigR&lt;/code&gt;, you can build the bioclimatic data sets you need for your study.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Let&amp;rsquo;s move on to the first important functionality of the &lt;code&gt;KrigR::BioClim()&lt;/code&gt; function: &lt;strong&gt;selection of time-frames&lt;/strong&gt;. With this, you can obtain bioclimatic data for exactly the duration that your study requires. Here, we query data for the period between 1951 and 1960:&lt;/p&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for file if download &amp; processing takes too long:&lt;/summary&gt;
    Download 
&lt;a href=&#34;https://github.com/ErikKusch/Homepage/raw/master/content/courses/krigr/Data/Past_BC.nc&#34;&gt;Past_BC.nc&lt;/a&gt; and place it into your data directory.
&lt;/details&gt; 
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;if(!file.exists(file.path(Dir.Data, &amp;quot;Past_BC.nc&amp;quot;))){
  BC1951_ras &amp;lt;- BioClim(
      Water_Var = &amp;quot;total_precipitation&amp;quot;,
      Y_start = 1951,
      Y_end = 1960,
      DataSet = &amp;quot;era5-land&amp;quot;,
      T_res = &amp;quot;day&amp;quot;,
      Extent = Shape_shp,
      Dir = Dir.Data,
      Keep_Monthly = FALSE,
      FileName = &amp;quot;Past_BC&amp;quot;,
      API_User = API_User,
      API_Key = API_Key,
      Cores = numberOfCores,
      TimeOut = 60^2*48,
      SingularDL = TRUE,
      verbose = TRUE
    )
}else{
  BC1951_ras &amp;lt;- stack(file.path(Dir.Data, &amp;quot;Past_BC.nc&amp;quot;))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I will forego plotting the data itself and instead plot the difference between our bioclimatic data of the present which we created prior and the newly created bioclimatic product of the past. Let me walk you through them 1 by 1.&lt;/p&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    The below plots show the differences in bioclimatic data products of the 2010-2020 and 1951-1960.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;annual-temperature&#34;&gt;Annual Temperature&lt;/h3&gt;
&lt;p&gt;As you can see below, the time period of 2010 to 2020 was about 1.5-1.9 Kelvin warmer than the period of 1951 to 1960:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Plot_BC(BC2010_ras-BC1951_ras, Shp = Shape_shp, which = 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-bioclim_files/figure-html/BC_Early1-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;temperatures&#34;&gt;Temperatures&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s bundle the differences for all remaining temperature-related bioclimatic variables:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Plot_BC(BC2010_ras-BC1951_ras, Shp = Shape_shp, which = 2:11)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-bioclim_files/figure-html/BC_Early2-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Again, you should easily identify just how much the data changes when setting different calculation time frames for bioclimatic variables.&lt;/p&gt;
&lt;h3 id=&#34;water-availability&#34;&gt;Water Availability&lt;/h3&gt;
&lt;p&gt;Now for the water-related bioclimatic variables:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Plot_BC(BC2010_ras-BC1951_ras, Shp = Shape_shp, which = 12:19)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-bioclim_files/figure-html/BC_Early8-1.png&#34; width=&#34;1440&#34; /&gt;
Clearly, my home area turned much drier with more pronounced seasonality and extreme precipitation events.&lt;/p&gt;
&lt;p&gt;I hope that the above has clearly demonstrated on thing:&lt;/p&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Appropriate use of bioclimatic variables is largely dependant on data retrieval for relevant time frames.&lt;/strong&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;water-availability-variables&#34;&gt;Water-Availability Variables&lt;/h2&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Precipitation&lt;/strong&gt; might not be the most useful or appropriate water availability metric for your study region or requirements.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-info&#34;&gt;
  &lt;div&gt;
    With &lt;code&gt;KrigR&lt;/code&gt;, you can decide which water availability variable from the ERA5(-Land) catalogue to use for calculation of bioclimatic data sets.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Contrary to current practices in macroecology, I have gripes with the use of precipitation data in bioclimatic variable computation. Why is that? I strongly believe that other water availability variables are much better suited for our analyses for two reasons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Bioclimatic products are usually derived from observation-based climate products (such as WorldClim) which do not do a terrific job at accurately representing precipitation to begin with.&lt;/li&gt;
&lt;li&gt;Further downscaling of bioclimatic products containing precipitation information is terribly difficult.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Both issues are related to one central problem: &lt;strong&gt;Statistical interpolation of precipitation data is difficult and usually done insufficiently&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Luckily, with ERA5(-Land), we aren&amp;rsquo;t tied to precipitation and can instead use other water availability metrics such as volumetric soil water content - also known as soil moisture. What&amp;rsquo;s more, this data is available in four distinct depth layers which can be linked to root depth and growth forms.&lt;/p&gt;
&lt;p&gt;Here, I demonstrate the use of the shallowest layer of soil moisture data. As you can see, we are using the same specification as for our basic bioclimatic product with the exception for the &lt;code&gt;Water_Var&lt;/code&gt; argument:&lt;/p&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for file if download &amp; processing takes too long:&lt;/summary&gt;
    Download 
&lt;a href=&#34;https://github.com/ErikKusch/Homepage/raw/master/content/courses/krigr/Data/Qsoil_BC.nc&#34;&gt;Qsoil_BC.nc&lt;/a&gt; and place it into your data directory.
&lt;/details&gt; 
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;if(file.exists(file.path(Dir.Data, &amp;quot;Qsoil_BC.nc&amp;quot;))){
  BCq_ras &amp;lt;- stack(file.path(Dir.Data, &amp;quot;Qsoil_BC.nc&amp;quot;))
}else{
  BCq_ras &amp;lt;- BioClim(
      Water_Var = &amp;quot;volumetric_soil_water_layer_1&amp;quot;,
      Y_start = 2010,
      Y_end = 2020,
      Extent = Shape_shp,
      Dir = Dir.Data,
      Keep_Monthly = FALSE,
      FileName = &amp;quot;Qsoil_BC&amp;quot;,
      API_User = API_User,
      API_Key = API_Key,
      Cores = numberOfCores,
      TimeOut = Inf,
      SingularDL = TRUE
    )
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That&amp;rsquo;s how easy it is to obtain different bioclimatic products with &lt;code&gt;KrigR&lt;/code&gt;. Let&amp;rsquo;s plot this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Plot_BC(BCq_ras, Shp = Shape_shp, Water_Var = &amp;quot;Soil Moisture&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-bioclim_files/figure-html/BC_QsoilB-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Again, I would like to investigate the changes in how we understand the climatic regimes across our study area now that we are using soil moisture for our water availability as compared to when we used precipitation data.&lt;/p&gt;
&lt;h3 id=&#34;temperatures-1&#34;&gt;Temperatures&lt;/h3&gt;
&lt;p&gt;As is hardly surprising, there are no differences in annual temperature data or any other temperature variable except for BIO8 and BIO9. Since we change by what we quantify dryness and wetness, there is tremendous potential in quantifying temperature of driest and wettest quarter differently:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Plot_BC(BC2010_ras-BCq_ras, Shp = Shape_shp, which = 8:9)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-bioclim_files/figure-html/BC_Q1-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    Changing water availability metric in bioclimatic considerations can drastically change even &lt;em&gt;temperature&lt;/em&gt; metrics.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-info&#34;&gt;
  &lt;div&gt;
    Volumetric soil moisture exhibits more pronounced spatial patterns than precipitation records do thus supplying bioclimatic modelling exercises with more pronounced information.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;water-availability-1&#34;&gt;Water Availability&lt;/h3&gt;
&lt;p&gt;Now for the water-related bioclimatic variables. This is where the rubber meets the road! Aside from the quantitative differences in water availability estimates when using soil moisture over precipitation records, please take note of the much more pronounced spatial patterns (particularly along the river throughout Saxony-Anhalt in the north-western region of our study area) when using soil moisture data. This is much more likely to accurately represent bioclimatic envelopes than the smooth patterns you can see for precipitation records.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Plot_BC(BC2010_ras-BCq_ras, Shp = Shape_shp, which = 12:19)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-bioclim_files/figure-html/BC_Q2-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I hope that the above has clearly demonstrated on thing:&lt;/p&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Choice of water availability variable has strong implications for how we quantify bioclimatic envelopes.&lt;/strong&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;extreme-value-calculations&#34;&gt;Extreme Value Calculations&lt;/h2&gt;
&lt;p&gt;Lastly, let us concern ourselves with the retrieval of extreme climate metrics which will affect almost all of our temperature-reliant bioclimatic variables.&lt;/p&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    Extreme event calculation is highly relevant for our understanding of bioclimatic envelopes and often turns into a blackbox exercise.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-info&#34;&gt;
  &lt;div&gt;
    With &lt;code&gt;KrigR&lt;/code&gt;, you can decide how to calculate extreme metrics.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;So far, we have calculated monthly minimum and maximum temperatures from daily aggregates. However, with &lt;code&gt;KrigR::BioClim()&lt;/code&gt; we can also obtain these extremes from hourly records simply by changing &lt;code&gt;T_res&lt;/code&gt;:&lt;/p&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for file if download &amp; processing takes too long:&lt;/summary&gt;
    Download 
&lt;a href=&#34;https://github.com/ErikKusch/Homepage/raw/master/content/courses/krigr/Data/Hourly_BC.nc&#34;&gt;Hourly_BC.nc&lt;/a&gt; and place it into your data directory.
&lt;/details&gt; 
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;if(file.exists(file.path(Dir.Data, &amp;quot;Hourly_BC.nc&amp;quot;))){
  BCh_ras &amp;lt;- stack(file.path(Dir.Data, &amp;quot;Hourly_BC.nc&amp;quot;))
}else{
  BCh_ras &amp;lt;- BioClim(
    Water_Var = &amp;quot;volumetric_soil_water_layer_1&amp;quot;,
    Y_start = 2010,
    Y_end = 2020,
    T_res = &amp;quot;hour&amp;quot;,
    Extent = Shape_shp,
    Dir = Dir.Data,
    Keep_Monthly = FALSE,
    FileName = &amp;quot;Hourly_BC&amp;quot;,
    API_User = API_User,
    API_Key = API_Key,
    Cores = numberOfCores,
    TimeOut = Inf,
    SingularDL = TRUE
  )
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once again, let me plot the outcome of this.&lt;/p&gt;
&lt;h3 id=&#34;annual-temperature-1&#34;&gt;Annual Temperature&lt;/h3&gt;
&lt;p&gt;The differences in annual temperature are negligible and only arise through slight deviations in hourly aggregates to monthly aggregates and daily aggregates.&lt;/p&gt;
&lt;details&gt;&lt;summary&gt; Click here for the plot &lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Plot_BC(BCq_ras - BCh_ras, Shp = Shape_shp, Water_Var = &amp;quot;Soil Moisture&amp;quot;, which = 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-bioclim_files/figure-html/BC_Diff1-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;h3 id=&#34;temperatures-2&#34;&gt;Temperatures&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s bundle the differences for all remaining temperature-related bioclimatic variables.&lt;/p&gt;
&lt;p&gt;You will immediately see that all metrics reliant of mean values such as BIO4 and BIO8-BIO11 remain almost completely unaltered when using hourly aggregates. The stark differences manifest in all temperature-extreme variables:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Plot_BC(BCq_ras - BCh_ras, Shp = Shape_shp, Water_Var = &amp;quot;Soil Moisture&amp;quot;, which = 2:11)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-bioclim_files/figure-html/BC_Diff2-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    Extraction of extremes at an hourly resolution amplifies said extremes.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;water-availability-2&#34;&gt;Water Availability&lt;/h3&gt;
&lt;p&gt;Unsurprisingly, there are no changes to our quantification of water availability metrics. You may plot this for yourself if you are interested.&lt;/p&gt;
&lt;p&gt;I hope that the above has clearly demonstrated on thing:&lt;/p&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Choice of temporal resolution of extreme metrics changes how we quantify bioclimatic envelopes drastically.&lt;/strong&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;kriging-bioclimatic-products&#34;&gt;Kriging Bioclimatic Products&lt;/h2&gt;
&lt;p&gt;You might be unhappy with the spatial resolution of the bioclimatic data products generated through &lt;code&gt;KrigR::BioClim()&lt;/code&gt;. You can remedy this through statistical interpolation which is conveniently built into &lt;code&gt;KrigR&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;When you do so, you do it at your own risk as I can not guarantee that the results will always be sensible. Investigate them before using them. It would be wiser to downscale the underlying data rather than the finished product, but I don&amp;rsquo;t feel like spending days on end kriging the underlying data so instead I show you how kriging can be performed, but I do so for the entire product.&lt;/p&gt;
&lt;p&gt;Since I mentioned earlier that statistical interpolation of precipitation data is fraught with errors, I am demonstrating how to downscale the soil moisture product (&lt;code&gt;BCq_ras&lt;/code&gt;). We have demonstrated capability of downscaling soil moisture data reliably using Kriging in this 
&lt;a href=&#34;https://iopscience.iop.org/article/10.1088/1748-9326/ac39bf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this publication (Figure 3)&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;temperatures-3&#34;&gt;Temperatures&lt;/h3&gt;
&lt;p&gt;Here, we follow the same basic kriging steps as demonstrated 
&lt;a href=&#34;https://www.erikkusch.com/courses/krigr/kriging/&#34;&gt;previously&lt;/a&gt; in this workshop material.&lt;/p&gt;
&lt;p&gt;First, we create our DEM covariate rasters:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Covs_ls &amp;lt;- download_DEM(Train_ras = BCq_ras,
                        Target_res = .02,
                        Shape = Shape_shp,
                        Dir = Dir.Covariates,
                        Keep_Temporary = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we carry out the interpolation. A few things of note here: (1) I only hand the first 11 layers to the kriging call because those are the temperature data, (2) I leave out the &lt;code&gt;Cores&lt;/code&gt; argument, so that &lt;code&gt;krigR()&lt;/code&gt; determines how many cores your machine has and uses all of them to speed up the computation of the multi-layer raster, and (3) I set &lt;code&gt;nmax&lt;/code&gt; to 80 to approximate a typical weather system in size:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;BC_Temperature_Krig &amp;lt;- krigR(Data = BCq_ras[[1:11]],
      Covariates_coarse = Covs_ls[[1]],
      Covariates_fine = Covs_ls[[2]],
      Keep_Temporary = FALSE,
      nmax = 80,
      FileName = &amp;quot;BC_Temperature_Krig&amp;quot;,
      Dir = Dir.Exports
      )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Commencing Kriging
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
  |                                                                                      
  |                                                                                |   0%
  |                                                                                      
  |=======                                                                         |   9%
  |                                                                                      
  |===============                                                                 |  18%
  |                                                                                      
  |======================                                                          |  27%
  |                                                                                      
  |=============================                                                   |  36%
  |                                                                                      
  |====================================                                            |  45%
  |                                                                                      
  |============================================                                    |  55%
  |                                                                                      
  |===================================================                             |  64%
  |                                                                                      
  |==========================================================                      |  73%
  |                                                                                      
  |=================================================================               |  82%
  |                                                                                      
  |=========================================================================       |  91%
  |                                                                                      
  |================================================================================| 100%
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: [writeCDF] for better results use file extension &#39;.nc&#39; or &#39;.cdf&#39;
## see: https://stackoverflow.com/a/65398262/635245
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: [rast] unknown extent
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: [writeCDF] for better results use file extension &#39;.nc&#39; or &#39;.cdf&#39;
## see: https://stackoverflow.com/a/65398262/635245
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: [rast] unknown extent
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we analyse the outputs of our plotting exercise. I break these up into smaller chunks for easier digestion.&lt;/p&gt;
&lt;h4 id=&#34;bio1---annual-mean-temperature&#34;&gt;BIO1 - Annual Mean Temperature&lt;/h4&gt;
&lt;div class=&#34;alert alert-normal&#34;&gt;
  &lt;div&gt;
    Interpolating this data is just like statistically downscaling any other temperature product and can be done without any problems.
  &lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for plotting call and plot:&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Plot_Krigs(lapply(BC_Temperature_Krig[-3], &amp;quot;[[&amp;quot;, 1), 
           Shp = Shape_shp,
           Dates = &amp;quot;BIO1 - Annual Mean Temperature&amp;quot;
           )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-bioclim_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;/details&gt; 
&lt;h4 id=&#34;bio2---mean-diurnal-range&#34;&gt;BIO2 - Mean Diurnal Range&lt;/h4&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    This data product is calculated from extreme values and would be interpolated better by first statistically downscaling the underlying data rather than the final bioclimatic variable.
  &lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for plotting call and plot:&lt;/summary&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    The smooth patterns in this plot clearly highlight the issue with using &lt;code&gt;krigr()&lt;/code&gt; on the final bioclimatic product.
  &lt;/div&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Plot_Krigs(lapply(BC_Temperature_Krig[-3], &amp;quot;[[&amp;quot;, 2), 
           Shp = Shape_shp,
           Dates = &amp;quot;BIO2 - Mean Diurnal Range&amp;quot;
           )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-bioclim_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;h4 id=&#34;bio3---isothermality&#34;&gt;BIO3 - Isothermality&lt;/h4&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    This data product is calculated from BIO2 and BIO7 and thus relies on extreme values. Conclusively, it would be interpolated better by first statistically downscaling the underlying data rather than the final bioclimatic variable.
  &lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for plotting call and plot:&lt;/summary&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    The smooth patterns in this plot clearly highlight the issue with using &lt;code&gt;krigr()&lt;/code&gt; on the final bioclimatic product.
  &lt;/div&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Plot_Krigs(lapply(BC_Temperature_Krig[-3], &amp;quot;[[&amp;quot;, 3), 
           Shp = Shape_shp,
           Dates = &amp;quot;BIO3 - Isothermality&amp;quot;
           )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-bioclim_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;h4 id=&#34;bio4---temperature-seasonality&#34;&gt;BIO4 - Temperature Seasonality&lt;/h4&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    This data product is calculated using the standard deviation of mean values throughout our time frame. Conclusively, it would be interpolated better by first statistically downscaling the underlying data rather than the final bioclimatic variable.
  &lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for plotting call and plot:&lt;/summary&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    The smooth patterns in this plot clearly highlight the issue with using &lt;code&gt;krigr()&lt;/code&gt; on the final bioclimatic product.
  &lt;/div&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Plot_Krigs(lapply(BC_Temperature_Krig[-3], &amp;quot;[[&amp;quot;, 4), 
           Shp = Shape_shp,
           Dates = &amp;quot;BIO4 - Temperature Seasonality&amp;quot;
           )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-bioclim_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;h4 id=&#34;bio5---max-temperature-of-warmest-month&#34;&gt;BIO5 - Max Temperature of Warmest Month&lt;/h4&gt;
&lt;div class=&#34;alert alert-normal&#34;&gt;
  &lt;div&gt;
    Interpolating this data is just like statistically downscaling any other temperature product and can be done without any problems.
  &lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for plotting call and plot:&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Plot_Krigs(lapply(BC_Temperature_Krig[-3], &amp;quot;[[&amp;quot;, 5), 
           Shp = Shape_shp,
           Dates = c(&amp;quot;BIO5 - Max Temperature of Warmest Month&amp;quot;)
           )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-bioclim_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;h4 id=&#34;bio6---min-temperature-of-coldest-month&#34;&gt;BIO6 - Min Temperature of Coldest Month&lt;/h4&gt;
&lt;div class=&#34;alert alert-normal&#34;&gt;
  &lt;div&gt;
    Interpolating this data is just like statistically downscaling any other temperature product and can be done without any problems.
  &lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for plotting call and plot:&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Plot_Krigs(lapply(BC_Temperature_Krig[-3], &amp;quot;[[&amp;quot;, 6), 
           Shp = Shape_shp,
           Dates = &amp;quot;BIO6 - Min Temperature of Coldest Month&amp;quot;
           )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-bioclim_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;h4 id=&#34;bio7---temperature-annual-range-bio5-bio6&#34;&gt;BIO7 - Temperature Annual Range (BIO5-BIO6)&lt;/h4&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    This data product is calculated from BIO5 and BIO6 and thus relies on extreme values. Conclusively, it would be interpolated better by first statistically downscaling the underlying data rather than the final bioclimatic variable.
  &lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for plotting call and plot:&lt;/summary&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    The smooth patterns in this plot clearly highlight the issue with using &lt;code&gt;krigr()&lt;/code&gt; on the final bioclimatic product.
  &lt;/div&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Plot_Krigs(lapply(BC_Temperature_Krig[-3], &amp;quot;[[&amp;quot;, 7), 
           Shp = Shape_shp,
           Dates = &amp;quot;BIO7 - Temperature Annual Range (BIO5-BIO6)&amp;quot;
           )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-bioclim_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;div class=&#34;alert alert-info&#34;&gt;
  &lt;div&gt;
    Since BIO5 and BIO6 can be interpolated well themselves, one may chose to use the downscaled versions of BIO5 and BIO6 to create a downscaled version of BIO7.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    Doing so, however, raises the question of how to integrate the downscaling uncertainty associated with BIO5 and BIO6 into the product for BIO7. I have submitted a research proposal to assess best practice for issues like these.
  &lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for calculation, plotting call, and plot:&lt;/summary&gt;
&lt;p&gt;Here, I visualise the differences between the interpolated BIO7 and the recalculated BIO7 (from interpolated BIO5 and BIO6):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;BIO7 &amp;lt;- lapply(BC_Temperature_Krig[1], &amp;quot;[[&amp;quot;, 5)[[1]] - lapply(BC_Temperature_Krig[1], &amp;quot;[[&amp;quot;, 6)[[1]]
Plot_Raw(lapply(BC_Temperature_Krig[1], &amp;quot;[[&amp;quot;, 7)[[1]]-BIO7, 
         Shp = Shape_shp,
         Dates = &amp;quot;BIO7 - Temperature Annual Range (BIO5-BIO6)&amp;quot;
         )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-bioclim_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To be fair, these differences are rather small when compared to the data range in BIO7.&lt;/p&gt;
&lt;/details&gt;
&lt;h4 id=&#34;bio8--bio9---temperatures-of-wettest-and-driest-quarter&#34;&gt;BIO8 &amp;amp; BIO9 - Temperatures of Wettest and Driest Quarter&lt;/h4&gt;
&lt;div class=&#34;alert alert-danger&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;I do not recommend you use these kriging outputs!&lt;/strong&gt; They rely on water availability data which is not being interpolated here. Subsequently, the patchiness of the underlying data is lost and with it: information.
  &lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for plotting call and plot:&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Plot_Krigs(lapply(BC_Temperature_Krig[-3], &amp;quot;[[&amp;quot;, 8:9), 
           Shp = Shape_shp,
           Dates = c(&amp;quot;BIO8 - Mean Temperature of Wettest Quarter&amp;quot;, 
                     &amp;quot;BIO9 - Mean Temperature of Driest Quarter&amp;quot;)
           )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-bioclim_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;h4 id=&#34;bio10--bio11---temperatures-of-warmest-and-coldest-quarter&#34;&gt;BIO10 &amp;amp; BIO11 - Temperatures of Warmest and Coldest Quarter&lt;/h4&gt;
&lt;div class=&#34;alert alert-danger&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;I do not recommend you use these kriging outputs!&lt;/strong&gt; They rely on mean quarterly temperature data which is not being interpolated here. Subsequently, the patchiness of the underlying data is lost and with it: information.
  &lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for plotting call and plot:&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Plot_Krigs(lapply(BC_Temperature_Krig[-3], &amp;quot;[[&amp;quot;, 10:11), 
           Shp = Shape_shp,
           Dates = c(&amp;quot;BIO10 - Mean Temperature of Warmest Quarter&amp;quot;, 
                     &amp;quot;BIO11 - Mean Temperature of Coldest Quarter&amp;quot;)
           )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-bioclim_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;h3 id=&#34;water-availability-3&#34;&gt;Water Availability&lt;/h3&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    Statistical downscaling of non-temperature data usually requires more than just elevation covariates.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-info&#34;&gt;
  &lt;div&gt;
    With &lt;code&gt;KrigR&lt;/code&gt;, you can use different sets of covariates. I demonstrate this in the workshop material regarding &lt;a href=&#34;https://www.erikkusch.com/courses/krigr/third-party/#third-party-data-covariates&#34;&gt;third-party covariates&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.2.3 (2023-03-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Big Sur ... 10.16
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] mapview_2.11.0          rnaturalearthdata_0.1.0 rnaturalearth_0.3.2    
##  [4] gimms_1.2.1             ggmap_3.0.2             cowplot_1.1.1          
##  [7] viridis_0.6.2           viridisLite_0.4.1       ggplot2_3.4.1          
## [10] tidyr_1.3.0             KrigR_0.1.2             terra_1.7-21           
## [13] httr_1.4.5              stars_0.6-0             abind_1.4-5            
## [16] fasterize_1.0.4         sf_1.0-12               lubridate_1.9.2        
## [19] automap_1.1-9           doSNOW_1.0.20           snow_0.4-4             
## [22] doParallel_1.0.17       iterators_1.0.14        foreach_1.5.2          
## [25] rgdal_1.6-5             raster_3.6-20           sp_1.6-0               
## [28] stringr_1.5.0           keyring_1.3.1           ecmwfr_1.5.0           
## [31] ncdf4_1.21             
## 
## loaded via a namespace (and not attached):
##  [1] leafem_0.2.0             colorspace_2.1-0         class_7.3-21            
##  [4] leaflet_2.1.2            satellite_1.0.4          base64enc_0.1-3         
##  [7] rstudioapi_0.14          proxy_0.4-27             farver_2.1.1            
## [10] fansi_1.0.4              codetools_0.2-19         cachem_1.0.7            
## [13] knitr_1.42               jsonlite_1.8.4           png_0.1-8               
## [16] Kendall_2.2.1            compiler_4.2.3           assertthat_0.2.1        
## [19] fastmap_1.1.1            cli_3.6.0                htmltools_0.5.4         
## [22] tools_4.2.3              gtable_0.3.1             glue_1.6.2              
## [25] dplyr_1.1.0              Rcpp_1.0.10              jquerylib_0.1.4         
## [28] vctrs_0.6.1              blogdown_1.16            crosstalk_1.2.0         
## [31] lwgeom_0.2-11            xfun_0.37                timechange_0.2.0        
## [34] lifecycle_1.0.3          rnaturalearthhires_0.2.1 zoo_1.8-11              
## [37] scales_1.2.1             gstat_2.1-0              yaml_2.3.7              
## [40] curl_5.0.0               memoise_2.0.1            gridExtra_2.3           
## [43] sass_0.4.5               reshape_0.8.9            stringi_1.7.12          
## [46] highr_0.10               e1071_1.7-13             boot_1.3-28.1           
## [49] intervals_0.15.3         RgoogleMaps_1.4.5.3      rlang_1.1.0             
## [52] pkgconfig_2.0.3          bitops_1.0-7             evaluate_0.20           
## [55] lattice_0.20-45          purrr_1.0.1              htmlwidgets_1.6.1       
## [58] labeling_0.4.2           tidyselect_1.2.0         plyr_1.8.8              
## [61] magrittr_2.0.3           bookdown_0.33            R6_2.5.1                
## [64] generics_0.1.3           DBI_1.1.3                pillar_1.8.1            
## [67] withr_2.5.0              units_0.8-1              xts_0.13.0              
## [70] tibble_3.2.1             spacetime_1.2-8          KernSmooth_2.23-20      
## [73] utf8_1.2.3               rmarkdown_2.20           jpeg_0.1-10             
## [76] grid_4.2.3               zyp_0.11-1               FNN_1.1.3.2             
## [79] digest_0.6.31            classInt_0.4-9           webshot_0.5.4           
## [82] stats4_4.2.3             munsell_0.5.0            bslib_0.4.2
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Third-Party Data</title>
      <link>https://www.erikkusch.com/courses/krigr/third-party/</link>
      <pubDate>Thu, 26 May 2022 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/krigr/third-party/</guid>
      <description>&lt;p&gt;&lt;div class=&#34;alert alert-danger&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;&lt;code&gt;KrigR&lt;/code&gt; is currently undergoing development. As a result, this part of the workshop has become deprecated. Please refer to the &lt;a href=&#34;https://www.erikkusch.com/courses/krigr/setup/&#34;&gt;setup&lt;/a&gt; &lt;a href=&#34;https://www.erikkusch.com/courses/krigr/quickstart/&#34;&gt;quick guide&lt;/a&gt; portions of this material as these are up-to-date. &lt;/strong&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;div class=&#34;alert alert-danger&#34;&gt;
  &lt;div&gt;
    This part of the workshop is dependant on set-up and preparation done previously &lt;a href=&#34;https://www.erikkusch.com/courses/krigr/prep/&#34;&gt;here&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;First, we load &lt;code&gt;KrigR&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(KrigR)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;matching-third-party-data&#34;&gt;Matching Third-Party Data&lt;/h2&gt;
&lt;p&gt;I expect that you won&amp;rsquo;t want to downscale to specific resolutions most of the time, but rather, match an already existing spatial data set in terms of spatial resolution and extent. Again, the &lt;code&gt;KrigR&lt;/code&gt; package got you covered!&lt;/p&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    Usually, you probably want to downscale data to match a certain pre-existing data set rather than a certain resolution.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Here, we illustrate this with an NDVI-based example. The NDVI is a satellite-derived vegetation index which tells us how green the Earth is. It comes in bi-weekly intervals and at a spatial resolution of &lt;code&gt;.08333&lt;/code&gt; (roughly 9km). Here, we download all NDVI data for the year 2015 and then create the annual mean. This time, we do so for all of Germany because of its size and topographical variety.&lt;/p&gt;
&lt;h3 id=&#34;third-party-data&#34;&gt;Third-Party Data&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Shape_shp &amp;lt;- ne_countries(country = &amp;quot;Germany&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## downloading gimms data
gimms_files &amp;lt;- downloadGimms(x = as.Date(&amp;quot;2015-01-01&amp;quot;), # download from January 1982
                             y = as.Date(&amp;quot;2015-12-31&amp;quot;), # download to December 1982
                             dsn = Dir.Data, # save downloads in data folder
                             quiet = FALSE # show download progress
                             )
## prcoessing gimms data
gimms_raster &amp;lt;- rasterizeGimms(x = gimms_files, # the data we rasterize
                               remove_header = TRUE # we don&#39;t need the header of the data
                               )
indices &amp;lt;- monthlyIndices(gimms_files) # generate month indices from the data
gimms_raster_mvc &amp;lt;- monthlyComposite(gimms_raster, # the data
                                     indices = indices # the indices
                                     )
Negatives &amp;lt;- which(values(gimms_raster_mvc) &amp;lt; 0) # identify all negative values
values(gimms_raster_mvc)[Negatives] &amp;lt;- 0 # set threshold for barren land (NDVI&amp;lt;0)
gimms_raster_mvc &amp;lt;- crop(gimms_raster_mvc, extent(Shape_shp)) # crop to extent
gimms_mask &amp;lt;- KrigR::mask_Shape(gimms_raster_mvc[[1]], Shape = Shape_shp) # create mask ith KrigR-internal function to ensure all edge cells are contained
NDVI_ras &amp;lt;- mask(gimms_raster_mvc, gimms_mask) # mask out shape
NDVI_ras &amp;lt;- calc(NDVI_ras, fun = mean, na.rm = TRUE) # annual mean
writeRaster(NDVI_ras, format = &amp;quot;CDF&amp;quot;, file = file.path(Dir.Data, &amp;quot;NDVI&amp;quot;)) # save file
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So what does this raster look like?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;NDVI_ras
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## class      : RasterStack 
## dimensions : 92, 108, 9936, 1  (nrow, ncol, ncell, nlayers)
## resolution : 0.08333333, 0.08333333  (x, y)
## extent     : 6, 15, 47.33333, 55  (xmin, xmax, ymin, ymax)
## crs        : +proj=longlat +datum=WGS84 +no_defs 
## names      :     layer 
## min values : 0.2430333 
## max values : 0.8339083
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And a visualisation of the same:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Plot_Raw(NDVI_ras, 
         Shp = Shape_shp,
         Dates = &amp;quot;Mean NDVI 2015&amp;quot;, 
         COL = viridis(100, begin = 0.5, direction = -1))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-third-party_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;1440&#34; /&gt;
As stated above, we want to match this with our output.&lt;/p&gt;
&lt;h3 id=&#34;krigr-workflow&#34;&gt;&lt;code&gt;KrigR&lt;/code&gt; Workflow&lt;/h3&gt;
&lt;p&gt;We could do this whole analysis in our three steps as outlined above, but why bother when the 
&lt;a href=&#34;https://www.erikkusch.com/courses/krigr/quickstart/#the-pipeline&#34;&gt;pipeline&lt;/a&gt; gets the job done just as well?&lt;/p&gt;
&lt;div class=&#34;alert alert-info&#34;&gt;
  &lt;div&gt;
    Matching Kriging outputs with a pre-existing data set is as easy as plugging the pre-existing raster into the &lt;code&gt;Target_res&lt;/code&gt; argument of the &lt;code&gt;krigR()&lt;/code&gt; or the &lt;code&gt;download_DEM()&lt;/code&gt; function.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;This time we want to downscale from ERA5 resolution (roughly 30km) because the ERA5-Land data already matches the NDVI resolution (roughly 9km). Here&amp;rsquo;s how we do this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;NDVI_Krig &amp;lt;- krigR(
  ## download_ERA block
  Variable = &#39;2m_temperature&#39;,
  Type = &#39;reanalysis&#39;,
  DataSet = &#39;era5&#39;,
  DateStart = &#39;2015-01-01&#39;,
  DateStop = &#39;2015-12-31&#39;,
  TResolution = &#39;year&#39;,
  TStep = 1,
  Extent = Shape_shp,
  API_User = API_User,
  API_Key = API_Key,
  SingularDL = TRUE,
  ## download_DEM block
  Target_res = NDVI_ras,
  Source = &amp;quot;Drive&amp;quot;,
  ## krigR block
  Cores = 1,
  FileName = &amp;quot;AirTemp_NDVI.nc&amp;quot;,
  nmax = 80, 
  Dir = Dir.Exports)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## download_ERA() is starting. Depending on your specifications, this can take a significant time.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## User 39340 for cds service added successfully in keychain
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Staging 1 download(s).
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Staging your request as a singular download now. This can take a long time due to size of required product.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 0001_2m_temperature_2015-01-01_2015-12-31_year.nc download queried
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Requesting data to the cds service with username 39340
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## - staging data transfer at url endpoint or request id:
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   4d24fc1f-2be1-4b65-b588-be3ba2b5938b
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## - timeout set to 10.0 hours
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## - polling server for a data transfer
\ polling server for a data transfer
| polling server for a data transfer
/ polling server for a data transfer
- polling server for a data transfer
\ polling server for a data transfer
| polling server for a data transfer
/ polling server for a data transfer
- polling server for a data transfer
\ polling server for a data transfer
| polling server for a data transfer
/ polling server for a data transfer
- polling server for a data transfer
\ polling server for a data transfer
| polling server for a data transfer
/ polling server for a data transfer
- polling server for a data transfer
\ polling server for a data transfer
| polling server for a data transfer
/ polling server for a data transfer
- polling server for a data transfer
\ polling server for a data transfer
| polling server for a data transfer
/ polling server for a data transfer
- polling server for a data transfer
## Downloading file
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
  |                                                                                      
  |                                                                                |   0%
  |                                                                                      
  |================================================================================| 100%
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## - moved temporary file to -&amp;gt; /Users/erikkus/Documents/HomePage/content/courses/krigr/Exports/0001_2m_temperature_2015-01-01_2015-12-31_year.nc
## - Delete data from queue for url endpoint or request id:
##   https://cds.climate.copernicus.eu/api/v2/tasks/4d24fc1f-2be1-4b65-b588-be3ba2b5938b
## 
## Checking for known data issues.
## Loading downloaded data for masking and aggregation.
## Masking according to shape/buffer polygon
## Aggregating to temporal resolution of choice
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
  |                                                                                      
  |                                                                                |   0%
  |                                                                                      
  |===========================                                                     |  33%
  |                                                                                      
  |=====================================================                           |  67%
  |                                                                                      
  |================================================================================| 100%
## 
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Commencing Kriging
## Kriging of remaining 0 data layers should finish around: 2023-04-03 16:54:51
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
  |                                                                                      
  |                                                                                |   0%
  |                                                                                      
  |================================================================================| 100%
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So? Did we match the pre-existing data?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;NDVI_Krig[[1]]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## class      : RasterBrick 
## dimensions : 92, 108, 9936, 1  (nrow, ncol, ncell, nlayers)
## resolution : 0.08333333, 0.08333333  (x, y)
## extent     : 6, 15, 47.33333, 55  (xmin, xmax, ymin, ymax)
## crs        : +proj=longlat +datum=WGS84 +no_defs 
## source     : memory
## names      : var1.pred 
## min values :  275.9705 
## max values :  285.7357
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We nailed this!&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s take one final look at our (A) raw ERA5 data, (B) NDVI data, (C) Kriged ERA5 data, and (D) standard error of our Kriging output:&lt;/p&gt;
&lt;details&gt;&lt;summary&gt; Click here for download plotting calls &lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;### ERA-Plot
ERA_df &amp;lt;- as.data.frame(raster(file.path(Dir.Exports, &amp;quot;2m_temperature_2015-01-01_2015-12-31_year.nc&amp;quot;)), xy = TRUE) # turn raster into dataframe
colnames(ERA_df)[c(-1,-2)] &amp;lt;- &amp;quot;Air Temperature 2015 (ERA5)&amp;quot;
ERA_df &amp;lt;- gather(data = ERA_df, key = Values, value = &amp;quot;value&amp;quot;, colnames(ERA_df)[c(-1,-2)]) #  make ggplot-ready
Raw_plot &amp;lt;- ggplot() + # create a plot
  geom_raster(data = ERA_df , aes(x = x, y = y, fill = value)) + # plot the raw data
  facet_wrap(~Values) + # split raster layers up
  theme_bw() + labs(x = &amp;quot;Longitude&amp;quot;, y = &amp;quot;Latitude&amp;quot;) + # make plot more readable
  scale_fill_gradientn(name = &amp;quot;Air Temperature [K]&amp;quot;, colours = inferno(100)) + # add colour and legend
  geom_polygon(data = Shape_shp, aes(x = long, y = lat, group = group), colour = &amp;quot;black&amp;quot;, fill = &amp;quot;NA&amp;quot;) # add shape
### NDVI-Plot
NDVI_df &amp;lt;- as.data.frame(NDVI_ras, xy = TRUE) # turn raster into dataframe
colnames(NDVI_df)[c(-1,-2)] &amp;lt;- &amp;quot;NDVI 2015&amp;quot;
NDVI_df &amp;lt;- gather(data = NDVI_df, key = Values, value = &amp;quot;value&amp;quot;, colnames(NDVI_df)[c(-1,-2)]) #  make ggplot-ready
NDVI_plot &amp;lt;- ggplot() + # create a plot
  geom_raster(data = NDVI_df , aes(x = x, y = y, fill = value)) + # plot the raw data
  facet_wrap(~Values) + # split raster layers up
  theme_bw() + labs(x = &amp;quot;Longitude&amp;quot;, y = &amp;quot;Latitude&amp;quot;) + # make plot more readable
  scale_fill_gradientn(name = &amp;quot;NDVI&amp;quot;, colours = rev(terrain.colors(100))) + # add colour and legend
  geom_polygon(data = Shape_shp, aes(x = long, y = lat, group = group), colour = &amp;quot;black&amp;quot;, fill = &amp;quot;NA&amp;quot;) # add shape
### KRIGED-Plots
Dates = c(&amp;quot;Kriged Air Temperature 2015 (NDVI Resolution)&amp;quot;)
Type_vec &amp;lt;- c(&amp;quot;Prediction&amp;quot;, &amp;quot;Standard Error&amp;quot;) # these are the output types of krigR
Colours_ls &amp;lt;- list(inferno(100), rev(viridis(100))) # we want separate colours for the types
Plots_ls &amp;lt;- as.list(NA, NA) # this list will be filled with the output plots
KrigDF_ls &amp;lt;- as.list(NA, NA) # this list will be filled with the output data
for(Plot in 1:2){ # loop over both output types
  Krig_df &amp;lt;- as.data.frame(NDVI_Krig[[Plot]], xy = TRUE) # turn raster into dataframe
  colnames(Krig_df)[c(-1,-2)] &amp;lt;- paste(Type_vec[Plot], Dates) # set colnames
  Krig_df &amp;lt;- gather(data = Krig_df, key = Values, value = &amp;quot;value&amp;quot;, colnames(Krig_df)[c(-1,-2)]) # make ggplot-ready
  Plots_ls[[Plot]] &amp;lt;- ggplot() + # create plot
    geom_raster(data = Krig_df , aes(x = x, y = y, fill = value)) + # plot the kriged data
    facet_wrap(~Values) + # split raster layers up
    theme_bw() + labs(x = &amp;quot;Longitude&amp;quot;, y = &amp;quot;Latitude&amp;quot;) + # make plot more readable
    scale_fill_gradientn(name = &amp;quot;Air Temperature [K]&amp;quot;, colours = Colours_ls[[Plot]]) + # add colour and legend
    theme(plot.margin = unit(c(0, 0, 0, 0), &amp;quot;cm&amp;quot;)) + # reduce margins (for fusing of plots)
    geom_polygon(data = Shape_shp, aes(x = long, y = lat, group = group), colour = &amp;quot;black&amp;quot;, fill = &amp;quot;NA&amp;quot;) # add shape
  KrigDF_ls[[Plot]] &amp;lt;- Krig_df
} # end of type-loop
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot_grid(plotlist = list(Raw_plot, NDVI_plot, Plots_ls[[1]], Plots_ls[[2]]), 
          nrow = 2, labels = &amp;quot;AUTO&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-third-party_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So what can we learn from this? Let&amp;rsquo;s plot the relation between temperature and NDVI:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot_df &amp;lt;- as.data.frame(cbind(KrigDF_ls[[1]][,4], 
                               KrigDF_ls[[2]][,4],
                               NDVI_df[,4]))
colnames(plot_df) &amp;lt;- c(&amp;quot;Temperature&amp;quot;, &amp;quot;Uncertainty&amp;quot;, &amp;quot;NDVI&amp;quot;)
ggplot(plot_df,
       aes(x = Temperature, y = NDVI, size = Uncertainty)) + 
  geom_point(alpha = 0.15) + 
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-third-party_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looks like NDVI increases as mean annual temperatures rise, but reaches a peak around 281-282 Kelvin with a subsequent decrease as mean annual temperatures rise higher.&lt;/p&gt;
&lt;h2 id=&#34;using-third-party-data&#34;&gt;Using Third-Party Data&lt;/h2&gt;
&lt;div class=&#34;alert alert-danger&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;ATTENTION:&lt;/strong&gt; Kriging only works on &lt;strong&gt;square-cell spatial products&lt;/strong&gt;!
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The &lt;code&gt;krigR()&lt;/code&gt; function is designed to work with non-ERA5(-Land) data as well as non-GMTED2010 covariate data. To downscale your own spatial products using different covariate data than the GMTED2010 DEM we use as a default, you need to step into the three-step workflow.&lt;/p&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    Most spatial products won&amp;rsquo;t be reliably downscaled using only elevation covariate data.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;code&gt;krigR()&lt;/code&gt; supports any combination of ERA5-family reanalysis, GMTED2010, third-party climate data, and third-party covariate data. Here, we just demonstrate the use of other covariates than the GMTED2010 used by &lt;code&gt;KrigR&lt;/code&gt; by default.&lt;/p&gt;
&lt;p&gt;The product we will focus on here is the soil moisture data contained in our &lt;code&gt;BCq_ras&lt;/code&gt; product established 
&lt;a href=&#34;https://www.erikkusch.com/courses/krigr/bioclim/#water-availability-variables&#34;&gt;here&lt;/a&gt;. With this data set, we also revert back to our original 
&lt;a href=&#34;https://www.erikkusch.com/courses/krigr/prep/#shape-of-interest-spatialpolygons&#34;&gt;study region&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;The reason we focus on soil moisture for this exercise? In 
&lt;a href=&#34;https://iopscience.iop.org/article/10.1088/1748-9326/ac39bf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this publication (Figure 3)&lt;/a&gt;, we demonstrate that soil moisture data can be statistically downscales using kriging with some third-party covariates. As such, we pick up from where we left off when we discussed 
&lt;a href=&#34;https://www.erikkusch.com/courses/krigr/bioclim/#water-availability-3&#34;&gt;kriging of bioclimatic products&lt;/a&gt;.&lt;/p&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for file:&lt;/summary&gt;
      Download 
&lt;a href=&#34;https://github.com/ErikKusch/Homepage/raw/master/content/courses/krigr/Data/Qsoil_BC.nc&#34;&gt;Qsoil_BC.nc&lt;/a&gt; and place it into your data directory.
&lt;/details&gt; 
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;BCq_ras &amp;lt;- stack(file.path(Dir.Data, &amp;quot;Qsoil_BC.nc&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;third-party-data-covariates&#34;&gt;Third-Party Data Covariates&lt;/h3&gt;
&lt;p&gt;In 
&lt;a href=&#34;https://iopscience.iop.org/article/10.1088/1748-9326/ac39bf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this publication&lt;/a&gt;, we demonstrate how soil moisture data can be reliably statistically downscaled using soil property data which we obtain from the 
&lt;a href=&#34;http://globalchange.bnu.edu.cn/research/soil4.jsp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Land-Atmosphere Interaction Research Group at Sun Yat-sen University&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Below, you will find the code needed to obtain the data of global coverage at roughly 1km spatial resolution. The code chunk below also crops and masks the data according to our study region and subsequently deletes the storage-heavy global files (3.5GB each in size). This process takes a long time due to download speeds.&lt;/p&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for the covariate file to save yourself downloading and processing of global data:&lt;/summary&gt;
      Download 
&lt;a href=&#34;https://github.com/ErikKusch/Homepage/raw/master/content/courses/krigr/Covariates/SoilCovs.nc&#34;&gt;SoilCovs.nc&lt;/a&gt; and place it into your covariates directory.
&lt;/details&gt; 
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# documentation of these can be found here http://globalchange.bnu.edu.cn/research/soil4.jsp
SoilCovs_vec &amp;lt;- c(&amp;quot;tkdry&amp;quot;, &amp;quot;tksat&amp;quot;, &amp;quot;csol&amp;quot;, &amp;quot;k_s&amp;quot;, &amp;quot;lambda&amp;quot;, &amp;quot;psi&amp;quot;, &amp;quot;theta_s&amp;quot;) # need these names for addressing soil covariates
if(!file.exists(file.path(Dir.Covariates, &amp;quot;SoilCovs.nc&amp;quot;))){
  print(&amp;quot;#### Loading SOIL PROPERTY covariate data. ####&amp;quot;) 
  # create lists to combine soil data into one
  SoilCovs_ls &amp;lt;- as.list(rep(NA, length(SoilCovs_vec)))
  names(SoilCovs_ls) &amp;lt;- c(SoilCovs_vec)
  ## Downloading, unpacking, and loading
  for(Soil_Iter in SoilCovs_vec){
    if(!file.exists(file.path(Dir.Covariates, paste0(Soil_Iter, &amp;quot;.nc&amp;quot;)))) { # if not downloaded and processed yet
      print(paste(&amp;quot;Handling&amp;quot;, Soil_Iter, &amp;quot;data.&amp;quot;))
      Dir.Soil &amp;lt;- file.path(Dir.Covariates, Soil_Iter)
      dir.create(Dir.Soil)
      download.file(paste0(&amp;quot;http://globalchange.bnu.edu.cn/download/data/worldptf/&amp;quot;, Soil_Iter,&amp;quot;.zip&amp;quot;),
                    destfile = file.path(Dir.Soil, paste0(Soil_Iter, &amp;quot;.zip&amp;quot;))
      ) # download data
      unzip(file.path(Dir.Soil, paste0(Soil_Iter, &amp;quot;.zip&amp;quot;)), exdir = Dir.Soil) # unzip data
      File &amp;lt;- list.files(Dir.Soil, pattern = &amp;quot;.nc&amp;quot;)[1] # only keep first soil layer
      Soil_ras &amp;lt;- raster(file.path(Dir.Soil, File)) # load data
      SoilCovs_ls[[which(names(SoilCovs_ls) == Soil_Iter)]] &amp;lt;- Soil_ras # save to list
      writeRaster(x = Soil_ras, filename = file.path(Dir.Covariates, Soil_Iter), format = &amp;quot;CDF&amp;quot;)
      unlink(Dir.Soil, recursive = TRUE)
    }else{
      print(paste(Soil_Iter, &amp;quot;already downloaded and processed.&amp;quot;))
      SoilCovs_ls[[which(names(SoilCovs_ls) == Soil_Iter)]] &amp;lt;- raster(file.path(Dir.Covariates, paste0(Soil_Iter, &amp;quot;.nc&amp;quot;)))
    }
  }
  ## data handling and manipulation
  SoilCovs_stack &amp;lt;- stack(SoilCovs_ls) # stacking raster layers from list
  SoilCovs_stack &amp;lt;- crop(SoilCovs_stack, extent(BCq_ras)) # cropping to extent of data we have
  SoilCovs_mask &amp;lt;- KrigR::mask_Shape(SoilCovs_stack[[1]], Shape = Shape_shp) # create mask with KrigR-internal function to ensure all edge cells are contained
  SoilCovs_stack &amp;lt;- mask(SoilCovs_stack, SoilCovs_mask) # mask out shape
  ## writing the data
  writeRaster(x = SoilCovs_stack, filename = file.path(Dir.Covariates, &amp;quot;SoilCovs&amp;quot;), format = &amp;quot;CDF&amp;quot;)
  ## removing the global files due to their size
  unlink(file.path(Dir.Covariates, paste0(SoilCovs_vec, &amp;quot;.nc&amp;quot;)))
}
SoilCovs_stack &amp;lt;- stack(file.path(Dir.Covariates, &amp;quot;SoilCovs.nc&amp;quot;))
names(SoilCovs_stack) &amp;lt;- SoilCovs_vec
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at these data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;SoilCovs_stack
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## class      : RasterStack 
## dimensions : 408, 648, 264384, 7  (nrow, ncol, ncell, nlayers)
## resolution : 0.008333333, 0.008333333  (x, y)
## extent     : 9.725, 15.125, 49.75, 53.15  (xmin, xmax, ymin, ymax)
## crs        : +proj=longlat +datum=WGS84 +no_defs 
## names      :         tkdry,         tksat,          csol,           k_s,        lambda,           psi,       theta_s 
## min values :  5.200000e-02,  1.337000e+00,  2.141000e+06,  5.212523e+00,  8.600000e-02, -5.307258e+01,  3.230000e-01 
## max values :  2.070000e-01,  2.862000e+00,  2.346400e+06,  2.461686e+02,  3.330000e-01, -5.205317e+00,  5.320000e-01
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we need to establish target and training resolution of our covariate data.&lt;/p&gt;
&lt;p&gt;First, we focus on the training resolution covariate data. We match our covariate data to our spatial product which we wish to downscale by resampling the covariate data to the coarser resolution:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Coarsecovs &amp;lt;- resample(x = SoilCovs_stack, y = BCq_ras)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Second, we aggregate the covariate data to our desired resolution. In this case, 0.02 as done previously 
&lt;a href=&#34;https://www.erikkusch.com/courses/krigr/bioclim/#kriging-bioclimatic-products&#34;&gt;here&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Finecovs &amp;lt;- aggregate(SoilCovs_stack, fact = 0.02/res(SoilCovs_stack)[1])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we combine these into a list like the output of &lt;code&gt;download_DEM()&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Covs_ls &amp;lt;- list(Coarsecovs, Finecovs)
Plot_Covs(Covs = Covs_ls, Shape_shp)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-third-party_files/figure-html/SoilCovs-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;div class=&#34;alert alert-&#34;&gt;
  &lt;div&gt;
    Our &lt;a href=&#34;https://www.erikkusch.com/courses/krigr/outlook/&#34;&gt;development goals&lt;/a&gt; include creating a function that automatically carries out all of the above for you with a specification alike to &lt;code&gt;download_DEM()&lt;/code&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;kriging-third-party-data&#34;&gt;Kriging Third-Party Data&lt;/h3&gt;
&lt;p&gt;Finally, we can statistically downscale our soil moisture data using the soil property covariates. For this, we need to specify a new &lt;code&gt;KrigingEquation&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;alert alert-&#34;&gt;
  &lt;div&gt;
    With the &lt;code&gt;KrigingEquation&lt;/code&gt; argument, you may specify non-linear combinations of covariates for your call to &lt;code&gt;krigR()&lt;/code&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-info&#34;&gt;
  &lt;div&gt;
    If you don&amp;rsquo;t specify a &lt;code&gt;KrigingEquation&lt;/code&gt; in &lt;code&gt;krigR()&lt;/code&gt; and your covariates do not contain a layer called &lt;code&gt;&amp;quot;DEM&amp;quot;&lt;/code&gt;, &lt;code&gt;krigR()&lt;/code&gt; will notify you that its default formula cannot be executed and will attempt to build an additive formula from the data it can find. &lt;code&gt;krigr()&lt;/code&gt; will inform you of this and ask for your approval before proceeding.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;This auto-generated formula would be the same as the one we specify here - an additive combination of all covariates found both at coarse and fine resolutions. Of course, this formula can also be specified to reflect interactive effects.&lt;/p&gt;
&lt;p&gt;Here, I automate the generation of our &lt;code&gt;KrigingEquation&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;KrigingEquation &amp;lt;- paste0(&amp;quot;ERA ~ &amp;quot;, paste(SoilCovs_vec, collapse = &amp;quot; + &amp;quot;))
KrigingEquation
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;ERA ~ tkdry + tksat + csol + k_s + lambda + psi + theta_s&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In accordance with our 
&lt;a href=&#34;https://www.erikkusch.com/courses/krigr/bioclim/#temperatures-3&#34;&gt;downscaling of the temperature-portion of the bioclimatic data&lt;/a&gt;, (1) I only hand the last 8 layers to the kriging call because those are the soil moisture data, (2) I leave out the &lt;code&gt;Cores&lt;/code&gt; argument, so that &lt;code&gt;krigR()&lt;/code&gt; determines how many cores your machine has and uses all of them to speed up the computation of the multi-layer raster, and (3) I set &lt;code&gt;nmax&lt;/code&gt; to 80 to approximate a typical weather system in size:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;BC_Water_Krig  &amp;lt;- krigR(Data = BCq_ras[[12:19]], 
                    Covariates_coarse = Covs_ls[[1]], 
                    Covariates_fine = Covs_ls[[2]],
                    KrigingEquation = KrigingEquation, 
                    FileName = &amp;quot;BC_Water_Krig&amp;quot;,
                    Dir = Dir.Covariates,
                    nmax = 80
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: [writeCDF] for better results use file extension &#39;.nc&#39; or &#39;.cdf&#39;
## see: https://stackoverflow.com/a/65398262/635245
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: [rast] unknown extent
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: [writeCDF] for better results use file extension &#39;.nc&#39; or &#39;.cdf&#39;
## see: https://stackoverflow.com/a/65398262/635245
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: [rast] unknown extent
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;bio12---annual-mean-soil-moisture&#34;&gt;BIO12 - Annual Mean Soil Moisture&lt;/h4&gt;
&lt;div class=&#34;alert alert-normal&#34;&gt;
  &lt;div&gt;
    Interpolating this data is just like statistically downscaling any other soil moisture product and can be done without any problems.
  &lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for plotting call and plot:&lt;/summary&gt;
&lt;div class=&#34;alert alert-normal&#34;&gt;
  &lt;div&gt;
    Look at how well the river Elbe sows up in this!
  &lt;/div&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Plot_Krigs(lapply(BC_Water_Krig[-3], &amp;quot;[[&amp;quot;, 1), 
           Shp = Shape_shp,
           Dates = &amp;quot;BIO12 - Annual Mean Soil Moisture&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-third-party_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;h4 id=&#34;bio13---soil-moisture-of-wettest-month&#34;&gt;BIO13 - Soil Moisture of Wettest Month&lt;/h4&gt;
&lt;div class=&#34;alert alert-normal&#34;&gt;
  &lt;div&gt;
    Interpolating this data is just like statistically downscaling any other soil moisture product and can be done without any problems.
  &lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for plotting call and plot:&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Plot_Krigs(lapply(BC_Water_Krig[-3], &amp;quot;[[&amp;quot;, 2), 
           Shp = Shape_shp,
           Dates = &amp;quot;BIO13 - Soil Moisture of Wettest Month&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-third-party_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;h4 id=&#34;bio14---soil-moisture-of-driest-month&#34;&gt;BIO14 - Soil Moisture of Driest Month&lt;/h4&gt;
&lt;div class=&#34;alert alert-normal&#34;&gt;
  &lt;div&gt;
    Interpolating this data is just like statistically downscaling any other soil moisture product and can be done without any problems.
  &lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for plotting call and plot:&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Plot_Krigs(lapply(BC_Water_Krig[-3], &amp;quot;[[&amp;quot;, 3), 
           Shp = Shape_shp,
           Dates = &amp;quot;BIO13 - Soil Moisture of Driest Month&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-third-party_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;h4 id=&#34;bio15---soil-moisture-seasonality&#34;&gt;BIO15 - Soil Moisture Seasonality&lt;/h4&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    This data product is calculated using the standard deviation of mean values throughout our time frame. Conclusively, it would be interpolated better by first statistically downscaling the underlying data rather than the final bioclimatic variable.
  &lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for plotting call and plot:&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Plot_Krigs(lapply(BC_Water_Krig[-3], &amp;quot;[[&amp;quot;, 4), 
           Shp = Shape_shp,
           Dates = &amp;quot;BIO15 - Precipitation Seasonality&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-third-party_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;h4 id=&#34;bio16--bio17---soil-moisture-of-wettest-and-driest-quarter&#34;&gt;BIO16 &amp;amp; BIO17 - Soil Moisture of Wettest and Driest Quarter&lt;/h4&gt;
&lt;div class=&#34;alert alert-danger&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;I do not recommend you use these kriging outputs!&lt;/strong&gt; They rely on mean quarterly soil moisture data which is not being interpolated here. Subsequently, the patchiness of the underlying data is lost and with it: information.
  &lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for plotting call and plot:&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Plot_Krigs(lapply(BC_Water_Krig[-3], &amp;quot;[[&amp;quot;, 5:6), 
           Shp = Shape_shp,
           Dates = c(&amp;quot;BIO16 - Soil Moisture of Wettest Quarter&amp;quot;, 
                     &amp;quot;BIO17 - Soil Moisture of Driest Quarter&amp;quot;)
           )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-third-party_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;h4 id=&#34;bio18--bio19---precipitation-of-warmest-and-coldest-quarter&#34;&gt;BIO18 &amp;amp; BIO19 - Precipitation of Warmest and Coldest Quarter&lt;/h4&gt;
&lt;div class=&#34;alert alert-danger&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;I do not recommend you use these kriging outputs!&lt;/strong&gt; They rely on mean quarterly temperature data which is not being interpolated here. Subsequently, the patchiness of the underlying data is lost and with it: information.
  &lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for plotting call and plot:&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Plot_Krigs(lapply(BC_Water_Krig[-3], &amp;quot;[[&amp;quot;, 7:8), 
           Shp = Shape_shp,
           Dates = c(&amp;quot;BIO16 - Soil Moisture of Warmest Quarter&amp;quot;, 
                     &amp;quot;BIO17 - Soil Moisture of Coldest Quarter&amp;quot;)
           )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-third-party_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;This concludes our exercise for using third-party data in &lt;code&gt;KrigR&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.2.3 (2023-03-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Big Sur ... 10.16
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] mapview_2.11.0          rnaturalearthdata_0.1.0 rnaturalearth_0.3.2    
##  [4] gimms_1.2.1             ggmap_3.0.2             cowplot_1.1.1          
##  [7] viridis_0.6.2           viridisLite_0.4.1       ggplot2_3.4.1          
## [10] tidyr_1.3.0             KrigR_0.1.2             terra_1.7-21           
## [13] httr_1.4.5              stars_0.6-0             abind_1.4-5            
## [16] fasterize_1.0.4         sf_1.0-12               lubridate_1.9.2        
## [19] automap_1.1-9           doSNOW_1.0.20           snow_0.4-4             
## [22] doParallel_1.0.17       iterators_1.0.14        foreach_1.5.2          
## [25] rgdal_1.6-5             raster_3.6-20           sp_1.6-0               
## [28] stringr_1.5.0           keyring_1.3.1           ecmwfr_1.5.0           
## [31] ncdf4_1.21             
## 
## loaded via a namespace (and not attached):
##  [1] leafem_0.2.0             colorspace_2.1-0         class_7.3-21            
##  [4] leaflet_2.1.2            satellite_1.0.4          base64enc_0.1-3         
##  [7] rstudioapi_0.14          proxy_0.4-27             farver_2.1.1            
## [10] fansi_1.0.4              codetools_0.2-19         cachem_1.0.7            
## [13] knitr_1.42               jsonlite_1.8.4           png_0.1-8               
## [16] Kendall_2.2.1            compiler_4.2.3           assertthat_0.2.1        
## [19] fastmap_1.1.1            cli_3.6.0                htmltools_0.5.4         
## [22] tools_4.2.3              gtable_0.3.1             glue_1.6.2              
## [25] dplyr_1.1.0              Rcpp_1.0.10              jquerylib_0.1.4         
## [28] vctrs_0.6.1              blogdown_1.16            crosstalk_1.2.0         
## [31] lwgeom_0.2-11            xfun_0.37                timechange_0.2.0        
## [34] lifecycle_1.0.3          rnaturalearthhires_0.2.1 zoo_1.8-11              
## [37] scales_1.2.1             gstat_2.1-0              yaml_2.3.7              
## [40] curl_5.0.0               memoise_2.0.1            gridExtra_2.3           
## [43] sass_0.4.5               reshape_0.8.9            stringi_1.7.12          
## [46] highr_0.10               e1071_1.7-13             boot_1.3-28.1           
## [49] intervals_0.15.3         RgoogleMaps_1.4.5.3      rlang_1.1.0             
## [52] pkgconfig_2.0.3          bitops_1.0-7             evaluate_0.20           
## [55] lattice_0.20-45          purrr_1.0.1              htmlwidgets_1.6.1       
## [58] labeling_0.4.2           tidyselect_1.2.0         plyr_1.8.8              
## [61] magrittr_2.0.3           bookdown_0.33            R6_2.5.1                
## [64] generics_0.1.3           DBI_1.1.3                pillar_1.8.1            
## [67] withr_2.5.0              units_0.8-1              xts_0.13.0              
## [70] tibble_3.2.1             spacetime_1.2-8          KernSmooth_2.23-20      
## [73] utf8_1.2.3               rmarkdown_2.20           jpeg_0.1-10             
## [76] grid_4.2.3               zyp_0.11-1               FNN_1.1.3.2             
## [79] digest_0.6.31            classInt_0.4-9           webshot_0.5.4           
## [82] stats4_4.2.3             munsell_0.5.0            bslib_0.4.2
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Projection Downscaling</title>
      <link>https://www.erikkusch.com/courses/krigr/projections/</link>
      <pubDate>Thu, 26 May 2022 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/krigr/projections/</guid>
      <description>&lt;p&gt;&lt;div class=&#34;alert alert-danger&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;&lt;code&gt;KrigR&lt;/code&gt; is currently undergoing development. As a result, this part of the workshop has become deprecated. Please refer to the &lt;a href=&#34;https://www.erikkusch.com/courses/krigr/setup/&#34;&gt;setup&lt;/a&gt; &lt;a href=&#34;https://www.erikkusch.com/courses/krigr/quickstart/&#34;&gt;quick guide&lt;/a&gt; portions of this material as these are up-to-date. &lt;/strong&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;div class=&#34;alert alert-danger&#34;&gt;
  &lt;div&gt;
    This part of the workshop is dependant on set-up and preparation done previously &lt;a href=&#34;https://www.erikkusch.com/courses/krigr/prep/&#34;&gt;here&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;First, we load &lt;code&gt;KrigR&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(KrigR)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I expect that you will often be interested not just in past and current climatic conditions, but also in future projections of climate data at high spatial resolutions.&lt;/p&gt;
&lt;div class=&#34;alert alert-info&#34;&gt;
  &lt;div&gt;
    The &lt;code&gt;KrigR&lt;/code&gt; workflow can be used to establish high-resolution, bias-corrected climate projection products.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;This time, we run our exercise for all of Germany because of its size and topographical variety.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Shape_shp &amp;lt;- ne_countries(country = &amp;quot;Germany&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;krigr-process-for-projections&#34;&gt;&lt;code&gt;KrigR&lt;/code&gt; Process for Projections&lt;/h2&gt;
&lt;p&gt;We published the the &lt;code&gt;KrigR&lt;/code&gt; workflow for downscaled climate projections in 
&lt;a href=&#34;https://iopscience.iop.org/article/10.1088/1748-9326/ac48b3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this publication (Section 3.5)&lt;/a&gt; and I will walk you through the contents thereof here.&lt;/p&gt;
&lt;p&gt;To achieve downscaled projection products we require three data products:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Historical climate data from ERA5(-Land)&lt;/li&gt;
&lt;li&gt;Historical climate data from projection source&lt;/li&gt;
&lt;li&gt;Future climate data from projection source&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Subsequently, the data products are downscaled to the desired spatial resolution using &lt;code&gt;krigR()&lt;/code&gt;. Finally, the difference between the downscaled projection-sourced data are added to the historical baseline obtained from (downscaled) ERA5(-Land) data. This achieves bias correction.&lt;/p&gt;
&lt;h3 id=&#34;obtaining-era5-land-data&#34;&gt;Obtaining ERA5(-Land) Data&lt;/h3&gt;
&lt;p&gt;Now, let&amp;rsquo;s obtain the historical baseline from ERA5-Land for the same time-period as our CMIP6 historical data.&lt;/p&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for file if download takes too long:&lt;/summary&gt;
    Download 
&lt;a href=&#34;https://github.com/ErikKusch/Homepage/raw/master/content/courses/krigr/Data/Germany_Hist_ERA.nc&#34;&gt;Germany_Hist_ERA.nc&lt;/a&gt; and place it into your data directory.
&lt;/details&gt; 
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;if(!file.exists(file.path(Dir.Data, &amp;quot;Germany_Hist_ERA.nc&amp;quot;))){
  Hist_ERA_ras &amp;lt;- download_ERA(Variable = &amp;quot;2m_temperature&amp;quot;,
                               DateStart = &amp;quot;1981-01-01&amp;quot;,
                               DateStop = &amp;quot;1999-12-31&amp;quot;,
                               TResolution = &amp;quot;month&amp;quot;,
                               TStep = 1,
                               Extent = Shape_shp,
                               Dir = Dir.Data,
                               FileName = &amp;quot;Germany_Hist_ERA&amp;quot;, 
                               API_Key = API_Key,
                               API_User = API_User,
                               SingularDL = TRUE)
  Index &amp;lt;- rep(1:12, length = nlayers(Hist_ERA_ras))
  Hist_ERA_ras &amp;lt;- stackApply(Hist_ERA_ras, indices = Index, fun = mean)
  writeRaster(Hist_ERA_ras, filename = file.path(Dir.Data, &amp;quot;Germany_Hist_ERA&amp;quot;), format = &amp;quot;CDF&amp;quot;)
}
Hist_ERA_ras &amp;lt;- mean(stack(file.path(Dir.Data, &amp;quot;Germany_Hist_ERA.nc&amp;quot;)))
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;obtaining-projection-data&#34;&gt;Obtaining Projection Data&lt;/h3&gt;
&lt;p&gt;Here, we use CMIP6 projection data manually sourced from the 
&lt;a href=&#34;https://cds.climate.copernicus.eu/cdsapp#!/dataset/projections-cmip6?tab=overview&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ECMWF CDS distribution&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;alert alert-&#34;&gt;
  &lt;div&gt;
    Our &lt;a href=&#34;https://www.erikkusch.com/courses/krigr/outlook/&#34;&gt;development goals&lt;/a&gt; include development of &lt;code&gt;download_ERA()&lt;/code&gt; to work with other ECWMF CDS data sets aside from ERA5(-Land). This includes this CMIP6 data set.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h4 id=&#34;historical-baseline&#34;&gt;Historical Baseline&lt;/h4&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for file:&lt;/summary&gt;
    Download 
&lt;a href=&#34;https://github.com/ErikKusch/Homepage/raw/master/content/courses/krigr/Data/historical_tas_1981-2000.nc&#34;&gt;historical_tas_1981-2000.nc&lt;/a&gt; and place it into your data directory.
&lt;/details&gt; 
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;train_HIST &amp;lt;- mean(stack(file.path(Dir.Data, &amp;quot;historical_tas_1981-2000.nc&amp;quot;)))
train_HIST &amp;lt;- crop(train_HIST,extent(Hist_ERA_ras))
train_mask &amp;lt;- KrigR::mask_Shape(train_HIST, Shape_shp)
train_HIST &amp;lt;- mask(train_HIST, train_mask)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;future-projection&#34;&gt;Future Projection&lt;/h4&gt;
&lt;details&gt;
  &lt;summary&gt;Click here for file:&lt;/summary&gt;
    Download 
&lt;a href=&#34;https://github.com/ErikKusch/Homepage/raw/master/content/courses/krigr/Data/ssp585_tas_2041-2060.nc&#34;&gt;ssp585_tas_2041-2060.nc&lt;/a&gt; and place it into your data directory.
&lt;/details&gt; 
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;train_SSP &amp;lt;- mean(stack(file.path(Dir.Data, &amp;quot;ssp585_tas_2041-2060.nc&amp;quot;)))
train_SSP &amp;lt;- crop(train_SSP,extent(Hist_ERA_ras))
train_mask &amp;lt;- KrigR::mask_Shape(train_SSP, Shape_shp)
train_SSP &amp;lt;- mask(train_SSP, train_mask)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;visualisation-of-cmip6-data&#34;&gt;Visualisation of CMIP6 Data&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Plot_Raw(stack(train_HIST, train_SSP), 
         Shp = Shape_shp,
         Dates = c(&amp;quot;Historic CMIP6&amp;quot;, &amp;quot;Future CMIP6&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-projections_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Already, we can see that quite a bit of warming is projected to happen all across Germany. However, we want to know about this at higher spatial resolutions. That&amp;rsquo;s where &lt;code&gt;KrigR&lt;/code&gt; comes in.&lt;/p&gt;
&lt;h3 id=&#34;establishing-kriged-products&#34;&gt;Establishing Kriged Products&lt;/h3&gt;
&lt;p&gt;For the first time in this workshop material, we will push our spatial resolution to the finest scale supported by our default GMTED 2010 DEM covariate data: 0.008333 / ~1km.&lt;/p&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    These operations take quite some time - grab a tea or coffee, go for a walk, or stretch a bit.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The downscaling calls should be familiar by now so I will forego explaining them. In case, the following code snippets do not make sense to you, please consult the portion of this workshop concerned with 
&lt;a href=&#34;https://www.erikkusch.com/courses/krigr/kriging/&#34;&gt;statistical downscaling&lt;/a&gt;.&lt;/p&gt;
&lt;h4 id=&#34;historical-cmip6&#34;&gt;Historical CMIP6&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Covariate Data
GMTED_DE &amp;lt;- download_DEM(
  Train_ras = train_HIST,
  Target_res = 0.008334,
  Shape = Shape_shp,
  Keep_Temporary = TRUE,
  Dir = Dir.Covariates
)
## Kriging
Output_HIST &amp;lt;- krigR(
  Data = train_HIST,
  Covariates_coarse = GMTED_DE[[1]], 
  Covariates_fine = GMTED_DE[[2]],  
  Keep_Temporary = FALSE,
  Cores = 1,
  Dir = Dir.Exports,  
  FileName = &amp;quot;DE_CMIP-HIST&amp;quot;, 
  nmax = 40
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Plot_Krigs(Output_HIST,
           Shp = Shape_shp,
           Dates = &amp;quot;CMIP6 Historical&amp;quot;, columns = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-projections_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h4 id=&#34;future-cmip6&#34;&gt;Future CMIP6&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Covariate Data
GMTED_DE &amp;lt;- download_DEM(
  Train_ras = train_SSP,
  Target_res = 0.008334,
  Shape = Shape_shp,
  Keep_Temporary = TRUE,
  Dir = Dir.Covariates
)
## Kriging
Output_SSP &amp;lt;- krigR(
  Data = train_SSP,
  Covariates_coarse = GMTED_DE[[1]], 
  Covariates_fine = GMTED_DE[[2]],   
  Keep_Temporary = FALSE,
  Cores = 1,
  Dir = Dir.Exports,  
  FileName = &amp;quot;DE_SSP585_2041-2060&amp;quot;, 
  nmax = 40
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Plot_Krigs(Output_SSP,
           Shp = Shape_shp,
           Dates = &amp;quot;CMIP6 Future&amp;quot;, columns = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-projections_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h4 id=&#34;historical-era5-land&#34;&gt;Historical ERA5-Land&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Covariate Data
GMTED_DE &amp;lt;- download_DEM(
  Train_ras = Hist_ERA_ras,
  Target_res = 0.008334,
  Shape = Shape_shp,
  Keep_Temporary = TRUE,
  Dir = Dir.Covariates
)
## Kriging
Output_ERA &amp;lt;- krigR(
  Data = Hist_ERA_ras,
  Covariates_coarse = GMTED_DE[[1]], 
  Covariates_fine = GMTED_DE[[2]],   
  Keep_Temporary = FALSE,
  Cores = 1,
  Dir = Dir.Exports,  
  FileName = &amp;quot;DE_hist&amp;quot;, 
  nmax = 40
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Plot_Krigs(Output_ERA,
           Shp = Shape_shp,
           Dates = &amp;quot;ERA5-Land Historical&amp;quot;, columns = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-projections_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;putting-it-all-together&#34;&gt;Putting It All Together&lt;/h3&gt;
&lt;p&gt;To establish a final product of high-resolution climate projection data, we simply add the difference between the kriged CMIP6 products to the kriged ERA5-Land product:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Creating Difference and Projection raster
Difference_ras &amp;lt;- Output_SSP[[1]] - Output_HIST[[1]]
Projection_ras &amp;lt;- Output_ERA[[1]] + Difference_ras
## Adding min and max values to ocean cells to ensure same colour scale
Output_ERA[[1]][10] &amp;lt;- maxValue(Projection_ras)
Output_ERA[[1]][12] &amp;lt;- minValue(Projection_ras)
Projection_ras[10] &amp;lt;- maxValue(Output_ERA[[1]])
Projection_ras[12] &amp;lt;- minValue(Output_ERA[[1]])
## Individual plots
A_gg &amp;lt;- Plot_Raw(Output_ERA[[1]], Shp = Shape_shp, 
                 Dates = &amp;quot;Historical ERA5-Land (1981-2000)&amp;quot;)
B_gg &amp;lt;- Plot_Raw(Difference_ras[[1]], Shp = Shape_shp, 
                 Dates = &amp;quot;Anomalies of SSP585 - Historical CMIP-6&amp;quot;,
                 COL = rev(viridis(100)))
C_gg &amp;lt;- Plot_Raw(Projection_ras[[1]], Shp = Shape_shp, 
                 Dates = &amp;quot;Future Projection (ERA5-Land + Anomalies)&amp;quot;)
## Fuse the plots into one big plot
ggPlot &amp;lt;- plot_grid(plotlist = list(A_gg, B_gg, C_gg), 
                    ncol = 3, labels = &amp;quot;AUTO&amp;quot;) 
ggPlot
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;krigr-projections_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;1440&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And there we have it - a downscaled, bias-corrected projection of air temperature across Germany.&lt;/p&gt;
&lt;h2 id=&#34;considerations-for-projection-kriging&#34;&gt;Considerations for Projection Kriging&lt;/h2&gt;
&lt;p&gt;Projection kriging is easily the most flexible exercise you can undertake with &lt;code&gt;KrigR&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    I have submitted a research proposal to establish best practice for projection kriging.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;So far, two particular aspects stand out to me and should be considered by you when using &lt;code&gt;KrigR&lt;/code&gt; to obtain high-resolution projection data.&lt;/p&gt;
&lt;div class=&#34;alert alert-danger&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Do not statistically downscale precipitation data and do not use products that do so!&lt;/strong&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;reliability&#34;&gt;Reliability&lt;/h3&gt;
&lt;p&gt;Just like with all statistical downscaling exercises, it is pivotal to consider variables interpolated and consistency of statistical relationships with covariates across spatial resolutions.&lt;/p&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    Kriging is a very flexible tool for statistical interpolation. Consider your choice of covariates and change in resolutions carefully. &lt;strong&gt;Always inspect your data&lt;/strong&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;uncertainty&#34;&gt;Uncertainty&lt;/h3&gt;
&lt;p&gt;Integration of multiple kriged data sets with statistical uncertainty and each of which comes with its own underlying dynamical data uncertainty raises the question of how to combine uncertainties for meaningful uncertainty flags.&lt;/p&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    I have submitted a research proposal to assess best practice for uncertainty integration across data products.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.2.3 (2023-03-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Big Sur ... 10.16
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] mapview_2.11.0          rnaturalearthdata_0.1.0 rnaturalearth_0.3.2    
##  [4] gimms_1.2.1             ggmap_3.0.2             cowplot_1.1.1          
##  [7] viridis_0.6.2           viridisLite_0.4.1       ggplot2_3.4.1          
## [10] tidyr_1.3.0             KrigR_0.1.2             terra_1.7-21           
## [13] httr_1.4.5              stars_0.6-0             abind_1.4-5            
## [16] fasterize_1.0.4         sf_1.0-12               lubridate_1.9.2        
## [19] automap_1.1-9           doSNOW_1.0.20           snow_0.4-4             
## [22] doParallel_1.0.17       iterators_1.0.14        foreach_1.5.2          
## [25] rgdal_1.6-5             raster_3.6-20           sp_1.6-0               
## [28] stringr_1.5.0           keyring_1.3.1           ecmwfr_1.5.0           
## [31] ncdf4_1.21             
## 
## loaded via a namespace (and not attached):
##  [1] leafem_0.2.0             colorspace_2.1-0         class_7.3-21            
##  [4] leaflet_2.1.2            satellite_1.0.4          base64enc_0.1-3         
##  [7] rstudioapi_0.14          proxy_0.4-27             farver_2.1.1            
## [10] fansi_1.0.4              codetools_0.2-19         cachem_1.0.7            
## [13] knitr_1.42               jsonlite_1.8.4           png_0.1-8               
## [16] Kendall_2.2.1            compiler_4.2.3           assertthat_0.2.1        
## [19] fastmap_1.1.1            cli_3.6.0                htmltools_0.5.4         
## [22] tools_4.2.3              gtable_0.3.1             glue_1.6.2              
## [25] dplyr_1.1.0              Rcpp_1.0.10              jquerylib_0.1.4         
## [28] vctrs_0.6.1              blogdown_1.16            crosstalk_1.2.0         
## [31] lwgeom_0.2-11            xfun_0.37                timechange_0.2.0        
## [34] lifecycle_1.0.3          rnaturalearthhires_0.2.1 zoo_1.8-11              
## [37] scales_1.2.1             gstat_2.1-0              yaml_2.3.7              
## [40] curl_5.0.0               memoise_2.0.1            gridExtra_2.3           
## [43] sass_0.4.5               reshape_0.8.9            stringi_1.7.12          
## [46] highr_0.10               e1071_1.7-13             boot_1.3-28.1           
## [49] intervals_0.15.3         RgoogleMaps_1.4.5.3      rlang_1.1.0             
## [52] pkgconfig_2.0.3          bitops_1.0-7             evaluate_0.20           
## [55] lattice_0.20-45          purrr_1.0.1              htmlwidgets_1.6.1       
## [58] labeling_0.4.2           tidyselect_1.2.0         plyr_1.8.8              
## [61] magrittr_2.0.3           bookdown_0.33            R6_2.5.1                
## [64] generics_0.1.3           DBI_1.1.3                pillar_1.8.1            
## [67] withr_2.5.0              units_0.8-1              xts_0.13.0              
## [70] tibble_3.2.1             spacetime_1.2-8          KernSmooth_2.23-20      
## [73] utf8_1.2.3               rmarkdown_2.20           jpeg_0.1-10             
## [76] grid_4.2.3               zyp_0.11-1               FNN_1.1.3.2             
## [79] digest_0.6.31            classInt_0.4-9           webshot_0.5.4           
## [82] stats4_4.2.3             munsell_0.5.0            bslib_0.4.2
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>TwinEco - A Unified Framework for Dynamic Data-Driven Digital Twins in Ecology</title>
      <link>https://www.erikkusch.com/publication/twineco-a-unified-framework-for-dynamic-data-driven-digital-twins-in-ecology/</link>
      <pubDate>Wed, 24 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/publication/twineco-a-unified-framework-for-dynamic-data-driven-digital-twins-in-ecology/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Prototype biodiversity digital twin - crop wild relatives genetic resources for food security</title>
      <link>https://www.erikkusch.com/publication/prototype-biodiversity-digital-twin-crop-wild-relatives-genetic-resources-for-food-security/</link>
      <pubDate>Tue, 11 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/publication/prototype-biodiversity-digital-twin-crop-wild-relatives-genetic-resources-for-food-security/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Plant trait and vegetation data along a 1314m elevation gradient with fire history in Puna grasslands, PerÃº</title>
      <link>https://www.erikkusch.com/publication/plant-trait-and-vegetation-data-along-a-1314-m-elevation-gradient-with-fire-history-in-puna-grasslands-peru/</link>
      <pubDate>Wed, 21 Feb 2024 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/publication/plant-trait-and-vegetation-data-along-a-1314-m-elevation-gradient-with-fire-history-in-puna-grasslands-peru/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Accessing, handling, and referencing open biodiversity data using the Global Biodiversity Information Facility (GBIF)</title>
      <link>https://www.erikkusch.com/courses/gbif/</link>
      <pubDate>Sun, 29 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/gbif/</guid>
      <description>&lt;p&gt;If you are seeing this page, something went awry in the build of the website. Please find the first course material for this course 
&lt;a href=&#34;https://www.erikkusch.com/courses/gbif/theory/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Integrating Ecological Networks in Macroecological Research - Enhancing Projections of Biodiversity in the Anthropocene</title>
      <link>https://www.erikkusch.com/talk/2023_08_tangledbank/</link>
      <pubDate>Thu, 17 Aug 2023 14:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/talk/2023_08_tangledbank/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Novel Simulation Framework for Validation of Ecological Network Inference</title>
      <link>https://www.erikkusch.com/publication/a-novel-simulation-framework-for-validation-of-ecological-network-inference/</link>
      <pubDate>Mon, 07 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/publication/a-novel-simulation-framework-for-validation-of-ecological-network-inference/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Ecological Network Resilience &amp; Extinction Proxies - Updating Projections of Ecological Networks</title>
      <link>https://www.erikkusch.com/publication/ecological-network-resilience-extinction-proxies-updating-projections-of-ecological-networks/</link>
      <pubDate>Mon, 07 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/publication/ecological-network-resilience-extinction-proxies-updating-projections-of-ecological-networks/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Ecological Network Inference is not Consistent Across Scales or Approaches</title>
      <link>https://www.erikkusch.com/publication/ecological-network-inference-is-not-consistent-across-sales-or-approaches/</link>
      <pubDate>Wed, 02 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/publication/ecological-network-inference-is-not-consistent-across-sales-or-approaches/</guid>
      <description></description>
    </item>
    
    <item>
      <title>NetworkExtinction - An R Package to Simulate Extinction Propagation and Rewiring Potential in Ecological Networks</title>
      <link>https://www.erikkusch.com/publication/networkextinction-an-r-package-to-simulate-extinctions-propagation-and-rewiring-potential-in-ecological-networks/</link>
      <pubDate>Fri, 02 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/publication/networkextinction-an-r-package-to-simulate-extinctions-propagation-and-rewiring-potential-in-ecological-networks/</guid>
      <description></description>
    </item>
    
    <item>
      <title>MOSAIC - A Unified Trait Database to Complement Structured Population Models</title>
      <link>https://www.erikkusch.com/publication/mosaic-a-unified-trait-database-to-complement-structured-population-models/</link>
      <pubDate>Thu, 01 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/publication/mosaic-a-unified-trait-database-to-complement-structured-population-models/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Biodiversity Digitial Twin (BioDT)</title>
      <link>https://www.erikkusch.com/project/biodt/</link>
      <pubDate>Sat, 04 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/project/biodt/</guid>
      <description></description>
    </item>
    
    <item>
      <title>NetworkExtinction - an R package to simulate extinctionâs propagation and rewiring potential in ecological networks</title>
      <link>https://www.erikkusch.com/publication/in-review/networkextinction-an-r-package-to-simulate-extinctions-propagation-and-rewiring-potential-in-ecological-networks/</link>
      <pubDate>Mon, 05 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/publication/in-review/networkextinction-an-r-package-to-simulate-extinctions-propagation-and-rewiring-potential-in-ecological-networks/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Ecological network inference is not consistent across sales or approaches</title>
      <link>https://www.erikkusch.com/publication/in-review/ecological-network-inference-is-not-consistent-across-sales-or-approaches/</link>
      <pubDate>Tue, 09 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/publication/in-review/ecological-network-inference-is-not-consistent-across-sales-or-approaches/</guid>
      <description></description>
    </item>
    
    <item>
      <title>KrigR - Climate Data for your Study Needs</title>
      <link>https://www.erikkusch.com/talk/2022_06_oikos/</link>
      <pubDate>Mon, 06 Jun 2022 11:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/talk/2022_06_oikos/</guid>
      <description></description>
    </item>
    
    <item>
      <title>KrigR â A tool for downloading and statistically downscaling climate reanalysis data</title>
      <link>https://www.erikkusch.com/talk/2022_05_clim4ecol/</link>
      <pubDate>Mon, 06 Jun 2022 11:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/talk/2022_05_clim4ecol/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A novel trophic cascade between cougars and feral donkeys shapes desert wetlands</title>
      <link>https://www.erikkusch.com/publication/a-novel-trophic-cascade-between-cougars-and-feral-donkeys-shapes-desert-wetlands/</link>
      <pubDate>Mon, 23 May 2022 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/publication/a-novel-trophic-cascade-between-cougars-and-feral-donkeys-shapes-desert-wetlands/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A novel trophic cascade between cougars and feral donkeys shapes desert wetlands</title>
      <link>https://www.erikkusch.com/publication/journal-article/a-novel-trophic-cascade-between-cougars-and-feral-donkeys-shapes-desert-wetlands/</link>
      <pubDate>Mon, 23 May 2022 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/publication/journal-article/a-novel-trophic-cascade-between-cougars-and-feral-donkeys-shapes-desert-wetlands/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Vegetation memory effects and their association with vegetation resilience in global drylands</title>
      <link>https://www.erikkusch.com/publication/journal-article/vegetation-memory-effects-and-their-association-with-vegetation-resilience-in-global-drylands/</link>
      <pubDate>Wed, 13 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/publication/journal-article/vegetation-memory-effects-and-their-association-with-vegetation-resilience-in-global-drylands/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Vegetation memory effects and their association with vegetation resilience in global drylands</title>
      <link>https://www.erikkusch.com/publication/vegetation-memory-effects-and-their-association-with-vegetation-resilience-in-global-drylands/</link>
      <pubDate>Wed, 13 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/publication/vegetation-memory-effects-and-their-association-with-vegetation-resilience-in-global-drylands/</guid>
      <description></description>
    </item>
    
    <item>
      <title>MOSAIC - A Unified Trait Database to Complement Structured Population Models</title>
      <link>https://www.erikkusch.com/publication/in-review/mosaic-a-unified-trait-database-to-complement-structured-population-models/</link>
      <pubDate>Mon, 14 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/publication/in-review/mosaic-a-unified-trait-database-to-complement-structured-population-models/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Climate Data Pipelines for the 21st Century - Efficient Data Retrieval and Processing</title>
      <link>https://www.erikkusch.com/talk/2022_02_climate-coffee/</link>
      <pubDate>Thu, 24 Feb 2022 11:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/talk/2022_02_climate-coffee/</guid>
      <description></description>
    </item>
    
    <item>
      <title>KrigR â A tool for downloading and statistically downscaling climate reanalysis data</title>
      <link>https://www.erikkusch.com/publication/journal-article/krigr-a-tool-for-statistically-downscaling-climate-reanalysis-data-for-ecological-applications/</link>
      <pubDate>Fri, 07 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/publication/journal-article/krigr-a-tool-for-statistically-downscaling-climate-reanalysis-data-for-ecological-applications/</guid>
      <description></description>
    </item>
    
    <item>
      <title>KrigR â A tool for downloading and statistically downscaling climate reanalysis data</title>
      <link>https://www.erikkusch.com/publication/krigr-a-tool-for-statistically-downscaling-climate-reanalysis-data-for-ecological-applications/</link>
      <pubDate>Fri, 07 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/publication/krigr-a-tool-for-statistically-downscaling-climate-reanalysis-data-for-ecological-applications/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Data Simplification  for Ecological Network Inference</title>
      <link>https://www.erikkusch.com/talk/2021_12_bes/</link>
      <pubDate>Mon, 13 Dec 2021 11:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/talk/2021_12_bes/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Reconciling high resolution climate datasets using KrigR</title>
      <link>https://www.erikkusch.com/publication/journal-article/reconciling-high-resolution-climate-datasets-using-krigr/</link>
      <pubDate>Tue, 16 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/publication/journal-article/reconciling-high-resolution-climate-datasets-using-krigr/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Reconciling high resolution climate datasets using KrigR</title>
      <link>https://www.erikkusch.com/publication/reconciling-high-resolution-climate-datasets-using-krigr/</link>
      <pubDate>Tue, 16 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/publication/reconciling-high-resolution-climate-datasets-using-krigr/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Statistical Education for Biologists</title>
      <link>https://www.erikkusch.com/talk/2021_09_statisticaleducation/</link>
      <pubDate>Thu, 09 Sep 2021 16:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/talk/2021_09_statisticaleducation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Coding Practices in R</title>
      <link>https://www.erikkusch.com/talk/2021_07_salgoteam/</link>
      <pubDate>Thu, 08 Jul 2021 10:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/talk/2021_07_salgoteam/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An Introduction to Biostatistics</title>
      <link>https://www.erikkusch.com/courses/an-introduction-to-biostatistics/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/an-introduction-to-biostatistics/</guid>
      <description>&lt;p&gt;If you are seeing this page, something went awry in the build of the website. Please find the first course material for this course 
&lt;a href=&#34;https://www.erikkusch.com/courses/an-introduction-to-biostatistics/01-an-introduction-to-basic-statistics-for-biologists/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian Networks</title>
      <link>https://www.erikkusch.com/courses/bayes-nets/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/bayes-nets/</guid>
      <description>&lt;p&gt;If you are seeing this page, something went awry in the build of the website. Please find the first course material for this course 
&lt;a href=&#34;https://www.erikkusch.com/courses/bayes-nets/introduction/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>BFTP - Biome Detection through Remote Sensing</title>
      <link>https://www.erikkusch.com/courses/bftp-biome-detection/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/bftp-biome-detection/</guid>
      <description>&lt;p&gt;If you are seeing this page, something went awry in the build of the website. Please find the first course material for this course 
&lt;a href=&#34;https://www.erikkusch.com/courses/bftp-biome-detection/data-allocation/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>BioStat 101 - An Introduction to Biostatistics</title>
      <link>https://www.erikkusch.com/courses/biostat101/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/biostat101/</guid>
      <description>&lt;p&gt;If you are seeing this page, something went awry in the build of the website. Please find the first course material for this course 
&lt;a href=&#34;https://www.erikkusch.com/courses/biostat101/01-an-introduction-to-basic-statistics-for-biologists/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Excursions into Biostatistics</title>
      <link>https://www.erikkusch.com/courses/excursions-into-biostatistics/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/excursions-into-biostatistics/</guid>
      <description>&lt;p&gt;If you are seeing this page, something went awry in the build of the website. Please find the first course material for this course 
&lt;a href=&#34;https://www.erikkusch.com/courses/excursions-into-biostatistics/1_biostatistics-wait.-what/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>KrigR Workshop</title>
      <link>https://www.erikkusch.com/courses/krigr/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/krigr/</guid>
      <description>&lt;p&gt;If you are seeing this page, something went awry in the build of the website. Please find the first course material for this course 
&lt;a href=&#34;https://www.erikkusch.com/courses/krigr/setup/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Statistical Rethinking</title>
      <link>https://www.erikkusch.com/courses/rethinking/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/courses/rethinking/</guid>
      <description>&lt;p&gt;If you are seeing this page, something went awry in the build of the website. Please find the first course material for this course 
&lt;a href=&#34;https://www.erikkusch.com/courses/rethinking/chapter-02/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Causes and Processes of Dryland Vegetation Memory</title>
      <link>https://www.erikkusch.com/project/functional-traits-and-life-history-effects-on-high-resolution-vegetation-memory/</link>
      <pubDate>Tue, 29 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/project/functional-traits-and-life-history-effects-on-high-resolution-vegetation-memory/</guid>
      <description>&lt;h1 id=&#34;disclaimer&#34;&gt;Disclaimer&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;This project is currently on hold.&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&#34;motivation&#34;&gt;Motivation&lt;/h1&gt;
&lt;p&gt;Throughout my previous project concerning 
&lt;a href=&#34;https://www.erikkusch.com/project/vegetation-memory-across-global-dryland-regions/&#34;&gt;Vegetation Memory across Global Dryland Regions&lt;/a&gt;, I identified prominent spatial patterns of intrinsic and extrinsic vegetation memory components. These patterns may serve us in discerning global and local resistance and recovery potential of vegetation communities to perturbations.&lt;/p&gt;
&lt;p&gt;However, these patterns alone only tell part of the story. How do these come about? How are they maintained? Understanding these mechanisms would undoubtedly go a long way in predicting responses of vegetation to anthropogenic and climate change-driven disturbances. Ultimately, I expect this exercise to create valuable knowledge for conservation biology and the agricultural sector.&lt;/p&gt;
&lt;h1 id=&#34;description&#34;&gt;Description&lt;/h1&gt;
&lt;p&gt;MODIS EVI data is aggregated at bi-weekly intervals and used as a proxy of vegetation response in the dryland study regions between January 2000 and December 2019. Independent climate data is provided using the &lt;code&gt;KrigR&lt;/code&gt; package (
&lt;a href=&#34;https://www.erikkusch.com/project/krigr/ar&#34;&gt;KrigR - Downloading and Downscaling of ERA5(-Land) data using R&lt;/a&gt;). Doing so allows us to make use of the high temporal resolution of the European Centre for Medium-range Weather Forecasts ReAnalysis 5 (ERA5) data from the European Centre for Medium-Range Weather Forecasts (ECMWF) at spatial resolutions of roughly 1x1km. Effectively, this increases the spatial resolution of vegetation memory products by almost one order of magnitude when compared to my previous project at 9x9km resolution. Vegetation memory is calculated the same way as during my previous project.&lt;/p&gt;
&lt;p&gt;Using these high-resolution vegetation memory products, we can reasonably argue that ground-data obtained through functional trait campaigns or vegetation plot exercises represents the 1x1km grid reasonably well. Thus we are able to assess correlations between the different vegetation components and ground-truthed expressions of plant life. To identify what shapes vegetation memory expressions, I am investigating a range of three potential predictor families:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Plant Functional Traits&lt;/strong&gt; - I expect investment in certain functional characteristics to greatly influence resistance and recovery potential. For example, sturdy leaves should grant resistance to temperature fluctuations but are costly to recover once lost.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Life History Traits&lt;/strong&gt; - Life history speed and timing undoubtedly alters vegetation memory expressions. But which life history traits fair best at explaining these processes? For example, a short-lived population may be less resistant to a perturbation than a long-lived one, but surely boasts a reproduction rate that achieves fast recovery.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Abiotic Legacies&lt;/strong&gt; - Biological organisms adapt to their surroundings. On varying time-scales and through varying processes, but they adapt nonetheless. I aim to investigate how strong these adaptations become by computing the impact of the legacy mean and standard deviation of abiotic conditions on vegetation memory components. For example, a species in a highly variable aridity regime may be more resistant to fluctuations in aridity than one used to stable, moist conditions&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>A Macroecological Perspective to Ecological Networks</title>
      <link>https://www.erikkusch.com/project/phd-packages/</link>
      <pubDate>Mon, 28 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/project/phd-packages/</guid>
      <description>&lt;p&gt;My PhD project was a part of the greater 
&lt;a href=&#34;https://www.erikkusch.com/project/biorates/&#34;&gt;BIORATES&lt;/a&gt; project.&lt;/p&gt;
&lt;h1 id=&#34;motivation&#34;&gt;Motivation&lt;/h1&gt;
&lt;p&gt;Species rarely occur in isolation. Instead, they assemble into multi-species communities wherein individuals interact within and across species groups. These pairwise species interactions affect responses of biological communities to environmental conditions and perturbations. To understand and forecast changes within the Ecosphere, it is thus vital to quantify such interactions and explore their implications. Changes in pairwise species interactions are largely spurred by processes of the Anthropocene (e.g., changes in temperature, habitat availability). These affect entire ecosystems simultaneously thus necessitating the exploration of biological interactions at macroecological scales. Despite the complexity of biological interactions and the networks they form at macroecological scales, their study can be carried out readily via ecological networks.&lt;/p&gt;
&lt;p&gt;Here, I develop methodology and carry out analyses to address (1) the failure of contemporary macroecological research to incorporate state-of-the-art climate data by updating macroecological research practices, (2) the implications of mechanisms of extinction cascades within ecological networks with respect to interaction magnitude and potential/realised interactions thus using ecological networks as forecast tools, and finally (3) the lack of ecological network quantification at macroecological scales by inferring biological interactions from proxies.&lt;/p&gt;
&lt;h1 id=&#34;work-packages&#34;&gt;Work Packages&lt;/h1&gt;
&lt;h2 id=&#34;chapter-1---updating-macroecological-research-practices&#34;&gt;Chapter 1 - Updating Macroecological Research Practices&lt;/h2&gt;
&lt;p&gt;Environmental conditions which regulate species distributions determine whether pairwise biological interactions can be realised (i.e., species which do not coincide cannot directly interact). Furthermore, changes in abiotic conditions alter the expression of realised interactions. Therefore, it is crucial to explicitly consider environmental conditions when quantifying and forecasting biological interactions, particularly at macroecological scales which are characterized by prominent environmental heterogeneity (e.g. thermal gradients). However, macroecological research, at present, does not leverage the most recent and accurate climate data products available. Therefore, we require changes to macroecological research practices to integrate state-of-the-art climate data.&lt;/p&gt;
&lt;p&gt;To resolve this issue, I have developed the &lt;code&gt;KrigR&lt;/code&gt; &lt;code&gt;R&lt;/code&gt; package which provides an easy-to-use and highly flexible infrastructure for access, temporal aggregation, spatial limitation, and statistical downloading of state of the art climate data from ECMWF. The resulting data products outperform legacy data products commonly used in macroecological research in (1) temporal resolution, (2) data accuracy, (3) provisioning of climate variables, (4) flexibility, and (5) applicability to specific research requirements.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;1.png&#34; alt=&#34;CH1 - Macroecological Climate Data Practices&#34;&gt;&lt;/p&gt;
&lt;p&gt;This work has been published in 
&lt;a href=&#34;https://www.erikkusch.com/publication/krigr-a-tool-for-statistically-downscaling-climate-reanalysis-data-for-ecological-applications/&#34;&gt;Kusch &amp;amp; Davy, 2022&lt;/a&gt; and 
&lt;a href=&#34;https://www.erikkusch.com/publication/reconciling-high-resolution-climate-datasets-using-krigr/&#34;&gt;Davy &amp;amp; Kusch, 2021&lt;/a&gt;. Due to the ongoing development of &lt;code&gt;KrigR&lt;/code&gt;, I have transferred any ongoing work on these issues to the 
&lt;a href=&#34;https://www.erikkusch.com/project/krigr/&#34;&gt;&lt;code&gt;KrigR&lt;/code&gt; project&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;chapter-2---using-ecological-networks-as-forecast-tools&#34;&gt;Chapter 2 - Using Ecological Networks as Forecast Tools&lt;/h2&gt;
&lt;p&gt;Biological interactions range in identity (i.e, present, absent), sign (i.e., positive and negative), and magnitude which determine the importance of an interaction for the persistence of interacting species and entire communities. This impact manifests especially through extinction cascades which exacerbate the biodiversity loss and change of network topologies spurred by Anthropogenic impacts. Such cascades are characterized by the loss of biological interactions following the extinction of a species leading to additional species extinctions. Thus, to forecast future community structures and quantify risk to ecosystem stability and functioning, it is vital to consider both the potential for and sign as well as magnitude of species interactions. Flexible and easy-to-use methodology for this purpose is currently lacking. Consequently, exploration of extinction cascades has remained simplistic and ought to be updated to facilitate the exploration of realistic future scenarios of the Ecosphere.&lt;/p&gt;
&lt;p&gt;In addressing this knowledge gap, I have co-developed the &lt;code&gt;NetworkExtinction&lt;/code&gt; &lt;code&gt;R&lt;/code&gt; package which enables simulations of extinction cascades within trophic as well as mutualistic networks with varying levels of link-importance and realisation of potential interactions. Using this tool, I subsequently explore network resilience landscapes defined by link-loss sensitivity and rewiring probability thresholds for a collection of empirical mutualistic networks across the Earth.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;2.png&#34; alt=&#34;CH2 - Ecological Networks as Forecasting Tools&#34;&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;NetworkExtinction&lt;/code&gt; package has been published as 
&lt;a href=&#34;https://www.erikkusch.com/publication/networkextinction-an-r-package-to-simulate-extinctions-propagation-and-rewiring-potential-in-ecological-networks/&#34;&gt;Ãvila-Thieme &amp;amp; Kusch et. al, 2023&lt;/a&gt; while the exploration of mutualistic network resilience landscapes (
&lt;a href=&#34;https://www.erikkusch.com/publication/ecological-network-resilience-extinction-proxies-updating-projections-of-ecological-networks/&#34;&gt;Kusch, Ordonez&lt;/a&gt;) is currently being prepared for submission.&lt;/p&gt;
&lt;h2 id=&#34;chapter-3---inferring-biological-interactions-from-proxies&#34;&gt;Chapter 3 - Inferring Biological Interactions from Proxies&lt;/h2&gt;
&lt;p&gt;Ecological networks are affected by the scales (e.g., local, regional, and continental) at which they are represented. Thus, locally quantified biological interactions cannot be used reliably to represent macroecological processes. However, sourcing networks at macroecological scales via traditional in-situ observations is prohibitively labour-intensive thus requiring the inference of biological interactions from macroecological proxies. However, the accuracy of already established biological interaction inference approaches remains understudied, calling into question their utility. Therefore, consistency and performance of interaction inference ought to be evaluated for use at macroecological scales.&lt;/p&gt;
&lt;p&gt;Within this final chapter of my PhD, I assess the consistency of four different interaction inference approaches (COOCCUR, NETASSOC, HMSC, and NDD-RIM) across ecologically relevant skills. Finding little consistency between the inferred ecological networks, I subsequently develop a demographic simulation framework of populations of interacting species across time and space and establish guidelines for assessment of ecological network inference performance.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;3.png&#34; alt=&#34;CH3 - Ecological Network Inference&#34;&gt;&lt;/p&gt;
&lt;p&gt;While the study of inference consistency is already submitted for review as 
&lt;a href=&#34;https://www.erikkusch.com/publication/ecological-network-inference-is-not-consistent-across-sales-or-approaches/&#34;&gt;Kusch et. al, 2023&lt;/a&gt;, a publication of 
&lt;a href=&#34;https://www.erikkusch.com/publication/a-novel-simulation-framework-for-validation-of-ecological-network-inference/&#34;&gt;Kusch &amp;amp; Vinton&lt;/a&gt; is to follow presenting the simulation framework.&lt;/p&gt;
&lt;h1 id=&#34;implications&#34;&gt;Implications&lt;/h1&gt;
&lt;p&gt;To ensure ecosystem management and conservation efforts are targeted appropriately under climate change, my work highlights that macroecological research practices ought to undergo a paradigm shift away from one-size-fits-all climate datasets towards reproducible and flexible data workflows for generation of climate datasets with respect to specific study purposes and requirements. Additionally, my explicit integration of ecological network resilience mechanisms towards extinction cascades reveals that contemporary approaches are likely overly optimistic in their projections of biodiversity loss throughout the Anthropocene. Lastly, I caution against naive use of ecological network inference within macroecological research. Instead, to render knowledge of ecological networks at macroecological scales, I argue that extensive assessments of network inference performance are required for which I present important groundwork.&lt;/p&gt;
&lt;p&gt;Ultimately, my dissertation represents a key advancement of use-cases of ecological networks at macroecological scales. Thus, adopting the novel methodology I have developed for macroecological use and inference of ecological networks is pressing in order to ensure adequate ecosystem management throughout the Anthropocene.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bayes Study Group</title>
      <link>https://www.erikkusch.com/project/aubayes/</link>
      <pubDate>Mon, 28 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/project/aubayes/</guid>
      <description>&lt;div class=&#34;alert alert-success&#34;&gt;
  &lt;div&gt;
    This study group has come to an end. All information about it can still be found below. We are now coordinating through Slack to get our fix of all things Bayes. Our Slack Workspace is open to anyone who wants to join us. To do so, simply let me know by &lt;a href=&#34;https://www.erikkusch.com/about#contact&#34;&gt;contacting me&lt;/a&gt; and shortly introduce yourself.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h1 id=&#34;motivation&#34;&gt;Motivation&lt;/h1&gt;
&lt;p&gt;Have you ever read a paper and thought: âWow, this is really coolâ, then read on only to find the word âBayesianâ in the analysis specification and promptly get lost at the terminology and approach to statistics?&lt;/p&gt;
&lt;p&gt;If you did/do, you are obviously not alone and I think it might be time we do something about it. To this end, I have created a Bayes Study Group (formerly AU Bayes Study Group) within which we work our way through (1) a 
&lt;a href=&#34;https://www.youtube.com/playlist?list=PLDcUM9US4XdNM4Edgs7weiyIguLSToZRI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;lecture series&lt;/a&gt; by Bayes Wunderkind Richard McElreath, (2) his corresponding book (â
&lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Statistical Rethinking&lt;/a&gt;â), and (3) a few practical examples in &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;h1 id=&#34;conduct--logistics&#34;&gt;Conduct &amp;amp; Logistics&lt;/h1&gt;
&lt;p&gt;Below, you will find a few key facts outlining how this Bayes Study Group sessions are run. You will find that what we do is quite a lot (I would like to think it quite exhaustive, actually). We are following the example set by some colleagues at the University of Oxford here and I have added some extra bits to the material and schedule. Go big or go home, right?&lt;/p&gt;
&lt;p&gt;That being said, I realise that what I propose is quite a big-time commitment â both in per-week hours and for how long this study group is supposed to run.&lt;/p&gt;
&lt;h2 id=&#34;meetings&#34;&gt;Meetings:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Scheduling:
&lt;ul&gt;
&lt;li&gt;Friday 1300-1600 GMT+1 (sessions end earlier if we are done earlier, of course)&lt;/li&gt;
&lt;li&gt;Weekly for 20 sessions including 4 input talks by practicioneers of Bayesian analyses&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;contents&#34;&gt;Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We follow the material noted further down under Group Material&lt;/li&gt;
&lt;li&gt;We prepare ourselves by working through the material noted for each session&lt;/li&gt;
&lt;li&gt;At the start of each session, either me or a volunteer quickly presents a summary of the preparation material. This does not mean a presenter should have mastered the material at all â simply be able to provide a red-thread through the contents and get a discussion going. We then go into questions/challenges concerning the material, practical examples, and personal implementations.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Lecture Recording âStatistical Rethinkingâ by Richard McElreath; available 
&lt;a href=&#34;https://www.youtube.com/playlist?list=PLDcUM9US4XdNM4Edgs7weiyIguLSToZRI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Book âStatistical Rethinkingâ by Richard McElreath; available 
&lt;a href=&#34;https://github.com/Booleans/statistical-rethinking/blob/master/Statistical%20Rethinking%202nd%20Edition.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Practical Examples:
&lt;ul&gt;
&lt;li&gt;Course Material 
&lt;a href=&#34;https://github.com/rmcelreath/stat_rethinking_2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Statistical Rethinking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Book âIntroduction to WinBUGS for Ecologistsâ by Marc KÃ©ry; available 
&lt;a href=&#34;https://www.sciencedirect.com/book/9780123786050/introduction-to-winbugs-for-ecologists&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Book âBayesian Population Analysis using WinBUGSâ by Marc KÃ©ry and Michael Schaub; available 
&lt;a href=&#34;https://www.sciencedirect.com/book/9780123870209/bayesian-population-analysis-using-winbugs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Further Input (all prospective)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian Networks Study Group</title>
      <link>https://www.erikkusch.com/project/bayesnets/</link>
      <pubDate>Mon, 28 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/project/bayesnets/</guid>
      <description>&lt;div class=&#34;alert alert-success&#34;&gt;
  &lt;div&gt;
    This study group has come to an end. All information about it can still be found below. We are now coordinating through Slack to get our fix of all things Bayes. Our Slack Workspace is open to anyone who wants to join us. To do so, simply let me know by &lt;a href=&#34;https://www.erikkusch.com/about#contact&#34;&gt;contacting me&lt;/a&gt; and shortly introduce yourself.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h1 id=&#34;motivation&#34;&gt;Motivation&lt;/h1&gt;
&lt;p&gt;Building on the success of the 
&lt;a href=&#34;https://www.erikkusch.com/project/aubayes/&#34;&gt;Bayes Study Group&lt;/a&gt; I ran two years ago, I have decided to create another study group this time focusing on explicit inference and analysis of Bayesian Networks which I believe are a neat tool for hypothesis testing.&lt;/p&gt;
&lt;h1 id=&#34;conduct--logistics&#34;&gt;Conduct &amp;amp; Logistics&lt;/h1&gt;
&lt;p&gt;Below, you will find a few key facts outlining how this Bayesian Network Study Group sessions are run. You will find that what we do is quite a lot (I would like to think it quite exhaustive, actually).&lt;/p&gt;
&lt;p&gt;That being said, I realise that what I propose is quite a big-time commitment â both in per-week hours and for how long this study group is supposed to run.&lt;/p&gt;
&lt;h2 id=&#34;meetings&#34;&gt;Meetings:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Scheduling:
&lt;ul&gt;
&lt;li&gt;Tuesday 1500-1700 CEST (sessions end earlier if we are done earlier, of course)&lt;/li&gt;
&lt;li&gt;Weekly for 8 sessions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;contents&#34;&gt;Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We follow the material noted further down under Group Material&lt;/li&gt;
&lt;li&gt;We prepare ourselves by working through the material noted for each session the proposed 
&lt;a href=&#34;https://www.erikkusch.com/project/bayesnets/Schedule.pdf&#34;&gt;schedule&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;At the start of each session, either me or a volunteer quickly presents a summary of the preparation material. This does not mean a presenter should have mastered the material at all â simply be able to provide a red-thread through the contents and get a discussion going. We then go into questions/challenges concerning the material, practical examples, and personal implementations.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Book âBayesian Networks With Examples in Râ by Marco Scutari &amp;amp; Jean-Baptiste Denis; available 
&lt;a href=&#34;https://www.routledge.com/Bayesian-Networks-With-Examples-in-R/Scutari-Denis/p/book/9780367366513&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Book âBayesian Networks in R with Applications in Systems Biologyâ by Radhakrishnan Nagarajan, Marco Scutari &amp;amp; Sophie LÃ¨bre; available 
&lt;a href=&#34;https://link.springer.com/book/10.1007/978-1-4614-6446-4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>BIOlogical response RATES to current rates of environmental changes</title>
      <link>https://www.erikkusch.com/project/biorates/</link>
      <pubDate>Mon, 28 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/project/biorates/</guid>
      <description></description>
    </item>
    
    <item>
      <title>KrigR - Downloading and Downscaling of ERA5(-Land) data using R</title>
      <link>https://www.erikkusch.com/project/krigr/</link>
      <pubDate>Mon, 28 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/project/krigr/</guid>
      <description>&lt;div class=&#34;alert alert-danger&#34;&gt;
  &lt;div&gt;
    I am continuing to develop &lt;code&gt;KrigR&lt;/code&gt;, add to its functionality, and broaden its scope. For any suggestions of development goals, novel functionality as well as any issues you might face with &lt;code&gt;KrigR&lt;/code&gt;, please register an issue on &lt;a href=&#34;https://github.com/ErikKusch/KrigR/issues&#34; target=&#34;_blank&#34;&gt;GitHub&lt;/a&gt;. where I track these issues. Please refrain from inquiries via direct E-mail.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h1 id=&#34;motivation&#34;&gt;Motivation&lt;/h1&gt;
&lt;p&gt;Contemporary studies of remote sensing and macroecology largely rely on legacy climate data which is surpassed by state-of-the-art climate reanalysis data sets such as the European Centre for Medium-range Weather Forecasts ReAnalysis 5 (ERA5) data catalogue which represents the forefront of advances in data assimilation and climate modelling.&lt;/p&gt;
&lt;p&gt;Using these data sets within &lt;code&gt;R&lt;/code&gt; â the most prominent statistical software in macroecology â is a complicated task that involves downloading of data via shell scripts, reformatting of coordinate system, and might even require rescaling of the climate data.&lt;/p&gt;
&lt;p&gt;In order to make the use of ERA5(-Land) in future analyses more streamlined, I am developing an &lt;code&gt;R&lt;/code&gt; Package (&lt;code&gt;KrigR&lt;/code&gt;) which will handle downloading and reformatting of ERA5(-Land) data. Furthermore, this package offers rescaling capabilities via the Kriging functionality of the &lt;code&gt;automap&lt;/code&gt; package whilst enabling multi-core processing for faster computation of time-series data.&lt;/p&gt;
&lt;h1 id=&#34;description&#34;&gt;Description&lt;/h1&gt;
&lt;p&gt;The superiority of ERA5(-Land) data over legacy data sets is largely due to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The volume of observational data used to create the ERA5 product. This includes a large volume of satellite data, ground-based weather station/independent institute data collection efforts, and other station data from a wide variety of data providers. This is in contrast to gridded observational datasets which are often characterised by their individual biases in sampling, coverage, and choice of interpolation and homogenization to create a gridded product.&lt;/li&gt;
&lt;li&gt;The advances in data assimilation procedures. Data assimilation for geophysical application has greatly developed as a field in recent decades. This has included fundamental methodological advancements, such as the development of the ensemble Kalman filter, as well developments specific to geophysical application of data assimilation, such as approaches to localisation.&lt;/li&gt;
&lt;li&gt;The complexity of the underlying models. Reanalysis products have been widely used to shed light on physical processes. ERA5 was created using the ECMWFâs integrated forecasting system which is the worldâs best forecast model and is continuously developed using the latest understanding of the physics of the climate system.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;KrigR&lt;/code&gt; is a tool to streamline and standardise the implementation of state-of-the-art ERA5(-Land) data and time series in large-scale analyses. As such, the &lt;code&gt;R&lt;/code&gt; Package includes the following functionality:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Download functions for:
&lt;ul&gt;
&lt;li&gt;ERA5(-Land) data&lt;/li&gt;
&lt;li&gt;GMTED2010 covariate data (kriging will not be limited to GMTED2010 alone)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Preprocessing of ERA5(-Land) data&lt;/li&gt;
&lt;li&gt;Statistical downscaling of ERA5(-Land) data using kriging methodology&lt;/li&gt;
&lt;li&gt;Selection of regions by shapefiles or extents&lt;/li&gt;
&lt;li&gt;Variable list with guidelines for statistical downscaling (this is currently being developed)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;KrigR&lt;/code&gt; downloads ERA5(-Land) upon having been provided with the required ECMWF API key and includes several self-checking statements to avoid the most common issues with the kriging methodology.&lt;/p&gt;
&lt;p&gt;Users are be able to start the process of the &lt;code&gt;KrigR&lt;/code&gt; workflow at any given function of the package, but it is recommended to let &lt;code&gt;KrigR&lt;/code&gt; handle the entire workflow as intended.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Vegetation Memory across Global Dryland Regions</title>
      <link>https://www.erikkusch.com/project/vegetation-memory-across-global-dryland-regions/</link>
      <pubDate>Mon, 28 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/project/vegetation-memory-across-global-dryland-regions/</guid>
      <description>&lt;h1 id=&#34;motivation&#34;&gt;Motivation&lt;/h1&gt;
&lt;p&gt;Vegetation memory describes the effect of antecedent environmental and ecosystem states on ecosystem state in the present and has been used as an important proxy for ecosystem recovery rates potentially a key component of vegetation resilience, at a global scale. In particular, strong vegetation memory effects have been identified in dryland regions coinciding with decreased vegetation sensitivity towards climatological drivers.&lt;/p&gt;
&lt;p&gt;Here, we aim to test the components and drivers of vegetation memory in dryland regions using state-of the-art climate reanalysis data and refined approaches to identify vegetation-memory characteristics across global dryland regions. We show that (1) dryland regions are characterised by strong vegetation memory (intrinsic and extrinsic), (2) it is possible to distinguish intrinsic and extrinsic vegetation memory to a hitherto unachieved degree using climate reanalysis data sets, (3) the link between intrinsic vegetation memory and resilience may be an oversimplification, and (4) dryland vegetation does not react to bioclimatic forcing in the same way across the Earth.&lt;/p&gt;
&lt;h1 id=&#34;description&#34;&gt;Description&lt;/h1&gt;
&lt;p&gt;GIMMS NDVI 3g data was used as a proxy of vegetation response in the dryland study regions between January 1982 and December 2015. The NDVI is a compound vegetation index made up from reflectance in the red and near-infrared reflectance bands.
For independent climate data in this study, we used European Centre for Medium-range Weather Forecasts ReAnalysis 5 (ERA5) data from the European Centre for Medium-Range Weather Forecasts (ECMWF). ERA5 data is available for hourly intervals (which we assembled to monthly time steps) from 1950 to present day at a 30km Ã 30km spatial resolution of global coverage making the resolution of ERA5 and AVHRR-based GIMMS NDVI 3g incompatible. We resolved this issue by statistically downscaling ERA5 data using the kriging methodology (&lt;code&gt;KrigR&lt;/code&gt; package of my 
&lt;a href=&#34;https://www.erikkusch.com/project/krigr/&#34;&gt;KrigR - Downloading and Downscaling of ERA5(-Land) data using R&lt;/a&gt; project).&lt;/p&gt;
&lt;p&gt;To assess the relative importance of intrinsic memory and extrinsic climate forcing we used a linear modelling approach akin to DeKeersmaecker et al, 2015. We use NDVI anomalies on a one-month lag, air temperature (Tair) and soil moisture (Qsoil) data on different temporal lags to model anomalies of monthly NDVI values over three decades. Due to issues of multi-collinearity, we do so using a principal component regression approach.&lt;/p&gt;
&lt;p&gt;The results contain information about relative importance of vegetation memory components, their temporal lags,and are presented in the context of contemporary vegetation memory and sensitivity literature.&lt;/p&gt;
&lt;p&gt;Our findings demonstrate novel observations of vegetation memory patterns across dryland regions such as regional differences of processes forming vegetation memory capabilities. Consequently, this study provides a helpful stepping stone for refining and combining already existing methodology which could, in turn, generate important knowledge of ecosystem functioning and resilience.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Species Associations Across Scales of Organisation</title>
      <link>https://www.erikkusch.com/talk/2020_12_salgoteam/</link>
      <pubDate>Thu, 03 Dec 2020 16:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/talk/2020_12_salgoteam/</guid>
      <description></description>
    </item>
    
    <item>
      <title>KrigR - Downscaling State-of-the-Art Climate Data for Macroecologists</title>
      <link>https://www.erikkusch.com/talk/2020_11-krigrworkshop/</link>
      <pubDate>Thu, 12 Nov 2020 13:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/talk/2020_11-krigrworkshop/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Intrinsic vegetation memory as a proxy of engineering resilience may be an oversimplification.</title>
      <link>https://www.erikkusch.com/talk/2020_06_isec_poster/</link>
      <pubDate>Thu, 25 Jun 2020 11:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/talk/2020_06_isec_poster/</guid>
      <description></description>
    </item>
    
    <item>
      <title>KrigR - Climate Data for Your Spatial Study</title>
      <link>https://www.erikkusch.com/talk/2020_06_isec_presentation/</link>
      <pubDate>Thu, 25 Jun 2020 10:30:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/talk/2020_06_isec_presentation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Identifying Ecological Memory Patterns in Drylands Using Remote Sensing and State-of-the-art Climate Reanalysis Products</title>
      <link>https://www.erikkusch.com/talk/2010_09_isem_talk/</link>
      <pubDate>Fri, 04 Oct 2019 13:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/talk/2010_09_isem_talk/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Intrinsic vegetation memory as a proxy of engineering resilience may be an oversimplification.</title>
      <link>https://www.erikkusch.com/talk/2010_09_isem_poster/</link>
      <pubDate>Thu, 03 Oct 2019 11:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/talk/2010_09_isem_poster/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Inferring Vegetation Memory from Remote Sensing Data using novel Climate Reconstruction Products</title>
      <link>https://www.erikkusch.com/talk/2019_mscdefense/</link>
      <pubDate>Fri, 05 Jul 2019 12:30:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/talk/2019_mscdefense/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Inferring Vegetation Memory from Remote Sensing Data using novel Climate Reconstruction Products</title>
      <link>https://www.erikkusch.com/publication/journal-article/m.sc._inferring-vegetation-memory-from-remote-copy/</link>
      <pubDate>Sun, 19 May 2019 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/publication/journal-article/m.sc._inferring-vegetation-memory-from-remote-copy/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Inferring Vegetation Memory from Remote Sensing Data using novel Climate Reconstruction Products</title>
      <link>https://www.erikkusch.com/publication/m.sc._inferring-vegetation-memory-from-remote-copy/</link>
      <pubDate>Sun, 19 May 2019 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/publication/m.sc._inferring-vegetation-memory-from-remote-copy/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Remote Sensing And Predicting Shifts In Biome Distribution And Resilience Using NDVI Data</title>
      <link>https://www.erikkusch.com/publication/b.sc._inferring-vegetation-memory-from-remote/</link>
      <pubDate>Fri, 10 Mar 2017 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/publication/b.sc._inferring-vegetation-memory-from-remote/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Remote Sensing And Predicting Shifts In Biome Distribution And Resilience Using NDVI Data</title>
      <link>https://www.erikkusch.com/publication/journal-article/b.sc._inferring-vegetation-memory-from-remote/</link>
      <pubDate>Fri, 10 Mar 2017 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/publication/journal-article/b.sc._inferring-vegetation-memory-from-remote/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Remote Sensing And Predicting Shifts In Biome Distribution And Resilience Using NDVI Data</title>
      <link>https://www.erikkusch.com/talk/2017_bscdefense/</link>
      <pubDate>Fri, 24 Feb 2017 12:30:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/talk/2017_bscdefense/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://www.erikkusch.com/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/about/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://www.erikkusch.com/contact/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://www.erikkusch.com/contact/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
