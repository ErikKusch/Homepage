<!DOCTYPE html><html lang="en-gb" >

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.8.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Erik Kusch">

  
  
  
    
  
  <meta name="description" content="Answers and solutions to the exercises belonging to chapter 14 in [Satistical Rethinking 2](https://xcelab.net/rm/statistical-rethinking/) by Richard McElreath.">

  
  <link rel="alternate" hreflang="en-gb" href="https://www.erikkusch.com/courses/rethinking/chapter-14/">

  


  
  
  
  <meta name="theme-color" content="#858383">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css" integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    

  

  
  
  
    
      
      
      <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Cutive+Mono%7CLora:400,700%7CRoboto:400,700&display=swap">
    
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  





<script async src="https://www.googletagmanager.com/gtag/js?id=UA-186216138-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url, target) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           if (target !== '_blank') {
             document.location = url;
           }
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target, event.target.getAttribute('target'));  
  }

  gtag('js', new Date());
  gtag('config', 'UA-186216138-1', {});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  


  
  

  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu7658c99d512131214e4d6c003a3f85f7_112593_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu7658c99d512131214e4d6c003a3f85f7_112593_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="https://www.erikkusch.com/courses/rethinking/chapter-14/">

  
  
  
  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@ErikKusch">
  <meta property="twitter:creator" content="@ErikKusch">
  
  <meta property="og:site_name" content="Erik Kusch">
  <meta property="og:url" content="https://www.erikkusch.com/courses/rethinking/chapter-14/">
  <meta property="og:title" content="Chapter 14 | Erik Kusch">
  <meta property="og:description" content="Answers and solutions to the exercises belonging to chapter 14 in [Satistical Rethinking 2](https://xcelab.net/rm/statistical-rethinking/) by Richard McElreath."><meta property="og:image" content="https://www.erikkusch.com/img/%C3%A5motdalshytta.jpg">
  <meta property="twitter:image" content="https://www.erikkusch.com/img/%C3%A5motdalshytta.jpg"><meta property="og:locale" content="en-gb">
  
    
      <meta property="article:published_time" content="2021-04-16T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2021-04-16T12:00:00&#43;01:00">
  

  



  


  


  





  <title>Chapter 14 | Erik Kusch</title>

</head>
<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  








  


<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/"><img src="/images/logo_huab2266ff59bbac6a094b6da1268adcf9_208479_0x70_resize_lanczos_2.png" alt="Erik Kusch"></a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/"><img src="/images/logo_huab2266ff59bbac6a094b6da1268adcf9_208479_0x70_resize_lanczos_2.png" alt="Erik Kusch"></a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-center" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link  active" href="/"><span>Home</span></a>
        </li>

        
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Courses & Workshops</span><span class="caret"></span>
          </a>
          <div class="dropdown-menu">
            
              <a class="dropdown-item" href="/courses/krigr/"><span>KrigR - State-of-the-Art Climate Data within R</span></a>
            
              <a class="dropdown-item" href="/courses/gbif/"><span>Accessing and Referencing GBIF Data for Research</span></a>
            
              <a class="dropdown-item" href="/courses/rethinking/"><span>Statistical Rethinking - Introduction to Bayesian Statistics</span></a>
            
              <a class="dropdown-item" href="/courses/bayes-nets/"><span>Bayesian Networks - Causality through Bayesian Approaches</span></a>
            
              <a class="dropdown-item" href="/courses/biostat101/"><span>Biostats 101 - An Introduction to Biostatistics</span></a>
            
              <a class="dropdown-item" href="/courses/excursions-into-biostatistics/"><span>Biostats 301 - Excursions into Biostatistics</span></a>
            
              <a class="dropdown-item" href="/courses/bftp-biome-detection/"><span>BFTP - First Steps in Macroecology in R</span></a>
            
          </div>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#featured"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#talks"><span>Talks</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/about"><span>Contact</span></a>
        </li>

        
        

        

        
        
        
          
            
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/media/CV_ErikKusch.html" target="_blank" rel="noopener"><span>CV</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link js-theme-selector" data-toggle="dropdown" aria-haspopup="true">
          <i class="fas fa-palette" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      

    </ul>

  </div>
</nav>


  

<div class="container-fluid docs">
  <div class="row flex-xl-nowrap">
    <div class="col-12 col-md-3 col-xl-2 docs-sidebar">
      





  
    
  




<form class="docs-search d-flex align-items-center">
  <button class="btn docs-toggle d-md-none p-0 mr-3" type="button" data-toggle="collapse" data-target="#docs-nav" aria-controls="docs-nav" aria-expanded="false" aria-label="Toggle section navigation">
    <span><i class="fas fa-bars"></i></span>
  </button>

  
  <input name="q" type="search" class="form-control" placeholder="Search..." autocomplete="off">
  
</form>

<nav class="collapse docs-links" id="docs-nav">
  

  
  
  
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/courses/rethinking/">Overview</a>

  </div>
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/courses/rethinking/chapter-02/">Seminars</a>
    <ul class="nav docs-sidenav">
      
      <li >
        <a href="/courses/rethinking/chapter-02/">Chapter 01 &amp; 02</a>
      </li>
      
      <li >
        <a href="/courses/rethinking/chapter-03/">Chapter 03</a>
      </li>
      
      <li >
        <a href="/courses/rethinking/chapter-04/">Chapter 04</a>
      </li>
      
      <li >
        <a href="/courses/rethinking/chapter-04b/">Chapter 04 (Extra Material)</a>
      </li>
      
      <li >
        <a href="/courses/rethinking/chapter-05/">Chapter 05</a>
      </li>
      
      <li >
        <a href="/courses/rethinking/chapter-06/">Chapter 06</a>
      </li>
      
      <li >
        <a href="/courses/rethinking/chapter-07/">Chapter 07</a>
      </li>
      
      <li >
        <a href="/courses/rethinking/chapter-08/">Chapter 08</a>
      </li>
      
      <li >
        <a href="/courses/rethinking/chapter-09/">Chapter 09</a>
      </li>
      
      <li >
        <a href="/courses/rethinking/chapter-11/">Chapter 10 &amp; 11</a>
      </li>
      
      <li >
        <a href="/courses/rethinking/chapter-12/">Chapter 12</a>
      </li>
      
      <li >
        <a href="/courses/rethinking/chapter-13/">Chapter 13</a>
      </li>
      
      <li class="active">
        <a href="/courses/rethinking/chapter-14/">Chapter 14</a>
      </li>
      
      <li >
        <a href="/courses/rethinking/chapter-15/">Chapter 15</a>
      </li>
      
      <li >
        <a href="/courses/rethinking/chapter-16/">Chapter 16</a>
      </li>
      
    </ul>
    

  </div>
  
  
</nav>

    </div>

    
    <div class="d-none d-xl-block col-xl-2 docs-toc">
      <ul class="nav toc-top">
        <li><a href="#" id="back_to_top" class="docs-toc-title">Contents</a></li>
      </ul>

      <nav id="TableOfContents">
  <ul>
    <li><a href="#material">Material</a></li>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#r-environment"><code>R</code> Environment</a></li>
    <li><a href="#easy-exercises">Easy Exercises</a>
      <ul>
        <li><a href="#practice-e1">Practice E1</a></li>
        <li><a href="#practice-e2">Practice E2</a></li>
        <li><a href="#practice-e3">Practice E3</a></li>
      </ul>
    </li>
    <li><a href="#medium-exercises">Medium Exercises</a>
      <ul>
        <li><a href="#practice-m1">Practice M1</a></li>
        <li><a href="#practice-m2">Practice M2</a></li>
        <li><a href="#practice-m3">Practice M3</a></li>
        <li><a href="#practice-m4">Practice M4</a></li>
        <li><a href="#practice-m5">Practice M5</a></li>
      </ul>
    </li>
    <li><a href="#hard-exercises">Hard Exercises</a>
      <ul>
        <li><a href="#practice-h1">Practice H1</a></li>
        <li><a href="#practice-h2">Practice H2</a></li>
        <li><a href="#practice-h3">Practice H3</a></li>
        <li><a href="#practice-h4">Practice H4</a></li>
      </ul>
    </li>
    <li><a href="#session-info">Session Info</a></li>
  </ul>
</nav>

      
    </div>
    

    <main class="col-12 col-md-9 col-xl-8 py-md-3 pl-md-5 docs-content" role="main">

      <article class="article">

        <div class="docs-article-container">
          <h1>Chapter 14</h1>

          <div class="article-style">
            <h1 id="adventures-in-covariance">Adventures in Covariance</h1>
<h2 id="material">Material</h2>
<ul>
<li>
<a href="https://htmlpreview.github.io/?https://github.com/ErikKusch/Homepage/blob/master/static/courses/rethinking/16__16-04-2021_SUMMARY_-Multi-Level-Models-II.html" target="_blank" rel="noopener">Slides Chapter 14</a></li>
</ul>
<h2 id="introduction">Introduction</h2>
<p>These are answers and solutions to the exercises at the end of chapter 14 in 
<a href="https://xcelab.net/rm/statistical-rethinking/" target="_blank" rel="noopener">Satistical Rethinking 2</a> by Richard McElreath. I have created these notes as a part of my ongoing involvement in the 
<a href="/project/aubayes/">AU Bayes Study Group</a>. Much of my inspiration for these solutions, where necessary, has been obtained from</p>
<!-- [Taras Svirskyi](https://github.com/jffist/statistical-rethinking-solutions/blob/master/ch10_hw.R), [William Wolf](https://github.com/cavaunpeu/statistical-rethinking/blob/master/chapter-10/homework.R), and [Corrie Bartelheimer](https://www.samples-of-thoughts.com/projects/statistical-rethinking/chapter_10/chp10-ex/) as well as  -->
<p>the solutions provided to instructors by Richard McElreath himself.</p>
<h2 id="r-environment"><code>R</code> Environment</h2>
<p>For today&rsquo;s exercise, I load the following packages:</p>
<pre><code class="language-r">library(rethinking)
library(rstan)
library(MASS)
library(ellipse)
library(ape)
library(ggplot2)
library(tidybayes)
</code></pre>
<h2 id="easy-exercises">Easy Exercises</h2>
<h3 id="practice-e1">Practice E1</h3>
<p><strong>Question:</strong>  Add to the following model varying slopes on the predictor $x$.</p>
<p>$$y_i ∼ Normal(µi, σ)$$ 
$$µ_i = α_{group[i]} + βx_i$$
$$α_{group} ∼ Normal(α, σ_α)$$ 
$$α ∼ Normal(0, 10)$$ 
$$β ∼ Normal(0, 1)$$ 
$$σ ∼ HalfCauchy(0, 2)$$ 
$$σ_α ∼ HalfCauchy(0, 2)$$</p>
<p><strong>Answer:</strong> To do this, our outcome distribution does not change. So keep it as is:</p>
<p>$$y_i ∼ Normal(μ_i, σ)$$</p>
<p>Next, we come to the linear model. This needs changing. Since we are now interested in a varying slope for each group ($\beta_{group}$), we need to exchange the original $\beta$ with $\beta_{group}$:</p>
<p>$$μ_i = α_{group[i]} + β_{group[i]}x_i$$</p>
<p>Consequently, we also need to change our prior. Since $\alpha_{group}$ and $\beta_{group}$ now stem from a joint distribution, we need to express them as such. $\alpha$ is still the average intercept. However, $\beta$ now turns into the average slope. Both of these serve as the mean expectations for $\alpha_{group}$ and $\beta_{group}$ in a multivariate normal distribution ($MVNormal()$) with a covariance matrix ($S$) defining how they are linked.</p>
<p>$$\begin{bmatrix} \alpha_{group} \ \beta_{group} \ \end{bmatrix}  \sim MVNormal \left(\begin{bmatrix} \alpha \ \beta \ \end{bmatrix}, S \right)$$</p>
<p>Since we have just introduced the need for a covariance matrix, we now need to define it. A covariance matrix is the product of a variance matrix and a correlation matrix ($R$). What we can do when determining the covariance matrix ($S$) is setting our variances for $\alpha$ and $\beta$ - $\sigma_\alpha$ and $\sigma_\beta$, respectively - and subsequently multiplying this with the correlation matrix ($R$):</p>
<p>$$S = \begin{pmatrix} \sigma_\alpha &amp; 0 \ 0 &amp; \sigma_\beta  \ \end{pmatrix} R \begin{pmatrix} \sigma_\alpha &amp; 0 \ 0 &amp; \sigma_\beta  \ \end{pmatrix} $$</p>
<p>The variances and correlation matrix referenced above need priors of their own - so called hyperpriors. Let&rsquo;s start with the priors of the variances:</p>
<p>$$σ_α ∼ HalfCauchy(0, 2)$$
$$σ_\beta ∼ HalfCauchy(0, 2)$$</p>
<p>And also add a somewhat regularising prior for $R$:</p>
<p>$$R ∼ LKJcorr(2)$$</p>
<p>Lastly, we simply keep the priors for $\alpha$, $\beta$, and $\sigma$ from the original model.
$$α ∼ Normal(0, 10)$$ 
$$β ∼ Normal(0, 1)$$ 
$$σ ∼ HalfCauchy(0, 2)$$</p>
<h3 id="practice-e2">Practice E2</h3>
<p><strong>Question:</strong>  Think up a context in which varying intercepts will be positively correlated with varying slopes. Provide a mechanistic explanation for the correlation.</p>
<p><strong>Answer:</strong> A setting within which there is positive correlation between varying intercepts and varying slopes can be put in laymen-terms as: &ldquo;A setting within which high intercepts come with steep slopes&rdquo;. With that in mind, what could be such a setting?</p>
<p>There are many settings which would meet this criterion. I am a biologist by training and the first thing that came to mind was that of an ant colony. Let&rsquo;s say we are interested studying ant hill size as a function of food availability. Ignoring the carrying capacity of a system, we can reasonably expect larger ant hills (higher intercepts) to benefit more strongly from increased food availability as their foraging will be much more efficient (steeper slope).</p>
<p>Of course, I realise that this thought experiment ignores some crucial bits of biological reality such as diminishing returns and structural integrity of ant hills after a certain size is reached. For the sake of keeping this example simple, I neglect them.</p>
<h3 id="practice-e3">Practice E3</h3>
<p><strong>Question:</strong>  When is it possible for a varying slopes model to have fewer effective parameters (as estimated by WAIC or DIC) than the corresponding model with fixed (unpooled) slopes? Explain.</p>
<p><strong>Answer:</strong> When there is little or next-to-no variation among clusters. The absence of this among-cluster variation induces very strong shrinkage. As a result, albeit containing more actual parameters in the posterior distribution, the varying slopes model may end up less flexible in fitting to the data because of adaptive regularisation forcing strong shrinkage. Consequently, our number of effective parameters - a proxy of overfitting risk and posterior flexibility - decreases.</p>
<p>For an example, consult the comparison of models <code>m13.1</code> and <code>m13.2</code> in R Code <code>13.4</code> in the book.</p>
<h2 id="medium-exercises">Medium Exercises</h2>
<h3 id="practice-m1">Practice M1</h3>
<p><strong>Question:</strong>  Repeat the café robot simulation from the beginning of the chapter. This time, set <code>rho</code> to zero, so that there is no correlation between intercepts and slopes. How does the posterior distribution of the correlation reflect this change in the underlying simulation?</p>
<p><strong>Answer:</strong> This is what was done in the book. <code>rho</code> has been adjusted to be $0$ now:</p>
<pre><code class="language-r"># set up parameters of population
a &lt;- 3.5 # average morning wait time
b &lt;- (-1) # average difference afternoon wait time
sigma_a &lt;- 1 # std dev in intercepts
sigma_b &lt;- 0.5 # std dev in slopes
rho &lt;- 0 # correlation between intercepts and slopes
Mu &lt;- c(a, b)
cov_ab &lt;- sigma_a * sigma_b * rho
Sigma &lt;- matrix(c(sigma_a^2, cov_ab, cov_ab, sigma_b^2), ncol = 2)
# simulate observations
N_cafes &lt;- 20
set.seed(6)
vary_effects &lt;- mvrnorm(N_cafes, Mu, Sigma)
a_cafe &lt;- vary_effects[, 1]
b_cafe &lt;- vary_effects[, 2]
N_visits &lt;- 10
afternoon &lt;- rep(0:1, N_visits * N_cafes / 2)
cafe_id &lt;- rep(1:N_cafes, each = N_visits)
mu &lt;- a_cafe[cafe_id] + b_cafe[cafe_id] * afternoon
sigma &lt;- 0.5 # std dev within cafes
wait &lt;- rnorm(N_visits * N_cafes, mu, sigma)
# package into  data frame
d &lt;- data.frame(cafe = cafe_id, afternoon = afternoon, wait = wait)
</code></pre>
<p>And now to run our model (<code>m14.1</code>) with the exact same specification as in the book:</p>
<pre><code class="language-r">m_M1 &lt;- ulam(
  alist(
    wait ~ normal(mu, sigma),
    mu &lt;- a_cafe[cafe] + b_cafe[cafe] * afternoon,
    c(a_cafe, b_cafe)[cafe] ~ multi_normal(c(a, b), Rho, sigma_cafe),
    a ~ normal(5, 2),
    b ~ normal(-1, 0.5),
    sigma_cafe ~ exponential(1),
    sigma ~ exponential(1),
    Rho ~ lkj_corr(2)
  ),
  data = d, chains = 6, cores = 6
)
</code></pre>
<p>So what about that posterior distribution for <code>Rho</code>?</p>
<pre><code class="language-r">post &lt;- extract.samples(m_M1)
ggplot() +
  stat_halfeye(aes(x = post$Rho[, 1, 2])) +
  theme_bw() +
  labs(x = &quot;Rho&quot;)
</code></pre>
<p><img src="2021-04-15-statistical-rethinking-chapter-14_files/figure-html/unnamed-chunk-4-1.png" width="1440" /></p>
<p>Jup. That accurately represents our underlying correlation of $0$. The <code>precis</code> output agrees:</p>
<pre><code class="language-r">precis(m_M1, pars = &quot;Rho[1,2]&quot;)
</code></pre>
<pre><code>##              result
## mean     0.01110476
## sd       0.23468860
## 5.5%    -0.36400650
## 94.5%    0.38982346
## n_eff 2833.41771477
## Rhat     1.00023118
</code></pre>
<h3 id="practice-m2">Practice M2</h3>
<p><strong>Question:</strong>  Fit this multilevel model to the simulated café data: 
$$W_i ∼ Normal(µ_i, σ)$$ 
$$µ_i = α_{café[i]} + β_{café[i]}A_i$$
$$α_{café} ∼ Normal(α, σ_α)$$ 
$$β_{café} ∼ Normal(β, σ_β)$$ 
$$α ∼ Normal(0, 10)$$ 
$$β ∼ Normal(0, 10)$$ 
$$σ ∼ HalfCauchy(0, 1)$$
$$σ_α ∼ HalfCauchy(0, 1)$$ 
$$σ_β ∼ HalfCauchy(0, 1)$$</p>
<p>Use WAIC to compare this model to the model from the chapter, the one that uses a multi-variate Gaussian prior. Explain the result.</p>
<p><strong>Answer:</strong> I am strongly assuming that this question is targeting the simulated café data used in the book. I create that data again here:</p>
<pre><code class="language-r"># set up parameters of population
a &lt;- 3.5 # average morning wait time
b &lt;- -1 # average difference afternoon wait time
sigma_a &lt;- 1 # std dev in intercepts
sigma_b &lt;- 0.5 # std dev in slopes
rho &lt;- -0.7 # correlation between intercepts and slopes
Mu &lt;- c(a, b)
cov_ab &lt;- sigma_a * sigma_b * rho
Sigma &lt;- matrix(c(sigma_a^2, cov_ab, cov_ab, sigma_b^2), ncol = 2)
# simulate observations
N_cafes &lt;- 20
set.seed(42)
vary_effects &lt;- mvrnorm(N_cafes, Mu, Sigma)
a_cafe &lt;- vary_effects[, 1]
b_cafe &lt;- vary_effects[, 2]
N_visits &lt;- 10
afternoon &lt;- rep(0:1, N_visits * N_cafes / 2)
cafe_id &lt;- rep(1:N_cafes, each = N_visits)
mu &lt;- a_cafe[cafe_id] + b_cafe[cafe_id] * afternoon
sigma &lt;- 0.5 # std dev within cafes
wait &lt;- rnorm(N_visits * N_cafes, mu, sigma)
# package into  data frame
d &lt;- data.frame(cafe = cafe_id, afternoon = afternoon, wait = wait)
</code></pre>
<p>With the data at hand, I now run our baseline model which is, again, <code>m14.1</code>:</p>
<pre><code class="language-r">m_M2Baseline &lt;- ulam(
  alist(
    wait ~ normal(mu, sigma),
    mu &lt;- a_cafe[cafe] + b_cafe[cafe] * afternoon,
    c(a_cafe, b_cafe)[cafe] ~ multi_normal(c(a, b), Rho, sigma_cafe),
    a ~ normal(5, 2),
    b ~ normal(-1, 0.5),
    sigma_cafe ~ exponential(1),
    sigma ~ exponential(1),
    Rho ~ lkj_corr(2)
  ),
  data = d, chains = 4, cores = 4
)
</code></pre>
<p>And now onto our new model for this task. What is already striking is the use of independent intercepts and slopes. There is no correlation parameter between them so the assumed correlation, by the model, is $0$:</p>
<pre><code class="language-r">m_M2 &lt;- ulam(
  alist(
    wait ~ dnorm(mu, sigma),
    mu &lt;- a_cafe[cafe] + b_cafe[cafe] * afternoon,
    a_cafe[cafe] ~ dnorm(a, sigma_alpha),
    b_cafe[cafe] ~ dnorm(b, sigma_beta),
    a ~ dnorm(0, 10),
    b ~ dnorm(0, 10),
    sigma ~ dexp(1),
    sigma_alpha ~ dexp(1),
    sigma_beta ~ dexp(1)
  ),
  data = d, chains = 4, cores = 4
)
</code></pre>
<p>But what actually distinguishes the model outputs now? Let&rsquo;s extract posterior samples for intercepts and slopes from both models and investigate visually:</p>
<pre><code class="language-r">post_Base &lt;- extract.samples(m_M2Baseline)
a_Base &lt;- apply(post_Base$a_cafe, 2, mean)
b_Base &lt;- apply(post_Base$b_cafe, 2, mean)
post_M2 &lt;- extract.samples(m_M2)
a_M2 &lt;- apply(post_M2$a_cafe, 2, mean)
b_M2 &lt;- apply(post_M2$b_cafe, 2, mean)
plot(a_M2, b_M2,
  xlab = &quot;intercept&quot;, ylab = &quot;slope&quot;,
  pch = 16, col = rangi2, ylim = c(min(b_M2) - 0.05, max(b_M2) + 0.05),
  xlim = c(min(a_M2) - 0.1, max(a_M2) + 0.1), cex = 2
)
points(a_Base, b_Base, pch = 1, cex = 2)
</code></pre>
<p><img src="2021-04-15-statistical-rethinking-chapter-14_files/figure-html/unnamed-chunk-9-1.png" width="1440" /></p>
<p>I have stuck to McElreath&rsquo;s colour scheme here once more. The filled circles represent samples from our new model, while the open circles represent samples from the posterior obtained via the model which accounts for correlation of slopes and intercepts. First and foremost, these are pretty similar I must say. This agreement is particularly pronounced towards the centre of the plot with increasing divergences of the posterior samples at the fringes of the intercept and slope ranges. This comes down to what the underlying models assume. Our baseline model assumes that slopes and intercepts are inherently related to one another and finds a negative correlation between them. This can be seen when looking at the lower right-hand and the upper left-hand corner of the plot above. Given the baseline model assumption, large intercepts are associated with strongly negative slopes and vice versa.</p>
<p>The correlation-informed model does better here because it leverages more information from the entire population and just so happens to exactly mirror the data generation process.</p>
<h3 id="practice-m3">Practice M3</h3>
<p><strong>Question:</strong> Re-estimate the varying slopes model for the <code>UCBadmit</code> data, now using a non-centered parametrization. Compare the efficiency of the forms of the model, using <code>n_eff</code>. Which is better? Which chain sampled faster?</p>
<p><strong>Answer:</strong> Ok&hellip; This is a headache because there is no varying slopes model for the <code>UCBadmit</code> data in the bookchapter. So let&rsquo;s make one ourselves and then re-parameterise it.</p>
<p>We start by loading and preparing the data. By defining an indicator variable for <code>male</code> we make it easier to fit a varying slopes model based on gender of applicant:</p>
<pre><code class="language-r">data(UCBadmit)
d &lt;- UCBadmit
dat_list &lt;- list(
  admit = d$admit,
  applications = d$applications,
  male = ifelse(d$applicant.gender == &quot;male&quot;, 1, 0),
  dept_id = rep(1:6, each = 2)
)
str(dat_list)
</code></pre>
<pre><code>## List of 4
##  $ admit       : int [1:12] 512 89 353 17 120 202 138 131 53 94 ...
##  $ applications: int [1:12] 825 108 560 25 325 593 417 375 191 393 ...
##  $ male        : num [1:12] 1 0 1 0 1 0 1 0 1 0 ...
##  $ dept_id     : int [1:12] 1 1 2 2 3 3 4 4 5 5 ...
</code></pre>
<p>Now with the data in hand, we can fit our own model on varying slopes. Let&rsquo;s think about this in theory first. What would this look like?</p>
<p>We start out with a binomial outcome distribution for our admitted applications:</p>
<p>$$admit_i ∼ Binomial(Applications, p_i)$$</p>
<p>Our next line is the linear model again. This time, the admittance rate ($p_i$) is a product of a department-specific intercept ($\alpha_{deptId}$) and slope ($\beta_{deptId}$):</p>
<p>$$p_i = \alpha_{deptID} + \beta_{deptID}*male$$</p>
<p>Since the varying slopes and intercepts are certain to be correlated, we specify a multivariate normal prior again:</p>
<p>$$\begin{bmatrix} \alpha_{deptID} \ \beta_{deptID} \ \end{bmatrix}  \sim MVNormal \left(\begin{bmatrix} \alpha \ \beta \ \end{bmatrix}, S \right)$$</p>
<p>And now for the covariance matrix:</p>
<p>$$S = \begin{pmatrix} \sigma_\alpha &amp; 0 \ 0 &amp; \sigma_\beta  \ \end{pmatrix} R \begin{pmatrix} \sigma_\alpha &amp; 0 \ 0 &amp; \sigma_\beta  \ \end{pmatrix} $$</p>
<p>Finally, we just need some priors and hyperpriors:</p>
<p>$$σ_α ∼ Exponential(1)$$
$$σ_\beta ∼ Exponential(1)$$</p>
<p>And also add a somewhat regularising prior for $R$:</p>
<p>$$R ∼ LKJcorr(2)$$</p>
<p>Lastly, we simply keep the priors for $\alpha$, $\beta$:
$$α ∼ Normal(0, 1)$$ 
$$β ∼ Normal(0, 1)$$</p>
<p>And now to do all of this in <code>R</code>:</p>
<pre><code class="language-r">Begin_C &lt;- Sys.time()
m_M3 &lt;- ulam(
  alist(
    admit ~ dbinom(applications, p),
    logit(p) &lt;- a[dept_id] + bm[dept_id] * male,
    c(a, bm)[dept_id] ~ multi_normal(c(a_bar, bm_bar), Rho, sigma_dept),
    a_bar ~ dnorm(0, 1),
    bm_bar ~ dnorm(0, 1),
    sigma_dept ~ dexp(1),
    Rho ~ dlkjcorr(2)
  ),
  data = dat_list, chains = 4, cores = 4
)
End_C &lt;- Sys.time()
precis(m_M3, 3)
</code></pre>
<pre><code>##                      mean           sd       5.5%      94.5%     n_eff     Rhat4
## bm[1]         -0.75467135 2.642890e-01 -1.1974575 -0.3419394  762.8361 1.0035396
## bm[2]         -0.21546963 3.141762e-01 -0.7172490  0.2702785 1444.8377 1.0007292
## bm[3]          0.07788729 1.395628e-01 -0.1457046  0.3005848 1519.8738 1.0005234
## bm[4]         -0.09706014 1.389302e-01 -0.3171928  0.1276948 1760.1554 0.9999004
## bm[5]          0.11606582 1.793774e-01 -0.1610577  0.4022199 1726.0739 1.0005376
## bm[6]         -0.10986997 2.643276e-01 -0.5302442  0.3009293 1397.2613 1.0030231
## a[1]           1.27128456 2.477604e-01  0.8819218  1.6722364  778.7511 1.0032979
## a[2]           0.74963140 3.154061e-01  0.2626239  1.2484753 1436.8953 1.0004966
## a[3]          -0.64605733 8.582742e-02 -0.7808221 -0.5086328 1617.3253 0.9995321
## a[4]          -0.61501279 1.019817e-01 -0.7774726 -0.4515837 1708.0376 0.9992410
## a[5]          -1.13005620 1.096386e-01 -1.3054035 -0.9559542 1993.4173 1.0004230
## a[6]          -2.60481876 2.045530e-01 -2.9411431 -2.2759003 1799.7746 1.0015531
## a_bar         -0.39106882 5.307969e-01 -1.2265922  0.4709463 1698.0086 0.9994382
## bm_bar        -0.16176143 2.137141e-01 -0.4947019  0.1694293 1403.2870 1.0018595
## sigma_dept[1]  1.48653705 4.716297e-01  0.9088349  2.3531967 1292.0985 1.0008335
## sigma_dept[2]  0.44566910 2.144169e-01  0.1819589  0.8410725  988.9796 1.0015728
## Rho[1,1]       1.00000000 0.000000e+00  1.0000000  1.0000000       NaN       NaN
## Rho[1,2]      -0.32154165 3.408218e-01 -0.8124723  0.2811236 1522.8640 1.0015113
## Rho[2,1]      -0.32154165 3.408218e-01 -0.8124723  0.2811236 1522.8640 1.0015113
## Rho[2,2]       1.00000000 8.265587e-17  1.0000000  1.0000000 1811.6478 0.9979980
</code></pre>
<p>Let&rsquo;s just acknowledge that the <code>precis()</code> output is here, but move on for now to the re-parametrised model.</p>
<p>I am not even going to attempt to come up with the mathematical notation of the non-centred version of the above model. Luckily, I don&rsquo;t have to because <code>ulam()</code> has helper functions which can do this for me:</p>
<pre><code class="language-r">Begin_NC &lt;- Sys.time()
m_M3NonCent &lt;- ulam(
  alist(
    admit ~ dbinom(applications, p),
    logit(p) &lt;- a_bar + v[dept_id, 1] + (bm_bar + v[dept_id, 2]) * male,
    transpars &gt; matrix[dept_id, 2]:v &lt;- compose_noncentered(sigma_dept, L_Rho, z),
    matrix[2, dept_id]:z ~ dnorm(0, 1),
    a_bar ~ dnorm(0, 1.5),
    bm_bar ~ dnorm(0, 1),
    vector[2]:sigma_dept ~ dexp(1),
    cholesky_factor_corr[2]:L_Rho ~ lkj_corr_cholesky(2)
  ),
  data = dat_list, chains = 4, cores = 4
)
End_NC &lt;- Sys.time()
precis(m_M3NonCent, 3)
</code></pre>
<pre><code>##                      mean        sd       5.5%       94.5%     n_eff     Rhat4
## z[1,1]         1.27847990 0.5262323  0.4705305  2.13326205  727.0227 1.0011087
## z[1,2]         0.88269092 0.4857079  0.1388867  1.68994558  719.7999 1.0031813
## z[1,3]        -0.12528283 0.3849925 -0.7106325  0.50669799  612.7119 1.0140975
## z[1,4]        -0.10433526 0.3866020 -0.6958663  0.51682268  601.9049 1.0154629
## z[1,5]        -0.48074322 0.4055438 -1.1153447  0.16675545  643.6182 1.0175388
## z[1,6]        -1.56200990 0.5688037 -2.4838992 -0.69872746  737.3167 1.0145782
## z[2,1]        -1.23644880 0.8164974 -2.6022559  0.05071129 1271.1801 1.0026798
## z[2,2]         0.18312727 0.8096899 -1.0820859  1.45719856 1573.6340 1.0003249
## z[2,3]         0.60192227 0.6276861 -0.3439741  1.64761142 1203.4125 1.0020469
## z[2,4]         0.10146041 0.5799030 -0.8100848  1.01529134 1291.1294 1.0015204
## z[2,5]         0.53484163 0.6684946 -0.4889643  1.64962250 1165.4873 1.0009753
## z[2,6]        -0.49127010 0.8038063 -1.8068248  0.74735922 1885.4880 1.0003286
## a_bar         -0.46792318 0.5611715 -1.3795380  0.39745615  588.0941 1.0121940
## bm_bar        -0.13969516 0.2136100 -0.4695941  0.18952741  799.7150 1.0034511
## sigma_dept[1]  1.46852953 0.4505571  0.9255641  2.31989495  833.1351 1.0030067
## sigma_dept[2]  0.44493698 0.2388977  0.1818530  0.86089979  757.1626 0.9991599
## L_Rho[1,1]     1.00000000 0.0000000  1.0000000  1.00000000       NaN       NaN
## L_Rho[1,2]     0.00000000 0.0000000  0.0000000  0.00000000       NaN       NaN
## L_Rho[2,1]    -0.31981947 0.3440124 -0.8022403  0.29104897 1687.4781 1.0001054
## L_Rho[2,2]     0.87223636 0.1365401  0.5970006  0.99912792 1190.3266 1.0002817
## v[1,1]         1.73536453 0.6011856  0.8128695  2.70174932  648.8099 1.0094272
## v[1,2]        -0.61370995 0.3128848 -1.1315494 -0.14674125 1028.2946 1.0005503
## v[2,1]         1.19934441 0.6279810  0.2169330  2.22040335  675.6765 1.0101770
## v[2,2]        -0.06176349 0.3328516 -0.5897528  0.46209909 1539.0956 1.0001901
## v[3,1]        -0.17733549 0.5673456 -1.0542767  0.75415175  593.0556 1.0109594
## v[3,2]         0.21846891 0.2395483 -0.1438607  0.60551012 1035.9533 1.0016130
## v[4,1]        -0.14832190 0.5675488 -1.0127159  0.76606136  575.1370 1.0125444
## v[4,2]         0.04602378 0.2363775 -0.3298587  0.41695086  890.5646 1.0022504
## v[5,1]        -0.65983984 0.5702765 -1.5558841  0.26053379  610.1289 1.0114797
## v[5,2]         0.25080258 0.2583262 -0.1081353  0.67319529 1159.0924 1.0024436
## v[6,1]        -2.12809664 0.5847887 -3.0186504 -1.19350046  633.0943 1.0111973
## v[6,2]         0.02683558 0.3048067 -0.4676814  0.50643307 1645.9028 1.0004084
</code></pre>
<p>First of all, we can see that the number of effective samples (<code>n_eff</code>) is higher for the centred model (<code>m_M3</code>) which is surprising to me. I thought that non-centred models were supposed to sample more efficiently. Maybe the underlying data just doesn&rsquo;t suffer from abrupt changes in posterior slope?</p>
<p>So what about running time?</p>
<pre><code class="language-r"># Centred
End_C - Begin_C
</code></pre>
<pre><code>## Time difference of 55.36985 secs
</code></pre>
<pre><code class="language-r"># Non-Centred
End_NC - Begin_NC
</code></pre>
<pre><code>## Time difference of 51.25241 secs
</code></pre>
<p>Ok. The non-centred model ran slightly faster.</p>
<h3 id="practice-m4">Practice M4</h3>
<p><strong>Question:</strong> Use WAIC to compare the Gaussian process model of Oceanic tools to the models fit to the same data in Chapter 11. Pay special attention to the effective numbers of parameters, as estimated by WAIC.</p>
<p><strong>Answer:</strong> So this needed some digging. The models in question are <code>m11.11</code> for the simpler model of CHapter 11 and <code>m14.8</code> from Chapter 14.</p>
<p>Before we can do any modelling, we need to load and prepare the data:</p>
<pre><code class="language-r">data(Kline)
d &lt;- Kline
# Chapter 11 stuff
d$P &lt;- scale(log(d$population))
d$contact_id &lt;- ifelse(d$contact == &quot;high&quot;, 2, 1)
# Chapter 14 stuff
d$society &lt;- 1:10
data(islandsDistMatrix)
</code></pre>
<p>With the data at hand, I simply use the exact same code as in the book to execute the respective models. Note that I have set the <code>ulam()</code> argument <code>log_lik=TRUE</code> for comparison with WAIC in the next step.</p>
<pre><code class="language-r">## Chapter 11 Model
dat2 &lt;- list(T = d$total_tools, P = d$population, cid = d$contact_id)
m11.11 &lt;- ulam(
  alist(
    T ~ dpois(lambda),
    lambda &lt;- exp(a[cid]) * P^b[cid] / g,
    a[cid] ~ dnorm(1, 1),
    b[cid] ~ dexp(1),
    g ~ dexp(1)
  ),
  data = dat2, chains = 4, cores = 4, log_lik = TRUE
)
## Chapter 14 Model
dat_list &lt;- list(T = d$total_tools, P = d$population, society = d$society, Dmat = islandsDistMatrix)
m14.8 &lt;- ulam(
  alist(
    T ~ dpois(lambda),
    lambda &lt;- (a * P^b / g) * exp(k[society]),
    vector[10]:k ~ multi_normal(0, SIGMA),
    matrix[10, 10]:SIGMA &lt;- cov_GPL2(Dmat, etasq, rhosq, 0.01), c(a, b, g) ~ dexp(1), etasq ~ dexp(2), rhosq ~ dexp(0.5)
  ),
  data = dat_list, chains = 4, cores = 4, iter = 2000, log_lik = TRUE
)
</code></pre>
<p>We have both models at the ready, let&rsquo;s do what the task asked of us and compare their out-of-sample accuracy predictions:</p>
<pre><code class="language-r">compare(m11.11, m14.8)
</code></pre>
<pre><code>##            WAIC        SE    dWAIC      dSE    pWAIC      weight
## m14.8  67.87559  2.370998  0.00000       NA 4.196489 0.998336383
## m11.11 80.66978 11.103427 12.79419 10.95919 5.148678 0.001663617
</code></pre>
<p>The more complex model taking into account spatial distances of societies - <code>m14.8</code> - outperforms the previously held &ldquo;best&rdquo; model (<code>m11.11</code>). We also see that the Gaussian process model has less effective parameters (<code>pWAIC</code>) than the simpler model. This is a sign of intense regularisation on the part of the Gaussian Process model.</p>
<h3 id="practice-m5">Practice M5</h3>
<p><strong>Question:</strong>  Modify the phylogenetic distance example to use group size as the outcome and brain size as a predictor. Assuming brain size influences group size, what is your estimate of the effect? How does phylogeny influence the estimate?</p>
<p><strong>Answer:</strong> This is the example from the book, but simply just switching the positions of group size and brain size in the model specification. Coincidentally, this is the model shown in the 
<a href="https://www.youtube.com/watch?v=pwMRbt2CbSU&amp;list=PLDcUM9US4XdNM4Edgs7weiyIguLSToZRI&amp;index=19&amp;ab_channel=RichardMcElreath" target="_blank" rel="noopener">YouTube Lecture series</a> by Richard McElreath.</p>
<p>For now, we start by loading the data as was done in the book and establish our first data list for subsequent modelling:</p>
<pre><code class="language-r">data(Primates301)
d &lt;- Primates301
d$name &lt;- as.character(d$name)
dstan &lt;- d[complete.cases(d$group_size, d$body, d$brain), ]
spp_obs &lt;- dstan$name
dat_list &lt;- list(
  N_spp = nrow(dstan),
  M = standardize(log(dstan$body)),
  B = standardize(log(dstan$brain)),
  G = standardize(log(dstan$group_size)),
  Imat = diag(nrow(dstan))
)
</code></pre>
<p>With this part of the observational data ready, I now turn to phylogenetic data which we can obtain and attach to our data list like so:</p>
<pre><code class="language-r">data(Primates301_nex)
tree_trimmed &lt;- keep.tip(Primates301_nex, spp_obs) # only keep tree that's relevant to our species
Rbm &lt;- corBrownian(phy = tree_trimmed) # calculate expected covariance given a Brownian model
V &lt;- vcv(Rbm) # compute expected variances and covariances
Dmat &lt;- cophenetic(tree_trimmed) # cophenetic distance matrix
dat_list$V &lt;- V[spp_obs, spp_obs] # covariances in speciesXspecies matrix
dat_list$R &lt;- dat_list$V / max(V) # relative covariances of speciesXspecies matrix
</code></pre>
<p>And we are ready to run our first model! Because they book went through multiple candidate models so do I.</p>
<p>Here, I start off with the basic, ordinary regression:</p>
<pre><code class="language-r">m_M5Ordi &lt;- ulam(
  alist(
    G ~ multi_normal(mu, SIGMA),
    mu &lt;- a + bM * M + bB * B,
    matrix[N_spp, N_spp]:SIGMA &lt;- Imat * sigma_sq,
    a ~ normal(0, 1),
    c(bM, bB) ~ normal(0, 0.5),
    sigma_sq ~ exponential(1)
  ),
  data = dat_list, chains = 4, cores = 4
)
</code></pre>
<p>Next, I run the Brownian motion model:</p>
<pre><code class="language-r">m_M5Brown &lt;- ulam(
  alist(
    G ~ multi_normal(mu, SIGMA),
    mu &lt;- a + bM * M + bB * B,
    matrix[N_spp, N_spp]:SIGMA &lt;- R * sigma_sq,
    a ~ normal(0, 1),
    c(bM, bB) ~ normal(0, 0.5),
    sigma_sq ~ exponential(1)
  ),
  data = dat_list, chains = 4, cores = 4
)
</code></pre>
<p>Lastly, I execute a Gaussian Process model. To do so, we need to convert our phylogenetic distance matrix into a relative measure of distance among our species:</p>
<pre><code class="language-r">dat_list$Dmat &lt;- Dmat[spp_obs, spp_obs] / max(Dmat)
m_M5GP &lt;- ulam(
  alist(
    G ~ multi_normal(mu, SIGMA),
    mu &lt;- a + bM * M + bB * B,
    matrix[N_spp, N_spp]:SIGMA &lt;- cov_GPL1(Dmat, etasq, rhosq, 0.01),
    a ~ normal(0, 1),
    c(bM, bB) ~ normal(0, 0.5),
    etasq ~ half_normal(1, 0.25),
    rhosq ~ half_normal(3, 0.25)
  ),
  data = dat_list, chains = 4, cores = 4
)
</code></pre>
<pre><code class="language-r">plot(coeftab(m_M5Ordi, m_M5Brown, m_M5GP), pars = c(&quot;bM&quot;, &quot;bB&quot;))
</code></pre>
<p><img src="2021-04-15-statistical-rethinking-chapter-14_files/figure-html/unnamed-chunk-22-1.png" width="1440" /></p>
<p>From the above, we clearly see that model which does not take into account our phylogeny - <code>m_M5Ordi</code> - finds a clearly non-zero dependence of brain size on group size. However, both model which do include phylogenetic information - <code>m_M5Brown</code> and <code>m_M5GP</code> - do not show this relationship. Adding phylogenetic information seems to reduce the evidence for a causal link between brain size and group size.</p>
<!-- ```{r} -->
<!-- post <- extract.samples(m_M5GP) -->
<!-- plot(NULL, xlim=c(0,max(dat_list$Dmat)), ylim=c(0,1.5), -->
<!--     xlab="phylogenetic distance", ylab="covariance") -->
<!-- # posterior -->
<!-- for (i in 1:30) -->
<!--     curve(post$etasq[i]*exp(-post$rhosq[i]*x), add=TRUE, col=rangi2) -->
<!-- # prior mean and 89% interval -->
<!-- eta <- abs(rnorm(1e3,1,0.25)) -->
<!-- rho <- abs(rnorm(1e3,3,0.25)) -->
<!-- d_seq <- seq(from=0,to=1,length.out=50) -->
<!-- K <- sapply(d_seq, function(x) eta*exp(-rho*x)) -->
<!-- lines(d_seq, colMeans(K), lwd=2) -->
<!-- shade(apply(K,2,PI), d_seq) -->
<!-- text(0.5, 0.5, "prior") -->
<!-- text(0.2, 0.1, "posterior", col=rangi2) -->
<!-- ``` -->
<!-- ```{r} -->
<!-- # OU model with different GP prior -->
<!-- m14M5.3 <- ulam( -->
<!--     alist( -->
<!--         G ~ multi_normal(mu, SIGMA), -->
<!--         mu <- a + bM*M + bB*B, -->
<!--         matrix[N_spp,N_spp]: SIGMA <- cov_GPL1(Dmat, etasq, rhosq, 0.01), -->
<!--         a ~ normal(0,1), -->
<!--         c(bM,bB) ~ normal(0,0.5), -->
<!--         etasq ~ half_normal(0.25,0.25), -->
<!--         rhosq ~ half_normal(3,0.25) -->
<!--  ), data=dat_list, chains=4, cores=4) -->
<!-- ``` -->
<!-- ```{r} -->
<!-- post <- extract.samples(m14M5.3) -->
<!-- plot(NULL, xlim=c(0,max(dat_list$Dmat)), ylim=c(0,1.5), -->
<!--     xlab="phylogenetic distance", ylab="covariance") -->
<!-- # posterior -->
<!-- for (i in 1:30) -->
<!--     curve(post$etasq[i]*exp(-post$rhosq[i]*x), add=TRUE, col=rangi2) -->
<!-- # prior mean and 89% interval -->
<!-- eta <- abs(rnorm(1e3,1,0.25)) -->
<!-- rho <- abs(rnorm(1e3,3,0.25)) -->
<!-- d_seq <- seq(from=0,to=1,length.out=50) -->
<!-- K <- sapply(d_seq, function(x) eta*exp(-rho*x)) -->
<!-- lines(d_seq, colMeans(K), lwd=2) -->
<!-- shade(apply(K,2,PI), d_seq) -->
<!-- text(0.5, 0.5, "prior") -->
<!-- text(0.2, 0.1, "posterior", col=rangi2) -->
<!-- ``` -->
<h2 id="hard-exercises">Hard Exercises</h2>
<h3 id="practice-h1">Practice H1</h3>
<p><strong>Question:</strong>  Let’s revisit the Bangladesh fertility data, <code>data(bangladesh)</code>, from the practice problems for Chapter 13. Fit a model with both varying intercepts by <code>district_id</code> and varying slopes of urban by <code>district_id</code>. You are still predicting <code>use.contraception</code>.</p>
<p>Inspect the correlation between the intercepts and slopes. Can you interpret this correlation, in terms of what it tells you about the pattern of contraceptive use in the sample? It might help to plot the mean (or median) varying effect estimates for both the intercepts and slopes, by district. Then you can visualize the correlation and maybe more easily think through what it means to have a particular correlation. Plotting predicted proportion of women using contraception, with urban women on one axis and rural on the other, might also help.</p>
<p><strong>Answer:</strong> Once more, I start by loading the data and preparing it as was done in a previous chapter:</p>
<pre><code class="language-r">data(bangladesh)
d &lt;- bangladesh
dat_list &lt;- list(
  C = d$use.contraception,
  did = as.integer(as.factor(d$district)),
  urban = d$urban
)
</code></pre>
<p>Now I can run my model:</p>
<pre><code class="language-r">m_H1 &lt;- ulam(
  alist(
    C ~ bernoulli(p),
    logit(p) &lt;- a[did] + b[did] * urban,
    c(a, b)[did] ~ multi_normal(c(abar, bbar), Rho, Sigma),
    abar ~ normal(0, 1),
    bbar ~ normal(0, 0.5),
    Rho ~ lkj_corr(2),
    Sigma ~ exponential(1)
  ),
  data = dat_list, chains = 4, cores = 4, iter = 4000
)
</code></pre>
<p>And now look at the posterior estimates of average effects:</p>
<pre><code class="language-r">precis(m_H1)
</code></pre>
<pre><code>##            mean        sd       5.5%      94.5%    n_eff    Rhat4
## abar -0.6842219 0.1003302 -0.8475050 -0.5301731 6158.562 1.000395
## bbar  0.6369826 0.1590421  0.3866908  0.8900233 4474.285 1.000145
</code></pre>
<p>Unsurprisingly, I find a positive effect for <code>bbar</code> which indicates that contraception is used more frequently in urban areas.</p>
<p>Looking deeper into the posterior estimates:</p>
<pre><code class="language-r">precis(m_H1, depth = 3, pars = c(&quot;Rho&quot;, &quot;Sigma&quot;))
</code></pre>
<pre><code>##                mean           sd       5.5%      94.5%     n_eff     Rhat4
## Rho[1,1]  1.0000000 0.000000e+00  1.0000000  1.0000000       NaN       NaN
## Rho[1,2] -0.6512997 1.696069e-01 -0.8680436 -0.3443487 1431.4457 1.0003693
## Rho[2,1] -0.6512997 1.696069e-01 -0.8680436 -0.3443487 1431.4457 1.0003693
## Rho[2,2]  1.0000000 5.972661e-17  1.0000000  1.0000000 7485.2487 0.9994999
## Sigma[1]  0.5762512 9.710360e-02  0.4308600  0.7404430 1990.0617 1.0021542
## Sigma[2]  0.7731623 1.991282e-01  0.4617857  1.0991674  967.7497 1.0034399
</code></pre>
<p>shows a negative correlation between the intercepts and slopes (<code>Rho[1,2]</code> or <code>Rho[2,1]</code>).</p>
<p>Let&rsquo;s plot this relationship between the varying effects to get a better understanding of what is happening:</p>
<pre><code class="language-r">post &lt;- extract.samples(m_H1)
a &lt;- apply(post$a, 2, mean)
b &lt;- apply(post$b, 2, mean)
plot(a, b, xlab = &quot;a (intercept)&quot;, ylab = &quot;b (urban slope)&quot;)
abline(h = 0, lty = 2)
abline(v = 0, lty = 2)
R &lt;- apply(post$Rho, 2:3, mean)
s &lt;- apply(post$Sigma, 2, mean)
S &lt;- diag(s) %*% R %*% diag(s)
ll &lt;- c(0.5, 0.67, 0.89, 0.97)
for (l in ll) {
  el &lt;- ellipse(S, centre = c(mean(post$abar), mean(post$bbar)), level = l)
  lines(el, col = &quot;black&quot;, lwd = 0.5)
}
</code></pre>
<p><img src="2021-04-15-statistical-rethinking-chapter-14_files/figure-html/unnamed-chunk-27-1.png" width="1440" /></p>
<p>So districts with higher use of contraception outside of urban areas come with smaller slopes. Basically, what this means is that districts which boast a high use of contraception outside of urban areas do not have a marked shift in use of contraceptives when moving to urban areas.</p>
<p>We can also show this in probability scale by applying inverse logit transformation to our estimates:</p>
<pre><code class="language-r">u0 &lt;- inv_logit(a)
u1 &lt;- inv_logit(a + b)
plot(u0, u1, xlim = c(0, 1), ylim = c(0, 1), xlab = &quot;urban = 0&quot;, ylab = &quot;urban = 1&quot;)
abline(h = 0.5, lty = 2)
abline(v = 0.5, lty = 2)
</code></pre>
<p><img src="2021-04-15-statistical-rethinking-chapter-14_files/figure-html/unnamed-chunk-28-1.png" width="1440" /></p>
<h3 id="practice-h2">Practice H2</h3>
<p><strong>Question:</strong>  Varying effects models are useful for modelling time series, as well as spatial clustering. In a time series, the observations cluster by entities that have continuity through time, such as individuals. Since observations within individuals are likely highly correlated, the multilevel structure can help quite a lot. You’ll use the data in <code>data(Oxboys)</code>, which is 234 height measurements on 26 boys from an Oxford Boys Club (I think these were like youth athletic leagues?), at 9 different ages (centred and standardized) per boy.</p>
<p>You’ll be interested in predicting <code>height</code>, using <code>age</code>, clustered by <code>Subject</code> (individual boy). Fit a model with varying intercepts and slopes (on <code>age</code>), clustered by <code>Subject</code>. Present and interpret the parameter estimates. Which varying effect contributes more variation to the heights, the intercept or the slope?</p>
<p><strong>Answer:</strong> I start with loading the data, standardising the age data, and making the subject IDs into an index:</p>
<pre><code class="language-r">data(Oxboys)
d &lt;- Oxboys
d$A &lt;- standardize(d$age)
d$id &lt;- coerce_index(d$Subject)
</code></pre>
<p>Armed with my data, I can now turn to modelling:</p>
<pre><code class="language-r">m_H2 &lt;- ulam(
  alist(
    height ~ dnorm(mu, sigma),
    mu &lt;- a_bar + a[id] + (b_bar + b[id]) * A,
    a_bar ~ dnorm(150, 10),
    b_bar ~ dnorm(0, 10),
    c(a, b)[id] ~ multi_normal(0, Rho_id, sigma_id),
    sigma_id ~ dexp(1),
    Rho_id ~ dlkjcorr(2),
    sigma ~ dexp(1)
  ),
  data = d, chains = 4, cores = 4, iter = 4000
)
</code></pre>
<p>The model has compiled and I am interested in the output it produced concerning average effects and variation:</p>
<pre><code class="language-r">precis(m_H2, depth = 2, pars = c(&quot;a_bar&quot;, &quot;b_bar&quot;, &quot;sigma_id&quot;))
</code></pre>
<pre><code>##                   mean        sd        5.5%      94.5%     n_eff    Rhat4
## a_bar       149.523993 1.3844230 147.2887666 151.690432  293.0144 1.011856
## b_bar         4.230315 0.2073667   3.8999943   4.554123  414.4092 1.003962
## sigma_id[1]   7.331832 0.8789976   6.0675622   8.840290 4173.7730 1.001038
## sigma_id[2]   1.065676 0.1525730   0.8508899   1.336301 4141.6398 1.000750
</code></pre>
<p>Since age is standardised, <code>a_bar</code> represent the average height at average age in the data set. The average slope <code>b_bar</code> represents change in height for a one-unit change in standardised age.</p>
<p>I don&rsquo;t like interpreting standardised data coefficients like that. Let&rsquo;s rather plot it:</p>
<pre><code class="language-r">plot(height ~ age, type = &quot;n&quot;, data = d)
for (i in 1:26) {
  h &lt;- d$height[d$Subject == i]
  a &lt;- d$age[d$Subject == i]
  lines(a, h, col = col.alpha(&quot;slateblue&quot;, 0.5), lwd = 2)
}
</code></pre>
<p><img src="2021-04-15-statistical-rethinking-chapter-14_files/figure-html/unnamed-chunk-32-1.png" width="1440" /></p>
<p>Now the the task at hand. Which effect contributes more to the overall heights of our individuals? Given the plot above, I&rsquo;d argue that it is the varying intercepts which provide us with most of the variation in heights. However, this is very much down to the data and should not be generalised beyond this data set. It might completely fall apart if we had longer time-series of data values.</p>
<h3 id="practice-h3">Practice H3</h3>
<p><strong>Question:</strong>  Now consider the correlation between the varying intercepts and slopes. Can you explain its value? How would this estimated correlation influence your predictions about a new sample of boys?</p>
<p><strong>Answer:</strong> For this, we look at the correlation matrix <code>Rho_id</code>:</p>
<pre><code class="language-r">precis(m_H2, depth = 3, pars = &quot;Rho_id&quot;)
</code></pre>
<pre><code>##                  mean           sd      5.5%     94.5%    n_eff     Rhat4
## Rho_id[1,1] 1.0000000 0.000000e+00 1.0000000 1.0000000      NaN       NaN
## Rho_id[1,2] 0.5307351 1.283373e-01 0.3046972 0.7170573 4627.842 1.0007918
## Rho_id[2,1] 0.5307351 1.283373e-01 0.3046972 0.7170573 4627.842 1.0007918
## Rho_id[2,2] 1.0000000 7.886203e-17 1.0000000 1.0000000 7829.277 0.9994999
</code></pre>
<p>So there is a positive correlation. Let&rsquo;s visualise that:</p>
<pre><code class="language-r">ggplot() +
  stat_halfeye(aes(x = extract.samples(m_H2)$Rho_id[, 1, 2])) +
  theme_bw() +
  labs(x = &quot;Rho&quot;)
</code></pre>
<p><img src="2021-04-15-statistical-rethinking-chapter-14_files/figure-html/unnamed-chunk-34-1.png" width="1440" /></p>
<p>The positive correlation implies that larger intercepts are associated with steeper slopes. What this means is that taller boys grow faster.</p>
<h3 id="practice-h4">Practice H4</h3>
<p><strong>Question:</strong>  Use <code>mvrnorm</code> (in <code>library(MASS)</code>) or <code>rmvnorm</code> (in <code>library(mvtnorm)</code>) to simulate a new sample of boys, based upon the posterior mean values of the parameters.</p>
<p>That is, try to simulate varying intercepts and slopes, using the relevant parameter estimates, and then plot the predicted trends of height on age, one trend for each simulated boy you produce. A sample of 10 simulated boys is plenty, to illustrate the lesson. You can ignore uncertainty in the posterior, just to make the problem a little easier. But if you want to include the uncertainty about the parameters, go for it.</p>
<p>Note that you can construct an arbitrary variance-covariance matrix to pass to either <code>mvrnorm</code> or <code>rmvnorm</code> with something like:</p>
<pre><code class="language-r">S &lt;- matrix(c(sa^2, sa * sb * rho, sa * sb * rho, sb^2), nrow = 2)
</code></pre>
<p>where <code>sa</code> is the standard deviation of the first variable, <code>sb</code> is the standard deviation of the second variable, and <code>rho</code> is the correlation between them.</p>
<p><strong>Answer:</strong> To simulate new observations we need to obtain the estimates of our model so far:</p>
<pre><code class="language-r">post &lt;- extract.samples(m_H2)
rho &lt;- mean(post$Rho_id[, 1, 2])
sb &lt;- mean(post$sigma_id[, 2])
sa &lt;- mean(post$sigma_id[, 1])
sigma &lt;- mean(post$sigma)
a &lt;- mean(post$a_bar)
b &lt;- mean(post$b_bar)
</code></pre>
<p>Now we can define the variance-covariance matrix:</p>
<pre><code class="language-r">S &lt;- matrix(c(sa^2, sa * sb * rho, sa * sb * rho, sb^2), nrow = 2)
round(S, 2)
</code></pre>
<pre><code>##       [,1] [,2]
## [1,] 53.76 4.15
## [2,]  4.15 1.14
</code></pre>
<p>Subsequently, we can sample from the multivariate normal distribution given our variance-covariance matrix to obtain a bivariate distribution of intercepts and slopes:</p>
<pre><code class="language-r">ve &lt;- mvrnorm(10, c(0, 0), Sigma = S)
ve
</code></pre>
<pre><code>##             [,1]       [,2]
##  [1,] -0.2914409 -0.8985609
##  [2,]  8.8137562  0.3436168
##  [3,] -1.5233131  1.5530926
##  [4,] -9.5179212 -0.6967038
##  [5,]  7.6547083 -0.3622056
##  [6,]  5.4710535 -0.3060008
##  [7,] -0.3548003  0.1445644
##  [8,]  7.2706590  3.0081540
##  [9,]  2.8143313  0.1653603
## [10,] -6.3582595 -1.0162374
</code></pre>
<p>These are individual intercepts and slopes of 10 random boys have which we only need to add to the average intercept and slope values to generate predicted heights for them. Here, we simulate the trend for each boy and add it to a plot:</p>
<pre><code class="language-r">age.seq &lt;- seq(from = -1, to = 1, length.out = 9)
plot(0, 0, type = &quot;n&quot;, xlim = range(d$age), ylim = range(d$height), xlab = &quot;age (centered)&quot;, ylab = &quot;height&quot;)
for (i in 1:nrow(ve)) {
  h &lt;- rnorm(9,
    mean = a + ve[i, 1] + (b + ve[i, 2]) * age.seq,
    sd = sigma
  )
  lines(age.seq, h, col = col.alpha(&quot;slateblue&quot;, 0.5))
}
</code></pre>
<p><img src="2021-04-15-statistical-rethinking-chapter-14_files/figure-html/unnamed-chunk-39-1.png" width="1440" /></p>
<h2 id="session-info">Session Info</h2>
<pre><code class="language-r">sessionInfo()
</code></pre>
<pre><code>## R version 4.0.5 (2021-03-31)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19043)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.1252  LC_CTYPE=English_United Kingdom.1252    LC_MONETARY=English_United Kingdom.1252 LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.1252    
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] tidybayes_2.3.1      ape_5.5              ellipse_0.4.2        MASS_7.3-53.1        rethinking_2.13      rstan_2.21.2         ggplot2_3.3.6        StanHeaders_2.21.0-7
## 
## loaded via a namespace (and not attached):
##  [1] sass_0.3.1           tidyr_1.1.3          jsonlite_1.7.2       R.utils_2.10.1       bslib_0.2.4          RcppParallel_5.1.2   assertthat_0.2.1     distributional_0.2.2 highr_0.9           
## [10] stats4_4.0.5         ggdist_2.4.0         yaml_2.2.1           pillar_1.6.0         backports_1.2.1      lattice_0.20-41      glue_1.4.2           arrayhelpers_1.1-0   digest_0.6.27       
## [19] colorspace_2.0-0     htmltools_0.5.1.1    R.oo_1.24.0          plyr_1.8.6           pkgconfig_2.0.3      svUnit_1.0.6         bookdown_0.22        purrr_0.3.4          mvtnorm_1.1-1       
## [28] scales_1.1.1         processx_3.5.1       tibble_3.1.1         styler_1.4.1         generics_0.1.0       farver_2.1.0         ellipsis_0.3.2       withr_2.4.2          cli_3.0.0           
## [37] magrittr_2.0.1       crayon_1.4.1         evaluate_0.14        ps_1.6.0             R.methodsS3_1.8.1    fansi_0.4.2          R.cache_0.14.0       nlme_3.1-152         forcats_0.5.1       
## [46] pkgbuild_1.2.0       blogdown_1.3         tools_4.0.5          loo_2.4.1            prettyunits_1.1.1    lifecycle_1.0.0      matrixStats_0.61.0   stringr_1.4.0        V8_3.4.1            
## [55] munsell_0.5.0        callr_3.7.0          compiler_4.0.5       jquerylib_0.1.4      rlang_0.4.11         grid_4.0.5           labeling_0.4.2       rmarkdown_2.7        gtable_0.3.0        
## [64] codetools_0.2-18     inline_0.3.17        DBI_1.1.1            curl_4.3.2           rematch2_2.1.2       R6_2.5.0             gridExtra_2.3        knitr_1.33           dplyr_1.0.5         
## [73] utf8_1.2.1           shape_1.4.5          stringi_1.5.3        Rcpp_1.0.7           vctrs_0.3.7          tidyselect_1.1.0     xfun_0.22            coda_0.19-4
</code></pre>

          </div>

          

<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/statistics/">Statistics</a>
  
  <a class="badge badge-light" href="/tag/bayes/">Bayes</a>
  
  <a class="badge badge-light" href="/tag/bayesian-statistics/">Bayesian Statistics</a>
  
  <a class="badge badge-light" href="/tag/au-bayes-study-group/">AU Bayes Study Group</a>
  
</div>



          
          <div class="article-widget">
            
<div class="post-nav">
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/courses/rethinking/chapter-13/" rel="next">Chapter 13</a>
  </div>
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Next</div>
    <a href="/courses/rethinking/chapter-15/" rel="prev">Chapter 15</a>
  </div>
  
</div>

          </div>
          
        </div>

        <div class="body-footer">
          <p>Last updated on 2021-04-16</p>

          





          


          


  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/courses/rethinking/chapter-16/">Chapter 16</a></li>
      
      <li><a href="/courses/rethinking/chapter-15/">Chapter 15</a></li>
      
      <li><a href="/courses/rethinking/chapter-13/">Chapter 13</a></li>
      
      <li><a href="/courses/rethinking/chapter-12/">Chapter 12</a></li>
      
      <li><a href="/courses/rethinking/chapter-11/">Chapter 10 &amp; 11</a></li>
      
    </ul>
  </div>
  



        </div>

      </article>

      <footer class="site-footer">
  

  <p class="powered-by">
    © 2024
  </p>

  
  






  <p class="powered-by">
    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
  </p>
</footer>


    </main>
  </div>
</div>


      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/r.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/latex.min.js"></script>
        
      

    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    <script>const isSiteThemeDark = false;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks",
        'slides' : "Slides"
        };
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.1.1/anchor.min.js" integrity="sha256-pB/deHc9CGfFpJRjC43imB29Rse8tak+5eXqntO94ck=" crossorigin="anonymous"></script>
    <script>
      anchors.add();
    </script>
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.66c553246b0f279a03be6e5597f72b52.js"></script>

    






  
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
